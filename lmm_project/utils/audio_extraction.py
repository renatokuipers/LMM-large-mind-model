"""
Audio Extraction Utilities

This module provides functions for extracting features from audio files,
particularly those generated by the text-to-speech system. It supports
the perception module by converting audio input into quantifiable
features at various levels of developmental sophistication.
"""

import os
import logging
import numpy as np
import librosa
import soundfile as sf
from typing import Dict, List, Tuple, Optional, Union, Any
from pathlib import Path

from lmm_project.utils.logging_utils import get_module_logger

# Initialize logger
logger = get_module_logger("utils.audio_extraction")

# Feature extraction developmental thresholds
# (minimum developmental age required for each feature type)
FEATURE_THRESHOLDS = {
    "basic": 0.0,        # Basic audio properties - always available
    "spectral": 0.3,     # Spectral features - early development
    "rhythm": 0.5,       # Rhythm features - intermediate development
    "pitch": 0.7,        # Pitch features - more advanced development
    "phonetic": 1.0      # Phonetic features - language development stage
}

def load_audio(file_path: Union[str, Path]) -> Tuple[np.ndarray, int]:
    """
    Load an audio file and return the audio data and sample rate.
    
    Args:
        file_path: Path to the audio file
        
    Returns:
        Tuple of (audio_data, sample_rate)
        
    Raises:
        FileNotFoundError: If the file does not exist
        ValueError: If the file cannot be loaded as audio
    """
    file_path = Path(file_path)
    
    if not file_path.exists():
        raise FileNotFoundError(f"Audio file not found: {file_path}")
    
    try:
        # Use librosa for robust audio loading
        audio_data, sample_rate = librosa.load(file_path, sr=None)
        return audio_data, sample_rate
    except Exception as e:
        logger.error(f"Error loading audio file {file_path}: {e}")
        raise ValueError(f"Could not load audio file: {e}")

def extract_audio_features(
    audio_data: np.ndarray, 
    sample_rate: int,
    developmental_age: float = 0.0,
    feature_types: Optional[List[str]] = None
) -> Dict[str, float]:
    """
    Extract audio features based on developmental age.
    
    Args:
        audio_data: Audio data as numpy array
        sample_rate: Sample rate of the audio
        developmental_age: Current developmental age of the mind
        feature_types: Specific feature types to extract, or None for all available
        
    Returns:
        Dictionary of extracted features with their values
    """
    features = {}
    
    # Determine which feature types to extract based on developmental age
    available_feature_types = []
    for feature_type, threshold in FEATURE_THRESHOLDS.items():
        if developmental_age >= threshold:
            available_feature_types.append(feature_type)
    
    # If specific feature types are requested, filter to those that are available
    if feature_types:
        feature_types = [ft for ft in feature_types if ft in available_feature_types]
    else:
        feature_types = available_feature_types
    
    # Extract features by type
    for feature_type in feature_types:
        if feature_type == "basic":
            features.update(_extract_basic_features(audio_data, sample_rate))
        
        if feature_type == "spectral" and developmental_age >= FEATURE_THRESHOLDS["spectral"]:
            features.update(_extract_spectral_features(audio_data, sample_rate))
        
        if feature_type == "rhythm" and developmental_age >= FEATURE_THRESHOLDS["rhythm"]:
            features.update(_extract_rhythm_features(audio_data, sample_rate))
        
        if feature_type == "pitch" and developmental_age >= FEATURE_THRESHOLDS["pitch"]:
            features.update(_extract_pitch_features(audio_data, sample_rate))
        
        if feature_type == "phonetic" and developmental_age >= FEATURE_THRESHOLDS["phonetic"]:
            features.update(_extract_phonetic_features(audio_data, sample_rate))
    
    return features

def _extract_basic_features(audio_data: np.ndarray, sample_rate: int) -> Dict[str, float]:
    """Extract basic features from audio data"""
    features = {}
    
    # Duration in seconds
    features["duration"] = len(audio_data) / sample_rate
    
    # Energy (volume) features
    features["energy_mean"] = np.mean(np.abs(audio_data))
    features["energy_std"] = np.std(np.abs(audio_data))
    features["energy_max"] = np.max(np.abs(audio_data))
    
    # Zero crossing rate (rough indicator of frequency content)
    zero_crossings = librosa.feature.zero_crossing_rate(audio_data)[0]
    features["zero_crossing_rate"] = np.mean(zero_crossings)
    
    # Silence ratio (proportion of audio that is silent)
    silence_threshold = 0.01  # Adjust based on your audio
    silent_points = np.sum(np.abs(audio_data) < silence_threshold)
    features["silence_ratio"] = silent_points / len(audio_data)
    
    return features

def _extract_spectral_features(audio_data: np.ndarray, sample_rate: int) -> Dict[str, float]:
    """Extract spectral features from audio data"""
    features = {}
    
    # Spectral centroid (brightness of sound)
    spectral_centroid = librosa.feature.spectral_centroid(y=audio_data, sr=sample_rate)[0]
    features["spectral_centroid_mean"] = np.mean(spectral_centroid)
    
    # Spectral bandwidth (width of the spectral distribution)
    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio_data, sr=sample_rate)[0]
    features["spectral_bandwidth_mean"] = np.mean(spectral_bandwidth)
    
    # Spectral rolloff (frequency below which most of the energy is contained)
    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sample_rate)[0]
    features["spectral_rolloff_mean"] = np.mean(spectral_rolloff)
    
    # Mel-frequency cepstral coefficients (MFCCs)
    # These are commonly used in speech recognition
    mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)
    for i in range(min(5, len(mfccs))):  # Just use first few MFCCs
        features[f"mfcc_{i+1}_mean"] = np.mean(mfccs[i])
    
    return features

def _extract_rhythm_features(audio_data: np.ndarray, sample_rate: int) -> Dict[str, float]:
    """Extract rhythm features from audio data"""
    features = {}
    
    # Tempo estimation
    onset_env = librosa.onset.onset_strength(y=audio_data, sr=sample_rate)
    tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sample_rate)[0]
    features["tempo"] = tempo
    
    # Beat positions
    beats = librosa.beat.beat_track(onset_envelope=onset_env, sr=sample_rate)[1]
    features["beat_count"] = len(beats)
    
    if len(beats) > 1:
        # Regularity of beats (standard deviation of inter-beat intervals)
        beat_times = librosa.frames_to_time(beats, sr=sample_rate)
        beat_intervals = np.diff(beat_times)
        features["beat_regularity"] = 1.0 / (1.0 + np.std(beat_intervals))
    else:
        features["beat_regularity"] = 0.0
    
    # Rhythm patterns
    # Simplified: measure energy in different frequency bands over time
    hop_length = 512
    mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate, hop_length=hop_length)
    
    # Measure variations in energy over time in bass, mid, and high frequencies
    if mel_spectrogram.shape[1] > 1:
        bass_band = np.mean(mel_spectrogram[:5], axis=0)
        mid_band = np.mean(mel_spectrogram[5:20], axis=0)
        high_band = np.mean(mel_spectrogram[20:], axis=0)
        
        features["rhythm_bass_variation"] = np.std(bass_band) / (np.mean(bass_band) + 1e-10)
        features["rhythm_mid_variation"] = np.std(mid_band) / (np.mean(mid_band) + 1e-10)
        features["rhythm_high_variation"] = np.std(high_band) / (np.mean(high_band) + 1e-10)
    
    return features

def _extract_pitch_features(audio_data: np.ndarray, sample_rate: int) -> Dict[str, float]:
    """Extract pitch-related features from audio data"""
    features = {}
    
    # Pitch tracking
    pitches, magnitudes = librosa.piptrack(y=audio_data, sr=sample_rate)
    
    # Extract pitch information where magnitude is strongest
    if pitches.size > 0 and magnitudes.size > 0:
        # Find the strongest frequency at each time
        strongest_freqs = []
        for t in range(pitches.shape[1]):
            mag_t = magnitudes[:, t]
            if np.any(mag_t > 0):
                strongest_idx = np.argmax(mag_t)
                strongest_freqs.append(pitches[strongest_idx, t])
        
        if strongest_freqs:
            features["pitch_mean"] = np.mean(strongest_freqs)
            features["pitch_std"] = np.std(strongest_freqs)
            features["pitch_min"] = np.min(strongest_freqs)
            features["pitch_max"] = np.max(strongest_freqs)
            features["pitch_range"] = features["pitch_max"] - features["pitch_min"]
    
    # Pitch contour variability (how much the pitch changes)
    if 'pitch_std' in features:
        features["pitch_variability"] = features["pitch_std"] / (features["pitch_mean"] + 1e-10)
    
    # Harmonicity (measure of harmonic vs. noisy sound)
    try:
        harmonic, percussive = librosa.effects.hpss(audio_data)
        features["harmonicity"] = np.sum(harmonic**2) / (np.sum(percussive**2) + 1e-10)
    except:
        # HPSS can fail on very short sounds
        features["harmonicity"] = 0.0
    
    return features

def _extract_phonetic_features(audio_data: np.ndarray, sample_rate: int) -> Dict[str, float]:
    """
    Extract phonetic features from audio data.
    
    Note: Full phonetic analysis requires specialized speech recognition,
    this is a simplified approximation using spectral characteristics
    that roughly correlate with different phoneme classes.
    """
    features = {}
    
    # Formant frequency estimation 
    # (simplified approximation using spectral peaks)
    n_fft = 2048
    spectrogram = np.abs(librosa.stft(audio_data, n_fft=n_fft))
    
    # Estimate formants by finding spectral peaks
    freqs = librosa.fft_frequencies(sr=sample_rate, n_fft=n_fft)
    
    if spectrogram.shape[1] > 0:
        # Average spectrum over time
        avg_spectrum = np.mean(spectrogram, axis=1)
        
        # Apply some smoothing
        smooth_spectrum = np.convolve(avg_spectrum, np.hamming(15), mode='same')
        
        # Find peaks
        from scipy.signal import find_peaks
        peaks, _ = find_peaks(smooth_spectrum, height=np.mean(smooth_spectrum), distance=20)
        
        # Convert peak positions to frequencies
        if len(peaks) > 0:
            formant_freqs = freqs[peaks]
            
            # Store first 3 formants (if available)
            for i in range(min(3, len(formant_freqs))):
                features[f"formant_{i+1}"] = formant_freqs[i]
    
    # Vowel-consonant balance estimation
    # (rough approximation - vowels have more energy in lower frequencies)
    if spectrogram.shape[1] > 1:
        low_freq_energy = np.sum(spectrogram[:int(n_fft/8)]) 
        high_freq_energy = np.sum(spectrogram[int(n_fft/8):])
        features["vowel_consonant_ratio"] = low_freq_energy / (high_freq_energy + 1e-10)
    
    # Voice onset time estimation (simplified)
    # Look at how quickly energy builds up at the start
    if len(audio_data) > 100:
        # Get the energy envelope
        energy_envelope = np.abs(audio_data)
        
        # Find the first significant sound
        threshold = 0.1 * np.max(energy_envelope)
        onset_indices = np.where(energy_envelope > threshold)[0]
        
        if len(onset_indices) > 0:
            first_onset = onset_indices[0]
            
            # Measure how quickly it reaches 90% of local maximum
            if first_onset < len(energy_envelope) - 100:
                local_max = np.max(energy_envelope[first_onset:first_onset+100])
                rise_threshold = 0.9 * local_max
                rise_indices = np.where(energy_envelope[first_onset:first_onset+100] > rise_threshold)[0]
                
                if len(rise_indices) > 0:
                    voice_onset_time = rise_indices[0] / sample_rate
                    features["voice_onset_time"] = voice_onset_time
    
    return features

def extract_features_from_file(
    file_path: Union[str, Path],
    developmental_age: float = 0.0,
    feature_types: Optional[List[str]] = None
) -> Dict[str, float]:
    """
    Extract features from an audio file.
    
    Args:
        file_path: Path to the audio file
        developmental_age: Current developmental age
        feature_types: Specific feature types to extract
        
    Returns:
        Dictionary of features
    """
    try:
        # Load the audio file
        audio_data, sample_rate = load_audio(file_path)
        
        # Extract features
        features = extract_audio_features(
            audio_data, 
            sample_rate, 
            developmental_age, 
            feature_types
        )
        
        # Add file metadata
        features["file_path"] = str(file_path)
        features["file_size"] = os.path.getsize(file_path)
        
        return features
        
    except Exception as e:
        logger.error(f"Error extracting features from {file_path}: {e}")
        return {"error": str(e)}

def get_available_feature_types(developmental_age: float) -> List[str]:
    """
    Get the feature types available at the current developmental age.
    
    Args:
        developmental_age: Current developmental age
        
    Returns:
        List of available feature type names
    """
    return [
        feature_type for feature_type, threshold in FEATURE_THRESHOLDS.items()
        if developmental_age >= threshold
    ] 