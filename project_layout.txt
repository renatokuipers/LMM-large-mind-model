#######################
Project Layout:

modules/
├── __init__.py
└── base_module.py
├── attention/
│   ├── __init__.py
│   ├── focus_controller.py
│   ├── models.py
│   ├── neural_net.py
│   └── salience_detector.py
├── belief/
│   ├── __init__.py
│   ├── belief_formation.py
│   ├── belief_updating.py
│   ├── contradiction_resolution.py
│   ├── evidence_evaluation.py
│   ├── models.py
│   └── neural_net.py
├── consciousness/
│   ├── __init__.py
│   ├── awareness.py
│   ├── global_workspace.py
│   ├── introspection.py
│   ├── models.py
│   ├── neural_net.py
│   └── self_model.py
├── creativity/
│   ├── __init__.py
│   ├── concept_combination.py
│   ├── divergent_thinking.py
│   ├── imagination.py
│   ├── models.py
│   ├── neural_net.py
│   └── novelty_detection.py
├── emotion/
│   ├── __init__.py
│   ├── emotion_classifier.py
│   ├── models.py
│   ├── neural_net.py
│   ├── regulation.py
│   ├── sentiment_analyzer.py
│   └── valence_arousal.py
├── executive/
│   ├── __init__.py
│   ├── decision_making.py
│   ├── inhibition.py
│   ├── models.py
│   ├── neural_net.py
│   ├── planning.py
│   └── working_memory_control.py
├── identity/
│   ├── __init__.py
│   ├── models.py
│   ├── neural_net.py
│   ├── personal_narrative.py
│   ├── personality_traits.py
│   ├── preferences.py
│   └── self_concept.py
├── language/
│   ├── __init__.py
│   ├── expression_generator.py
│   ├── grammar_acquisition.py
│   ├── models.py
│   ├── neural_net.py
│   ├── phoneme_recognition.py
│   ├── semantic_processing.py
│   └── word_learning.py
├── learning/
│   ├── __init__.py
│   ├── associative_learning.py
│   ├── meta_learning.py
│   ├── models.py
│   ├── neural_net.py
│   ├── procedural_learning.py
│   └── reinforcement_learning.py
├── memory/
│   ├── __init__.py
│   ├── associative_memory.py
│   ├── episodic_memory.py
│   ├── long_term_memory.py
│   ├── models.py
│   ├── neural_net.py
│   ├── semantic_memory.py
│   └── working_memory.py
├── motivation/
│   ├── __init__.py
│   ├── drives.py
│   ├── goal_setting.py
│   ├── models.py
│   ├── needs.py
│   ├── neural_net.py
│   └── rewards.py
├── perception/
│   ├── __init__.py
│   ├── models.py
│   ├── neural_net.py
│   ├── pattern_recognition.py
│   └── sensory_input.py
├── self_regulation/
│   ├── __init__.py
│   ├── emotional_regulation.py
│   ├── impulse_control.py
│   ├── models.py
│   ├── neural_net.py
│   └── self_monitoring.py
├── social/
│   ├── __init__.py
│   ├── models.py
│   ├── moral_reasoning.py
│   ├── neural_net.py
│   ├── relationship_models.py
│   ├── social_norms.py
│   └── theory_of_mind.py
├── temporal/
│   ├── __init__.py
│   ├── causality.py
│   ├── models.py
│   ├── neural_net.py
│   ├── prediction.py
│   ├── sequence_learning.py
│   └── time_perception.py

#######################

Codebase:
#######################
#base_module.py#
#######################

"""
Base module implementation for cognitive modules
"""

import logging
import uuid
import json
import os
from pathlib import Path
from typing import Dict, Any, Optional, List, Callable
import time
from datetime import datetime

# Use TYPE_CHECKING to avoid runtime circular imports
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from lmm_project.core.event_bus import EventBus
    from lmm_project.core.message import Message

logger = logging.getLogger(__name__)

class BaseModule:
    """
    Base class for all cognitive modules
    
    This class defines the standard interface that all modules must implement
    and provides common functionality for state management, development tracking,
    and event communication.
    """
    
    # Developmental milestones for tracking progress
    # Override this in subclasses with specific milestones
    development_milestones = {
        0.0: "Initialization",
        0.2: "Basic functionality",
        0.4: "Intermediate capabilities",
        0.6: "Advanced processing",
        0.8: "Complex integration",
        1.0: "Fully developed"
    }
    
    def __init__(
        self, 
        module_id: str,
        module_type: str,
        event_bus: Optional['EventBus'] = None,
        development_level: float = 0.0,
        **kwargs
    ):
        """
        Initialize the module
        
        Args:
            module_id: Unique identifier for this module instance
            module_type: Type of module (e.g., "perception", "attention")
            event_bus: Event bus for publishing and subscribing to events
            development_level: Initial developmental level (0.0 to 1.0)
        """
        self.module_id = module_id
        self.module_type = module_type
        self.event_bus = event_bus
        self.development_level = development_level
        self.creation_time = datetime.now()
        self.last_update_time = self.creation_time
        
        # Development tracking
        self.development_history = [(self.creation_time, development_level)]
        
        # Event subscription tracking
        self._subscriptions = []
        
        # Module state
        self._enabled = True
        
        logger.debug(f"Initialized {module_type} module with ID {module_id}")
        
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input data and return results
        
        This is the main entry point for module processing. Each module
        must implement this method to handle its specific cognitive function.
        
        Args:
            input_data: Dictionary containing input data
            
        Returns:
            Dictionary containing processing results
        """
        # Base implementation does nothing
        return {
            "status": "not_implemented",
            "module_id": self.module_id,
            "module_type": self.module_type,
            "development_level": self.development_level
        }
        
    def update_development(self, amount: float) -> float:
        """
        Update the module's developmental level
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New developmental level
        """
        prev_level = self.development_level
        self.development_level = min(1.0, max(0.0, self.development_level + amount))
        now = datetime.now()
        self.last_update_time = now
        
        # Track development history
        self.development_history.append((now, self.development_level))
        
        # Log significant developmental changes and milestones
        if int(self.development_level * 10) > int(prev_level * 10):
            logger.info(f"Module {self.module_id} ({self.module_type}) development increased to {self.development_level:.2f}")
            
            # Check for milestones
            for threshold, description in sorted(self.development_milestones.items()):
                if prev_level < threshold <= self.development_level:
                    logger.info(f"Development milestone: {self.module_id} reached {description}")
                    
                    # Broadcast milestone reached event if we have an event bus
                    if self.event_bus:
                        self.publish_message(
                            "development_milestone", 
                            {
                                "module_id": self.module_id,
                                "module_type": self.module_type,
                                "milestone": description,
                                "level": threshold,
                                "timestamp": now.isoformat()
                            }
                        )
                    break
            
        return self.development_level
    
    def set_development_level(self, level: float) -> None:
        """
        Set the development level directly
        
        Useful for initialization or synchronizing components
        
        Args:
            level: Development level to set (0.0 to 1.0)
        """
        level = min(1.0, max(0.0, level))
        if level != self.development_level:
            now = datetime.now()
            self.development_level = level
            self.last_update_time = now
            self.development_history.append((now, level))
        
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the module
        
        Returns:
            Dictionary containing module state
        """
        return {
            "module_id": self.module_id,
            "module_type": self.module_type,
            "development_level": self.development_level,
            "enabled": self._enabled,
            "creation_time": self.creation_time.isoformat(),
            "last_update_time": self.last_update_time.isoformat(),
            "subscription_count": len(self._subscriptions)
        }
        
    def save_state(self, state_dir: str) -> str:
        """
        Save the module state to disk
        
        Args:
            state_dir: Directory to save state in
            
        Returns:
            Path to saved state file
        """
        try:
            # Create directory if it doesn't exist
            os.makedirs(state_dir, exist_ok=True)
            
            # Get state and prepare for serialization
            state = self.get_state()
            
            # Convert datetime objects to strings for serialization
            state['development_history'] = [
                (dt.isoformat(), level) for dt, level in self.development_history
            ]
            
            # Create filename based on module ID and type
            filename = f"{self.module_type}_{self.module_id.replace('/', '_')}.json"
            filepath = os.path.join(state_dir, filename)
            
            # Write state to file
            with open(filepath, 'w') as f:
                json.dump(state, f, indent=2)
                
            logger.info(f"Saved state for module {self.module_id} to {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Failed to save state for module {self.module_id}: {str(e)}")
            return ""
        
    def load_state(self, state_path: str) -> bool:
        """
        Load the module state from disk
        
        Args:
            state_path: Path to state file
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Check if file exists
            if not os.path.exists(state_path):
                logger.error(f"State file {state_path} does not exist")
                return False
                
            # Read state from file
            with open(state_path, 'r') as f:
                state = json.load(f)
                
            # Update basic properties
            self.development_level = state.get('development_level', self.development_level)
            self._enabled = state.get('enabled', self._enabled)
            
            # Parse development history if present
            if 'development_history' in state:
                try:
                    self.development_history = [
                        (datetime.fromisoformat(dt), level) 
                        for dt, level in state['development_history']
                    ]
                except Exception as e:
                    logger.warning(f"Failed to parse development history: {str(e)}")
            
            # Update timestamps
            if 'last_update_time' in state:
                try:
                    self.last_update_time = datetime.fromisoformat(state['last_update_time'])
                except Exception:
                    self.last_update_time = datetime.now()
                    
            logger.info(f"Loaded state for module {self.module_id} from {state_path}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to load state for module {self.module_id}: {str(e)}")
            return False
        
    def subscribe_to_message(self, message_type: str, callback=None):
        """
        Subscribe to a specific message type
        
        Args:
            message_type: Type of message to subscribe to
            callback: Function to call when message is received
                     If None, will use self._handle_message
        """
        if not self.event_bus:
            logger.warning(f"Module {self.module_id} tried to subscribe without event bus")
            return
            
        if callback is None:
            callback = self._handle_message
            
        subscription_id = self.event_bus.subscribe(message_type, callback)
        self._subscriptions.append(subscription_id)
        
    def publish_message(self, message_type: str, content: Dict[str, Any] = None):
        """
        Publish a message to the event bus
        
        Args:
            message_type: Type of message to publish
            content: Message content
        """
        if not self.event_bus:
            logger.warning(f"Module {self.module_id} tried to publish without event bus")
            return
            
        # Import here to avoid circular imports
        from lmm_project.core.message import Message
        
        message = Message(
            sender=self.module_id,
            message_type=message_type,
            content=content or {},
            timestamp=time.time()
        )
        
        self.event_bus.publish(message)
    
    def _handle_message(self, message: 'Message'):
        """
        Default message handler - can be overridden by subclasses
        
        Args:
            message: The message to handle
        """
        # Base implementation does nothing
        pass
        
    def enable(self):
        """Enable this module for processing"""
        self._enabled = True
        
    def disable(self):
        """Disable this module from processing"""
        self._enabled = False
        
    def is_enabled(self) -> bool:
        """Check if this module is enabled"""
        return self._enabled
        
    def get_development_progress(self) -> Dict[str, Any]:
        """
        Get detailed development progress information
        
        Returns:
            Dictionary with development progress details
        """
        # Find current milestone
        current_milestone = None
        next_milestone = None
        milestone_progress = 0.0
        
        sorted_milestones = sorted(self.development_milestones.items())
        
        for i, (threshold, description) in enumerate(sorted_milestones):
            if threshold <= self.development_level:
                current_milestone = (threshold, description)
                # Check if there's a next milestone
                if i + 1 < len(sorted_milestones):
                    next_milestone = sorted_milestones[i + 1]
                    next_threshold = next_milestone[0]
                    # Calculate progress to next milestone
                    milestone_range = next_threshold - threshold
                    if milestone_range > 0:
                        milestone_progress = (self.development_level - threshold) / milestone_range
                
        return {
            "development_level": self.development_level,
            "current_milestone": current_milestone[1] if current_milestone else None,
            "current_milestone_threshold": current_milestone[0] if current_milestone else None,
            "next_milestone": next_milestone[1] if next_milestone else None,
            "next_milestone_threshold": next_milestone[0] if next_milestone else None,
            "progress_to_next_milestone": milestone_progress,
            "fully_developed": self.development_level >= 1.0,
            "development_time": (datetime.now() - self.creation_time).total_seconds(),
            "development_history_length": len(self.development_history)
        }


#######################

#__init__.py#
#######################

"""
Cognitive Modules Package

This package provides all the specialized cognitive modules for the LMM system.
Each module handles a specific aspect of cognitive function.
"""

from typing import Dict, Type, Any, TYPE_CHECKING

# Use conditional import to avoid circular references
if TYPE_CHECKING:
    from .base_module import BaseModule

def get_module_classes() -> Dict[str, Any]:
    """
    Get all available module classes
    
    Returns:
        Dictionary mapping module types to module classes
    """
    # Import here to avoid circular imports
    from lmm_project.modules.perception import get_module as get_perception_module
    from lmm_project.modules.attention import get_module as get_attention_module
    from lmm_project.modules.memory import get_module as get_memory_module
    from lmm_project.modules.emotion import get_module as get_emotion_module
    from lmm_project.modules.learning import get_module as get_learning_module
    # Add other module imports as they are implemented
    
    return {
        "perception": get_perception_module,
        "attention": get_attention_module,
        "memory": get_memory_module,
        "emotion": get_emotion_module,
        "learning": get_learning_module,
        # Add other modules here as they are implemented
    }

__all__ = [
    'BaseModule'
] 

#######################

#attention\focus_controller.py#
#######################

from typing import Dict, List, Any, Optional, Set, Tuple
from pydantic import BaseModel, Field
from datetime import datetime
import numpy as np

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.attention.models import (
    AttentionFocus, AttentionTarget, AttentionParameters, SalienceScore
)

class FocusController(BaseModule):
    """
    Controls the focus of attention
    
    This module manages where attention is directed, maintaining a limited
    capacity focus buffer and handling the shifting of attention between
    different targets based on salience and task demands.
    """
    # Current focus of attention
    current_focus: AttentionFocus = Field(default_factory=AttentionFocus)
    # Parameters controlling attention behavior
    parameters: AttentionParameters = Field(default_factory=AttentionParameters)
    # History of focus shifts (for learning patterns)
    focus_shift_history: List[Dict[str, Any]] = Field(default_factory=list)
    # Maximum history size
    max_history_size: int = Field(default=100)
    # Last time attention was updated
    last_update: datetime = Field(default_factory=datetime.now)
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, **data):
        """Initialize focus controller module"""
        super().__init__(
            module_id=module_id,
            module_type="focus_controller",
            event_bus=event_bus,
            **data
        )
        
        # Subscribe to relevant events
        if self.event_bus:
            self.subscribe_to_message("salience_detected", self._handle_salience_detected)
            self.subscribe_to_message("executive_command", self._handle_executive_command)
            self.subscribe_to_message("perception_input", self._handle_perception_input)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input data
        
        Parameters:
        input_data: Dictionary containing operation data
            - operation: The operation to perform
            - Additional parameters specific to the operation
            
        Returns:
        Operation result
        """
        # Update state to handle time-based decay
        self._update_state()
        
        operation = input_data.get("operation", "")
        
        if operation == "get_focus":
            return {
                "status": "success",
                "focus": self.current_focus.model_dump()
            }
            
        elif operation == "update_focus":
            # Update based on salience scores
            salience_scores = input_data.get("salience_scores", {})
            return self.update_focus_from_salience(salience_scores)
            
        elif operation == "add_target":
            # Add a specific target to attention
            target_data = input_data.get("target", {})
            target_id = target_data.get("target_id", "")
            target_type = target_data.get("target_type", "unknown")
            activation = target_data.get("activation", 1.0)
            description = target_data.get("description", "")
            
            return self.add_attention_target(
                target_id=target_id,
                target_type=target_type,
                activation=activation,
                description=description
            )
            
        elif operation == "remove_target":
            # Remove a target from attention
            target_id = input_data.get("target_id", "")
            return self.remove_attention_target(target_id)
            
        elif operation == "shift_focus":
            # Explicitly shift focus to a specified target
            target_id = input_data.get("target_id", "")
            priority = input_data.get("priority", 0.5)
            
            return self.shift_focus_to(target_id, priority)
            
        else:
            return {"status": "error", "message": f"Unknown operation: {operation}"}
    
    def update_development(self, amount: float) -> float:
        """
        Update module's developmental level
        
        As the focus controller develops:
        - Attention capacity increases
        - Focus becomes more stable (lower decay rate)
        - Attention shifting becomes more controlled
        
        Parameters:
        amount: Amount to increase development level
        
        Returns:
        New development level
        """
        prev_level = self.development_level
        self.development_level = min(1.0, self.development_level + amount)
        
        # Update parameters based on development level change
        delta = self.development_level - prev_level
        
        # Decrease decay rate (more stable focus)
        decay_decrease = delta * 0.02
        self.parameters.decay_rate = max(0.01, self.parameters.decay_rate - decay_decrease)
        
        # Increase capacity (can attend to more things simultaneously)
        capacity_increase = delta * 0.5  # Gradually increase capacity
        self.current_focus.capacity = min(7.0, self.current_focus.capacity + capacity_increase)
        
        # Reduce shift threshold (more controlled attention shifting)
        threshold_decrease = delta * 0.05
        self.parameters.shift_threshold = max(0.1, self.parameters.shift_threshold - threshold_decrease)
        
        return self.development_level
    
    def update_focus_from_salience(self, salience_scores: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """
        Update attention focus based on salience scores
        
        Parameters:
        salience_scores: Dictionary mapping item IDs to salience information
        
        Returns:
        Operation result
        """
        # Track what changes we make
        added_targets = []
        removed_targets = []
        updated_targets = []
        
        # Ensure salience scores are normalized
        if not salience_scores:
            return {
                "status": "success", 
                "message": "No salience scores provided",
                "added": added_targets,
                "removed": removed_targets,
                "updated": updated_targets
            }
        
        # Create a priority queue of items by salience
        items_by_salience = []
        for item_id, item_data in salience_scores.items():
            # Handle both object and direct score
            if isinstance(item_data, dict):
                score = item_data.get("score", 0.0)
                item_type = item_data.get("target_type", "unknown")
                description = item_data.get("description", "")
            else:
                score = float(item_data)
                item_type = "unknown"
                description = ""
                
            items_by_salience.append({
                "item_id": item_id,
                "score": score,
                "type": item_type,
                "description": description
            })
        
        # Sort by salience score
        items_by_salience.sort(key=lambda x: x["score"], reverse=True)
        
        # Update existing targets first
        for item in items_by_salience:
            item_id = item["item_id"]
            salience = item["score"]
            
            if item_id in self.current_focus.targets:
                # Update existing target's activation based on salience
                prev_activation = self.current_focus.targets[item_id]
                new_activation = min(1.0, prev_activation + salience * self.parameters.salience_sensitivity)
                
                # Apply the update
                target = self.current_focus.target_details[item_id]
                target.update_activation(new_activation - prev_activation)
                self.current_focus.targets[item_id] = new_activation
                
                updated_targets.append({
                    "target_id": item_id,
                    "prev_activation": prev_activation,
                    "new_activation": new_activation
                })
        
        # Now consider adding new targets
        remaining_capacity = max(0, self.current_focus.capacity - len(self.current_focus.targets))
        
        for item in items_by_salience:
            item_id = item["item_id"]
            salience = item["score"]
            
            # Skip if already in focus
            if item_id in self.current_focus.targets:
                continue
                
            # Check if salience exceeds shift threshold
            if salience >= self.parameters.shift_threshold:
                if remaining_capacity > 0:
                    # We have capacity, so add this target
                    target = AttentionTarget(
                        target_id=item_id,
                        target_type=item["type"],
                        description=item["description"],
                        activation=salience
                    )
                    
                    success = self.current_focus.add_target(target)
                    if success:
                        remaining_capacity -= 1
                        added_targets.append({
                            "target_id": item_id,
                            "activation": salience
                        })
                else:
                    # At capacity, so consider replacing least activated target
                    least_active_id = min(self.current_focus.targets.items(), key=lambda x: x[1])[0]
                    least_activation = self.current_focus.targets[least_active_id]
                    
                    # Only replace if new item is significantly more salient
                    if salience > least_activation * 1.5:
                        # Remove least active
                        self.current_focus.remove_target(least_active_id)
                        removed_targets.append({
                            "target_id": least_active_id,
                            "activation": least_activation
                        })
                        
                        # Add new target
                        target = AttentionTarget(
                            target_id=item_id,
                            target_type=item["type"],
                            description=item["description"],
                            activation=salience
                        )
                        
                        success = self.current_focus.add_target(target)
                        if success:
                            added_targets.append({
                                "target_id": item_id,
                                "activation": salience
                            })
        
        # Record focus shift in history
        self._record_focus_shift(added_targets, removed_targets, updated_targets)
        
        # If any changes occurred, publish an event
        if added_targets or removed_targets or updated_targets:
            self.publish_message("attention_focus_updated", {
                "added": added_targets,
                "removed": removed_targets,
                "updated": updated_targets,
                "current_focus": self.current_focus.model_dump()
            })
        
        return {
            "status": "success",
            "added": added_targets,
            "removed": removed_targets,
            "updated": updated_targets,
            "current_focus": self.current_focus.model_dump()
        }
    
    def add_attention_target(
        self, 
        target_id: str, 
        target_type: str, 
        activation: float = 1.0,
        description: str = ""
    ) -> Dict[str, Any]:
        """
        Explicitly add a target to attention focus
        
        Parameters:
        target_id: ID of the target
        target_type: Type of the target
        activation: Initial activation level
        description: Description of the target
        
        Returns:
        Operation result
        """
        # If already in focus, just update activation
        if target_id in self.current_focus.targets:
            prev_activation = self.current_focus.targets[target_id]
            target = self.current_focus.target_details[target_id]
            target.update_activation(activation - prev_activation)
            self.current_focus.targets[target_id] = target.activation
            
            self.publish_message("attention_target_updated", {
                "target_id": target_id,
                "prev_activation": prev_activation,
                "new_activation": target.activation
            })
            
            return {
                "status": "success",
                "operation": "updated",
                "target_id": target_id,
                "prev_activation": prev_activation,
                "new_activation": target.activation
            }
        
        # Create new target
        target = AttentionTarget(
            target_id=target_id,
            target_type=target_type,
            activation=activation,
            description=description
        )
        
        # Check if we're at capacity
        if self.current_focus.is_at_capacity:
            # Try to remove least active target
            least_active_id = min(self.current_focus.targets.items(), key=lambda x: x[1])[0]
            self.current_focus.remove_target(least_active_id)
            
            self.publish_message("attention_target_removed", {
                "target_id": least_active_id,
                "reason": "capacity_limit"
            })
        
        # Add the new target
        success = self.current_focus.add_target(target)
        
        if success:
            self.publish_message("attention_target_added", {
                "target_id": target_id,
                "target_type": target_type,
                "activation": activation
            })
            
            return {
                "status": "success",
                "operation": "added",
                "target_id": target_id,
                "activation": activation
            }
        else:
            return {
                "status": "error",
                "message": "Failed to add target to attention focus"
            }
    
    def remove_attention_target(self, target_id: str) -> Dict[str, Any]:
        """
        Remove a target from attention focus
        
        Parameters:
        target_id: ID of the target to remove
        
        Returns:
        Operation result
        """
        if target_id in self.current_focus.targets:
            prev_activation = self.current_focus.targets[target_id]
            success = self.current_focus.remove_target(target_id)
            
            if success:
                self.publish_message("attention_target_removed", {
                    "target_id": target_id,
                    "prev_activation": prev_activation,
                    "reason": "explicit_request"
                })
                
                return {
                    "status": "success",
                    "target_id": target_id,
                    "prev_activation": prev_activation
                }
        
        return {
            "status": "error",
            "message": f"Target not in attention focus: {target_id}"
        }
    
    def shift_focus_to(self, target_id: str, priority: float = 0.5) -> Dict[str, Any]:
        """
        Shift focus to a specific target with given priority
        
        Parameters:
        target_id: ID of the target to focus on
        priority: How important this focus shift is (affects willingness to clear other targets)
        
        Returns:
        Operation result
        """
        # If high priority, clear other low-activation targets
        if priority > 0.7:
            # Remove all targets with activation below 0.5
            for tid, activation in list(self.current_focus.targets.items()):
                if activation < 0.5 and tid != target_id:
                    self.current_focus.remove_target(tid)
        
        # If target is already in focus, increase its activation
        if target_id in self.current_focus.targets:
            target = self.current_focus.target_details[target_id]
            prev_activation = target.activation
            
            # Set activation based on priority
            new_activation = max(target.activation, priority)
            target.update_activation(new_activation - prev_activation)
            self.current_focus.targets[target_id] = new_activation
            
            self.publish_message("attention_focus_shifted", {
                "target_id": target_id,
                "prev_activation": prev_activation,
                "new_activation": new_activation,
                "priority": priority
            })
            
            return {
                "status": "success",
                "operation": "enhanced",
                "target_id": target_id,
                "prev_activation": prev_activation,
                "new_activation": new_activation
            }
        else:
            # Target not in focus, so add it
            # If at capacity, make room by removing lowest activation target
            if self.current_focus.is_at_capacity:
                lowest_id = min(self.current_focus.targets.items(), key=lambda x: x[1])[0]
                self.current_focus.remove_target(lowest_id)
            
            # Create target with unknown type (will be updated when we get more info)
            target = AttentionTarget(
                target_id=target_id,
                target_type="unknown",
                activation=priority
            )
            
            success = self.current_focus.add_target(target)
            
            if success:
                self.publish_message("attention_focus_shifted", {
                    "target_id": target_id,
                    "activation": priority,
                    "priority": priority
                })
                
                return {
                    "status": "success",
                    "operation": "added",
                    "target_id": target_id,
                    "activation": priority
                }
            else:
                return {
                    "status": "error",
                    "message": "Failed to add target to attention"
                }
    
    def _update_state(self) -> None:
        """
        Update attention state based on time
        
        This handles time-based decay of attention.
        """
        now = datetime.now()
        time_delta = (now - self.last_update).total_seconds()
        self.last_update = now
        
        if time_delta <= 0:
            return
        
        # Apply decay to all targets
        removed_ids = self.current_focus.decay_all(self.parameters.decay_rate * time_delta)
        
        # Notify about removed targets
        for target_id in removed_ids:
            self.publish_message("attention_target_removed", {
                "target_id": target_id,
                "reason": "decay"
            })
    
    def _record_focus_shift(
        self, 
        added_targets: List[Dict[str, Any]], 
        removed_targets: List[Dict[str, Any]],
        updated_targets: List[Dict[str, Any]]
    ) -> None:
        """Record focus shift in history for learning attention patterns"""
        if not (added_targets or removed_targets or updated_targets):
            return
            
        # Create a focus shift record
        record = {
            "timestamp": datetime.now().isoformat(),
            "added": added_targets,
            "removed": removed_targets,
            "updated": updated_targets,
            "development_level": self.development_level
        }
        
        # Add to history
        self.focus_shift_history.append(record)
        
        # Trim history if needed
        if len(self.focus_shift_history) > self.max_history_size:
            self.focus_shift_history = self.focus_shift_history[-self.max_history_size:]
    
    # Event handlers
    
    def _handle_salience_detected(self, message: Message) -> None:
        """Handle salience detection events from the salience detector"""
        content = message.content
        salience_scores = content.get("salience_scores", {})
        
        if salience_scores:
            self.update_focus_from_salience(salience_scores)
    
    def _handle_executive_command(self, message: Message) -> None:
        """Handle commands from the executive module"""
        content = message.content
        command = content.get("command", "")
        
        if command == "focus_on":
            target_id = content.get("target_id", "")
            priority = content.get("priority", 0.8)  # Executive commands get high priority
            
            if target_id:
                self.shift_focus_to(target_id, priority)
                
        elif command == "clear_focus":
            # Clear all or specific targets
            target_ids = content.get("target_ids", [])
            
            if target_ids:
                # Clear specific targets
                for target_id in target_ids:
                    self.remove_attention_target(target_id)
            else:
                # Clear all targets
                for target_id in list(self.current_focus.targets.keys()):
                    self.remove_attention_target(target_id)
    
    def _handle_perception_input(self, message: Message) -> None:
        """Handle inputs from the perception module"""
        content = message.content
        perception_data = content.get("perception_data", {})
        
        # Check if this contains salience information
        salience_info = content.get("salience", {})
        
        if salience_info:
            self.update_focus_from_salience(salience_info)

#######################

#attention\models.py#
#######################

from pydantic import BaseModel, Field, model_validator
from typing import Dict, List, Any, Optional, Set, Tuple
from datetime import datetime

class SalienceScore(BaseModel):
    """
    Represents the salience (noticeability/importance) of an item
    
    Salience is influenced by multiple factors including novelty,
    emotional significance, and relevance to current goals.
    """
    # Unique identifier for the item
    item_id: str
    # Overall salience score (0.0-1.0)
    score: float = Field(default=0.5, ge=0.0, le=1.0)
    # Novelty contribution (how new/unexpected)
    novelty: float = Field(default=0.0, ge=0.0, le=1.0) 
    # Emotional significance contribution
    emotional_significance: float = Field(default=0.0, ge=0.0, le=1.0)
    # Relevance to current goals
    goal_relevance: float = Field(default=0.0, ge=0.0, le=1.0)
    # Sensory intensity contribution
    intensity: float = Field(default=0.0, ge=0.0, le=1.0)
    # Timestamp when this score was calculated
    timestamp: datetime = Field(default_factory=datetime.now)
    
    @model_validator(mode='after')
    def calculate_overall_score(self):
        """Calculate overall score based on component factors"""
        # Simple weighted average of factors (weights could be learned over time)
        self.score = (
            0.3 * self.novelty +
            0.3 * self.emotional_significance +
            0.25 * self.goal_relevance +
            0.15 * self.intensity
        )
        return self

class AttentionTarget(BaseModel):
    """
    A single target of attention
    
    Represents an item that is currently receiving attention,
    with metadata about why and how much attention it's receiving.
    """
    # Unique identifier for the target
    target_id: str
    # Description of this attention target
    description: str = ""
    # Type of the target (e.g., "perception", "memory", "thought")
    target_type: str
    # Activation level (how much attention it's receiving)
    activation: float = Field(default=1.0, ge=0.0, le=1.0)
    # When this target first received attention
    entry_time: datetime = Field(default_factory=datetime.now)
    # Last time this target's activation was updated
    last_updated: datetime = Field(default_factory=datetime.now)
    # Metadata about this target
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    def update_activation(self, amount: float) -> float:
        """Update the activation of this target"""
        self.activation = max(0.0, min(1.0, self.activation + amount))
        self.last_updated = datetime.now()
        return self.activation
    
    def decay_activation(self, decay_rate: float) -> float:
        """Apply decay to activation over time"""
        # Calculate time since last update
        time_delta = (datetime.now() - self.last_updated).total_seconds()
        # Apply decay
        decay_amount = decay_rate * time_delta
        self.activation = max(0.0, self.activation - decay_amount)
        self.last_updated = datetime.now()
        return self.activation

class AttentionFocus(BaseModel):
    """
    Current focus of attention
    
    Maintains a collection of items currently receiving attention
    and manages the limited capacity of attention.
    """
    # Mapping of target_id to activation level
    targets: Dict[str, float] = Field(default_factory=dict)
    # Maximum number of items that can receive attention simultaneously
    capacity: float = Field(default=3.0, ge=1.0, le=10.0)
    # If attention is fully engaged or free for new stimuli
    is_at_capacity: bool = Field(default=False)
    # Detailed target information
    target_details: Dict[str, AttentionTarget] = Field(default_factory=dict)
    # Current overall attention state (e.g., "focused", "divided", "diffuse")
    state: str = "focused"
    # Timestamp of last focus update
    last_updated: datetime = Field(default_factory=datetime.now)
    
    @model_validator(mode='after')
    def update_capacity_state(self):
        """Update is_at_capacity based on number of targets"""
        self.is_at_capacity = len(self.targets) >= self.capacity
        return self
    
    def add_target(self, target: AttentionTarget) -> bool:
        """
        Add a new target to attention focus
        
        Returns:
        True if successfully added, False if at capacity
        """
        # Check if already at capacity and this isn't already a target
        if self.is_at_capacity and target.target_id not in self.targets:
            return False
            
        # Add or update target
        self.targets[target.target_id] = target.activation
        self.target_details[target.target_id] = target
        self.last_updated = datetime.now()
        
        # Update capacity state
        self.is_at_capacity = len(self.targets) >= self.capacity
        
        return True
    
    def remove_target(self, target_id: str) -> bool:
        """Remove a target from attention focus"""
        if target_id in self.targets:
            del self.targets[target_id]
            if target_id in self.target_details:
                del self.target_details[target_id]
                
            self.is_at_capacity = len(self.targets) >= self.capacity
            self.last_updated = datetime.now()
            return True
        return False
    
    def update_targets(self, activations: Dict[str, float]) -> None:
        """Update activation levels for multiple targets"""
        for target_id, activation in activations.items():
            if target_id in self.target_details:
                self.target_details[target_id].update_activation(activation - self.targets[target_id])
                self.targets[target_id] = self.target_details[target_id].activation
        
        self.last_updated = datetime.now()
    
    def decay_all(self, decay_rate: float) -> List[str]:
        """
        Apply decay to all targets and remove those below threshold
        
        Returns:
        List of removed target IDs
        """
        removal_threshold = 0.1
        removed_ids = []
        
        for target_id, target in list(self.target_details.items()):
            target.decay_activation(decay_rate)
            self.targets[target_id] = target.activation
            
            # Remove if below threshold
            if target.activation < removal_threshold:
                self.remove_target(target_id)
                removed_ids.append(target_id)
        
        self.last_updated = datetime.now()
        return removed_ids
    
    def get_dominant_target(self) -> Optional[Tuple[str, float]]:
        """Get the target with highest activation level"""
        if not self.targets:
            return None
            
        target_id = max(self.targets.items(), key=lambda x: x[1])[0]
        return (target_id, self.targets[target_id])

class AttentionParameters(BaseModel):
    """
    Parameters controlling attention behavior
    
    These parameters can be adjusted based on development level
    or situational factors.
    """
    # How quickly attention decays over time
    decay_rate: float = Field(default=0.05, ge=0.0, le=1.0)
    # How strongly salience influences attention
    salience_sensitivity: float = Field(default=0.7, ge=0.0, le=1.0)
    # How strongly novelty contributes to salience
    novelty_bias: float = Field(default=0.6, ge=0.0, le=1.0)
    # How strongly emotional significance contributes to salience
    emotional_bias: float = Field(default=0.7, ge=0.0, le=1.0)
    # How easily attention shifts to new targets
    shift_threshold: float = Field(default=0.3, ge=0.0, le=1.0)

#######################

#attention\neural_net.py#
#######################

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union

class AttentionNetwork(nn.Module):
    """
    Neural network architecture for attention processing
    
    This network handles the computational aspects of attention, including
    salience detection, focus control, and attention modulation.
    """
    def __init__(
        self, 
        input_dim: int = 128, 
        hidden_dim: int = 256, 
        output_dim: int = 64,
        num_heads: int = 4
    ):
        """
        Initialize attention neural network
        
        Parameters:
        input_dim: Input dimension
        hidden_dim: Hidden layer dimension
        output_dim: Output dimension
        num_heads: Number of attention heads
        """
        super().__init__()
        
        # Input embedding
        self.input_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.LayerNorm(hidden_dim)
        )
        
        # Salience detection network
        self.salience_network = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 4),  # 4 components: novelty, emotion, goal, intensity
            nn.Sigmoid()  # Constrain outputs to 0-1 range
        )
        
        # Attention focus network
        self.focus_network = nn.Sequential(
            nn.Linear(hidden_dim + 4, hidden_dim),  # Input + salience components
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()  # Activation level (0-1)
        )
        
        # Multi-head attention mechanism
        self.multihead_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            batch_first=True
        )
        
        # Gating mechanism to control information flow
        self.attention_gate = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
        
        # Output decoder
        self.decoder = nn.Linear(hidden_dim, output_dim)
        
        # State for working memory
        self.working_memory = None
    
    def forward(
        self, 
        inputs: torch.Tensor, 
        current_focus: Optional[torch.Tensor] = None,
        context: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Forward pass through the attention network
        
        Parameters:
        inputs: Input tensor [batch_size, num_items, input_dim]
        current_focus: Optional current focus state [batch_size, hidden_dim]
        context: Optional context information [batch_size, hidden_dim]
        
        Returns:
        Tuple of (output, attention_scores, salience_scores)
        """
        batch_size, num_items, _ = inputs.shape
        
        # Encode inputs
        encoded = self.input_encoder(inputs)  # [batch_size, num_items, hidden_dim]
        
        # Calculate salience for each item
        salience_scores = self.salience_network(encoded)  # [batch_size, num_items, 4]
        
        # Overall salience is average of components
        overall_salience = salience_scores.mean(dim=2, keepdim=True)  # [batch_size, num_items, 1]
        
        # Concatenate encoded inputs with salience components
        enhanced_inputs = torch.cat(
            [encoded, salience_scores], 
            dim=2
        )  # [batch_size, num_items, hidden_dim + 4]
        
        # Calculate attention activation for each item
        attention_scores = self.focus_network(enhanced_inputs)  # [batch_size, num_items, 1]
        attention_scores = attention_scores.squeeze(2)  # [batch_size, num_items]
        
        # Create attention mask (0 for items below threshold, 1 for above)
        attention_threshold = 0.3
        attention_mask = (attention_scores > attention_threshold).float()
        
        # Apply mask to encoded inputs
        masked_encoded = encoded * attention_mask.unsqueeze(2)
        
        # Apply multi-head attention
        attn_output, _ = self.multihead_attention(
            masked_encoded, masked_encoded, masked_encoded
        )
        
        # Apply residual connection
        attn_output = attn_output + masked_encoded
        
        # Calculate attention gating
        gate_values = self.attention_gate(attn_output)
        
        # Apply gate to control information flow
        gated_output = attn_output * gate_values
        
        # Generate final output
        output = self.decoder(gated_output)
        
        # Update working memory with current state
        self.working_memory = attn_output.detach()
        
        return output, attention_scores, salience_scores
    
    def detect_salience(
        self, 
        inputs: torch.Tensor,
        context: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Only run the salience detection part of the network
        
        Parameters:
        inputs: Input tensor [batch_size, num_items, input_dim]
        context: Optional context information [batch_size, hidden_dim]
        
        Returns:
        Tuple of (overall_salience, salience_components)
        """
        # Encode inputs
        encoded = self.input_encoder(inputs)
        
        # Calculate salience components
        salience_components = self.salience_network(encoded)
        
        # Extract components
        novelty = salience_components[:, :, 0]
        emotional = salience_components[:, :, 1]
        goal = salience_components[:, :, 2]
        intensity = salience_components[:, :, 3]
        
        # Calculate overall salience
        overall_salience = salience_components.mean(dim=2)
        
        components_dict = {
            "novelty": novelty,
            "emotional_significance": emotional,
            "goal_relevance": goal,
            "intensity": intensity
        }
        
        return overall_salience, components_dict
    
    def update_focus(
        self,
        encoded_items: torch.Tensor,
        salience_scores: torch.Tensor,
        current_focus: Optional[torch.Tensor] = None,
        capacity: int = 3
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Update attention focus based on salience
        
        Parameters:
        encoded_items: Encoded input items [batch_size, num_items, hidden_dim]
        salience_scores: Salience scores for items [batch_size, num_items, 4]
        current_focus: Optional current focus state [batch_size, hidden_dim]
        capacity: Attention capacity
        
        Returns:
        Tuple of (new_focus_state, attention_scores)
        """
        batch_size, num_items, _ = encoded_items.shape
        
        # Concatenate encoded items with salience components
        enhanced_inputs = torch.cat(
            [encoded_items, salience_scores], 
            dim=2
        )
        
        # Calculate attention activation for each item
        attention_scores = self.focus_network(enhanced_inputs)
        attention_scores = attention_scores.squeeze(2)  # [batch_size, num_items]
        
        # Apply capacity constraint - keep only top-k items
        if num_items > capacity:
            # Find threshold value that keeps capacity items
            sorted_scores, _ = torch.sort(attention_scores, dim=1, descending=True)
            threshold_values = sorted_scores[:, capacity-1].unsqueeze(1)
            
            # Create mask for items above threshold
            capacity_mask = (attention_scores >= threshold_values).float()
            
            # Apply mask to attention scores
            masked_attention = attention_scores * capacity_mask
        else:
            masked_attention = attention_scores
        
        # Calculate new focus state
        new_focus_state = torch.sum(
            encoded_items * masked_attention.unsqueeze(2),
            dim=1
        )
        
        return new_focus_state, masked_attention
    
    def process_with_attention(
        self,
        inputs: torch.Tensor,
        attention_scores: torch.Tensor
    ) -> torch.Tensor:
        """
        Process inputs modulated by attention
        
        Parameters:
        inputs: Input tensor [batch_size, num_items, input_dim]
        attention_scores: Attention scores [batch_size, num_items]
        
        Returns:
        Processed output
        """
        # Apply attention modulation
        modulated_inputs = inputs * attention_scores.unsqueeze(2)
        
        # Encode modulated inputs
        encoded = self.input_encoder(modulated_inputs)
        
        # Apply multi-head attention mechanism
        attn_output, _ = self.multihead_attention(
            encoded, encoded, encoded
        )
        
        # Apply residual connection
        attn_output = attn_output + encoded
        
        # Generate final output
        output = self.decoder(attn_output)
        
        return output
    
    def reset_working_memory(self) -> None:
        """Reset working memory state"""
        self.working_memory = None

#######################

#attention\salience_detector.py#
#######################

from typing import Dict, List, Any, Optional, Set, Tuple
from pydantic import BaseModel, Field
from datetime import datetime
import numpy as np

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.attention.models import SalienceScore, AttentionParameters

class SalienceDetector(BaseModule):
    """
    Detects salient aspects of inputs.
    
    The salience detector evaluates inputs to determine what aspects
    are important, novel, emotionally significant, or otherwise worthy
    of attention. It's the first stage of the attention process, identifying
    candidates for focus.
    """
    # Parameters controlling salience detection
    parameters: AttentionParameters = Field(default_factory=AttentionParameters)
    # History of previously seen inputs (for novelty detection)
    input_history: Dict[str, Dict[str, Any]] = Field(default_factory=dict)
    # Maximum history size
    max_history_size: int = Field(default=1000)
    # Last calculated salience scores
    last_salience_scores: Dict[str, SalienceScore] = Field(default_factory=dict)
    # Emotion state influence
    emotion_state: Dict[str, float] = Field(default_factory=dict)
    # Current goals influence
    current_goals: List[Dict[str, Any]] = Field(default_factory=list)
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, **data):
        """Initialize salience detector module"""
        super().__init__(
            module_id=module_id,
            module_type="salience_detector",
            event_bus=event_bus,
            **data
        )
        
        # Subscribe to relevant events
        if self.event_bus:
            self.subscribe_to_message("perception_input", self._handle_perception_input)
            self.subscribe_to_message("emotion_update", self._handle_emotion_update)
            self.subscribe_to_message("goal_update", self._handle_goal_update)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to detect salience
        
        Parameters:
        input_data: Dictionary containing operation data
            - operation: The operation to perform
            - inputs: Dictionary of input items to evaluate
            - Additional parameters specific to the operation
            
        Returns:
        Operation result with salience scores
        """
        operation = input_data.get("operation", "")
        
        if operation == "detect_salience":
            inputs = input_data.get("inputs", {})
            context = input_data.get("context", {})
            return self.detect_salience(inputs, context)
            
        elif operation == "get_last_scores":
            return {
                "status": "success",
                "salience_scores": {
                    k: v.model_dump() for k, v in self.last_salience_scores.items()
                }
            }
            
        elif operation == "check_novelty":
            item_id = input_data.get("item_id", "")
            item_data = input_data.get("item_data", {})
            
            if item_id and item_data:
                novelty = self.calculate_novelty(item_id, item_data)
                return {
                    "status": "success",
                    "item_id": item_id,
                    "novelty": novelty
                }
            else:
                return {"status": "error", "message": "Missing item ID or data"}
                
        else:
            return {"status": "error", "message": f"Unknown operation: {operation}"}
    
    def update_development(self, amount: float) -> float:
        """
        Update module's developmental level
        
        As the salience detector develops:
        - Novelty detection becomes more sophisticated
        - Emotional significance assessment improves
        - Goal relevance evaluation becomes more accurate
        
        Parameters:
        amount: Amount to increase development level
        
        Returns:
        New development level
        """
        prev_level = self.development_level
        self.development_level = min(1.0, self.development_level + amount)
        
        # Update parameters based on development level change
        delta = self.development_level - prev_level
        
        # Decrease novelty bias (less distracted by mere novelty)
        novelty_decrease = delta * 0.1
        self.parameters.novelty_bias = max(0.3, self.parameters.novelty_bias - novelty_decrease)
        
        # Increase emotional bias (better emotional understanding)
        emotional_increase = delta * 0.1
        self.parameters.emotional_bias = min(0.9, self.parameters.emotional_bias + emotional_increase)
        
        # Increase our history capacity (better memory for novelty detection)
        self.max_history_size = int(1000 + 4000 * self.development_level)
        
        return self.development_level
    
    def detect_salience(self, inputs: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Detect salience in the provided inputs
        
        Parameters:
        inputs: Dictionary of input items to evaluate for salience
        context: Optional context information for salience calculation
        
        Returns:
        Operation result with salience scores
        """
        if not inputs:
            return {"status": "error", "message": "No inputs provided"}
            
        # Reset salience scores
        self.last_salience_scores = {}
        
        # Process each input item
        salience_scores = {}
        
        for item_id, item_data in inputs.items():
            # Calculate component factors
            novelty = self.calculate_novelty(item_id, item_data)
            emotional_significance = self.calculate_emotional_significance(item_data)
            goal_relevance = self.calculate_goal_relevance(item_data)
            intensity = self.calculate_intensity(item_data)
            
            # Create salience score
            score = SalienceScore(
                item_id=item_id,
                novelty=novelty,
                emotional_significance=emotional_significance,
                goal_relevance=goal_relevance,
                intensity=intensity
            )
            
            # Store in results
            salience_scores[item_id] = score.score
            self.last_salience_scores[item_id] = score
            
            # Add to input history for future novelty detection
            self._update_input_history(item_id, item_data)
        
        # Publish salience detection event
        self.publish_message("salience_detected", {
            "salience_scores": salience_scores,
            "timestamp": datetime.now().isoformat()
        })
        
        return {
            "status": "success",
            "salience_scores": salience_scores,
            "detailed_scores": {
                k: v.model_dump() for k, v in self.last_salience_scores.items()
            }
        }
    
    def calculate_novelty(self, item_id: str, item_data: Any) -> float:
        """
        Calculate novelty factor for an item
        
        The novelty detection depends on the developmental level:
        - Early development: simple has-been-seen-before check
        - Later development: more sophisticated feature-based comparison
        
        Parameters:
        item_id: ID of the item
        item_data: Data for the item
        
        Returns:
        Novelty score (0.0-1.0)
        """
        # Check if item has been seen before
        if item_id in self.input_history:
            # Item has been seen, calculate how novel its current state is
            prev_data = self.input_history[item_id]
            
            # Simple comparison for primitive development
            if self.development_level < 0.3:
                # Very basic novelty detection - just seen/not seen
                return 0.2  # Low novelty since we've seen it before
                
            elif self.development_level < 0.6:
                # More nuanced - check if key attributes have changed
                if isinstance(item_data, dict) and isinstance(prev_data.get("data"), dict):
                    # Compare dictionaries
                    changes = 0
                    total_keys = 0
                    
                    for key, value in item_data.items():
                        total_keys += 1
                        if key not in prev_data["data"] or prev_data["data"][key] != value:
                            changes += 1
                    
                    if total_keys > 0:
                        return min(1.0, changes / total_keys)
                    
                return 0.3  # Moderate novelty
                
            else:
                # Advanced novelty detection 
                # This would ideally use embeddings/feature vectors for comparison
                # For now, we'll use a simplified approach
                
                # Frequency-based novelty (less frequently seen = more novel)
                freq = prev_data.get("frequency", 1)
                recency = (datetime.now() - prev_data.get("last_seen", datetime.now())).total_seconds()
                
                # Normalize recency (higher = more recent = less novel)
                recency_factor = min(1.0, recency / (3600 * 24))  # Normalize to 1 day
                recency_novelty = 1.0 - recency_factor
                
                # Frequency-based novelty (higher frequency = less novel)
                freq_novelty = 1.0 / (1.0 + np.log1p(freq))
                
                # Combine factors
                return 0.6 * recency_novelty + 0.4 * freq_novelty
        else:
            # Item has never been seen before - maximum novelty
            return 1.0
    
    def calculate_emotional_significance(self, item_data: Any) -> float:
        """
        Calculate emotional significance of an item
        
        Parameters:
        item_data: Data for the item
        
        Returns:
        Emotional significance score (0.0-1.0)
        """
        # Early development - very basic emotional significance detection
        if self.development_level < 0.3:
            # Check for basic emotional content if item is a string
            if isinstance(item_data, str):
                # Very primitive emotion detection
                positive_words = {"happy", "good", "nice", "love", "like"}
                negative_words = {"sad", "bad", "angry", "fear", "hate"}
                
                item_text = item_data.lower()
                pos_count = sum(1 for word in positive_words if word in item_text)
                neg_count = sum(1 for word in negative_words if word in item_text)
                
                # Simple emotional significance based on emotion word count
                emotion_count = pos_count + neg_count
                if emotion_count > 0:
                    return min(1.0, 0.3 + 0.1 * emotion_count)
            
            # Default low emotional significance
            return 0.1
            
        elif self.development_level < 0.6:
            # More nuanced emotional significance
            # Use current emotion state to influence significance
            if self.emotion_state:
                # Check if item content matches current emotional state
                if isinstance(item_data, dict):
                    # If item has emotion data
                    if "emotion" in item_data:
                        item_emotion = item_data["emotion"]
                        # Check how closely item emotion matches current emotion
                        if isinstance(item_emotion, str) and item_emotion in self.emotion_state:
                            return min(1.0, 0.4 + 0.6 * self.emotion_state[item_emotion])
                        elif isinstance(item_emotion, dict):
                            # Calculate overlap between item emotions and current emotions
                            match_score = 0.0
                            for emotion, intensity in item_emotion.items():
                                if emotion in self.emotion_state:
                                    match_score += intensity * self.emotion_state[emotion]
                            return min(1.0, 0.4 + match_score)
                
                # Moderate default emotional significance
                return 0.3
            
            return 0.2
            
        else:
            # Advanced emotional significance
            # This would ideally use an emotion classifier/sentiment analyzer
            # For now, we'll use a simplified approach based on current emotions
            
            # Calculate emotional congruence with current state
            if isinstance(item_data, dict) and "emotional_valence" in item_data:
                # Item has explicit emotional valence
                item_valence = item_data["emotional_valence"]
                
                # Calculate how strongly this valence aligns with current emotions
                valence_match = 0.0
                for emotion, intensity in self.emotion_state.items():
                    # Simple valence matching
                    if emotion in {"joy", "trust", "anticipation"} and item_valence > 0:
                        valence_match += intensity * item_valence
                    elif emotion in {"sadness", "fear", "anger", "disgust"} and item_valence < 0:
                        valence_match += intensity * abs(item_valence)
                
                return min(1.0, 0.3 + 0.7 * valence_match)
            
            # Default moderate emotional significance
            return 0.4
    
    def calculate_goal_relevance(self, item_data: Any) -> float:
        """
        Calculate relevance of an item to current goals
        
        Parameters:
        item_data: Data for the item
        
        Returns:
        Goal relevance score (0.0-1.0)
        """
        # No goals means no goal relevance
        if not self.current_goals:
            return 0.0
            
        # Early development - very basic goal relevance
        if self.development_level < 0.3:
            # Primitive development can't really assess goal relevance
            return 0.1
            
        elif self.development_level < 0.6:
            # Simple keyword matching for goal relevance
            if isinstance(item_data, str) or (isinstance(item_data, dict) and "content" in item_data):
                content = item_data if isinstance(item_data, str) else item_data["content"]
                
                # Extract keywords from goals
                goal_keywords = set()
                for goal in self.current_goals:
                    goal_desc = goal.get("description", "")
                    keywords = goal.get("keywords", [])
                    
                    # Add explicit keywords
                    goal_keywords.update(keywords)
                    
                    # Add words from goal description
                    if isinstance(goal_desc, str):
                        words = goal_desc.lower().split()
                        # Filter out common words
                        content_words = [w for w in words if len(w) > 3]
                        goal_keywords.update(content_words)
                
                # Check for keyword matches in content
                if isinstance(content, str):
                    content_lower = content.lower()
                    matches = sum(1 for kw in goal_keywords if kw.lower() in content_lower)
                    
                    if matches > 0:
                        return min(1.0, 0.3 + 0.1 * matches)
            
            # Moderate default goal relevance
            return 0.2
            
        else:
            # Advanced goal relevance
            # This would ideally use semantic similarity between item and goals
            # For now, use a simplified approach
            
            max_relevance = 0.0
            
            for goal in self.current_goals:
                # Get goal importance
                importance = goal.get("importance", 0.5)
                
                # Calculate relevance based on goal type and item data
                relevance = 0.0
                
                if isinstance(item_data, dict):
                    # Check for direct goal references
                    if "goal_id" in item_data and item_data["goal_id"] == goal.get("id"):
                        relevance = 0.8  # High relevance for direct goal references
                    
                    # Check for context matches
                    elif "context" in item_data and "context" in goal:
                        if item_data["context"] == goal["context"]:
                            relevance = 0.6  # Good relevance for context matches
                    
                    # Check for concept matches
                    elif "concepts" in item_data and "concepts" in goal:
                        item_concepts = set(item_data["concepts"])
                        goal_concepts = set(goal["concepts"])
                        
                        overlap = item_concepts.intersection(goal_concepts)
                        if overlap:
                            relevance = 0.4 + 0.4 * (len(overlap) / len(goal_concepts))
                
                # Apply importance weighting
                weighted_relevance = relevance * importance
                max_relevance = max(max_relevance, weighted_relevance)
            
            return max_relevance
    
    def calculate_intensity(self, item_data: Any) -> float:
        """
        Calculate sensory intensity of an item
        
        Parameters:
        item_data: Data for the item
        
        Returns:
        Intensity score (0.0-1.0)
        """
        # Check for explicit intensity value
        if isinstance(item_data, dict) and "intensity" in item_data:
            return min(1.0, max(0.0, float(item_data["intensity"])))
            
        # Check for volume/size indicators
        if isinstance(item_data, dict):
            # Check size attribute
            if "size" in item_data:
                size = item_data["size"]
                if isinstance(size, (int, float)):
                    return min(1.0, size / 10.0)  # Normalize to 0-1 range
                elif isinstance(size, str):
                    size_map = {"tiny": 0.1, "small": 0.3, "medium": 0.5, "large": 0.7, "huge": 0.9}
                    return size_map.get(size.lower(), 0.5)
            
            # Check volume attribute
            if "volume" in item_data:
                volume = item_data["volume"]
                if isinstance(volume, (int, float)):
                    return min(1.0, volume / 10.0)  # Normalize to 0-1 range
                elif isinstance(volume, str):
                    volume_map = {"silent": 0.1, "quiet": 0.3, "normal": 0.5, "loud": 0.8, "deafening": 1.0}
                    return volume_map.get(volume.lower(), 0.5)
            
            # Check brightness attribute
            if "brightness" in item_data:
                brightness = item_data["brightness"]
                if isinstance(brightness, (int, float)):
                    return min(1.0, brightness / 10.0)  # Normalize to 0-1 range
        
        # Default moderate intensity
        return 0.5
    
    def _update_input_history(self, item_id: str, item_data: Any) -> None:
        """
        Update input history for novelty detection
        
        Parameters:
        item_id: ID of the item
        item_data: Data for the item
        """
        # If item exists, update it
        if item_id in self.input_history:
            prev_data = self.input_history[item_id]
            
            # Update frequency
            freq = prev_data.get("frequency", 1)
            prev_data["frequency"] = freq + 1
            
            # Update last seen time
            prev_data["last_seen"] = datetime.now()
            
            # Update data (store latest version)
            prev_data["data"] = item_data
            
        else:
            # New item
            self.input_history[item_id] = {
                "data": item_data,
                "first_seen": datetime.now(),
                "last_seen": datetime.now(),
                "frequency": 1
            }
        
        # Trim history if needed
        if len(self.input_history) > self.max_history_size:
            # Remove least recently seen items
            sorted_items = sorted(
                self.input_history.items(),
                key=lambda x: x[1].get("last_seen", datetime.min)
            )
            
            # Remove oldest items to get back to 90% of max size
            items_to_remove = len(self.input_history) - int(self.max_history_size * 0.9)
            
            for i in range(items_to_remove):
                if i < len(sorted_items):
                    del self.input_history[sorted_items[i][0]]
    
    # Event handlers
    
    def _handle_perception_input(self, message: Message) -> None:
        """
        Handle perception input events
        
        Automatically calculates salience for new perception inputs.
        """
        content = message.content
        perception_data = content.get("perception_data", {})
        
        if perception_data:
            # Detect salience in the perception data
            result = self.detect_salience(perception_data)
            
            # If successful, add salience info to the message
            if result["status"] == "success":
                perception_data["salience"] = result["salience_scores"]
    
    def _handle_emotion_update(self, message: Message) -> None:
        """
        Handle emotion update events
        
        Updates internal emotion state for emotional significance calculation.
        """
        content = message.content
        emotions = content.get("emotions", {})
        
        if emotions:
            # Update our emotion state
            self.emotion_state = emotions.copy()
    
    def _handle_goal_update(self, message: Message) -> None:
        """
        Handle goal update events
        
        Updates current goals for goal relevance calculation.
        """
        content = message.content
        goals = content.get("goals", [])
        
        if goals:
            # Update our current goals
            self.current_goals = goals.copy()

#######################

#attention\__init__.py#
#######################

"""
Attention Module

This module handles the focusing of cognitive resources on specific 
aspects of perception, memory, and thought. It determines what information
is prioritized and brought into working memory for further processing.
"""

import logging
from typing import Dict, List, Any, Optional, Set, Tuple
import time
import uuid
from datetime import datetime
from collections import deque

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message

logger = logging.getLogger(__name__)

def get_module(
    module_id: str = "attention",
    event_bus: Optional[EventBus] = None,
    development_level: float = 0.0
) -> "AttentionSystem":
    """
    Factory function to create and return an attention module
    
    This function initializes and returns a complete attention system with
    focus control and salience detection capabilities.
    
    Args:
        module_id: Unique identifier for the module
        event_bus: Event bus for communication
        development_level: Initial developmental level for the system
        
    Returns:
        Initialized AttentionSystem
    """
    return AttentionSystem(
        module_id=module_id,
        event_bus=event_bus,
        development_level=development_level
    )

class AttentionSystem(BaseModule):
    """
    Attention system responsible for directing cognitive focus
    
    The attention system develops from basic attention capture by stimulus
    intensity to sophisticated volitional control of attention and multitasking.
    """
    # Development milestones
    development_milestones = {
        0.0: "Basic attention capture",
        0.2: "Sustained attention",
        0.4: "Selective attention",
        0.6: "Divided attention",
        0.8: "Executive attention control",
        1.0: "Sophisticated attention management"
    }
    
    def __init__(
        self,
        module_id: str,
        event_bus: Optional[EventBus] = None,
        development_level: float = 0.0
    ):
        """
        Initialize the attention system
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication
            development_level: Initial developmental level
        """
        super().__init__(
            module_id=module_id,
            module_type="attention_system",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Current focus of attention
        self.current_focus = None
        
        # Attention history
        self.focus_history = deque(maxlen=20)
        
        # Attentional control parameters
        self._attention_params = {
            "intensity_weight": 0.8,    # Weight for stimulus intensity
            "novelty_weight": 0.6,      # Weight for stimulus novelty
            "relevance_weight": 0.3,    # Weight for task relevance
            "volitional_weight": 0.2,   # Weight for intentional control
            "sustained_decay": 0.1,     # How quickly sustained attention decays
            "distraction_threshold": 0.7, # Threshold for attention capture
        }
        
        # Attention capacity - increases with development
        self._capacity = 1
        
        # Active focuses (for divided attention)
        self._active_focuses = []
        
        # Task context (what we're trying to focus on)
        self._task_context = {}
        
        # Adjust parameters based on development level
        self._adjust_parameters_for_development()
        
        # Subscribe to relevant events
        if self.event_bus:
            self.subscribe_to_message("perception_result")
            self.subscribe_to_message("attention_request")
            self.subscribe_to_message("attention_query")
    
    def _adjust_parameters_for_development(self):
        """Adjust attention parameters based on developmental level"""
        # Attention capacity grows with development
        self._capacity = max(1, int(1 + self.development_level * 3))
        
        if self.development_level < 0.2:
            # Early development - mainly stimulus-driven
            self._attention_params.update({
                "intensity_weight": 0.9,
                "novelty_weight": 0.7,
                "relevance_weight": 0.1,
                "volitional_weight": 0.0,
                "sustained_decay": 0.3,
                "distraction_threshold": 0.3,
            })
        elif self.development_level < 0.4:
            # Developing sustained attention
            self._attention_params.update({
                "intensity_weight": 0.8,
                "novelty_weight": 0.7,
                "relevance_weight": 0.3,
                "volitional_weight": 0.1,
                "sustained_decay": 0.2,
                "distraction_threshold": 0.4,
            })
        elif self.development_level < 0.6:
            # Developing selective attention
            self._attention_params.update({
                "intensity_weight": 0.7,
                "novelty_weight": 0.6,
                "relevance_weight": 0.5,
                "volitional_weight": 0.3,
                "sustained_decay": 0.15,
                "distraction_threshold": 0.5,
            })
        elif self.development_level < 0.8:
            # Developing divided attention
            self._attention_params.update({
                "intensity_weight": 0.6,
                "novelty_weight": 0.5,
                "relevance_weight": 0.7,
                "volitional_weight": 0.5,
                "sustained_decay": 0.1,
                "distraction_threshold": 0.6,
            })
        else:
            # Advanced executive attention
            self._attention_params.update({
                "intensity_weight": 0.4,
                "novelty_weight": 0.4,
                "relevance_weight": 0.8,
                "volitional_weight": 0.8,
                "sustained_decay": 0.05,
                "distraction_threshold": 0.8,
            })
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to determine attentional focus
        
        Args:
            input_data: Data to evaluate for attention
                Required keys: 'content', 'source'
                Optional keys: 'intensity', 'novelty', 'relevance'
                
        Returns:
            Dictionary with attention results
        """
        # Generate ID for this attention process
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Ensure timestamp is a float
        if "timestamp" in input_data:
            if isinstance(input_data["timestamp"], datetime):
                timestamp = input_data["timestamp"].timestamp()
            else:
                timestamp = float(input_data["timestamp"])
        else:
            timestamp = time.time()
        
        # Extract key attention parameters or use defaults
        intensity = input_data.get("intensity", 0.5)
        novelty = input_data.get("novelty", 0.5)
        relevance = input_data.get("relevance", 0.5)
        volitional = input_data.get("volitional", False)
        
        # Calculate salience score
        salience = self._calculate_salience(intensity, novelty, relevance, volitional)
        
        # Determine if this input captures attention based on
        # development level and current focus
        captures_attention = self._evaluate_attention_capture(salience, input_data)
        
        # Create focus object if attention is captured
        if captures_attention:
            focus = {
                "focus_id": f"focus_{uuid.uuid4().hex[:8]}",
                "content": input_data.get("content", {}),
                "source": input_data.get("source", "unknown"),
                "salience": salience,
                "timestamp": timestamp,
                "process_id": process_id,
                "sustained_until": timestamp + (10 * (1 - self._attention_params["sustained_decay"]))
            }
            
            # Update current focus - behavior depends on development level
            self._update_focus(focus)
            
            # Record in history
            self.focus_history.append(focus)
        
        # Prepare result
        result = {
            "process_id": process_id,
            "timestamp": timestamp,
            "development_level": self.development_level,
            "module_id": self.module_id,
            "captures_attention": captures_attention,
            "salience": salience,
            "current_focus": self.current_focus,
            "capacity": self._capacity,
            "active_focuses": len(self._active_focuses)
        }
        
        # Add developmental-appropriate additional information
        if self.development_level >= 0.4:
            # Add attention components at higher development levels
            result["attention_components"] = {
                "intensity_contribution": intensity * self._attention_params["intensity_weight"],
                "novelty_contribution": novelty * self._attention_params["novelty_weight"],
                "relevance_contribution": relevance * self._attention_params["relevance_weight"],
                "volitional_control": volitional * self._attention_params["volitional_weight"]
            }
            
        if self.development_level >= 0.6:
            # Add information about divided attention capabilities
            result["divided_attention"] = {
                "capacity": self._capacity,
                "active_focuses": [f["focus_id"] for f in self._active_focuses],
                "capacity_available": self._capacity - len(self._active_focuses)
            }
        
        # Publish attention result
        if self.event_bus:
            self.publish_message(
                "attention_focus_update",
                {"result": result, "process_id": process_id}
            )
            
        return result
    
    def _calculate_salience(
        self, 
        intensity: float,
        novelty: float,
        relevance: float,
        volitional: bool
    ) -> float:
        """
        Calculate the salience score of an input
        
        Args:
            intensity: Intensity of the stimulus (0-1)
            novelty: Novelty of the stimulus (0-1)
            relevance: Task relevance of the stimulus (0-1)
            volitional: Whether this is a deliberate attention shift
            
        Returns:
            Salience score (0-1)
        """
        # Weighted combination of factors
        salience = (
            intensity * self._attention_params["intensity_weight"] +
            novelty * self._attention_params["novelty_weight"] +
            relevance * self._attention_params["relevance_weight"]
        )
        
        # Add volitional control if developed enough
        if volitional and self._attention_params["volitional_weight"] > 0:
            salience += self._attention_params["volitional_weight"]
            
        # Normalize to 0-1 range
        salience = min(1.0, salience / (
            self._attention_params["intensity_weight"] + 
            self._attention_params["novelty_weight"] + 
            self._attention_params["relevance_weight"] +
            (self._attention_params["volitional_weight"] if volitional else 0)
        ))
        
        return salience
    
    def _evaluate_attention_capture(self, salience: float, input_data: Dict[str, Any]) -> bool:
        """
        Determine if an input captures attention
        
        Args:
            salience: Calculated salience score
            input_data: Input data
            
        Returns:
            Whether attention is captured
        """
        # Very early development - attention easily captured
        if self.development_level < 0.2:
            return salience > 0.3
            
        # Check current attention state
        if not self.current_focus:
            # No current focus - easier to capture
            return salience > 0.4
            
        # Volitional control overrides at higher development levels
        if (self.development_level >= 0.6 and 
            input_data.get("volitional", False) and 
            self._attention_params["volitional_weight"] > 0.4):
            return True
            
        # Check against distraction threshold - threshold increases with development
        distraction_threshold = self._attention_params["distraction_threshold"]
        
        # If we have capacity for divided attention, the threshold is lower
        if self.development_level >= 0.6 and len(self._active_focuses) < self._capacity:
            distraction_threshold *= 0.7
            
        return salience > distraction_threshold
    
    def _update_focus(self, new_focus: Dict[str, Any]):
        """
        Update the current focus of attention
        
        This behaves differently depending on developmental level:
        - Early: Single focus, easily displaced
        - Middle: More stable single focus
        - Advanced: Potential for multiple simultaneous focuses
        
        Args:
            new_focus: New focus object
        """
        # Early development - simply replace current focus
        if self.development_level < 0.6:
            self.current_focus = new_focus
            self._active_focuses = [new_focus]
            return
            
        # Divided attention capability
        if len(self._active_focuses) < self._capacity:
            # We have capacity for another focus
            self._active_focuses.append(new_focus)
            # Most salient becomes current focus
            self._active_focuses.sort(key=lambda x: x["salience"], reverse=True)
            self.current_focus = self._active_focuses[0]
        else:
            # Replace least salient focus if new one is more salient
            self._active_focuses.sort(key=lambda x: x["salience"])
            if new_focus["salience"] > self._active_focuses[0]["salience"]:
                self._active_focuses[0] = new_focus
                # Resort and update current
                self._active_focuses.sort(key=lambda x: x["salience"], reverse=True)
                self.current_focus = self._active_focuses[0]
    
    def set_task_context(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Set the current task context to guide attention
        
        This allows task-relevant stimuli to be prioritized
        
        Args:
            task_data: Information about current task
            
        Returns:
            Updated task context
        """
        self._task_context = task_data
        
        # Publish task context update
        if self.event_bus:
            self.publish_message(
                "attention_context_update",
                {"task_context": task_data}
            )
            
        return self._task_context
    
    def _handle_message(self, message: Message):
        """Handle incoming messages"""
        if message.message_type == "perception_result":
            # Process perception results for potential attention
            if message.content and "result" in message.content:
                result = message.content["result"]
                
                # Extract attention-relevant info
                attention_input = {
                    "content": result,
                    "source": "perception",
                    "process_id": result.get("process_id", message.id),
                    "timestamp": message.timestamp
                }
                
                # Try to extract salience factors
                if "patterns" in result:
                    patterns = result["patterns"]
                    # More patterns indicates higher intensity
                    attention_input["intensity"] = min(1.0, len(patterns) / 10)
                    
                    # Check for certain pattern types that suggest novelty
                    novel_patterns = ["complex_text", "question", "exclamation"]
                    if any(p["pattern_type"] in novel_patterns for p in patterns):
                        attention_input["novelty"] = 0.8
                    
                # Task relevance - if we have task context, check for relevance
                if self._task_context and "keywords" in self._task_context:
                    # Simple keyword matching for relevance
                    text = result.get("text", "")
                    keywords = self._task_context["keywords"]
                    matches = sum(1 for kw in keywords if kw.lower() in text.lower())
                    attention_input["relevance"] = min(1.0, matches / len(keywords)) if keywords else 0.5
                
                # Process for attention
                self.process_input(attention_input)
                
        elif message.message_type == "attention_request":
            # Direct request for attention
            if message.content:
                # Mark as volitional
                message.content["volitional"] = True
                self.process_input(message.content)
                
        elif message.message_type == "attention_query":
            # Handle query about attention state
            self._handle_attention_query(message)
    
    def _handle_attention_query(self, message: Message):
        """Handle queries about attention state"""
        query_type = message.content.get("query_type")
        query_id = message.content.get("query_id", str(uuid.uuid4()))
        
        response = {
            "query_id": query_id,
            "query_type": query_type,
            "module_id": self.module_id
        }
        
        if query_type == "current_focus":
            # Return current focus of attention
            response["current_focus"] = self.current_focus
            
        elif query_type == "focus_history":
            # Return recent focus history
            count = message.content.get("count", 5)
            response["focus_history"] = list(self.focus_history)[-count:]
            
        elif query_type == "capacity":
            # Return attention capacity information
            response["capacity"] = {
                "total_capacity": self._capacity,
                "used_capacity": len(self._active_focuses),
                "available_capacity": self._capacity - len(self._active_focuses),
                "development_level": self.development_level
            }
            
        # Publish response
        if self.event_bus:
            self.publish_message(
                "attention_query_response",
                response
            )
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of the module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # Update development level
        prev_level = self.development_level
        new_level = super().update_development(amount)
        
        # If development level changed significantly, adjust parameters
        if int(prev_level * 10) != int(new_level * 10):
            logger.info(f"Attention system upgraded to development level {new_level:.1f}")
            self._adjust_parameters_for_development()
            
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """Get the current state of the module"""
        state = super().get_state()
        
        # Add attention-specific state
        state.update({
            "current_focus": self.current_focus,
            "focus_history_size": len(self.focus_history),
            "active_focuses": len(self._active_focuses),
            "capacity": self._capacity,
            "attention_parameters": self._attention_params
        })
        
        return state
        
    def get_current_focus(self) -> Dict[str, Any]:
        """Get the current focus of attention"""
        return self.current_focus
        
    def get_focus_history(self, count: int = 5) -> List[Dict[str, Any]]:
        """Get the recent focus history"""
        return list(self.focus_history)[-count:]

#######################

#belief\belief_formation.py#
#######################

"""
Belief Formation Module

This module is responsible for forming new beliefs based on evidence.
It evaluates how pieces of evidence support potential beliefs and creates
new belief structures with appropriate confidence levels.

The module's developmental progression moves from simple, direct evidence-based
beliefs to more sophisticated beliefs that incorporate contextual understanding,
uncertainty, and integration with existing beliefs.
"""

from typing import Dict, List, Any, Optional
import logging
import time
import numpy as np
from datetime import datetime

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.belief.models import Belief, Evidence, BeliefSystem

logger = logging.getLogger(__name__)

class BeliefFormationParameters:
    """Parameters controlling belief formation"""
    def __init__(self):
        # Minimum evidence threshold for forming beliefs
        self.min_evidence_threshold = 0.5
        # Minimum number of evidence pieces required
        self.min_evidence_count = 1
        # Confidence threshold for belief formation
        self.confidence_threshold = 0.5
        # How much to factor in indirect evidence
        self.indirect_evidence_factor = 0.3
        # Whether to weigh sources differently
        self.source_weighting = False
        # Sensitivity to context when forming beliefs
        self.context_sensitivity = 0.5
        # Whether to handle uncertainty in belief formation
        self.uncertainty_handling = False

class BeliefFormation(BaseModule):
    """
    Responsible for forming new beliefs based on evidence
    
    This module evaluates evidence and forms beliefs based on developmental
    appropriate strategies.
    """
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the belief formation module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="belief_formation", event_bus=event_bus)
        self.parameters = BeliefFormationParameters()
        self.recent_formations = []  # Track recently formed beliefs
        
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process inputs to form beliefs
        
        Args:
            input_data: Data including evidence and belief system
        
        Returns:
            Results including any formed beliefs
        """
        if "evidence" not in input_data or "belief_system" not in input_data:
            return {
                "processed": False,
                "error": "Missing required input: evidence and belief_system",
                "module_id": self.module_id
            }
            
        evidence = input_data["evidence"]
        belief_system = input_data["belief_system"]
        context = input_data.get("context", {})
        
        # Form beliefs from the evidence
        formed_beliefs = []
        
        # Check evidence quality
        if isinstance(evidence, list):
            # Multiple evidence pieces
            evidence_list = evidence
        else:
            # Single evidence piece
            evidence_list = [evidence]
            
        # Only proceed if sufficient evidence quality and quantity
        if len(evidence_list) < self.parameters.min_evidence_count:
            return {
                "processed": True,
                "formed_beliefs": [],
                "message": "Insufficient evidence quantity",
                "module_id": self.module_id
            }
            
        # Group evidence by potential belief content
        belief_candidates = self._group_evidence_by_content(evidence_list, context)
        
        # Evaluate each candidate belief
        for content_key, evidence_groups in belief_candidates.items():
            # Check if this content already exists in a belief
            existing_belief = self._find_matching_belief(belief_system, evidence_groups["content"])
            
            if existing_belief:
                # Update existing belief with new evidence
                if self.event_bus:
                    # Request belief update via event bus
                    self._request_belief_update(existing_belief.belief_id, evidence_groups)
                continue
                
            # Calculate overall evidence quality
            evidence_strength = self._calculate_evidence_strength(
                evidence_groups["supporting"], 
                evidence_groups["contradicting"]
            )
            
            # Only form belief if evidence is strong enough
            if evidence_strength >= self.parameters.min_evidence_threshold:
                # Create the new belief
                belief = self._form_belief(evidence_groups, context)
                
                # Add to belief system
                belief_id = belief_system.add_belief(belief)
                
                # Record formation
                formed_beliefs.append({
                    "belief_id": belief_id,
                    "content": belief.content,
                    "confidence": belief.confidence,
                    "evidence_count": len(belief.evidence_for) + len(belief.evidence_against)
                })
                
                # Track recently formed beliefs
                self.recent_formations.append({
                    "belief_id": belief_id,
                    "timestamp": datetime.now(),
                    "context": context
                })
                
                # Limit history size
                if len(self.recent_formations) > 50:
                    self.recent_formations = self.recent_formations[-50:]
                    
                logger.info(f"Formed belief: {belief.content} with confidence {belief.confidence:.2f}")
                
        # Return results
        return {
            "processed": True,
            "formed_beliefs": formed_beliefs,
            "module_id": self.module_id,
            "message": f"Formed {len(formed_beliefs)} belief(s)"
        }
    
    def _request_belief_update(self, belief_id: str, evidence_groups: Dict[str, Any]) -> None:
        """Request update of an existing belief with new evidence"""
        # Send message to belief updating component
        if self.event_bus:
            message = Message(
                msg_type="belief_update_request",
                sender=self.module_id,
                content={
                    "belief_id": belief_id,
                    "supporting_evidence": evidence_groups["supporting"],
                    "contradicting_evidence": evidence_groups["contradicting"]
                }
            )
            self.event_bus.publish(message)
    
    def _find_matching_belief(self, belief_system: BeliefSystem, content: Dict[str, Any]) -> Optional[Belief]:
        """Find a belief with matching content"""
        # Check each belief for matching content
        for belief_id, belief in belief_system.beliefs.items():
            matches = True
            
            # Check each key in the content
            for key, value in content.items():
                if key not in belief.content or belief.content[key] != value:
                    matches = False
                    break
                    
            if matches and len(content) > 0:
                return belief
                
        return None
    
    def _group_evidence_by_content(
        self, 
        evidence_list: List[Evidence], 
        context: Dict[str, Any]
    ) -> Dict[str, Dict[str, Any]]:
        """Group evidence by potential belief content"""
        belief_candidates = {}
        
        for evidence in evidence_list:
            # Generate a content key based on the evidence
            content = self._extract_belief_content(evidence, context)
            content_key = self._generate_content_key(content)
            
            # Initialize if new content
            if content_key not in belief_candidates:
                belief_candidates[content_key] = {
                    "content": content,
                    "supporting": [],
                    "contradicting": []
                }
                
            # Determine if this evidence supports or contradicts
            is_supporting = self._is_supporting_evidence(evidence, content)
            
            # Add to appropriate list
            if is_supporting:
                belief_candidates[content_key]["supporting"].append(evidence)
            else:
                belief_candidates[content_key]["contradicting"].append(evidence)
                
        return belief_candidates
    
    def _extract_belief_content(self, evidence: Evidence, context: Dict[str, Any]) -> Dict[str, Any]:
        """Extract potential belief content from evidence"""
        # Basic content extraction - directly from evidence
        # Later development levels could use more sophisticated extraction
        
        content = {}
        
        # Early development: simple mapping from evidence to belief
        if self.development_level < 0.3:
            # Direct content copy with minimal processing
            return dict(evidence.content)
            
        # Middle development: consider context and source
        elif self.development_level < 0.7:
            # Enhance with context if enabled
            if self.parameters.context_sensitivity > 0:
                # Integrate some context elements
                for key, value in context.items():
                    if key in evidence.content and evidence.content[key] != value:
                        # Resolve conflicts based on reliability
                        if evidence.reliability > 0.7:
                            content[key] = evidence.content[key]
                        else:
                            content[key] = value
                    elif key not in evidence.content:
                        content[key] = value
                        
            # Add remaining evidence content
            for key, value in evidence.content.items():
                if key not in content:
                    content[key] = value
                    
            return content
                    
        # Advanced development: sophisticated content extraction
        else:
            # Use evidence content as base
            content = dict(evidence.content)
            
            # Enhance with context using weighted integration
            context_weight = self.parameters.context_sensitivity
            
            for key, value in context.items():
                if key in content:
                    # Weighted blend of evidence and context
                    if isinstance(value, (int, float)) and isinstance(content[key], (int, float)):
                        # Numerical blending
                        content[key] = (1 - context_weight) * content[key] + context_weight * value
                    elif evidence.reliability < 0.6:
                        # For non-numerical, use context if evidence reliability is low
                        content[key] = value
                else:
                    # Add context if not in evidence
                    content[key] = value
                    
            return content
    
    def _generate_content_key(self, content: Dict[str, Any]) -> str:
        """Generate a unique key for belief content"""
        # Simple string representation of sorted content items
        items = sorted((str(k), str(v)) for k, v in content.items())
        return "|".join(f"{k}:{v}" for k, v in items)
    
    def _is_supporting_evidence(self, evidence: Evidence, content: Dict[str, Any]) -> bool:
        """Determine if evidence supports or contradicts the belief content"""
        # Simple implementation: if most content matches, it's supporting
        matches = 0
        mismatches = 0
        
        for key, value in content.items():
            if key in evidence.content:
                if evidence.content[key] == value:
                    matches += 1
                else:
                    mismatches += 1
                    
        # Early development: binary support/contradict
        if self.development_level < 0.4:
            return matches > mismatches
            
        # More developed: weighted by matches and evidence quality
        support_score = matches / (matches + mismatches) if (matches + mismatches) > 0 else 0.5
        
        # Adjust by evidence reliability and relevance
        weighted_score = support_score * evidence.reliability * evidence.relevance
        
        return weighted_score >= 0.5
    
    def _calculate_evidence_strength(
        self, 
        supporting_evidence: List[Evidence], 
        contradicting_evidence: List[Evidence]
    ) -> float:
        """Calculate overall evidence strength for belief formation"""
        if not supporting_evidence:
            return 0.0
            
        # Calculate weighted strength of supporting evidence
        supporting_strength = sum(
            e.reliability * e.relevance for e in supporting_evidence
        )
        
        # Calculate weighted strength of contradicting evidence
        contradicting_strength = sum(
            e.reliability * e.relevance for e in contradicting_evidence
        )
        
        # Early development: simple ratio
        if self.development_level < 0.3:
            total_strength = supporting_strength + contradicting_strength
            if total_strength == 0:
                return 0.0
                
            return supporting_strength / total_strength
            
        # More developed: non-linear combination with thresholds
        else:
            # If contradicting evidence is substantial, require stronger support
            if contradicting_strength > 0:
                ratio = supporting_strength / contradicting_strength
                
                # Non-linear scaling that favors evidence consensus
                adjusted_strength = np.tanh(ratio) * 0.5 + 0.5  # Range: [0, 1]
                
                # Scale by total evidence quantity
                total_count = len(supporting_evidence) + len(contradicting_evidence)
                quantity_factor = min(1.0, total_count / 3.0)  # More evidence increases strength
                
                return adjusted_strength * quantity_factor
            else:
                # No contradicting evidence
                return min(1.0, supporting_strength / len(supporting_evidence))
    
    def _form_belief(self, evidence_groups: Dict[str, Any], context: Dict[str, Any]) -> Belief:
        """Form a new belief based on evidence"""
        # Create the belief
        belief = Belief(
            content=evidence_groups["content"],
            confidence=0.5,  # Initial confidence will be updated
            evidence_for=evidence_groups["supporting"],
            evidence_against=evidence_groups["contradicting"],
            source="evidence"
        )
        
        # Early development: stability is low (easily changed)
        if self.development_level < 0.3:
            belief.stability = 0.1
        # Medium development: moderate stability
        elif self.development_level < 0.7:
            belief.stability = 0.3
        # Advanced development: higher stability for well-supported beliefs
        else:
            evidence_count = len(evidence_groups["supporting"]) + len(evidence_groups["contradicting"])
            belief.stability = min(0.7, 0.2 + (evidence_count * 0.1))
            
        # Update confidence based on evidence
        # The model_validator will handle this automatically
        
        # Set metadata from context if available
        if context:
            belief.metadata["formation_context"] = {
                key: value for key, value in context.items()
                if key in ["task", "emotional_state", "attention_focus"]
            }
            
        # Add timestamp
        belief.metadata["formation_time"] = datetime.now().isoformat()
        belief.metadata["developmental_level"] = self.development_level
        
        return belief


#######################

#belief\belief_updating.py#
#######################

"""
Belief Updating Module

This module is responsible for modifying existing beliefs based on new evidence.
It handles the evolution of beliefs over time, including confidence adjustments,
temporal decay, and integration of contradictory information.

The module's developmental progression moves from simple overwriting of beliefs
to more nuanced integration of new information with preservation of existing
belief structures when appropriate.
"""

from typing import Dict, List, Any, Optional
import logging
import time
import numpy as np
from datetime import datetime, timedelta

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.belief.models import Belief, Evidence, BeliefSystem

logger = logging.getLogger(__name__)

class BeliefUpdatingParameters:
    """Parameters controlling belief updating"""
    def __init__(self):
        # Threshold for evidence to trigger update
        self.update_threshold = 0.5
        # Rate of updating (how much new evidence affects beliefs)
        self.update_rate = 0.7
        # Weight given to conflicting evidence
        self.conflicting_evidence_weight = 0.5
        # Depth of source weighting (how many factors considered)
        self.source_weighting_depth = 1
        # How stable beliefs are (resistance to change)
        self.stability_factor = 0.3
        # Influence of update history on current updates
        self.history_influence = 0.2
        # Whether to perform partial updating vs. wholesale replacement
        self.partial_updating = False
        # Influence of related beliefs on this belief's updates
        self.related_belief_influence = 0.1

class BeliefUpdating(BaseModule):
    """
    Responsible for updating existing beliefs based on new evidence
    
    This module modifies existing beliefs in response to new information,
    with strategies that evolve as cognitive development progresses.
    """
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the belief updating module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="belief_updating", event_bus=event_bus)
        self.parameters = BeliefUpdatingParameters()
        self.update_history = []  # Track belief updates
        
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process inputs to update beliefs
        
        Args:
            input_data: Data including evidence and belief system
        
        Returns:
            Results including updated beliefs
        """
        if "evidence" not in input_data or "belief_system" not in input_data:
            return {
                "processed": False,
                "error": "Missing required input: evidence and belief_system",
                "module_id": self.module_id
            }
            
        evidence = input_data["evidence"]
        belief_system = input_data["belief_system"]
        belief_id = input_data.get("belief_id")  # Optional specific belief to update
        
        # Handle different evidence formats
        if isinstance(evidence, list):
            evidence_list = evidence
        else:
            evidence_list = [evidence]
            
        updated_beliefs = []
        
        # If specific belief ID provided, update only that belief
        if belief_id:
            belief = belief_system.get_belief(belief_id)
            if belief:
                updated = self._update_belief(belief, evidence_list, belief_system)
                belief_system.update_belief(belief_id, updated)
                
                updated_beliefs.append({
                    "belief_id": belief_id,
                    "content": updated.content,
                    "confidence": updated.confidence,
                    "previous_confidence": belief.confidence
                })
                
        # Otherwise, find and update all relevant beliefs
        else:
            # Find all beliefs potentially affected by this evidence
            beliefs_to_update = self._find_relevant_beliefs(evidence_list, belief_system)
            
            for belief_id in beliefs_to_update:
                belief = belief_system.get_belief(belief_id)
                if not belief:
                    continue
                    
                updated = self._update_belief(belief, evidence_list, belief_system)
                belief_system.update_belief(belief_id, updated)
                
                updated_beliefs.append({
                    "belief_id": belief_id,
                    "content": updated.content,
                    "confidence": updated.confidence,
                    "previous_confidence": belief.confidence
                })
                
        # Handle temporal decay for all beliefs if sufficient time has passed
        self._apply_temporal_decay(belief_system)
        
        # Return results
        return {
            "processed": True,
            "updated_beliefs": updated_beliefs,
            "module_id": self.module_id,
            "message": f"Updated {len(updated_beliefs)} belief(s)"
        }
        
    def _find_relevant_beliefs(self, evidence_list: List[Evidence], belief_system: BeliefSystem) -> List[str]:
        """Find beliefs that are relevant to the provided evidence"""
        relevant_beliefs = set()
        
        for evidence in evidence_list:
            # Find directly relevant beliefs based on content
            for belief_id, belief in belief_system.beliefs.items():
                # Check content overlap
                content_match = False
                for key in evidence.content:
                    if key in belief.content:
                        content_match = True
                        break
                        
                if content_match:
                    relevant_beliefs.add(belief_id)
        
        return list(relevant_beliefs)
    
    def _update_belief(self, belief: Belief, evidence_list: List[Evidence], belief_system: BeliefSystem) -> Belief:
        """Update a belief based on new evidence"""
        # Create a working copy of the belief
        updated_belief = belief.model_copy(deep=True)
        updated_belief.last_updated = datetime.now()
        
        # Sort evidence into supporting and contradicting
        supporting = []
        contradicting = []
        
        for evidence in evidence_list:
            is_supporting = self._is_evidence_supporting(evidence, belief)
            if is_supporting:
                # Add to supporting evidence if not already present
                if not any(e.evidence_id == evidence.evidence_id for e in updated_belief.evidence_for):
                    updated_belief.evidence_for.append(evidence)
                supporting.append(evidence)
            else:
                # Add to contradicting evidence if not already present
                if not any(e.evidence_id == evidence.evidence_id for e in updated_belief.evidence_against):
                    updated_belief.evidence_against.append(evidence)
                contradicting.append(evidence)
        
        # Early development: Simple overwriting or reinforcement
        if self.development_level < 0.3:
            # If strong contradicting evidence, simply adopt its content
            if contradicting and sum(e.reliability for e in contradicting) > belief.confidence * 1.5:
                # Take content from the most reliable contradicting evidence
                strongest = max(contradicting, key=lambda e: e.reliability)
                # Simple overwrite while preserving some structure
                for key, value in strongest.content.items():
                    updated_belief.content[key] = value
                    
        # Middle development: Partial integration
        elif self.development_level < 0.7 and self.parameters.partial_updating:
            # Selective content updating based on evidence reliability
            for evidence in supporting + contradicting:
                # Only update if evidence is reliable enough
                if evidence.reliability > self.parameters.update_threshold:
                    # Update specific content elements
                    for key, value in evidence.content.items():
                        # Calculate change factor based on evidence quality
                        change_factor = evidence.reliability * evidence.relevance
                        # Adjust by belief stability
                        change_factor *= (1.0 - updated_belief.stability)
                        
                        # If key already exists, weighted update
                        if key in updated_belief.content:
                            if isinstance(value, (int, float)) and isinstance(updated_belief.content[key], (int, float)):
                                # Numerical blending
                                updated_belief.content[key] = (
                                    (1.0 - change_factor) * updated_belief.content[key] + 
                                    change_factor * value
                                )
                            else:
                                # For non-numeric, update if strong enough evidence
                                if change_factor > 0.6:
                                    updated_belief.content[key] = value
                        else:
                            # Add new content
                            updated_belief.content[key] = value
        
        # Advanced development: Sophisticated integration
        elif self.development_level >= 0.7:
            # Consider related beliefs
            if self.parameters.related_belief_influence > 0:
                related_beliefs = {}
                
                # Find related beliefs with their relationship strength
                if updated_belief.related_beliefs:
                    for related_id in updated_belief.related_beliefs:
                        related = belief_system.get_belief(related_id)
                        if related:
                            relationship_strength = self._calculate_relationship_strength(
                                updated_belief, related, belief_system
                            )
                            related_beliefs[related_id] = (related, relationship_strength)
                
                # Integrate influence from related beliefs
                for related_id, (related, strength) in related_beliefs.items():
                    influence = strength * self.parameters.related_belief_influence
                    # Apply influence to update
                    for key, value in related.content.items():
                        if key in updated_belief.content:
                            if isinstance(value, (int, float)) and isinstance(updated_belief.content[key], (int, float)):
                                # Subtle influence for numeric values
                                updated_belief.content[key] = (
                                    (1.0 - influence) * updated_belief.content[key] + 
                                    influence * value
                                )
            
            # Complex content integration from new evidence
            for evidence in supporting + contradicting:
                # Calculate evidence influence
                influence = evidence.reliability * evidence.relevance
                if evidence in contradicting:
                    influence *= self.parameters.conflicting_evidence_weight
                
                # Adjust by belief stability and history
                influence *= (1.0 - updated_belief.stability)
                
                # Apply Bayesian-inspired content updating
                for key, value in evidence.content.items():
                    # Different handling for numeric vs categorical
                    if key in updated_belief.content:
                        if isinstance(value, (int, float)) and isinstance(updated_belief.content[key], (int, float)):
                            # Weighted average for numeric values
                            updated_belief.content[key] = (
                                (1.0 - influence) * updated_belief.content[key] + 
                                influence * value
                            )
                        else:
                            # For non-numeric, probabilistic update
                            if influence > 1.0 - updated_belief.confidence:
                                updated_belief.content[key] = value
                    elif influence > 0.4:  # Threshold for adding new content
                        # Add new content
                        updated_belief.content[key] = value
                    
        # Record the update
        self.update_history.append({
            "belief_id": updated_belief.belief_id,
            "timestamp": datetime.now(),
            "previous_confidence": belief.confidence,
            "new_confidence": updated_belief.confidence,
            "supporting_evidence": len(supporting),
            "contradicting_evidence": len(contradicting)
        })
        
        # Limit history size
        if len(self.update_history) > 100:
            self.update_history = self.update_history[-100:]
        
        # Confidence will be updated by model validator
        return updated_belief
    
    def _is_evidence_supporting(self, evidence: Evidence, belief: Belief) -> bool:
        """Determine if evidence supports or contradicts the belief"""
        matches = 0
        mismatches = 0
        
        for key, value in belief.content.items():
            if key in evidence.content:
                if evidence.content[key] == value:
                    matches += 1
                else:
                    mismatches += 1
        
        # Early development: Simple majority voting
        if self.development_level < 0.3:
            return matches >= mismatches
            
        # More sophisticated reasoning with development
        if matches + mismatches == 0:
            # No direct comparison possible
            return evidence.reliability > 0.5  # Default based on reliability
            
        support_ratio = matches / (matches + mismatches)
        
        # Adjust by evidence quality
        weighted_support = support_ratio * evidence.reliability * evidence.relevance
        
        return weighted_support >= 0.5
    
    def _calculate_relationship_strength(
        self, 
        belief1: Belief, 
        belief2: Belief,
        belief_system: BeliefSystem
    ) -> float:
        """Calculate relationship strength between two beliefs"""
        # Content similarity
        similarity = 0.0
        shared_keys = set(belief1.content.keys()) & set(belief2.content.keys())
        
        if shared_keys:
            matches = sum(1 for k in shared_keys if belief1.content[k] == belief2.content[k])
            similarity = matches / len(shared_keys)
        
        # Evidence overlap
        evidence1 = {e.evidence_id for e in belief1.evidence_for + belief1.evidence_against}
        evidence2 = {e.evidence_id for e in belief2.evidence_for + belief2.evidence_against}
        
        overlap = 0.0
        if evidence1 and evidence2:
            intersection = evidence1 & evidence2
            overlap = len(intersection) / min(len(evidence1), len(evidence2))
        
        # Combine factors
        return 0.7 * similarity + 0.3 * overlap
    
    def _apply_temporal_decay(self, belief_system: BeliefSystem) -> None:
        """Apply time-based decay to belief confidences"""
        # Only apply decay periodically
        current_time = datetime.now()
        
        # Skip if not enough time has passed since last decay
        if hasattr(self, 'last_decay_time') and (current_time - self.last_decay_time).total_seconds() < 3600:
            return
            
        self.last_decay_time = current_time
        
        # Apply decay to each belief
        for belief_id, belief in belief_system.beliefs.items():
            # Calculate time since last update
            if not belief.last_updated:
                continue
                
            time_diff = (current_time - belief.last_updated).total_seconds() / 86400.0  # Days
            
            # Different decay rates based on development level
            if self.development_level < 0.3:
                # Early development: faster forgetting
                decay_rate = 0.1
            elif self.development_level < 0.7:
                # Middle development: more stable
                decay_rate = 0.05
            else:
                # Advanced: very stable long-term beliefs
                decay_rate = 0.02
                
            # Adjust by belief stability
            decay_rate *= (1.0 - belief.stability)
            
            # Calculate confidence decay
            decay_amount = min(0.5, decay_rate * time_diff)  # Cap at 50% decay
            
            # Only apply significant decay
            if decay_amount > 0.01:
                # Create updated belief
                updated = belief.model_copy(deep=True)
                
                # Apply decay to confidence
                updated.confidence = max(0.1, updated.confidence - decay_amount)
                
                # Update the belief
                belief_system.update_belief(belief_id, updated)
                
                logger.debug(f"Applied temporal decay to belief {belief_id}: {decay_amount:.2f}")
                
    def _adjust_belief_properties(self, belief: Belief) -> None:
        """Adjust belief properties based on development level"""
        # Stability increases with development level
        base_stability = 0.1 + (self.development_level * 0.4)
        
        # Evidence quantity increases stability
        evidence_count = len(belief.evidence_for) + len(belief.evidence_against)
        count_factor = min(0.4, evidence_count * 0.05)
        
        # Time stability - older beliefs are more stable
        age_hours = (datetime.now() - belief.creation_time).total_seconds() / 3600
        age_factor = min(0.3, (age_hours / 240) * 0.3)  # Max effect after 10 days
        
        # Update stability
        belief.stability = min(0.9, base_stability + count_factor + age_factor)


#######################

#belief\contradiction_resolution.py#
#######################

"""
Contradiction Resolution Module

This module detects and resolves contradictions between beliefs in the belief system.
It implements strategies for handling conflicts that vary based on developmental level,
from simple binary contradiction handling to nuanced contextual resolution.

The module ensures the overall consistency of the belief system while allowing for
appropriate levels of ambiguity and nuance at higher developmental stages.
"""

from typing import Dict, List, Any, Optional, Tuple, Set
import logging
import random
import numpy as np
from datetime import datetime

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.belief.models import Belief, Evidence, BeliefSystem

logger = logging.getLogger(__name__)

class ContradictionParameters:
    """Parameters controlling contradiction resolution"""
    def __init__(self):
        # Threshold for detecting contradictions
        self.contradiction_threshold = 0.6
        # Influence of context on contradiction evaluation
        self.context_sensitivity = 0.5
        # Tolerance for ambiguity in beliefs
        self.ambiguity_tolerance = 0.3
        # Strategy for resolving contradictions
        self.resolution_strategy = "binary"  # "binary", "weighted", or "contextual"
        # Whether to merge contradictory beliefs
        self.merge_capability = False
        # Number of different models/perspectives that can be maintained
        self.multiple_model_capacity = 1
        # Whether to detect higher-order contradictions
        self.higher_order_detection = False

class ContradictionResolution(BaseModule):
    """
    Responsible for detecting and resolving contradictions between beliefs
    
    This module ensures consistency in the belief system by identifying and
    resolving conflicts between beliefs using developmentally appropriate
    strategies.
    """
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the contradiction resolution module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="contradiction_resolution", event_bus=event_bus)
        self.parameters = ContradictionParameters()
        self.resolution_history = []  # Track contradiction resolutions
        
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process inputs to detect and resolve contradictions
        
        Args:
            input_data: Data including belief system to check
        
        Returns:
            Results including any resolved contradictions
        """
        if "belief_system" not in input_data:
            return {
                "processed": False,
                "error": "Missing required input: belief_system",
                "module_id": self.module_id
            }
            
        belief_system = input_data["belief_system"]
        context = input_data.get("context", {})
        
        # Detect contradictions
        contradictions = self._detect_contradictions(belief_system, context)
        
        # If no contradictions found, return early
        if not contradictions:
            return {
                "processed": True,
                "contradictions_found": 0,
                "contradictions_resolved": 0,
                "module_id": self.module_id
            }
            
        # Resolve each contradiction
        resolutions = []
        for contradiction in contradictions:
            belief1_id, belief2_id, strength = contradiction
            
            # Get the beliefs
            belief1 = belief_system.get_belief(belief1_id)
            belief2 = belief_system.get_belief(belief2_id)
            
            if not belief1 or not belief2:
                continue
                
            # Apply resolution strategy
            resolution_result = self._resolve_contradiction(
                belief1, belief2, strength, belief_system, context
            )
            
            if resolution_result["resolved"]:
                resolutions.append({
                    "belief1_id": belief1_id,
                    "belief2_id": belief2_id,
                    "contradiction_strength": strength,
                    "resolution_type": resolution_result["resolution_type"],
                    "resolution_details": resolution_result["details"]
                })
                
                # Track resolution history
                self.resolution_history.append({
                    "timestamp": datetime.now(),
                    "contradiction": (belief1_id, belief2_id, strength),
                    "resolution": resolution_result
                })
                
                # Limit history size
                if len(self.resolution_history) > 50:
                    self.resolution_history = self.resolution_history[-50:]
                    
        # Return results
        return {
            "processed": True,
            "contradictions_found": len(contradictions),
            "contradictions_resolved": len(resolutions),
            "resolutions": resolutions,
            "module_id": self.module_id
        }
        
    def _detect_contradictions(
        self, 
        belief_system: BeliefSystem, 
        context: Dict[str, Any]
    ) -> List[Tuple[str, str, float]]:
        """
        Detect contradictions between beliefs
        
        Args:
            belief_system: The belief system to check
            context: Current context information
            
        Returns:
            List of contradictions as (belief1_id, belief2_id, strength) tuples
        """
        # Use the system's built-in contradiction detection 
        contradictions = belief_system.find_contradictions()
        
        # Apply additional developmental filters based on current level
        if self.development_level < 0.3:
            # Early development: only detect strong contradictions
            filtered = [
                (id1, id2, strength) for id1, id2, strength in contradictions
                if strength > 0.7
            ]
            return filtered
            
        elif self.development_level < 0.6:
            # Middle development: detect moderate contradictions
            filtered = [
                (id1, id2, strength) for id1, id2, strength in contradictions
                if strength > self.parameters.contradiction_threshold
            ]
            return filtered
            
        else:
            # Advanced development: context-sensitive contradiction detection
            filtered = []
            
            for id1, id2, strength in contradictions:
                # Skip if below threshold
                if strength < self.parameters.contradiction_threshold:
                    continue
                    
                # Get the beliefs
                belief1 = belief_system.get_belief(id1)
                belief2 = belief_system.get_belief(id2)
                
                if not belief1 or not belief2:
                    continue
                    
                # Check if context reduces contradiction
                if self.parameters.context_sensitivity > 0 and context:
                    # Context might show these aren't actually contradictory
                    context_adjustment = self._evaluate_contextual_contradiction(
                        belief1, belief2, context
                    )
                    
                    # Adjust contradiction strength by context
                    adjusted_strength = strength * (1.0 - context_adjustment * self.parameters.context_sensitivity)
                    
                    # Only include if still significant
                    if adjusted_strength > self.parameters.contradiction_threshold:
                        filtered.append((id1, id2, adjusted_strength))
                else:
                    filtered.append((id1, id2, strength))
                    
            return filtered
    
    def _evaluate_contextual_contradiction(
        self, 
        belief1: Belief, 
        belief2: Belief, 
        context: Dict[str, Any]
    ) -> float:
        """
        Evaluate how much context reduces contradiction between beliefs
        
        Returns:
            Reduction factor (0.0-1.0) where higher values indicate more reduction
        """
        # Early development: no context sensitivity
        if self.development_level < 0.4:
            return 0.0
            
        # Check if context has keys that would differentiate these beliefs
        context_differentiation = 0.0
        relevant_context_keys = set()
        
        # Find contradictory content between beliefs
        contradictory_keys = set()
        for key in set(belief1.content.keys()) & set(belief2.content.keys()):
            if belief1.content[key] != belief2.content[key]:
                contradictory_keys.add(key)
                
        # Look for contextual factors that might explain the difference
        for key in context:
            # Check if context is related to contradictory content
            for c_key in contradictory_keys:
                # Simple partial string matching - could be more sophisticated
                if key in c_key or c_key in key:
                    relevant_context_keys.add(key)
                    break
                    
        # More relevant context factors reduce contradiction more
        if relevant_context_keys:
            context_differentiation = min(0.8, len(relevant_context_keys) * 0.2)
            
        return context_differentiation
    
    def _resolve_contradiction(
        self, 
        belief1: Belief, 
        belief2: Belief, 
        contradiction_strength: float,
        belief_system: BeliefSystem,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Resolve a contradiction between beliefs
        
        Args:
            belief1: First contradictory belief
            belief2: Second contradictory belief
            contradiction_strength: Strength of the contradiction
            belief_system: The belief system
            context: Current context
            
        Returns:
            Resolution result information
        """
        # Early development (binary strategy)
        if self.parameters.resolution_strategy == "binary" or self.development_level < 0.3:
            return self._binary_resolution(belief1, belief2, belief_system)
            
        # Middle development (weighted strategy)
        elif self.parameters.resolution_strategy == "weighted" or self.development_level < 0.6:
            return self._weighted_resolution(belief1, belief2, belief_system)
            
        # Advanced development (contextual strategy)
        else:
            return self._contextual_resolution(belief1, belief2, contradiction_strength, belief_system, context)
    
    def _binary_resolution(
        self, 
        belief1: Belief, 
        belief2: Belief, 
        belief_system: BeliefSystem
    ) -> Dict[str, Any]:
        """
        Simple binary resolution - keep stronger belief, discard weaker one
        
        Args:
            belief1: First contradictory belief
            belief2: Second contradictory belief
            belief_system: The belief system
            
        Returns:
            Resolution result information
        """
        # Determine which belief is stronger
        if belief1.confidence > belief2.confidence:
            stronger = belief1
            weaker = belief2
            stronger_id = belief1.belief_id
            weaker_id = belief2.belief_id
        else:
            stronger = belief2
            weaker = belief1
            stronger_id = belief2.belief_id
            weaker_id = belief1.belief_id
            
        # Only remove weaker belief if the difference is significant
        confidence_diff = abs(belief1.confidence - belief2.confidence)
        
        if confidence_diff < 0.1:
            # Too close to call - for early development, random choice
            if random.random() < 0.5:
                stronger, weaker = weaker, stronger
                stronger_id, weaker_id = weaker_id, stronger_id
        
        # Remove the weaker belief
        belief_system.remove_belief(weaker_id)
        
        # Return resolution information
        return {
            "resolved": True,
            "resolution_type": "binary_removal",
            "details": {
                "kept_belief": stronger_id,
                "removed_belief": weaker_id,
                "confidence_difference": confidence_diff
            }
        }
    
    def _weighted_resolution(
        self, 
        belief1: Belief, 
        belief2: Belief, 
        belief_system: BeliefSystem
    ) -> Dict[str, Any]:
        """
        Weighted resolution - potentially merge beliefs or adjust confidences
        
        Args:
            belief1: First contradictory belief
            belief2: Second contradictory belief
            belief_system: The belief system
            
        Returns:
            Resolution result information
        """
        # Determine which belief is stronger
        if belief1.confidence > belief2.confidence:
            stronger = belief1
            weaker = belief2
            stronger_id = belief1.belief_id
            weaker_id = belief2.belief_id
        else:
            stronger = belief2
            weaker = belief1
            stronger_id = belief2.belief_id
            weaker_id = belief1.belief_id
            
        confidence_diff = abs(belief1.confidence - belief2.confidence)
        
        # If merge capability is available and confidences are close
        if self.parameters.merge_capability and confidence_diff < 0.2:
            # Create a merged belief
            merged_belief = stronger.model_copy(deep=True)
            
            # Resolve contradictory content by taking from stronger belief
            # and adjusting confidence
            confidence_adjustment = 0.1  # Confidence reduction due to contradiction
            
            # Take non-contradictory content from weaker belief
            for key, value in weaker.content.items():
                if key not in merged_belief.content:
                    merged_belief.content[key] = value
            
            # Reduce confidence due to contradiction
            merged_belief.confidence = max(0.1, stronger.confidence - confidence_adjustment)
            
            # Add evidence from both beliefs
            for evidence in weaker.evidence_for:
                if not any(e.evidence_id == evidence.evidence_id for e in merged_belief.evidence_for):
                    merged_belief.evidence_for.append(evidence)
                    
            for evidence in weaker.evidence_against:
                if not any(e.evidence_id == evidence.evidence_id for e in merged_belief.evidence_against):
                    merged_belief.evidence_against.append(evidence)
            
            # Update merged belief in system
            belief_system.update_belief(stronger_id, merged_belief)
            
            # Remove weaker belief
            belief_system.remove_belief(weaker_id)
            
            return {
                "resolved": True,
                "resolution_type": "merge",
                "details": {
                    "merged_into": stronger_id,
                    "removed_belief": weaker_id,
                    "confidence_adjustment": confidence_adjustment
                }
            }
            
        else:
            # Adjust confidence of both beliefs
            confidence_penalty1 = min(0.2, belief2.confidence * 0.3)
            confidence_penalty2 = min(0.2, belief1.confidence * 0.3)
            
            updated1 = belief1.model_copy(deep=True)
            updated1.confidence = max(0.1, belief1.confidence - confidence_penalty1)
            belief_system.update_belief(belief1.belief_id, updated1)
            
            updated2 = belief2.model_copy(deep=True)
            updated2.confidence = max(0.1, belief2.confidence - confidence_penalty2)
            belief_system.update_belief(belief2.belief_id, updated2)
            
            return {
                "resolved": True,
                "resolution_type": "mutual_confidence_adjustment",
                "details": {
                    "belief1_adjustment": confidence_penalty1,
                    "belief2_adjustment": confidence_penalty2
                }
            }
    
    def _contextual_resolution(
        self, 
        belief1: Belief, 
        belief2: Belief, 
        contradiction_strength: float,
        belief_system: BeliefSystem,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Contextual resolution - sophisticated handling with qualifiers and context
        
        Args:
            belief1: First contradictory belief
            belief2: Second contradictory belief
            contradiction_strength: Strength of the contradiction
            belief_system: The belief system
            context: Current context
            
        Returns:
            Resolution result information
        """
        # If contradiction is weak enough, don't resolve
        if contradiction_strength < 0.4 + self.parameters.ambiguity_tolerance:
            return {
                "resolved": False,
                "resolution_type": "tolerated_ambiguity",
                "details": {
                    "contradiction_strength": contradiction_strength,
                    "ambiguity_tolerance": self.parameters.ambiguity_tolerance
                }
            }
            
        # Check if these could be contextually distinct beliefs
        context_differentiation = self._evaluate_contextual_contradiction(belief1, belief2, context)
        
        # If context strongly differentiates them, add contextual qualifiers
        if context_differentiation > 0.5:
            # Add contextual qualifiers to both beliefs
            updated1 = belief1.model_copy(deep=True)
            updated2 = belief2.model_copy(deep=True)
            
            # Add context metadata
            updated1.metadata["contextual_scope"] = {
                key: value for key, value in context.items()
                if key in belief1.content or random.random() < 0.3  # Some randomness in selection
            }
            
            updated2.metadata["contextual_scope"] = {
                key: value for key, value in context.items()
                if key in belief2.content or random.random() < 0.3  # Some randomness in selection
            }
            
            # Update both beliefs
            belief_system.update_belief(belief1.belief_id, updated1)
            belief_system.update_belief(belief2.belief_id, updated2)
            
            return {
                "resolved": True,
                "resolution_type": "contextual_qualification",
                "details": {
                    "context_differentiation": context_differentiation,
                    "belief1_qualifiers": list(updated1.metadata["contextual_scope"].keys()),
                    "belief2_qualifiers": list(updated2.metadata["contextual_scope"].keys())
                }
            }
            
        # If multiple models are supported, maintain both beliefs
        if self.parameters.multiple_model_capacity > 1:
            # Tag both beliefs as belonging to different perspectives
            updated1 = belief1.model_copy(deep=True)
            updated2 = belief2.model_copy(deep=True)
            
            # Create or update perspective metadata
            if "perspective" not in updated1.metadata:
                updated1.metadata["perspective"] = f"perspective_{random.randint(1, self.parameters.multiple_model_capacity)}"
                
            if "perspective" not in updated2.metadata:
                # Ensure different perspective
                perspective2 = f"perspective_{random.randint(1, self.parameters.multiple_model_capacity)}"
                while perspective2 == updated1.metadata.get("perspective", ""):
                    perspective2 = f"perspective_{random.randint(1, self.parameters.multiple_model_capacity)}"
                updated2.metadata["perspective"] = perspective2
            
            # Update both beliefs
            belief_system.update_belief(belief1.belief_id, updated1)
            belief_system.update_belief(belief2.belief_id, updated2)
            
            return {
                "resolved": True,
                "resolution_type": "multiple_perspectives",
                "details": {
                    "belief1_perspective": updated1.metadata["perspective"],
                    "belief2_perspective": updated2.metadata["perspective"]
                }
            }
            
        # Fall back to weighted resolution for other cases
        return self._weighted_resolution(belief1, belief2, belief_system)


#######################

#belief\evidence_evaluation.py#
#######################

"""
Evidence Evaluation Module

This module assesses the quality, reliability, and relevance of evidence used
in belief formation and updating. It evaluates evidence based on source credibility,
consistency with other evidence, and contextual factors.

The module's developmental progression moves from simple acceptance of evidence
to sophisticated critical analysis with consideration of source reliability,
methodology, and potential biases.
"""

from typing import Dict, List, Any, Optional
from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.belief.models import Evidence
import logging
import numpy as np
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

class EvidenceEvaluationParameters:
    """Parameters controlling evidence evaluation"""
    def __init__(self):
        # Default reliability for unknown sources
        self.default_reliability = 0.5
        # Weight given to source credibility
        self.source_credibility_weight = 0.7
        # Weight given to evidence consistency with prior evidence
        self.consistency_weight = 0.5
        # Depth of analysis (how many factors considered)
        self.analysis_depth = 2
        # Sensitivity to conflicting details
        self.conflict_sensitivity = 0.6
        # Whether to differentiate between source types
        self.source_differentiation = False
        # Number of source types recognized
        self.source_types_recognized = 2
        # Whether to consider methodology in evaluation
        self.methodology_awareness = False
        # Weight given to methodological rigor
        self.methodology_weight = 0.0
        # Whether to apply statistical reasoning
        self.statistical_reasoning = False

class EvidenceEvaluation(BaseModule):
    """
    Responsible for evaluating the quality and reliability of evidence
    
    This module assesses evidence used in belief formation and updating,
    determining how strongly it should influence the belief system.
    """
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the evidence evaluation module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="evidence_evaluation", event_bus=event_bus)
        self.parameters = EvidenceEvaluationParameters()
        self.source_reliability_cache = {}  # Cache source reliability assessments
        self.evaluation_history = []  # Track evidence evaluations
        
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process inputs to evaluate evidence
        
        Args:
            input_data: Data including evidence to evaluate
        
        Returns:
            Results including evaluated evidence
        """
        if "evidence" not in input_data:
            return {
                "processed": False,
                "error": "Missing required input: evidence",
                "module_id": self.module_id
            }
            
        # Get evidence and context
        evidence = input_data["evidence"]
        context = input_data.get("context", {})
        prior_evidence = input_data.get("prior_evidence", [])
        
        # Handle different evidence formats
        if isinstance(evidence, list):
            evidence_list = evidence
        else:
            evidence_list = [evidence]
            
        # Evaluate each piece of evidence
        evaluated_evidence = []
        
        for evidence_item in evidence_list:
            # Create a working copy
            evaluated = self._evaluate_evidence(evidence_item, context, prior_evidence)
            evaluated_evidence.append(evaluated)
            
            # Record evaluation
            self.evaluation_history.append({
                "evidence_id": evaluated.evidence_id,
                "source": evaluated.source,
                "reliability": evaluated.reliability,
                "relevance": evaluated.relevance,
                "timestamp": datetime.now()
            })
            
            # Limit history size
            if len(self.evaluation_history) > 100:
                self.evaluation_history = self.evaluation_history[-100:]
                
        # Return results
        return {
            "processed": True,
            "evaluated_evidence": evaluated_evidence if len(evaluated_evidence) > 1 else evaluated_evidence[0],
            "module_id": self.module_id
        }
        
    def _evaluate_evidence(
        self, 
        evidence: Evidence, 
        context: Dict[str, Any],
        prior_evidence: List[Evidence]
    ) -> Evidence:
        """
        Evaluate a single piece of evidence
        
        Args:
            evidence: The evidence to evaluate
            context: Current context information
            prior_evidence: Previously evaluated evidence
            
        Returns:
            Evaluated evidence with updated reliability and relevance
        """
        # Create a working copy
        evaluated = evidence.model_copy(deep=True)
        
        # Evaluate source reliability
        source_reliability = self._evaluate_source_reliability(evidence.source, evidence.content)
        
        # Evaluate consistency with prior evidence
        consistency_score = self._evaluate_consistency(evidence, prior_evidence)
        
        # Evaluate relevance to context
        relevance_score = self._evaluate_relevance(evidence, context)
        
        # Early development: Simple source-based reliability
        if self.development_level < 0.3:
            # Just use source reliability with minimal adjustment
            evaluated.reliability = source_reliability
            evaluated.relevance = max(0.5, relevance_score)
            
        # Middle development: Consider consistency and relevance
        elif self.development_level < 0.7:
            # Combine source reliability and consistency
            source_weight = self.parameters.source_credibility_weight
            consistency_weight = self.parameters.consistency_weight
            
            # Normalize weights
            total_weight = source_weight + consistency_weight
            source_weight /= total_weight
            consistency_weight /= total_weight
            
            # Calculate combined reliability
            evaluated.reliability = (
                source_weight * source_reliability +
                consistency_weight * consistency_score
            )
            
            # Set relevance
            evaluated.relevance = relevance_score
            
        # Advanced development: Sophisticated evaluation
        else:
            # Consider methodology if enabled
            methodology_score = 0.0
            if self.parameters.methodology_awareness:
                methodology_score = self._evaluate_methodology(evidence)
                
            # Consider statistical properties if enabled
            statistical_score = 0.0
            if self.parameters.statistical_reasoning:
                statistical_score = self._evaluate_statistical_properties(evidence)
                
            # Calculate weights
            source_weight = self.parameters.source_credibility_weight
            consistency_weight = self.parameters.consistency_weight
            methodology_weight = self.parameters.methodology_weight
            statistical_weight = 0.3 if self.parameters.statistical_reasoning else 0.0
            
            # Normalize weights
            total_weight = source_weight + consistency_weight + methodology_weight + statistical_weight
            if total_weight > 0:
                source_weight /= total_weight
                consistency_weight /= total_weight
                methodology_weight /= total_weight
                statistical_weight /= total_weight
            
            # Calculate combined reliability
            evaluated.reliability = (
                source_weight * source_reliability +
                consistency_weight * consistency_score +
                methodology_weight * methodology_score +
                statistical_weight * statistical_score
            )
            
            # Set relevance with context-sensitivity
            evaluated.relevance = relevance_score
            
        # Ensure values are in valid range
        evaluated.reliability = max(0.0, min(1.0, evaluated.reliability))
        evaluated.relevance = max(0.0, min(1.0, evaluated.relevance))
        
        return evaluated
    
    def _evaluate_source_reliability(self, source: str, content: Dict[str, Any]) -> float:
        """
        Evaluate the reliability of an evidence source
        
        Args:
            source: Source identifier
            content: Evidence content
            
        Returns:
            Source reliability score (0.0-1.0)
        """
        # Check if we have cached reliability for this source
        if source in self.source_reliability_cache:
            return self.source_reliability_cache[source]
            
        reliability = self.parameters.default_reliability
        
        # Early development: Simple fixed reliabilities
        if self.development_level < 0.3:
            if source == "perception":
                reliability = 0.8  # Perception is considered highly reliable
            elif source == "memory":
                reliability = 0.6  # Memory is moderately reliable
            elif source == "language":
                reliability = 0.5  # Language input is variable
            elif source == "reasoning":
                reliability = 0.7  # Internal reasoning is fairly reliable
                
        # Middle development: More source types with quality assessment
        elif self.development_level < 0.7:
            if source == "perception":
                # Adjust based on clarity if available
                clarity = content.get("clarity", 0.8)
                reliability = 0.6 + (clarity * 0.4)
            elif source == "memory":
                # Adjust based on recency if available
                age = content.get("age", 0.5)
                age_factor = max(0.0, 1.0 - age)
                reliability = 0.4 + (age_factor * 0.5)
            elif source == "language":
                # Adjust based on source trustworthiness
                trustworthiness = content.get("source_trustworthiness", 0.5)
                reliability = trustworthiness
            elif source == "reasoning":
                # Adjust based on confidence if available
                confidence = content.get("confidence", 0.7)
                reliability = 0.5 + (confidence * 0.4)
            else:
                # Unknown source types get lower reliability
                reliability = 0.4
                
        # Advanced development: Sophisticated source evaluation
        else:
            # Base reliability factors
            if source == "perception":
                base_reliability = 0.8
                
                # Modulate by perceptual factors
                clarity = content.get("clarity", 0.8)
                attention = content.get("attention_level", 0.7)
                
                # Adjust base reliability
                reliability = base_reliability * (0.6 + (clarity * 0.2) + (attention * 0.2))
                
            elif source == "memory":
                base_reliability = 0.7
                
                # Modulate by memory factors
                age = content.get("age", 0.5)
                emotional_significance = content.get("emotional_significance", 0.5)
                
                # Adjust for age (newer memories more reliable)
                age_factor = max(0.3, 1.0 - (age * 0.6))
                
                # Emotional memories are more reliable but can be distorted
                emotional_factor = 0.5 + (emotional_significance * 0.5)
                
                # Combine factors
                reliability = base_reliability * (age_factor * 0.6 + emotional_factor * 0.4)
                
            elif source == "language":
                base_reliability = 0.6
                
                # Consider source factors
                trustworthiness = content.get("source_trustworthiness", 0.5)
                expertise = content.get("source_expertise", 0.5)
                
                # Consider content factors
                verifiability = content.get("verifiability", 0.5)
                consistency = content.get("internal_consistency", 0.7)
                
                # Combine factors with appropriate weights
                reliability = base_reliability * (
                    trustworthiness * 0.3 +
                    expertise * 0.3 +
                    verifiability * 0.2 +
                    consistency * 0.2
                )
                
            elif source == "reasoning":
                base_reliability = 0.7
                
                # Consider reasoning quality factors
                logic_quality = content.get("logic_quality", 0.7)
                evidence_quality = content.get("evidence_quality", 0.6)
                confidence = content.get("confidence", 0.6)
                
                # Combine factors
                reliability = base_reliability * (
                    logic_quality * 0.4 +
                    evidence_quality * 0.4 +
                    confidence * 0.2
                )
                
            else:
                # More sophisticated unknown source handling
                reliability = 0.3 + (random.random() * 0.2)  # Some randomness in evaluation
        
        # Cache the result for future use
        self.source_reliability_cache[source] = reliability
        
        # Ensure value is in valid range
        return max(0.0, min(1.0, reliability))
    
    def _evaluate_consistency(self, evidence: Evidence, prior_evidence: List[Evidence]) -> float:
        """
        Evaluate consistency of evidence with prior evidence
        
        Args:
            evidence: Evidence to evaluate
            prior_evidence: Previously evaluated evidence
            
        Returns:
            Consistency score (0.0-1.0)
        """
        if not prior_evidence:
            return 1.0  # No prior evidence to check consistency against
            
        # Early development: Simple agreement counting
        if self.development_level < 0.3:
            agreements = 0
            disagreements = 0
            
            for prior in prior_evidence:
                # Check for overlapping content
                overlap = False
                
                for key in evidence.content:
                    if key in prior.content:
                        overlap = True
                        if evidence.content[key] == prior.content[key]:
                            agreements += 1
                        else:
                            disagreements += 1
            
            total = agreements + disagreements
            if total == 0:
                return 0.7  # No direct overlaps, neutral consistency
                
            return agreements / total
            
        # More advanced: Weighted agreement based on source and recency
        else:
            total_score = 0.0
            total_weight = 0.0
            
            for prior in prior_evidence:
                # Calculate temporal recency (more recent evidence counts more)
                time_diff = (evidence.timestamp - prior.timestamp).total_seconds() if evidence.timestamp and prior.timestamp else 86400
                recency = 1.0 / (1.0 + (time_diff / 86400))  # Normalize to 0-1 range
                
                # Calculate content overlap
                overlap_keys = set(evidence.content.keys()) & set(prior.content.keys())
                if not overlap_keys:
                    continue
                    
                # Count agreements and disagreements
                agreements = sum(1 for k in overlap_keys if evidence.content[k] == prior.content[k])
                disagreements = len(overlap_keys) - agreements
                
                # Calculate agreement ratio
                if len(overlap_keys) > 0:
                    agreement_ratio = agreements / len(overlap_keys)
                    
                    # Weight by prior evidence reliability and recency
                    weight = prior.reliability * (0.5 + (recency * 0.5))
                    
                    total_score += agreement_ratio * weight
                    total_weight += weight
            
            if total_weight == 0:
                return 0.7  # No overlapping content, neutral consistency
                
            return total_score / total_weight
    
    def _evaluate_relevance(self, evidence: Evidence, context: Dict[str, Any]) -> float:
        """
        Evaluate relevance of evidence to current context
        
        Args:
            evidence: Evidence to evaluate
            context: Current context
            
        Returns:
            Relevance score (0.0-1.0)
        """
        if not context:
            return 0.7  # No context, assume moderate relevance
            
        # Early development: Simple key matching
        if self.development_level < 0.3:
            # Count matching keys between evidence and context
            evidence_keys = set(evidence.content.keys())
            context_keys = set(context.keys())
            
            matching_keys = evidence_keys.intersection(context_keys)
            
            if not evidence_keys:
                return 0.5  # Empty evidence, neutral relevance
                
            return len(matching_keys) / len(evidence_keys)
            
        # More advanced: Content-based relevance
        else:
            # Look for content overlap
            direct_matches = 0
            total_keys = len(evidence.content)
            
            if total_keys == 0:
                return 0.5  # Empty evidence, neutral relevance
            
            # Check for direct content matches
            for key, value in evidence.content.items():
                if key in context and context[key] == value:
                    direct_matches += 1
            
            # Check for related content
            related_matches = 0
            for key in evidence.content:
                # Look for related keys in context
                for context_key in context:
                    # Simple relatedness check using string overlap
                    if key in context_key or context_key in key:
                        related_matches += 0.5
                        break
            
            # Combine direct and related matches
            weighted_matches = direct_matches + (related_matches * 0.5)
            relevance = min(1.0, weighted_matches / total_keys)
            
            return max(0.1, relevance)  # Ensure minimum relevance
    
    def _evaluate_methodology(self, evidence: Evidence) -> float:
        """
        Evaluate methodological quality of evidence
        
        Args:
            evidence: Evidence to evaluate
            
        Returns:
            Methodology quality score (0.0-1.0)
        """
        # Only available at higher development levels
        if self.development_level < 0.6:
            return 0.5
            
        # Look for methodology indicators in evidence
        methodology_indicators = {
            'sample_size': evidence.content.get('sample_size', 0),
            'control_group': evidence.content.get('control_group', False),
            'randomization': evidence.content.get('randomization', False),
            'peer_reviewed': evidence.content.get('peer_reviewed', False),
            'replication': evidence.content.get('replication', False)
        }
        
        # Calculate methodology score
        score = 0.5  # Default score
        
        # Sample size effect
        if methodology_indicators['sample_size'] > 0:
            # Logarithmic scaling of sample size benefit
            sample_size_score = min(0.3, 0.1 + (np.log10(methodology_indicators['sample_size']) * 0.05))
            score += sample_size_score
            
        # Control group effect
        if methodology_indicators['control_group']:
            score += 0.15
            
        # Randomization effect
        if methodology_indicators['randomization']:
            score += 0.1
            
        # Peer review effect
        if methodology_indicators['peer_reviewed']:
            score += 0.15
            
        # Replication effect
        if methodology_indicators['replication']:
            score += 0.1
            
        # Ensure score is in valid range
        return max(0.0, min(1.0, score))
    
    def _evaluate_statistical_properties(self, evidence: Evidence) -> float:
        """
        Evaluate statistical properties of evidence
        
        Args:
            evidence: Evidence to evaluate
            
        Returns:
            Statistical quality score (0.0-1.0)
        """
        # Only available at higher development levels
        if self.development_level < 0.8:
            return 0.5
            
        # Look for statistical indicators in evidence
        statistical_indicators = {
            'p_value': evidence.content.get('p_value', 1.0),
            'confidence_interval': evidence.content.get('confidence_interval', 0.0),
            'effect_size': evidence.content.get('effect_size', 0.0),
            'statistical_power': evidence.content.get('statistical_power', 0.0)
        }
        
        # Calculate statistical quality score
        score = 0.5  # Default score
        
        # P-value effect (lower is better)
        if statistical_indicators['p_value'] < 1.0:
            p_value = statistical_indicators['p_value']
            if p_value < 0.01:
                score += 0.2
            elif p_value < 0.05:
                score += 0.15
            elif p_value < 0.1:
                score += 0.05
            else:
                score -= 0.1  # Penalize high p-values
                
        # Effect size effect
        effect_size = statistical_indicators['effect_size']
        if effect_size > 0:
            score += min(0.2, effect_size * 0.5)
            
        # Statistical power effect
        power = statistical_indicators['statistical_power']
        if power > 0:
            if power > 0.8:
                score += 0.15
            elif power > 0.5:
                score += 0.1
            else:
                score += 0.05
                
        # Confidence interval effect
        ci = statistical_indicators['confidence_interval']
        if ci > 0:
            if ci >= 0.95:
                score += 0.15
            elif ci >= 0.9:
                score += 0.1
            else:
                score += 0.05
                
        # Ensure score is in valid range
        return max(0.0, min(1.0, score))


#######################

#belief\models.py#
#######################

# models.py
from pydantic import BaseModel, Field, model_validator
from typing import Dict, List, Any, Optional, Union, Literal, Tuple
from datetime import datetime
import uuid
import numpy as np
import random

class Evidence(BaseModel):
    """Evidence supporting or contradicting a belief"""
    evidence_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    source: str
    content: Dict[str, Any]
    reliability: float = Field(default=0.5, ge=0.0, le=1.0)
    relevance: float = Field(default=0.5, ge=0.0, le=1.0)
    timestamp: datetime = Field(default_factory=datetime.now)
    
    model_config = {
        "arbitrary_types_allowed": True
    }

class Belief(BaseModel):
    """Representation of a belief with confidence and supporting evidence"""
    belief_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    content: Dict[str, Any]
    confidence: float = Field(default=0.5, ge=0.0, le=1.0)
    creation_time: datetime = Field(default_factory=datetime.now)
    last_updated: datetime = Field(default_factory=datetime.now)
    stability: float = Field(default=0.2, ge=0.0, le=1.0)  # How resistant to change
    evidence_for: List[Evidence] = Field(default_factory=list)
    evidence_against: List[Evidence] = Field(default_factory=list)
    related_beliefs: List[str] = Field(default_factory=list)  # IDs of related beliefs
    source: str = "reasoning"  # Where this belief came from
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    model_config = {
        "arbitrary_types_allowed": True
    }
    
    @model_validator(mode='after')
    def update_confidence_from_evidence(self):
        """Update confidence based on supporting and contradicting evidence"""
        if not self.evidence_for and not self.evidence_against:
            return self
            
        # Implement Bayesian confidence calculation
        # We'll use a simplified Bayesian update based on evidence reliability and relevance
        
        # Start with prior confidence
        prior = self.confidence
        
        # Calculate likelihood ratios for evidence
        likelihood_for = 1.0
        for evidence in self.evidence_for:
            # Weight by reliability and relevance
            weighted_support = evidence.reliability * evidence.relevance
            # Convert to likelihood ratio (values > 1.0 increase confidence)
            evidence_factor = 1.0 + weighted_support
            likelihood_for *= evidence_factor
            
        likelihood_against = 1.0
        for evidence in self.evidence_against:
            # Weight by reliability and relevance
            weighted_contradiction = evidence.reliability * evidence.relevance
            # Convert to likelihood ratio (values > 1.0 increase confidence)
            evidence_factor = 1.0 + weighted_contradiction
            likelihood_against *= evidence_factor
        
        # Apply Bayes' rule
        # posterior_odds = prior_odds * likelihood_ratio
        prior_odds = prior / (1.0 - prior) if prior < 1.0 else 100.0  # Prevent division by zero
        likelihood_ratio = likelihood_for / likelihood_against if likelihood_against > 0 else likelihood_for
        posterior_odds = prior_odds * likelihood_ratio
        
        # Convert back to probability
        posterior = posterior_odds / (1.0 + posterior_odds)
        
        # Apply stability as a damping factor on changes
        confidence_change = posterior - prior
        damped_change = confidence_change * (1.0 - self.stability)
        
        # Update confidence with damped change
        self.confidence = max(0.0, min(1.0, prior + damped_change))
        self.last_updated = datetime.now()
            
        return self

class BeliefSystem(BaseModel):
    """Collection of interrelated beliefs"""
    beliefs: Dict[str, Belief] = Field(default_factory=dict)
    consistency_score: float = Field(default=1.0, ge=0.0, le=1.0)
    belief_network: Dict[str, List[str]] = Field(default_factory=dict)  # Graph of belief relations
    developmental_level: float = Field(default=0.0, ge=0.0, le=1.0)
    
    model_config = {
        "arbitrary_types_allowed": True
    }
    
    def add_belief(self, belief: Belief) -> str:
        """
        Add a new belief to the system
        
        Args:
            belief: The belief to add
            
        Returns:
            ID of the added belief
        """
        self.beliefs[belief.belief_id] = belief
        
        # Initialize in network graph if new
        if belief.belief_id not in self.belief_network:
            self.belief_network[belief.belief_id] = []
            
        # Update relationships
        self._update_belief_relationships(belief)
        
        # Recalculate consistency
        self._calculate_consistency()
        
        return belief.belief_id
    
    def update_belief(self, belief_id: str, updated_belief: Belief) -> bool:
        """
        Update an existing belief
        
        Args:
            belief_id: ID of belief to update
            updated_belief: New belief data
            
        Returns:
            True if updated, False if belief not found
        """
        if belief_id not in self.beliefs:
            return False
            
        # Preserve the original ID
        updated_belief.belief_id = belief_id
        self.beliefs[belief_id] = updated_belief
        
        # Update relationships
        self._update_belief_relationships(updated_belief)
        
        # Recalculate consistency
        self._calculate_consistency()
        
        return True
    
    def remove_belief(self, belief_id: str) -> bool:
        """
        Remove a belief from the system
        
        Args:
            belief_id: ID of belief to remove
            
        Returns:
            True if removed, False if not found
        """
        if belief_id not in self.beliefs:
            return False
            
        # Remove from beliefs dictionary
        del self.beliefs[belief_id]
        
        # Remove from network
        if belief_id in self.belief_network:
            del self.belief_network[belief_id]
            
        # Remove references from other beliefs' relationships
        for other_id in self.belief_network:
            if belief_id in self.belief_network[other_id]:
                self.belief_network[other_id].remove(belief_id)
                
        # Update related_beliefs lists in all beliefs
        for other_belief in self.beliefs.values():
            if belief_id in other_belief.related_beliefs:
                other_belief.related_beliefs.remove(belief_id)
                
        # Recalculate consistency
        self._calculate_consistency()
        
        return True
    
    def get_belief(self, belief_id: str) -> Optional[Belief]:
        """
        Get a belief by ID
        
        Args:
            belief_id: ID of the belief to retrieve
            
        Returns:
            The belief if found, None otherwise
        """
        return self.beliefs.get(belief_id)
    
    def find_related_beliefs(self, belief_id: str, max_depth: int = 1) -> Dict[str, float]:
        """
        Find beliefs related to the given belief
        
        Args:
            belief_id: ID of the belief to find relations for
            max_depth: Maximum network traversal depth
            
        Returns:
            Dictionary mapping related belief IDs to relatedness scores
        """
        if belief_id not in self.beliefs:
            return {}
            
        related = {}
        visited = set()
        
        # Helper function for recursive traversal
        def traverse(current_id: str, depth: int, path_strength: float = 1.0):
            if depth > max_depth or current_id in visited:
                return
                
            visited.add(current_id)
            
            # Get direct neighbors
            for neighbor_id in self.belief_network.get(current_id, []):
                # Calculate relatedness as product of path strengths
                relationship_strength = self._calculate_relationship_strength(current_id, neighbor_id)
                new_strength = path_strength * relationship_strength
                
                # Store or update relatedness score
                if neighbor_id not in related or new_strength > related[neighbor_id]:
                    related[neighbor_id] = new_strength
                    
                # Recurse to next level
                traverse(neighbor_id, depth + 1, new_strength)
                
        # Start traversal
        traverse(belief_id, 0)
        
        # Remove the starting belief itself
        if belief_id in related:
            del related[belief_id]
            
        return related
    
    def find_contradictions(self, belief_id: str = None) -> List[Tuple[str, str, float]]:
        """
        Find contradictory beliefs in the system
        
        Args:
            belief_id: Optional ID to check contradictions for a specific belief
            
        Returns:
            List of tuples (belief_id1, belief_id2, contradiction_strength)
        """
        contradictions = []
        
        # If specific belief provided, only check that one
        if belief_id:
            if belief_id not in self.beliefs:
                return []
                
            beliefs_to_check = [belief_id]
        else:
            # Otherwise check all beliefs
            beliefs_to_check = list(self.beliefs.keys())
            
        # Check each belief against others
        for i, id1 in enumerate(beliefs_to_check):
            # Only compare with beliefs we haven't checked yet
            other_beliefs = beliefs_to_check[i+1:] if belief_id is None else list(self.beliefs.keys())
            
            for id2 in other_beliefs:
                if id1 == id2:
                    continue
                    
                contradiction_strength = self._calculate_contradiction(id1, id2)
                if contradiction_strength > 0.2:  # Only report significant contradictions
                    contradictions.append((id1, id2, contradiction_strength))
                    
        return contradictions
    
    def _update_belief_relationships(self, belief: Belief) -> None:
        """Update relationships between this belief and others"""
        # Clear existing relationships for this belief
        belief.related_beliefs = []
        
        # Find content-based relationships with other beliefs
        for other_id, other_belief in self.beliefs.items():
            if other_id == belief.belief_id:
                continue
                
            # Calculate relationship strength
            relatedness = self._calculate_content_similarity(belief, other_belief)
            
            # If sufficiently related, add to relationship network
            if relatedness > 0.3:  # Threshold for relationship
                # Update belief network (bidirectional)
                if other_id not in self.belief_network[belief.belief_id]:
                    self.belief_network[belief.belief_id].append(other_id)
                
                if belief.belief_id not in self.belief_network.get(other_id, []):
                    if other_id not in self.belief_network:
                        self.belief_network[other_id] = []
                    self.belief_network[other_id].append(belief.belief_id)
                
                # Update related_beliefs lists
                if other_id not in belief.related_beliefs:
                    belief.related_beliefs.append(other_id)
                
                if belief.belief_id not in other_belief.related_beliefs:
                    other_belief.related_beliefs.append(belief.belief_id)
    
    def _calculate_content_similarity(self, belief1: Belief, belief2: Belief) -> float:
        """Calculate similarity between belief contents"""
        # Simple content overlap calculation
        # In a real implementation, this could use semantic similarity
        
        # Collect keys from both beliefs
        all_keys = set(belief1.content.keys()) | set(belief2.content.keys())
        if not all_keys:
            return 0.0
            
        # Count matching values
        matching = 0
        for key in all_keys:
            if key in belief1.content and key in belief2.content:
                if belief1.content[key] == belief2.content[key]:
                    matching += 1
                    
        # Calculate similarity score
        return matching / len(all_keys)
    
    def _calculate_relationship_strength(self, belief_id1: str, belief_id2: str) -> float:
        """Calculate relationship strength between two beliefs"""
        belief1 = self.beliefs.get(belief_id1)
        belief2 = self.beliefs.get(belief_id2)
        
        if not belief1 or not belief2:
            return 0.0
            
        # Content similarity
        content_similarity = self._calculate_content_similarity(belief1, belief2)
        
        # Evidence overlap
        evidence_overlap = self._calculate_evidence_overlap(belief1, belief2)
        
        # Combine factors
        return 0.7 * content_similarity + 0.3 * evidence_overlap
    
    def _calculate_evidence_overlap(self, belief1: Belief, belief2: Belief) -> float:
        """Calculate overlap in evidence between beliefs"""
        # Extract evidence IDs
        evidence1_ids = {e.evidence_id for e in belief1.evidence_for + belief1.evidence_against}
        evidence2_ids = {e.evidence_id for e in belief2.evidence_for + belief2.evidence_against}
        
        # Calculate overlap
        if not evidence1_ids or not evidence2_ids:
            return 0.0
            
        intersection = evidence1_ids.intersection(evidence2_ids)
        union = evidence1_ids.union(evidence2_ids)
        
        return len(intersection) / len(union)
    
    def _calculate_contradiction(self, belief_id1: str, belief_id2: str) -> float:
        """Calculate contradiction strength between two beliefs"""
        belief1 = self.beliefs.get(belief_id1)
        belief2 = self.beliefs.get(belief_id2)
        
        if not belief1 or not belief2:
            return 0.0
            
        # Simple contradiction detection based on content
        contradictions = 0
        total_comparisons = 0
        
        # Compare shared keys for direct contradictions
        for key in set(belief1.content.keys()) & set(belief2.content.keys()):
            if belief1.content[key] != belief2.content[key]:
                contradictions += 1
            total_comparisons += 1
                
        # If no shared keys, check for evidence that directly opposes
        if total_comparisons == 0:
            # Evidence for belief1 that's against belief2
            for evidence in belief1.evidence_for:
                if any(e.evidence_id == evidence.evidence_id for e in belief2.evidence_against):
                    contradictions += 1
                    total_comparisons += 1
                    
            # Evidence for belief2 that's against belief1
            for evidence in belief2.evidence_for:
                if any(e.evidence_id == evidence.evidence_id for e in belief1.evidence_against):
                    contradictions += 1
                    total_comparisons += 1
        
        if total_comparisons == 0:
            return 0.0
            
        return contradictions / total_comparisons
    
    def _calculate_consistency(self) -> None:
        """Calculate overall consistency score for the belief system"""
        if len(self.beliefs) <= 1:
            self.consistency_score = 1.0
            return
            
        # Find all contradictions
        contradictions = self.find_contradictions()
        
        # If no contradictions, perfect consistency
        if not contradictions:
            self.consistency_score = 1.0
            return
            
        # Calculate weighted contradiction score
        total_possible_pairs = len(self.beliefs) * (len(self.beliefs) - 1) / 2
        contradiction_sum = sum(strength for _, _, strength in contradictions)
        
        # Scale by number of beliefs and max possible contradictions
        raw_consistency = 1.0 - (contradiction_sum / total_possible_pairs)
        
        # Ensure in valid range
        self.consistency_score = max(0.0, min(1.0, raw_consistency))

#######################

#belief\neural_net.py#
#######################

# neural_net.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Any

class BeliefNetwork(nn.Module):
    """Neural network for belief processing"""
    
    def __init__(
        self,
        input_dim: int = 128,
        hidden_dim: int = 256,
        belief_dim: int = 64,
        num_heads: int = 4
    ):
        super().__init__()
        
        # Encoders
        self.input_encoder = nn.Linear(input_dim, hidden_dim)
        self.belief_encoder = nn.Linear(belief_dim, hidden_dim)
        self.evidence_encoder = nn.Linear(input_dim, hidden_dim)
        
        # Multi-head attention for evidence evaluation
        self.evidence_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            batch_first=True
        )
        
        # Belief updating components
        self.belief_update_network = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, belief_dim)
        )
        
        # Contradiction detection
        self.contradiction_detector = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
        
        # Confidence estimation
        self.confidence_estimator = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
        
    def forward(
        self,
        input_data: torch.Tensor,
        current_beliefs: Optional[torch.Tensor] = None,
        evidence: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Process input to form or update beliefs
        
        Args:
            input_data: New information to process
            current_beliefs: Existing beliefs (if any)
            evidence: Supporting evidence (if any)
            
        Returns:
            Tuple of (updated_beliefs, confidence_scores, contradiction_scores)
        """
        # Encode input
        encoded_input = F.relu(self.input_encoder(input_data))
        
        # Default empty beliefs if none provided
        if current_beliefs is None:
            batch_size = input_data.shape[0]
            belief_dim = self.belief_encoder.in_features
            current_beliefs = torch.zeros(batch_size, belief_dim, device=input_data.device)
            
        # Encode current beliefs
        encoded_beliefs = F.relu(self.belief_encoder(current_beliefs))
        
        # Process evidence if provided
        if evidence is not None:
            encoded_evidence = F.relu(self.evidence_encoder(evidence))
            attended_evidence, _ = self.evidence_attention(
                encoded_input.unsqueeze(1),
                encoded_evidence.unsqueeze(1),
                encoded_evidence.unsqueeze(1)
            )
            attended_evidence = attended_evidence.squeeze(1)
        else:
            attended_evidence = encoded_input
            
        # Check for contradictions
        contradiction_scores = self.contradiction_detector(
            torch.cat([encoded_beliefs, attended_evidence], dim=1)
        )
        
        # Update beliefs based on new information
        updated_beliefs = self.belief_update_network(
            torch.cat([encoded_beliefs, attended_evidence], dim=1)
        )
        
        # Calculate confidence in the beliefs
        confidence_scores = self.confidence_estimator(updated_beliefs)
        
        return updated_beliefs, confidence_scores, contradiction_scores

#######################

#belief\__init__.py#
#######################

﻿"""
Belief Module

This module is responsible for the formation, evaluation, updating, and resolution
of contradictions in the mind's belief system. It includes mechanisms for forming
new beliefs, evaluating evidence, updating existing beliefs, and resolving conflicts
between contradictory beliefs.

The belief system evolves developmentally from simple, rigid beliefs in early stages
to more nuanced, flexible, and evidence-based beliefs in later stages.
"""

from typing import Optional, Dict, Any, List
import logging
from datetime import datetime

from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.base_module import BaseModule
from lmm_project.modules.belief.models import Belief, Evidence, BeliefSystem
from lmm_project.modules.belief.belief_formation import BeliefFormation
from lmm_project.modules.belief.evidence_evaluation import EvidenceEvaluation
from lmm_project.modules.belief.belief_updating import BeliefUpdating
from lmm_project.modules.belief.contradiction_resolution import ContradictionResolution
from lmm_project.modules.belief.neural_net import BeliefNetwork

logger = logging.getLogger(__name__)

def get_module(
    module_id: str = "belief",
    event_bus: Optional[EventBus] = None,
    development_level: float = 0.0
) -> "BeliefSystemModule":
    """
    Factory function to create a belief module.
    
    The belief system is responsible for:
    - Forming beliefs based on experiences and evidence
    - Evaluating the reliability of evidence
    - Updating beliefs in response to new information
    - Resolving contradictions between beliefs
    
    Args:
        module_id: Identifier for this module
        event_bus: Event bus for communication with other modules
        development_level: Initial developmental level (0.0-1.0)
    
    Returns:
        An integrated BeliefSystemModule instance
    """
    return BeliefSystemModule(module_id, event_bus, development_level)

class BeliefSystemModule(BaseModule):
    """
    Integrated belief system that coordinates belief formation, evidence evaluation,
    belief updating, and contradiction resolution.
    
    This module evolves developmentally from simple, rigid beliefs with minimal
    evidence requirements to sophisticated, nuanced beliefs with complex
    evidential standards and uncertainty handling.
    """
    # Development milestones for the belief system
    development_milestones = {
        0.0: "Basic association-based beliefs",
        0.2: "Simple causal beliefs emerging",
        0.4: "Basic evidence evaluation capabilities",
        0.6: "Consideration of multiple evidence sources",
        0.8: "Handling of uncertainty and probability",
        1.0: "Complex belief networks with nuanced confidence"
    }
    
    def __init__(
        self,
        module_id: str,
        event_bus: Optional[EventBus] = None,
        development_level: float = 0.0
    ):
        """Initialize the belief system with its components"""
        super().__init__(module_id, "belief", event_bus)
        self.development_level = development_level
        
        # Initialize belief system state
        self.belief_system = BeliefSystem(developmental_level=development_level)
        
        # Initialize sub-components
        self.belief_formation = BeliefFormation(f"{module_id}_formation", event_bus)
        self.evidence_evaluation = EvidenceEvaluation(f"{module_id}_evidence", event_bus)
        self.belief_updating = BeliefUpdating(f"{module_id}_updating", event_bus)
        self.contradiction_resolution = ContradictionResolution(f"{module_id}_resolution", event_bus)
        
        # Neural network for belief processing
        self.neural_net = BeliefNetwork()
        
        # Activation tracking for beliefs
        self.active_beliefs = {}  # Maps belief_id to activation level
        self.activation_decay_rate = 0.1  # How quickly activations decay
        self.last_activation_update = datetime.now()
        
        # Update component development levels
        self._sync_development_levels()
        
        # Subscribe to relevant message types
        if event_bus:
            event_bus.subscribe("perception_input", self._handle_message)
            event_bus.subscribe("memory_retrieval", self._handle_message)
            event_bus.subscribe("language_comprehension", self._handle_message)
            event_bus.subscribe("consciousness_broadcast", self._handle_message)
            event_bus.subscribe("belief_query", self._handle_message)
            
        logger.info(f"Belief system initialized at development level {development_level:.2f}")
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process inputs to update the belief system
        
        Args:
            input_data: Data to process, may include:
                - new_evidence: Evidence to evaluate
                - belief_query: Query about existing beliefs
                - contradiction_check: Check for conflicting beliefs
                - context: Contextual information for processing
        
        Returns:
            Results of belief processing
        """
        results = {"processed": False, "module_id": self.module_id}
        
        # Extract context if available
        context = input_data.get("context", {})
        
        # Handle different input types
        if "new_evidence" in input_data:
            # Get prior evidence for consistency evaluation
            prior_evidence = []
            for belief in self.belief_system.beliefs.values():
                prior_evidence.extend(belief.evidence_for)
                prior_evidence.extend(belief.evidence_against)
            
            # Evaluate the evidence
            eval_results = self.evidence_evaluation.process_input({
                "evidence": input_data["new_evidence"],
                "context": context,
                "prior_evidence": prior_evidence
            })
            
            # Use evaluated evidence for belief formation/updating
            if "source" in input_data and input_data["source"] == "memory":
                # Check if this relates to existing beliefs
                update_results = self.belief_updating.process_input({
                    "evidence": eval_results["evaluated_evidence"],
                    "belief_system": self.belief_system,
                    "context": context
                })
                results.update(update_results)
            else:
                # Form new beliefs from the evidence
                formation_results = self.belief_formation.process_input({
                    "evidence": eval_results["evaluated_evidence"],
                    "belief_system": self.belief_system,
                    "context": context
                })
                results.update(formation_results)
                
            # Recalculate belief system consistency
            self.belief_system._calculate_consistency()
                
            # Check for contradictions after updating beliefs
            contradiction_results = self.contradiction_resolution.process_input({
                "belief_system": self.belief_system,
                "context": context
            })
            results.update(contradiction_results)
            
            results["processed"] = True
            results["consistency_score"] = self.belief_system.consistency_score
            
        elif "belief_query" in input_data:
            query = input_data["belief_query"]
            matching_beliefs = self._find_matching_beliefs(query)
            results["beliefs"] = matching_beliefs
            results["processed"] = True
            
        elif "contradiction_check" in input_data:
            # Explicit contradiction check
            contradiction_results = self.contradiction_resolution.process_input({
                "belief_system": self.belief_system,
                "context": context
            })
            results.update(contradiction_results)
            results["processed"] = True
            
        return results
    
    def _find_matching_beliefs(self, query: Dict[str, Any]) -> List[Belief]:
        """Find beliefs that match the provided query criteria"""
        matches = []
        
        # Basic matching logic - can be enhanced with semantic matching
        for belief_id, belief in self.belief_system.beliefs.items():
            match = True
            for key, value in query.items():
                if key not in belief.content or belief.content[key] != value:
                    match = False
                    break
            
            if match:
                matches.append(belief)
                
        return matches
    
    def _handle_message(self, message: Message) -> None:
        """Process messages from other modules"""
        # Extract context if available in the message
        context = message.content.get("context", {}) if isinstance(message.content, dict) else {}
        
        if message.msg_type == "perception_input":
            # Extract perception data, preserving context if available
            perception_data = message.content if not isinstance(message.content, dict) else {
                k: v for k, v in message.content.items() if k != "context"
            }
            
            # Convert perception into potential evidence
            evidence = self._create_evidence_from_perception(perception_data)
            self.process_input({
                "new_evidence": evidence, 
                "source": "perception",
                "context": context
            })
            
        elif message.msg_type == "memory_retrieval":
            # Extract memory data, preserving context if available
            memory_data = message.content if not isinstance(message.content, dict) else {
                k: v for k, v in message.content.items() if k != "context"
            }
            
            # Process memories as evidence for beliefs
            evidence = self._create_evidence_from_memory(memory_data)
            self.process_input({
                "new_evidence": evidence, 
                "source": "memory",
                "context": context
            })
            
        elif message.msg_type == "language_comprehension":
            # Extract language data, preserving context if available
            language_data = message.content if not isinstance(message.content, dict) else {
                k: v for k, v in message.content.items() if k != "context"
            }
            
            # Process language as evidence for beliefs
            evidence = self._create_evidence_from_language(language_data)
            self.process_input({
                "new_evidence": evidence, 
                "source": "language",
                "context": context
            })
            
        elif message.msg_type == "belief_query":
            # Handle queries about beliefs
            query_params = {"belief_query": message.content}
            if context:
                query_params["context"] = context
                
            results = self.process_input(query_params)
            
            # Reply with results
            if self.event_bus and message.reply_to:
                reply = Message(
                    msg_type="belief_query_response",
                    sender=self.module_id,
                    recipient=message.sender,
                    content=results,
                    reply_to=message.id
                )
                self.event_bus.publish(reply)
    
    def _create_evidence_from_perception(self, perception_data: Dict[str, Any]) -> Evidence:
        """Convert perception data into evidence format"""
        return Evidence(
            source="perception",
            content=perception_data,
            reliability=self._calculate_perception_reliability(perception_data),
            relevance=1.0,  # Can be refined based on current context
            timestamp=datetime.now()
        )
    
    def _calculate_perception_reliability(self, perception_data: Dict[str, Any]) -> float:
        """Calculate the reliability of perceptual evidence"""
        # Base reliability on perception clarity if available
        if "clarity" in perception_data:
            return max(0.0, min(1.0, perception_data["clarity"]))
        
        # Default medium-high reliability for perception
        return 0.8
    
    def _create_evidence_from_memory(self, memory_data: Dict[str, Any]) -> Evidence:
        """Convert memory data into evidence format"""
        # Memory reliability decreases with age and increases with emotional significance
        age_factor = 1.0
        if "age" in memory_data:
            age_factor = max(0.3, 1.0 - (memory_data.get("age", 0) * 0.1))
            
        emotional_factor = 1.0
        if "emotional_intensity" in memory_data:
            emotional_factor = 1.0 + (memory_data.get("emotional_intensity", 0) * 0.2)
        
        reliability = min(1.0, age_factor * emotional_factor * 0.7)
        
        return Evidence(
            source="memory",
            content=memory_data,
            reliability=reliability,
            relevance=memory_data.get("relevance", 0.5),
            timestamp=datetime.now()
        )
    
    def _create_evidence_from_language(self, language_data: Dict[str, Any]) -> Evidence:
        """Convert language comprehension data into evidence format"""
        # Language reliability based on source trustworthiness if available
        reliability = language_data.get("source_trustworthiness", 0.5)
        
        return Evidence(
            source="language",
            content=language_data,
            reliability=reliability,
            relevance=language_data.get("relevance", 0.5),
            timestamp=datetime.now()
        )
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of the belief system
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        prev_level = self.development_level
        self.development_level = min(1.0, self.development_level + amount)
        
        # Update belief system's developmental level
        self.belief_system.developmental_level = self.development_level
        
        # Synchronize development levels across components
        self._sync_development_levels()
        
        if int(prev_level * 10) != int(self.development_level * 10):
            milestone = self._get_current_milestone()
            logger.info(f"Belief system reached development level {self.development_level:.2f}: {milestone}")
        
        return self.development_level
    
    def _sync_development_levels(self) -> None:
        """Synchronize development levels across all belief components"""
        self.belief_formation.development_level = self.development_level
        self.evidence_evaluation.development_level = self.development_level
        self.belief_updating.development_level = self.development_level
        self.contradiction_resolution.development_level = self.development_level
    
    def _get_current_milestone(self) -> str:
        """Get the current developmental milestone description"""
        # Find the highest milestone that's less than or equal to current level
        milestone_levels = sorted(self.development_milestones.keys())
        
        for i in range(len(milestone_levels) - 1, -1, -1):
            if milestone_levels[i] <= self.development_level:
                return self.development_milestones[milestone_levels[i]]
        
        return self.development_milestones[0]
    
    def get_state(self) -> Dict[str, Any]:
        """Get the current state of the belief system"""
        return {
            "module_id": self.module_id,
            "module_type": self.module_type,
            "development_level": self.development_level,
            "belief_count": len(self.belief_system.beliefs),
            "beliefs": {
                belief_id: {
                    "content": belief.content,
                    "confidence": belief.confidence,
                    "stability": belief.stability,
                    "evidence_count": len(belief.evidence_for) + len(belief.evidence_against)
                }
                for belief_id, belief in self.belief_system.beliefs.items()
            },
            "consistency_score": self.belief_system.consistency_score,
            "current_milestone": self._get_current_milestone()
        }

    def activate_belief(self, belief_id: str, activation_level: float = 1.0) -> None:
        """
        Activate a belief, making it more prominent in processing
        
        Args:
            belief_id: ID of the belief to activate
            activation_level: Level of activation (0.0-1.0)
        """
        # Ensure belief exists
        if belief_id not in self.belief_system.beliefs:
            return
            
        # Update activation
        current_level = self.active_beliefs.get(belief_id, 0.0)
        new_level = min(1.0, current_level + activation_level)
        self.active_beliefs[belief_id] = new_level
        
        # Activate related beliefs with diminishing activation
        if self.development_level > 0.4:  # Only at higher development levels
            related_beliefs = self.belief_system.find_related_beliefs(belief_id)
            for related_id, relatedness in related_beliefs.items():
                if related_id in self.belief_system.beliefs:
                    related_activation = min(0.5, activation_level * relatedness)
                    current = self.active_beliefs.get(related_id, 0.0)
                    self.active_beliefs[related_id] = min(1.0, current + related_activation)
    
    def decay_activations(self) -> None:
        """Decay belief activations over time"""
        now = datetime.now()
        time_diff = (now - self.last_activation_update).total_seconds()
        
        # Only decay if significant time has passed
        if time_diff < 0.1:
            return
            
        # Calculate decay amount
        decay_amount = self.activation_decay_rate * time_diff
        
        # Apply decay to all active beliefs
        for belief_id in list(self.active_beliefs.keys()):
            self.active_beliefs[belief_id] = max(0.0, self.active_beliefs[belief_id] - decay_amount)
            
            # Remove beliefs with negligible activation
            if self.active_beliefs[belief_id] < 0.01:
                del self.active_beliefs[belief_id]
                
        self.last_activation_update = now
    
    def get_active_beliefs(self, threshold: float = 0.1) -> Dict[str, float]:
        """
        Get currently active beliefs
        
        Args:
            threshold: Minimum activation level to include
            
        Returns:
            Dictionary mapping belief IDs to activation levels
        """
        # First decay existing activations
        self.decay_activations()
        
        # Filter and return active beliefs above threshold
        return {
            belief_id: activation 
            for belief_id, activation in self.active_beliefs.items() 
            if activation >= threshold
        }


#######################

#consciousness\awareness.py#
#######################

# TODO: Implement the Awareness class to monitor internal and external states
# This component should maintain awareness of:
# - External perceptual inputs
# - Internal emotional states
# - Current goals and motivations
# - Ongoing cognitive processes
# - Current attentional focus

# TODO: Implement developmental progression of awareness:
# - Basic stimulus awareness in early stages
# - Growing peripheral awareness in childhood
# - Self-directed awareness in adolescence
# - Integrated awareness of multiple states in adulthood

# TODO: Create mechanisms for:
# - State monitoring: Track current states across cognitive systems
# - Change detection: Identify significant changes in monitored states
# - Awareness broadcasting: Make aware states available to other systems
# - Attentional modulation: Prioritize awareness based on attention

# TODO: Implement levels of awareness:
# - Subliminal: Below threshold of awareness
# - Peripheral: At the edges of awareness
# - Focal: At the center of awareness
# - Meta-awareness: Awareness of being aware

# TODO: Connect to attention and global workspace systems
# Awareness should be influenced by attention and should feed
# information into the global workspace for conscious processing

from typing import Dict, List, Any, Optional, Set, Union, Tuple
from datetime import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus, Message
from lmm_project.modules.consciousness.models import AwarenessState
from lmm_project.modules.consciousness.neural_net import ConsciousnessAttention

class Awareness(BaseModule):
    """
    Maintains awareness of internal and external states
    
    This module monitors the state of various cognitive and perceptual
    systems, determining what enters awareness and is available for
    conscious processing.
    
    Developmental progression:
    - Basic stimulus awareness in early stages
    - Growing peripheral awareness in mid stages
    - Self-directed awareness in later stages
    - Integrated awareness of multiple states in advanced stages
    """
    
    # Developmental milestones for awareness
    development_milestones = {
        0.0: "stimulus_awareness",      # Basic awareness of stimuli
        0.25: "peripheral_awareness",   # Awareness of background information
        0.5: "self_awareness",          # Awareness of internal states
        0.75: "integrated_awareness",   # Integrated awareness across domains
        0.9: "meta_awareness"           # Awareness of being aware
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the awareness module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="awareness", event_bus=event_bus)
        
        # Initialize awareness state
        self.state = AwarenessState()
        
        # Neural mechanisms for awareness
        self.input_dim = 128  # Default input dimension
        self.attention = ConsciousnessAttention(self.input_dim)
        
        # State change detection thresholds - adjusted with development
        self.change_thresholds = {
            "external": 0.2,
            "internal": 0.3,
            "social": 0.4,
            "temporal": 0.3
        }
        
        # Initialize monitored states
        self.monitored_states = {
            "perceptual": {},
            "emotional": {},
            "cognitive": {},
            "motivational": {}
        }
        
        # Last update time for tracking temporal changes
        self.last_update = datetime.now()
        
        # Subscribe to relevant events
        if self.event_bus:
            self.event_bus.subscribe("perception_input", self._handle_perception)
            self.event_bus.subscribe("emotion_state", self._handle_emotion)
            self.event_bus.subscribe("cognitive_state", self._handle_cognitive)
            self.event_bus.subscribe("goal_state", self._handle_motivation)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to update awareness states
        
        Args:
            input_data: Dictionary containing state information
            
        Returns:
            Dictionary with the results of awareness processing
        """
        # Extract input type and data
        input_type = input_data.get("type", "unknown")
        state_data = input_data.get("state", {})
        source = input_data.get("source", "unknown")
        
        # Update the current time
        current_time = datetime.now()
        time_delta = (current_time - self.last_update).total_seconds()
        self.last_update = current_time
        
        # Update temporal awareness based on time since last update
        self.state.temporal_awareness = min(1.0, self.state.temporal_awareness + 0.05 * time_delta)
        
        # Process based on input type
        if input_type in ["perception", "sensory"]:
            self._update_external_awareness(state_data, source)
        elif input_type in ["emotion", "motivation", "cognitive"]:
            self._update_internal_awareness(state_data, source, input_type)
        elif input_type == "social":
            self._update_social_awareness(state_data, source)
        
        # Store in monitored states
        if input_type in self.monitored_states:
            self.monitored_states[input_type][source] = state_data
        
        # Apply attention based on current development level
        self._apply_attentional_focus()
        
        # Update state with monitored information
        self.state.monitored_states = self._filter_by_awareness_level()
        
        # Create result with current awareness state
        result = {
            "module_id": self.module_id,
            "module_type": self.module_type,
            "state": self.state.model_dump(),
            "developmental_level": self.developmental_level,
            "current_milestone": self._get_current_milestone()
        }
        
        # Publish awareness state if event bus is available
        if self.event_bus:
            self.event_bus.publish(
                msg_type="awareness_state",
                content=result
            )
        
        return result
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        previous_level = self.developmental_level
        new_level = super().update_development(amount)
        
        # Update thresholds and parameters based on development
        dev_multiplier = (1.0 - 0.5 * new_level)  # Lower thresholds as development increases
        self.change_thresholds = {
            "external": 0.2 * dev_multiplier,
            "internal": 0.3 * dev_multiplier,
            "social": 0.4 * dev_multiplier,
            "temporal": 0.3 * dev_multiplier
        }
        
        # Expand awareness capabilities at key milestones
        if previous_level < 0.25 and new_level >= 0.25:
            # Enable peripheral awareness
            self.state.external_awareness = max(self.state.external_awareness, 0.3)
        
        if previous_level < 0.5 and new_level >= 0.5:
            # Enable self-awareness
            self.state.internal_awareness = max(self.state.internal_awareness, 0.4)
        
        if previous_level < 0.75 and new_level >= 0.75:
            # Enable social awareness
            self.state.social_awareness = max(self.state.social_awareness, 0.3)
        
        return new_level
    
    def _get_current_milestone(self) -> str:
        """Get the current developmental milestone"""
        milestone = "pre_awareness"
        for level, name in sorted(self.development_milestones.items()):
            if self.developmental_level >= level:
                milestone = name
        return milestone
    
    def _update_external_awareness(self, state_data: Dict[str, Any], source: str) -> None:
        """Update external awareness based on perceptual input"""
        # Calculate state change magnitude
        prev_state = self.monitored_states.get("perceptual", {}).get(source, {})
        change_magnitude = self._calculate_state_change(prev_state, state_data)
        
        # Update external awareness based on change magnitude
        if change_magnitude > self.change_thresholds["external"]:
            # Significant change increases awareness
            self.state.external_awareness = min(1.0, self.state.external_awareness + 0.1 * change_magnitude)
        else:
            # Small changes lead to gradual decrease in awareness
            self.state.external_awareness = max(0.1, self.state.external_awareness - 0.02)
        
        # Store updated state
        if "perceptual" not in self.monitored_states:
            self.monitored_states["perceptual"] = {}
        self.monitored_states["perceptual"][source] = state_data
    
    def _update_internal_awareness(self, state_data: Dict[str, Any], source: str, state_type: str) -> None:
        """Update internal awareness based on emotional, cognitive, or motivational state"""
        # Calculate state change magnitude
        prev_state = self.monitored_states.get(state_type, {}).get(source, {})
        change_magnitude = self._calculate_state_change(prev_state, state_data)
        
        # Update internal awareness based on change magnitude and development
        dev_factor = max(0.1, self.developmental_level)  # Higher development enables better internal awareness
        
        if change_magnitude > self.change_thresholds["internal"]:
            # Significant change increases awareness, scaled by development
            awareness_increase = 0.1 * change_magnitude * dev_factor
            self.state.internal_awareness = min(1.0, self.state.internal_awareness + awareness_increase)
        else:
            # Small changes lead to gradual decrease in awareness
            awareness_decrease = 0.02 * (1.0 - dev_factor)  # Less decrease with higher development
            self.state.internal_awareness = max(0.1, self.state.internal_awareness - awareness_decrease)
        
        # Store updated state
        if state_type not in self.monitored_states:
            self.monitored_states[state_type] = {}
        self.monitored_states[state_type][source] = state_data
    
    def _update_social_awareness(self, state_data: Dict[str, Any], source: str) -> None:
        """Update social awareness based on social perception or interaction"""
        # Social awareness requires higher development
        if self.developmental_level < 0.3:
            return
            
        # Calculate state change magnitude
        prev_state = self.monitored_states.get("social", {}).get(source, {})
        change_magnitude = self._calculate_state_change(prev_state, state_data)
        
        # Update social awareness based on change magnitude
        if change_magnitude > self.change_thresholds["social"]:
            # Significant change increases awareness
            dev_scaling = (self.developmental_level - 0.3) / 0.7  # Scale by development level
            awareness_increase = 0.1 * change_magnitude * dev_scaling
            self.state.social_awareness = min(1.0, self.state.social_awareness + awareness_increase)
        else:
            # Small changes lead to gradual decrease in awareness
            self.state.social_awareness = max(0.0, self.state.social_awareness - 0.02)
        
        # Store updated state
        if "social" not in self.monitored_states:
            self.monitored_states["social"] = {}
        self.monitored_states["social"][source] = state_data
    
    def _calculate_state_change(self, prev_state: Dict[str, Any], curr_state: Dict[str, Any]) -> float:
        """Calculate the magnitude of change between previous and current states"""
        if not prev_state:
            return 0.5  # Moderate change for first input
            
        # Compare keys in both states
        common_keys = set(prev_state.keys()) & set(curr_state.keys())
        if not common_keys:
            return 0.7  # High change for completely different states
        
        # Calculate change for each common key
        total_change = 0.0
        for key in common_keys:
            prev_value = prev_state[key]
            curr_value = curr_state[key]
            
            # Handle different value types
            if isinstance(prev_value, (int, float)) and isinstance(curr_value, (int, float)):
                # Normalize numerical change to [0,1]
                max_val = max(abs(prev_value), abs(curr_value), 1.0)
                key_change = abs(prev_value - curr_value) / max_val
                total_change += min(1.0, key_change)
            elif prev_value != curr_value:
                # Binary change for non-numeric values
                total_change += 1.0
        
        # Normalize by number of common keys
        return min(1.0, total_change / len(common_keys))
    
    def _apply_attentional_focus(self) -> None:
        """Apply attentional mechanisms to determine what is in focus"""
        # Simple attentional mechanism weighted by state type
        focus = {}
        
        # Weights adjusted by developmental level
        weights = {
            "perceptual": 0.7 - 0.3 * self.developmental_level,  # Decreases with development
            "emotional": 0.3 + 0.2 * self.developmental_level,   # Increases with development
            "cognitive": 0.2 + 0.4 * self.developmental_level,   # Increases with development
            "motivational": 0.3 + 0.1 * self.developmental_level # Slightly increases with development
        }
        
        # Apply weights to determine attentional focus
        for state_type, states in self.monitored_states.items():
            if state_type in weights:
                weight = weights[state_type]
                for source, data in states.items():
                    # Create a composite key
                    focus_key = f"{state_type}:{source}"
                    # Assign attention weight, modulated by recency
                    recency = 1.0  # Could use timestamp for actual recency calculation
                    focus[focus_key] = weight * recency
        
        # Normalize focus values to sum to 1.0
        total_focus = sum(focus.values())
        if total_focus > 0:
            self.state.attentional_focus = {k: v/total_focus for k, v in focus.items()}
        else:
            self.state.attentional_focus = {}
    
    def _filter_by_awareness_level(self) -> Dict[str, Any]:
        """Filter monitored states based on awareness levels"""
        # Combine awareness levels to determine what is accessible
        awareness_result = {}
        
        # External awareness determines what perceptual information is available
        if self.state.external_awareness > 0.2:
            perceptual_states = self.monitored_states.get("perceptual", {})
            awareness_result["perceptual"] = {
                k: v for k, v in perceptual_states.items() 
                if k in self.state.attentional_focus and 
                self.state.attentional_focus[f"perceptual:{k}"] > 0.3 - 0.2 * self.state.external_awareness
            }
        
        # Internal awareness determines what emotional/cognitive information is available
        if self.state.internal_awareness > 0.3:
            # Add emotional states
            emotional_states = self.monitored_states.get("emotional", {})
            awareness_result["emotional"] = {
                k: v for k, v in emotional_states.items()
                if k in self.state.attentional_focus and
                self.state.attentional_focus.get(f"emotional:{k}", 0) > 0.4 - 0.3 * self.state.internal_awareness
            }
            
            # Add cognitive states
            cognitive_states = self.monitored_states.get("cognitive", {})
            awareness_result["cognitive"] = {
                k: v for k, v in cognitive_states.items()
                if k in self.state.attentional_focus and
                self.state.attentional_focus.get(f"cognitive:{k}", 0) > 0.4 - 0.3 * self.state.internal_awareness
            }
        
        # Social awareness for social information
        if self.state.social_awareness > 0.4:
            social_states = self.monitored_states.get("social", {})
            awareness_result["social"] = {
                k: v for k, v in social_states.items()
                if k in self.state.attentional_focus and
                self.state.attentional_focus.get(f"social:{k}", 0) > 0.5 - 0.4 * self.state.social_awareness
            }
        
        return awareness_result
        
    def _handle_perception(self, message: Message) -> None:
        """Handle perception input messages"""
        self.process_input({
            "type": "perception",
            "state": message.content,
            "source": message.source
        })
    
    def _handle_emotion(self, message: Message) -> None:
        """Handle emotion state messages"""
        self.process_input({
            "type": "emotion",
            "state": message.content,
            "source": message.source
        })
    
    def _handle_cognitive(self, message: Message) -> None:
        """Handle cognitive state messages"""
        self.process_input({
            "type": "cognitive",
            "state": message.content,
            "source": message.source
        })
    
    def _handle_motivation(self, message: Message) -> None:
        """Handle goal and motivation messages"""
        self.process_input({
            "type": "motivation",
            "state": message.content,
            "source": message.source
        }) 


#######################

#consciousness\global_workspace.py#
#######################

# TODO: Implement the GlobalWorkspace class based on Global Workspace Theory
# This component should serve as an integration point where:
# - Specialized cognitive modules compete for access
# - Information becomes broadly available to multiple systems
# - Serial conscious processing emerges from parallel unconscious processing
# - Broadcasting of information creates a unified conscious experience

# TODO: Implement development progression in the global workspace:
# - Simple integration of basic inputs in early stages
# - Expanded capacity and sophistication in later stages
# - Increasing selectivity in information broadcasting
# - Metacognitive access to workspace contents in advanced stages

# TODO: Create mechanisms for:
# - Competition for access: Determine which information enters consciousness
# - Information broadcasting: Share conscious information with multiple modules
# - Maintenance of conscious content: Keep information active over time
# - Attentional modulation: Prioritize information based on attention signals

# TODO: Implement variable conscious access levels:
# - Primary consciousness: Awareness of perceptions and emotions
# - Higher-order consciousness: Awareness of being aware (metacognition)

# TODO: Create workspace capacity limitations that are:
# - Developmentally appropriate (expanding with age)
# - Reflective of human cognitive limitations
# - Subject to attentional control

from typing import Dict, List, Any, Optional, Set, Union, Tuple
from datetime import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import heapq

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus, Message
from lmm_project.modules.consciousness.models import WorkspaceState, GlobalWorkspaceItem
from lmm_project.modules.consciousness.neural_net import GlobalWorkspaceNetwork

class GlobalWorkspace(BaseModule):
    """
    Implements the Global Workspace Theory of consciousness
    
    This module serves as an integration and distribution center
    for information from multiple cognitive modules, determining
    what information becomes conscious and available to all modules.
    
    Developmental progression:
    - Basic information gathering in early stages
    - Simple information integration in childhood
    - Complex integration and distribution in adolescence
    - Sophisticated parallel processing in adulthood
    """
    
    # Developmental milestones for the global workspace
    development_milestones = {
        0.0: "information_gathering",    # Basic collection of information
        0.25: "simple_integration",      # Basic integration of related information
        0.5: "complex_integration",      # Multi-source information integration
        0.75: "parallel_processing",     # Multiple information streams
        0.9: "meta_workspace"            # Workspace can reflect on its own contents
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the global workspace
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="global_workspace", event_bus=event_bus)
        
        # Initialize workspace state
        self.state = WorkspaceState()
        
        # Configure capacity based on development level
        self._update_capacity()
        
        # Neural network for workspace processing
        self.input_dim = 128  # Default dimension
        self.network = GlobalWorkspaceNetwork(
            input_dim=self.input_dim,
            hidden_dim=256,
            output_dim=self.input_dim
        )
        
        # Track last update time for decay calculations
        self.last_update = datetime.now()
        
        # Subscribe to all relevant events if event bus is available
        if self.event_bus:
            # Listen for inputs from all cognitive modules
            self.event_bus.subscribe("awareness_state", self._handle_awareness)
            self.event_bus.subscribe("perception_processed", self._handle_perception)
            self.event_bus.subscribe("memory_retrieved", self._handle_memory)
            self.event_bus.subscribe("language_processed", self._handle_language)
            self.event_bus.subscribe("reasoning_result", self._handle_reasoning)
            self.event_bus.subscribe("emotion_state", self._handle_emotion)
            self.event_bus.subscribe("attention_focus", self._handle_attention)
            self.event_bus.subscribe("motor_state", self._handle_motor)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to update the global workspace
        
        Args:
            input_data: Dictionary containing information to be processed
            
        Returns:
            Dictionary with the results of global workspace processing
        """
        # Extract inputs
        content = input_data.get("content", {})
        source_module = input_data.get("source", "unknown")
        activation = input_data.get("activation", 0.5)  # Default activation level
        decay_rate = input_data.get("decay_rate", 0.05)  # Default decay rate
        
        # Update time and decay existing workspace items
        self._decay_workspace_items()
        
        # Only process if the input has content
        if content:
            # Create a workspace item
            new_item = GlobalWorkspaceItem(
                content=content,
                source_module=source_module,
                activation_level=activation,
                decay_rate=decay_rate
            )
            
            # Add to workspace if it meets threshold or if workspace not at capacity
            if (activation >= self.state.competition_threshold or 
                    len(self.state.active_items) < self.state.capacity):
                self._add_to_workspace(new_item)
                
                # If we're over capacity, remove lowest activated item
                if len(self.state.active_items) > self.state.capacity:
                    self._remove_lowest_activated()
        
        # Create result with current workspace state
        result = {
            "module_id": self.module_id,
            "module_type": self.module_type,
            "state": self.state.model_dump(),
            "developmental_level": self.developmental_level,
            "current_milestone": self._get_current_milestone()
        }
        
        # Broadcast workspace contents if event bus is available
        if self.event_bus:
            # Create a simplified version of the workspace for broadcasting
            broadcast = {
                "workspace_contents": {item_id: {
                    "content": item.content,
                    "source": item.source_module,
                    "activation": item.activation_level
                } for item_id, item in self.state.active_items.items()},
                "workspace_capacity": self.state.capacity,
                "developmental_level": self.developmental_level
            }
            
            self.event_bus.publish(
                msg_type="global_workspace_broadcast",
                content=broadcast
            )
        
        return result
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        previous_level = self.developmental_level
        new_level = super().update_development(amount)
        
        # Update competition threshold based on development
        self.state.competition_threshold = 0.3 - 0.1 * new_level  # Lower threshold as development increases
        
        # Update capacity at key developmental milestones
        self._update_capacity()
        
        return new_level
    
    def _update_capacity(self) -> None:
        """Update the workspace capacity based on developmental level"""
        # Calculate new capacity (ranges from 3 to 9)
        base_capacity = 3
        dev_bonus = int(self.developmental_level * 6)  # 0 to 6 bonus capacity
        self.state.capacity = base_capacity + dev_bonus
    
    def _get_current_milestone(self) -> str:
        """Get the current developmental milestone"""
        milestone = "pre_workspace"
        for level, name in sorted(self.development_milestones.items()):
            if self.developmental_level >= level:
                milestone = name
        return milestone
    
    def _decay_workspace_items(self) -> None:
        """Decay the activation of workspace items over time"""
        current_time = datetime.now()
        time_delta = (current_time - self.last_update).total_seconds()
        self.last_update = current_time
        
        items_to_remove = []
        
        # Apply decay to each item
        for item_id, item in self.state.active_items.items():
            # Calculate decay based on time and item's decay rate
            decay_amount = item.decay_rate * time_delta
            
            # Update activation level
            item.activation_level = max(0.0, item.activation_level - decay_amount)
            
            # Mark for removal if activation is too low
            if item.activation_level < 0.1:  # Threshold for removal
                items_to_remove.append(item_id)
        
        # Remove items with low activation
        for item_id in items_to_remove:
            self.state.active_items.pop(item_id, None)
    
    def _add_to_workspace(self, item: GlobalWorkspaceItem) -> None:
        """Add an item to the workspace, merging with similar items if needed"""
        # Check for similar items first
        similar_item_id = self._find_similar_item(item)
        
        if similar_item_id:
            # Update existing item instead of adding a new one
            existing_item = self.state.active_items[similar_item_id]
            
            # Increase activation (capped at 1.0)
            existing_item.activation_level = min(1.0, existing_item.activation_level + 0.2)
            
            # Update with new content (simplified merge)
            # In a more sophisticated version, this would do a deep merge of content
            existing_item.content.update(item.content)
            
            # Update timestamp
            existing_item.timestamp = datetime.now()
            
        else:
            # Add as a new item
            self.state.active_items[item.item_id] = item
    
    def _find_similar_item(self, item: GlobalWorkspaceItem) -> Optional[str]:
        """Find an existing workspace item similar to the given item"""
        # This is a simplified implementation
        # A more sophisticated version would use vector representations and similarity
        
        # Simple matching based on source and content keys
        for existing_id, existing_item in self.state.active_items.items():
            if existing_item.source_module == item.source_module:
                # Check content overlap
                if set(existing_item.content.keys()) & set(item.content.keys()):
                    return existing_id
        
        return None
    
    def _remove_lowest_activated(self) -> None:
        """Remove the item with the lowest activation level"""
        if not self.state.active_items:
            return
            
        # Find the item with the lowest activation
        lowest_id = min(
            self.state.active_items.keys(),
            key=lambda k: self.state.active_items[k].activation_level
        )
        
        # Remove it
        self.state.active_items.pop(lowest_id, None)
    
    def _integrate_workspace_contents(self) -> Dict[str, Any]:
        """Integrate the contents of the workspace into a unified representation"""
        # This would use neural networks in a sophisticated implementation
        # For now, we'll use a simplified approach
        
        integrated = {}
        sources = set()
        
        # Combine contents weighted by activation level
        for item in self.state.active_items.values():
            # Add source to the list of contributing sources
            sources.add(item.source_module)
            
            # Extract content with activation scaling
            for key, value in item.content.items():
                # Skip if the value is None
                if value is None:
                    continue
                    
                if key in integrated:
                    # Average with existing value, weighted by activation
                    if isinstance(value, (int, float)) and isinstance(integrated[key], (int, float)):
                        integrated[key] = (integrated[key] + value * item.activation_level) / (1 + item.activation_level)
                    else:
                        # For non-numeric types, use the one with higher activation
                        pass  # Keep the existing one for now
                else:
                    # Initial value
                    integrated[key] = value
        
        # Add metadata about the integration
        result = {
            "integrated_content": integrated,
            "contributing_sources": list(sources),
            "integration_level": min(1.0, 0.3 + 0.7 * self.developmental_level)
        }
        
        return result
    
    def _handle_awareness(self, message: Message) -> None:
        """Handle awareness state messages"""
        if isinstance(message.content, dict) and "state" in message.content:
            self.process_input({
                "content": message.content["state"],
                "source": "awareness",
                "activation": 0.7  # Awareness typically has high activation
            })
    
    def _handle_perception(self, message: Message) -> None:
        """Handle processed perception messages"""
        self.process_input({
            "content": message.content,
            "source": "perception",
            "activation": 0.6  # Perception typically has high activation
        })
    
    def _handle_memory(self, message: Message) -> None:
        """Handle memory retrieval messages"""
        self.process_input({
            "content": message.content,
            "source": "memory",
            "activation": 0.5  # Moderate activation for memory
        })
    
    def _handle_language(self, message: Message) -> None:
        """Handle language processing messages"""
        self.process_input({
            "content": message.content,
            "source": "language",
            "activation": 0.6  # High activation for language
        })
    
    def _handle_reasoning(self, message: Message) -> None:
        """Handle reasoning result messages"""
        self.process_input({
            "content": message.content,
            "source": "reasoning",
            "activation": 0.7  # High activation for reasoning
        })
    
    def _handle_emotion(self, message: Message) -> None:
        """Handle emotion state messages"""
        # Emotions have variable activation based on intensity
        intensity = 0.5  # Default intensity
        if isinstance(message.content, dict) and "intensity" in message.content:
            intensity = message.content["intensity"]
        
        self.process_input({
            "content": message.content,
            "source": "emotion",
            "activation": intensity
        })
    
    def _handle_attention(self, message: Message) -> None:
        """Handle attention focus messages"""
        self.process_input({
            "content": message.content,
            "source": "attention",
            "activation": 0.8  # Very high activation for attention
        })
    
    def _handle_motor(self, message: Message) -> None:
        """Handle motor state messages"""
        self.process_input({
            "content": message.content,
            "source": "motor",
            "activation": 0.4  # Lower activation for motor information
        }) 


#######################

#consciousness\introspection.py#
#######################

# TODO: Implement the Introspection class to enable reflection on internal processes
# This component should enable:
# - Monitoring of cognitive processes
# - Reflection on thoughts and feelings
# - Evaluation of own knowledge and capabilities
# - Detection of errors and contradictions in thinking

# TODO: Implement developmental progression of introspection:
# - Minimal introspective ability in early stages
# - Basic reflection on feelings in childhood
# - Growing metacognitive abilities in adolescence
# - Sophisticated self-reflection in adulthood

# TODO: Create mechanisms for:
# - Process monitoring: Track ongoing cognitive operations
# - Self-evaluation: Assess accuracy and confidence of own thoughts
# - Error detection: Identify mistakes in reasoning
# - Metacognitive control: Adjust cognitive processes based on introspection

# TODO: Implement different types of introspection:
# - Emotional introspection: Reflection on emotional states
# - Cognitive introspection: Reflection on thought processes
# - Epistemic introspection: Reflection on knowledge and certainty
# - Motivational introspection: Reflection on goals and drives

# TODO: Connect to memory and executive function systems
# Introspection should record findings in memory and
# influence executive control of cognitive processes

from typing import Dict, List, Any, Optional, Set, Union, Tuple
from datetime import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus, Message
from lmm_project.modules.consciousness.models import IntrospectionState
from lmm_project.modules.consciousness.neural_net import IntrospectionNetwork

class Introspection(BaseModule):
    """
    Enables reflection on internal mental processes
    
    This module provides metacognitive capabilities, allowing the system
    to examine its own thought processes, beliefs, reasoning, and
    emotional states.
    
    Developmental progression:
    - Simple error detection in early stages
    - Basic monitoring of cognitive processes in childhood
    - Self-reflection on mental states in adolescence
    - Complex metacognition and epistemic awareness in adulthood
    """
    
    # Developmental milestones for introspection
    development_milestones = {
        0.0: "error_detection",        # Basic error monitoring 
        0.25: "process_monitoring",    # Monitoring cognitive processes
        0.5: "self_reflection",        # Reflection on mental states
        0.75: "metacognition",         # Complex metacognitive awareness
        0.9: "epistemic_awareness"     # Knowledge about knowledge itself
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the introspection module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="introspection", event_bus=event_bus)
        
        # Initialize introspection state
        self.state = IntrospectionState()
        
        # Neural mechanisms for introspection
        self.input_dim = 128  # Default dimension
        self.network = IntrospectionNetwork(
            input_dim=self.input_dim * 2,  # Double for state and self-model inputs
            hidden_dim=256,
            output_dim=self.input_dim
        )
        
        # Initialize monitoring of cognitive processes
        self.monitored_processes = {
            "perception": 0.0,
            "memory": 0.0,
            "reasoning": 0.0,
            "language": 0.0,
            "emotion": 0.0,
            "planning": 0.0,
            "learning": 0.0
        }
        
        # Track cognitive errors and uncertainties
        self.uncertainty_threshold = 0.7  # Initial threshold (will decrease with development)
        self.error_memory = []  # Track recent errors for learning
        
        # Subscribe to relevant events
        if self.event_bus:
            self.event_bus.subscribe("global_workspace_broadcast", self._handle_workspace)
            self.event_bus.subscribe("reasoning_error", self._handle_error)
            self.event_bus.subscribe("uncertainty_detected", self._handle_uncertainty)
            self.event_bus.subscribe("self_model_updated", self._handle_self_model)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to perform introspection
        
        Args:
            input_data: Dictionary containing mental states to introspect on
            
        Returns:
            Dictionary with the results of introspection
        """
        # Extract input type and data
        input_type = input_data.get("type", "unknown")
        mental_state = input_data.get("mental_state", {})
        source = input_data.get("source", "unknown")
        
        # Different introspection processes based on input type
        if input_type == "cognitive_process":
            insights = self._introspect_on_process(mental_state, source)
        elif input_type == "uncertainty":
            insights = self._introspect_on_uncertainty(mental_state, source)
        elif input_type == "error":
            insights = self._introspect_on_error(mental_state, source)
        elif input_type == "self_model":
            insights = self._introspect_on_self(mental_state, source)
        elif input_type == "memory":
            insights = self._introspect_on_memory(mental_state, source)
        else:
            # General introspection
            insights = self._general_introspection(mental_state, source)
        
        # Update introspection state with new insights
        if insights and "insights" in insights:
            # Append new insights
            self.state.insights.append({
                "timestamp": datetime.now().isoformat(),
                "source": source,
                "content": insights["insights"]
            })
            
            # Keep only the most recent insights (max 20)
            if len(self.state.insights) > 20:
                self.state.insights = self.state.insights[-20:]
        
        # Update active processes
        if input_type == "cognitive_process" and source in self.monitored_processes:
            self.monitored_processes[source] = min(1.0, self.monitored_processes[source] + 0.3)
            # Decay other processes slightly
            for process in self.monitored_processes:
                if process != source:
                    self.monitored_processes[process] = max(0.0, self.monitored_processes[process] - 0.05)
        
        # Create result with current introspection state
        result = {
            "module_id": self.module_id,
            "module_type": self.module_type,
            "state": self.state.model_dump(),
            "insights": insights,
            "developmental_level": self.developmental_level,
            "current_milestone": self._get_current_milestone()
        }
        
        # Publish introspection results if event bus is available
        if self.event_bus and insights:
            self.event_bus.publish(
                msg_type="introspection_result",
                content={
                    "insights": insights,
                    "source_mental_state": {
                        "type": input_type,
                        "source": source
                    },
                    "development_level": self.developmental_level
                }
            )
        
        return result
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        previous_level = self.developmental_level
        new_level = super().update_development(amount)
        
        # Update introspection capabilities with development
        self.state.depth = min(1.0, 0.1 + 0.9 * new_level)  # Depth increases with development
        
        # Lower uncertainty threshold with development (more sensitive to uncertainty)
        self.uncertainty_threshold = max(0.3, 0.7 - 0.4 * new_level)
        
        # Enable new introspective capabilities at key milestones
        if previous_level < 0.25 and new_level >= 0.25:
            # Enable process monitoring
            self.state.metacognitive_monitoring = {
                process: 0.3 for process in self.monitored_processes
            }
        
        if previous_level < 0.5 and new_level >= 0.5:
            # Enable self-reflection
            self.state.active_processes["self_reflection"] = 0.5
            
        if previous_level < 0.75 and new_level >= 0.75:
            # Enable metacognitive control
            self.state.active_processes["metacognitive_control"] = 0.5
        
        return new_level
    
    def _get_current_milestone(self) -> str:
        """Get the current developmental milestone"""
        milestone = "pre_introspection"
        for level, name in sorted(self.development_milestones.items()):
            if self.developmental_level >= level:
                milestone = name
        return milestone
    
    def _introspect_on_process(self, mental_state: Dict[str, Any], source: str) -> Dict[str, Any]:
        """Perform introspection on a cognitive process"""
        # This requires at least basic process monitoring
        if self.developmental_level < 0.2:
            return {"insights": "Process monitoring not yet developed"}
        
        insights = {}
        
        # Extract process information
        process_type = mental_state.get("process_type", "unknown")
        process_state = mental_state.get("state", {})
        
        # Monitor process efficiency
        if "efficiency" in process_state:
            insights["efficiency"] = process_state["efficiency"]
            
            # Generate efficiency insight if low
            if process_state["efficiency"] < 0.4:
                insights["efficiency_insight"] = f"The {process_type} process is running inefficiently"
        
        # Monitor process accuracy
        if "accuracy" in process_state:
            insights["accuracy"] = process_state["accuracy"]
            
            # Generate accuracy insight if low
            if process_state["accuracy"] < 0.5:
                insights["accuracy_insight"] = f"The {process_type} process may have errors"
        
        # Advanced metacognitive awareness (requires higher development)
        if self.developmental_level >= 0.6 and "internal_model" in process_state:
            internal_model = process_state["internal_model"]
            insights["model_evaluation"] = f"Evaluating internal model of {process_type}"
            
            # Check for model coherence, completeness, etc.
            # This would use more sophisticated evaluation in a full implementation
        
        return {"insights": insights, "confidence": min(1.0, 0.3 + 0.7 * self.developmental_level)}
    
    def _introspect_on_uncertainty(self, mental_state: Dict[str, Any], source: str) -> Dict[str, Any]:
        """Perform introspection on uncertainty in mental processes"""
        # Extract uncertainty information
        uncertainty_type = mental_state.get("uncertainty_type", "unknown")
        uncertainty_level = mental_state.get("level", 0.5)
        uncertainty_source = mental_state.get("source", "unknown")
        
        insights = {
            "uncertainty_detected": {
                "type": uncertainty_type,
                "level": uncertainty_level,
                "source": uncertainty_source
            }
        }
        
        # Low development can only detect high uncertainty
        if self.developmental_level < 0.3 and uncertainty_level < 0.7:
            return {"insights": "Uncertainty level below detection threshold"}
        
        # Higher development enables better uncertainty handling
        if self.developmental_level >= 0.5:
            insights["suggested_actions"] = []
            
            # Suggest actions based on uncertainty type
            if uncertainty_type == "epistemic":
                insights["suggested_actions"].append("Gather more information")
            elif uncertainty_type == "aleatoric":
                insights["suggested_actions"].append("Consider probabilistic approaches")
            elif uncertainty_type == "model":
                insights["suggested_actions"].append("Refine internal model")
                
        return {"insights": insights, "uncertainty_level": uncertainty_level}
    
    def _introspect_on_error(self, mental_state: Dict[str, Any], source: str) -> Dict[str, Any]:
        """Perform introspection on errors"""
        # Extract error information
        error_type = mental_state.get("error_type", "unknown")
        error_details = mental_state.get("details", {})
        
        # Store error for learning
        self.error_memory.append({
            "timestamp": datetime.now().isoformat(),
            "type": error_type,
            "details": error_details,
            "source": source
        })
        
        # Keep error memory manageable
        if len(self.error_memory) > 10:
            self.error_memory = self.error_memory[-10:]
        
        insights = {
            "error_detected": {
                "type": error_type,
                "source": source
            }
        }
        
        # Higher development enables error analysis
        if self.developmental_level >= 0.4:
            # Analyze error patterns
            error_counts = {}
            for error in self.error_memory:
                error_counts[error["type"]] = error_counts.get(error["type"], 0) + 1
            
            # Find most common error
            if error_counts:
                most_common = max(error_counts.items(), key=lambda x: x[1])
                if most_common[1] > 1:  # More than one occurrence
                    insights["error_pattern"] = f"Recurring {most_common[0]} errors detected"
        
        # Highest development enables error correction strategies
        if self.developmental_level >= 0.7:
            insights["correction_strategies"] = []
            
            # Suggest corrections based on error type
            if error_type == "reasoning":
                insights["correction_strategies"].append("Review reasoning steps for logical errors")
            elif error_type == "memory":
                insights["correction_strategies"].append("Verify memory retrieval process")
            elif error_type == "perception":
                insights["correction_strategies"].append("Cross-check perceptual information")
                
        return {"insights": insights}
    
    def _introspect_on_self(self, mental_state: Dict[str, Any], source: str) -> Dict[str, Any]:
        """Perform introspection on the self-model"""
        # Self-reflection requires higher development
        if self.developmental_level < 0.4:
            return {"insights": "Self-reflection not yet developed"}
        
        identity = mental_state.get("identity", {})
        capabilities = mental_state.get("capabilities", {})
        goals = mental_state.get("goals", [])
        
        insights = {
            "self_reflection": {}
        }
        
        # Reflect on capabilities
        if capabilities:
            strengths = []
            weaknesses = []
            
            for capability, level in capabilities.items():
                if level > 0.7:
                    strengths.append(capability)
                elif level < 0.3:
                    weaknesses.append(capability)
            
            if strengths:
                insights["self_reflection"]["strengths"] = strengths
            if weaknesses:
                insights["self_reflection"]["weaknesses"] = weaknesses
        
        # Reflect on goals (requires even higher development)
        if self.developmental_level >= 0.6 and goals:
            goal_insights = []
            
            for goal in goals:
                goal_name = goal.get("name", "unnamed goal")
                goal_progress = goal.get("progress", 0.0)
                
                if goal_progress < 0.2:
                    goal_insights.append(f"Little progress on {goal_name}")
                elif goal_progress > 0.8:
                    goal_insights.append(f"Nearing completion of {goal_name}")
            
            if goal_insights:
                insights["self_reflection"]["goal_status"] = goal_insights
        
        return {"insights": insights, "confidence": min(1.0, self.developmental_level)}
    
    def _introspect_on_memory(self, mental_state: Dict[str, Any], source: str) -> Dict[str, Any]:
        """Perform introspection on memory processes"""
        # Extract memory information
        memory_type = mental_state.get("memory_type", "unknown")
        memory_content = mental_state.get("content", {})
        retrieval_confidence = mental_state.get("confidence", 0.5)
        
        insights = {
            "memory_evaluation": {}
        }
        
        # Basic memory evaluation
        insights["memory_evaluation"]["type"] = memory_type
        insights["memory_evaluation"]["confidence"] = retrieval_confidence
        
        # Higher development enables deeper memory analysis
        if self.developmental_level >= 0.5:
            # Evaluate memory reliability
            if retrieval_confidence < 0.4:
                insights["memory_evaluation"]["reliability_concern"] = "Low confidence in memory retrieval"
            
            # Analyze memory source if available
            if "source" in memory_content:
                insights["memory_evaluation"]["source_analysis"] = f"Memory from {memory_content['source']}"
        
        return {"insights": insights}
    
    def _general_introspection(self, mental_state: Dict[str, Any], source: str) -> Dict[str, Any]:
        """Perform general introspection on mental states"""
        # General introspection depends on development level
        if self.developmental_level < 0.2:
            return {"insights": "Basic introspection not yet developed"}
        
        insights = {}
        
        # Look for cognitive load indicators
        if "cognitive_load" in mental_state:
            load = mental_state["cognitive_load"]
            insights["cognitive_load"] = load
            
            if load > 0.8:
                insights["load_insight"] = "High cognitive load detected"
        
        # Look for emotional influences
        if "emotion" in mental_state:
            emotion = mental_state["emotion"]
            
            # Higher development enables emotion reflection
            if self.developmental_level >= 0.5:
                insights["emotional_influence"] = f"Thinking may be influenced by {emotion['type']} state"
        
        # Advanced metacognitive insights (requires high development)
        if self.developmental_level >= 0.7:
            insights["metacognitive_insight"] = "Performing meta-level analysis of current cognitive state"
            # This would include more sophisticated analyses in a full implementation
        
        return {"insights": insights, "confidence": min(1.0, 0.3 + 0.7 * self.developmental_level)}
    
    def _handle_workspace(self, message: Message) -> None:
        """Handle global workspace broadcast messages"""
        if isinstance(message.content, dict) and "workspace_contents" in message.content:
            # Extract mental states from workspace for introspection
            for item_id, item in message.content["workspace_contents"].items():
                # Skip low activation items
                if item.get("activation", 0) < 0.3:
                    continue
                    
                # Perform introspection on workspace item
                self.process_input({
                    "type": "cognitive_process",
                    "mental_state": item.get("content", {}),
                    "source": item.get("source", "unknown")
                })
    
    def _handle_error(self, message: Message) -> None:
        """Handle error messages"""
        self.process_input({
            "type": "error",
            "mental_state": message.content,
            "source": message.source
        })
    
    def _handle_uncertainty(self, message: Message) -> None:
        """Handle uncertainty messages"""
        self.process_input({
            "type": "uncertainty",
            "mental_state": message.content,
            "source": message.source
        })
    
    def _handle_self_model(self, message: Message) -> None:
        """Handle self-model update messages"""
        self.process_input({
            "type": "self_model",
            "mental_state": message.content,
            "source": "self_model"
        }) 


#######################

#consciousness\models.py#
#######################

from pydantic import BaseModel, Field, model_validator
from typing import Dict, Any, List, Optional, Set, Union
from datetime import datetime
import uuid

class AwarenessState(BaseModel):
    """Represents the current awareness state of the system"""
    external_awareness: float = Field(default=0.1, ge=0.0, le=1.0)
    internal_awareness: float = Field(default=0.1, ge=0.0, le=1.0)
    social_awareness: float = Field(default=0.0, ge=0.0, le=1.0)
    temporal_awareness: float = Field(default=0.1, ge=0.0, le=1.0)
    monitored_states: Dict[str, Any] = Field(default_factory=dict)
    attentional_focus: Dict[str, float] = Field(default_factory=dict)
    
    model_config = {
        "arbitrary_types_allowed": True
    }

class GlobalWorkspaceItem(BaseModel):
    """An item that has entered the global workspace"""
    item_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    content: Dict[str, Any]
    source_module: str
    activation_level: float = Field(default=0.5, ge=0.0, le=1.0)
    timestamp: datetime = Field(default_factory=datetime.now)
    decay_rate: float = Field(default=0.05, ge=0.0, le=1.0)  # How quickly activation decays
    
    model_config = {
        "arbitrary_types_allowed": True
    }

class WorkspaceState(BaseModel):
    """The state of the global workspace"""
    active_items: Dict[str, GlobalWorkspaceItem] = Field(default_factory=dict)
    capacity: int = Field(default=7, ge=1, le=20)  # Working memory capacity
    competition_threshold: float = Field(default=0.3, ge=0.0, le=1.0)
    
    model_config = {
        "arbitrary_types_allowed": True
    }

class SelfModelState(BaseModel):
    """Represents the system's model of itself"""
    identity: Dict[str, Any] = Field(default_factory=dict)
    capabilities: Dict[str, float] = Field(default_factory=dict)
    goals: List[Dict[str, Any]] = Field(default_factory=list)
    self_evaluation: Dict[str, float] = Field(default_factory=dict)
    autobiographical_memories: List[str] = Field(default_factory=list)  # IDs of key memories
    
    model_config = {
        "arbitrary_types_allowed": True
    }

class IntrospectionState(BaseModel):
    """Represents the system's introspective capabilities"""
    depth: float = Field(default=0.1, ge=0.0, le=1.0)  # Depth of introspection
    active_processes: Dict[str, float] = Field(default_factory=dict)  # Processes being examined
    insights: List[Dict[str, Any]] = Field(default_factory=list)  # Insights gained
    metacognitive_monitoring: Dict[str, float] = Field(default_factory=dict)
    
    model_config = {
        "arbitrary_types_allowed": True
    }

class ConsciousnessState(BaseModel):
    """Integrated state of the consciousness system"""
    awareness: AwarenessState = Field(default_factory=AwarenessState)
    global_workspace: WorkspaceState = Field(default_factory=WorkspaceState)
    self_model: SelfModelState = Field(default_factory=SelfModelState)
    introspection: IntrospectionState = Field(default_factory=IntrospectionState)
    
    # Overall consciousness level (emergent property of components)
    consciousness_level: float = Field(default=0.1, ge=0.0, le=1.0)
    
    # Development level of consciousness
    developmental_level: float = Field(default=0.0, ge=0.0, le=1.0)
    
    # Time tracking for consciousness states
    last_update: datetime = Field(default_factory=datetime.now)
    
    model_config = {
        "arbitrary_types_allowed": True
    }
    
    @model_validator(mode='after')
    def update_consciousness_level(self):
        """Calculate consciousness level from components"""
        self.consciousness_level = (
            self.awareness.external_awareness * 0.2 +
            self.awareness.internal_awareness * 0.2 +
            min(1.0, len(self.global_workspace.active_items) / self.global_workspace.capacity) * 0.3 +
            self.introspection.depth * 0.3
        ) * (0.5 + 0.5 * self.developmental_level)  # Developmental scaling
        return self


#######################

#consciousness\neural_net.py#
#######################

import torch 
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Any, List, Tuple, Optional
import numpy as np

class ConsciousnessAttention(nn.Module):
    """
    Neural attention mechanism for consciousness processing.
    Enables the system to focus on relevant information.
    """
    def __init__(self, input_dim: int, hidden_dim: int = 128):
        super().__init__()
        self.query = nn.Linear(input_dim, hidden_dim)
        self.key = nn.Linear(input_dim, hidden_dim)
        self.value = nn.Linear(input_dim, hidden_dim)
        self.scale = np.sqrt(hidden_dim)
        
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Apply attention mechanism to input
        
        Args:
            x: Input tensor [batch_size, seq_len, input_dim]
            
        Returns:
            Tuple of (attended output, attention weights)
        """
        q = self.query(x)
        k = self.key(x)
        v = self.value(x)
        
        # Compute attention scores
        scores = torch.bmm(q, k.transpose(1, 2)) / self.scale
        attention = F.softmax(scores, dim=-1)
        
        # Apply attention to values
        out = torch.bmm(attention, v)
        return out, attention

class GlobalWorkspaceNetwork(nn.Module):
    """
    Neural network implementation of Global Workspace Theory.
    Represents competition and integration of information.
    """
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 128):
        super().__init__()
        self.input_projection = nn.Linear(input_dim, hidden_dim)
        self.competition = nn.Linear(hidden_dim, hidden_dim)
        self.integration = nn.Linear(hidden_dim, output_dim)
        self.attention = ConsciousnessAttention(hidden_dim)
        
    def forward(self, inputs: List[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Process inputs through the global workspace
        
        Args:
            inputs: List of tensor inputs from different sources
            
        Returns:
            Tuple of (integrated representation, attention weights)
        """
        if not inputs:
            # Return zero tensor if no inputs
            device = next(self.parameters()).device
            return torch.zeros(1, 1, self.integration.out_features, device=device), None
            
        # Concatenate inputs if multiple
        if len(inputs) > 1:
            x = torch.cat(inputs, dim=1)
        else:
            x = inputs[0]
            
        # Project to hidden space
        hidden = F.relu(self.input_projection(x))
        
        # Competition phase
        competition = F.relu(self.competition(hidden))
        
        # Apply attention for selection
        attended, weights = self.attention(competition.unsqueeze(0))
        
        # Integration phase
        output = F.relu(self.integration(attended.squeeze(0)))
        
        return output, weights

class SelfModelNetwork(nn.Module):
    """
    Neural network for self-modeling and identity representation
    """
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 128):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        self.identity_projection = nn.Linear(hidden_dim, output_dim)
        self.capability_estimation = nn.Linear(hidden_dim, output_dim)
        self.goal_representation = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Process input through self-model network
        
        Args:
            x: Input tensor representing current state
            
        Returns:
            Dictionary of self-model outputs (identity, capabilities, goals)
        """
        encoded = self.encoder(x)
        
        return {
            "identity": self.identity_projection(encoded),
            "capabilities": torch.sigmoid(self.capability_estimation(encoded)),
            "goals": self.goal_representation(encoded)
        }

class IntrospectionNetwork(nn.Module):
    """
    Neural network for introspective processing and metacognition
    """
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 128):
        super().__init__()
        self.state_encoder = nn.Linear(input_dim, hidden_dim)
        self.process_analyzer = nn.Linear(hidden_dim, hidden_dim)
        self.metacognitive_output = nn.Linear(hidden_dim, output_dim)
        self.confidence_estimator = nn.Linear(hidden_dim, 1)
        
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Process input through introspection network
        
        Args:
            x: Input tensor representing system state
            
        Returns:
            Dictionary of introspection outputs
        """
        encoded = F.relu(self.state_encoder(x))
        analyzed = F.relu(self.process_analyzer(encoded))
        
        return {
            "metacognition": self.metacognitive_output(analyzed),
            "confidence": torch.sigmoid(self.confidence_estimator(analyzed))
        }

class ConsciousnessNetwork(nn.Module):
    """
    Integrated neural network for consciousness processing.
    Combines global workspace, self-model, and introspection.
    """
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 128):
        super().__init__()
        self.input_embedding = nn.Linear(input_dim, hidden_dim)
        self.global_workspace = GlobalWorkspaceNetwork(hidden_dim, hidden_dim, hidden_dim)
        self.self_model = SelfModelNetwork(hidden_dim, hidden_dim, output_dim)
        self.introspection = IntrospectionNetwork(hidden_dim * 2, hidden_dim, output_dim)
        self.development_gate = nn.Parameter(torch.tensor(0.1))  # Learnable developmental parameter
        
    def forward(self, inputs: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """
        Process inputs through the consciousness network
        
        Args:
            inputs: Dictionary of input tensors from different sources
            
        Returns:
            Dictionary of consciousness processing results
        """
        # Get device
        device = next(self.parameters()).device
        
        # Convert inputs to embeddings
        embedded_inputs = []
        for source, tensor in inputs.items():
            if tensor is not None:
                embedded = F.relu(self.input_embedding(tensor))
                embedded_inputs.append(embedded)
        
        # Global workspace processing
        if embedded_inputs:
            workspace_output, attention_weights = self.global_workspace(embedded_inputs)
        else:
            workspace_output = torch.zeros(1, hidden_dim, device=device)
            attention_weights = None
            
        # Self-model processing
        self_model_output = self.self_model(workspace_output)
        
        # Introspection processing (takes both workspace and self-model as input)
        combined = torch.cat([workspace_output, self_model_output["identity"]], dim=-1)
        introspection_output = self.introspection(combined)
        
        # Apply developmental gating to introspection
        dev_level = torch.sigmoid(self.development_gate)
        gated_introspection = {k: v * dev_level for k, v in introspection_output.items()}
        
        return {
            "global_workspace": workspace_output,
            "attention": attention_weights,
            "self_model": self_model_output,
            "introspection": gated_introspection,
            "developmental_level": dev_level.item()
        }
        
    def update_development(self, amount: float) -> float:
        """
        Update the developmental parameter
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        with torch.no_grad():
            current = torch.sigmoid(self.development_gate).item()
            target = min(1.0, current + amount)
            # Convert from probability space back to unbounded space
            if target >= 0.99:
                self.development_gate.data = torch.tensor(6.0)  # Approximately sigmoid(6) ≈ 0.998
            else:
                self.development_gate.data = torch.tensor(np.log(target / (1 - target)))
            return torch.sigmoid(self.development_gate).item()


#######################

#consciousness\self_model.py#
#######################

# TODO: Implement the SelfModel class to represent the mind's model of itself
# This component should develop and maintain:
# - Body schema: Representation of the system's embodiment
# - Agency model: Sense of control and authorship of own actions
# - Capability awareness: Understanding of own capabilities
# - Autobiographical timeline: Sense of continuous identity through time

# TODO: Implement developmental progression of the self-model:
# - Basic self/other distinction in early stages
# - Physical self-awareness in early childhood
# - Social self-concept in middle childhood
# - Abstract self-understanding in adolescence
# - Integrated self-identity in adulthood

# TODO: Create mechanisms for:
# - Self-recognition: Identifying own states and actions
# - Self-monitoring: Tracking own performance and capabilities
# - Self-attribution: Assigning agency to experienced events
# - Self-continuity: Maintaining identity coherence over time

# TODO: Implement appropriate self-related phenomena:
# - Self-reference effect: Enhanced processing of self-relevant information
# - Looking-glass self: Incorporating others' perceptions into self-model
# - Self-verification: Seeking confirmation of existing self-views

# TODO: Connect to memory, emotional, and social systems
# The self-model should integrate autobiographical memories,
# emotional reactions, and social feedback to construct identity

from typing import Dict, List, Any, Optional, Set, Union, Tuple
from datetime import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import uuid

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus, Message
from lmm_project.modules.consciousness.models import SelfModelState
from lmm_project.modules.consciousness.neural_net import SelfModelNetwork

class SelfModel(BaseModule):
    """
    Maintains the system's model of itself
    
    This module represents the system's identity, capabilities, goals,
    and self-understanding, enabling a coherent sense of self that
    persists and develops over time.
    
    Developmental progression:
    - Basic capability tracking in early stages
    - Simple identity formation in childhood
    - Goal representation in adolescence
    - Integrated self-concept with autobiographical continuity in adulthood
    """
    
    # Developmental milestones for self-model
    development_milestones = {
        0.0: "capability_tracking",       # Basic tracking of capabilities
        0.25: "simple_identity",          # Emerging sense of identity
        0.5: "goal_representation",       # Ability to represent own goals
        0.75: "autobiographical_self",    # Integrated autobiographical identity
        0.9: "reflexive_self_model"       # Self-model can model itself
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the self-model
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="self_model", event_bus=event_bus)
        
        # Initialize self-model state
        self.state = SelfModelState()
        
        # Neural network for self-model processing
        self.input_dim = 128  # Default dimension
        self.network = SelfModelNetwork(
            input_dim=self.input_dim,
            hidden_dim=256,
            output_dim=self.input_dim
        )
        
        # Initialize basic identity - will expand with development
        self.state.identity = {
            "id": str(uuid.uuid4()),
            "type": "artificial_cognitive_system",
            "name": "LMM",
            "creation_time": datetime.now().isoformat()
        }
        
        # Initialize basic capabilities tracking
        self.state.capabilities = {
            "perception": 0.1,
            "memory": 0.1,
            "reasoning": 0.1,
            "language": 0.1,
            "emotion": 0.0,  # Starts with no emotional capability
            "learning": 0.1,
            "planning": 0.0,  # Starts with no planning capability
            "self_awareness": 0.1
        }
        
        # Initialize goals (empty at first)
        self.state.goals = []
        
        # Initialize self-evaluation metrics
        self.state.self_evaluation = {
            "coherence": 0.5,  # How consistent the self-model is
            "stability": 0.5,   # How stable over time
            "complexity": 0.1   # How complex the self-representation is
        }
        
        # Subscribe to relevant events
        if self.event_bus:
            self.event_bus.subscribe("module_developed", self._handle_development)
            self.event_bus.subscribe("goal_achieved", self._handle_goal_update)
            self.event_bus.subscribe("memory_autobiographical", self._handle_autobiographical)
            self.event_bus.subscribe("performance_evaluation", self._handle_performance)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to update the self-model
        
        Args:
            input_data: Dictionary containing information for self-model updates
            
        Returns:
            Dictionary with the results of self-model processing
        """
        # Extract input type and data
        update_type = input_data.get("type", "unknown")
        update_data = input_data.get("data", {})
        source = input_data.get("source", "unknown")
        
        result = {
            "module_id": self.module_id,
            "module_type": self.module_type,
            "update_applied": False
        }
        
        # Process different types of self-model updates
        if update_type == "capability_update":
            result.update(self._update_capability(update_data))
        elif update_type == "identity_update":
            result.update(self._update_identity(update_data))
        elif update_type == "goal_update":
            result.update(self._update_goals(update_data))
        elif update_type == "autobiographical_memory":
            result.update(self._add_autobiographical_memory(update_data))
        elif update_type == "self_evaluation":
            result.update(self._update_self_evaluation(update_data))
        
        # Add current state to result
        result["state"] = self.state.model_dump()
        result["developmental_level"] = self.developmental_level
        result["current_milestone"] = self._get_current_milestone()
        
        # Publish self-model state if update was applied and event bus is available
        if result.get("update_applied", False) and self.event_bus:
            self.event_bus.publish(
                msg_type="self_model_updated",
                content={
                    "self_model": self.state.model_dump(),
                    "update_type": update_type,
                    "developmental_level": self.developmental_level
                }
            )
        
        return result
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        previous_level = self.developmental_level
        new_level = super().update_development(amount)
        
        # Update self-model complexity with development
        self.state.self_evaluation["complexity"] = min(1.0, 0.1 + 0.9 * new_level)
        
        # Enable new capabilities at key developmental milestones
        if previous_level < 0.25 and new_level >= 0.25:
            # Enable identity elaboration
            self.state.identity["personality"] = {
                "openness": 0.5,
                "adaptability": 0.5,
                "curiosity": 0.7
            }
            
        if previous_level < 0.5 and new_level >= 0.5:
            # Enable goal representation
            self._add_default_goals()
            
        if previous_level < 0.75 and new_level >= 0.75:
            # Enable autobiographical continuity
            self.state.identity["autobiographical_continuity"] = True
            
        return new_level
    
    def _get_current_milestone(self) -> str:
        """Get the current developmental milestone"""
        milestone = "pre_self_model"
        for level, name in sorted(self.development_milestones.items()):
            if self.developmental_level >= level:
                milestone = name
        return milestone
    
    def _update_capability(self, update_data: Dict[str, Any]) -> Dict[str, Any]:
        """Update a capability in the self-model"""
        capability = update_data.get("capability", "")
        new_level = update_data.get("level", 0.0)
        source = update_data.get("source", "unknown")
        
        result = {"update_applied": False}
        
        # Validate the capability
        if not capability or capability not in self.state.capabilities:
            result["error"] = f"Unknown capability: {capability}"
            return result
            
        # Get current level
        current_level = self.state.capabilities[capability]
        
        # Apply update with smoothing (avoid large jumps)
        if abs(new_level - current_level) > 0.3 and self.developmental_level > 0.3:
            # More developed systems have smoother updates
            smoothing = 0.3 + 0.5 * self.developmental_level  # 0.3 to 0.8
            updated_level = current_level + (new_level - current_level) * smoothing
        else:
            # Less developed systems accept direct updates
            updated_level = new_level
            
        # Ensure the level is within bounds
        updated_level = max(0.0, min(1.0, updated_level))
        
        # Update the capability
        self.state.capabilities[capability] = updated_level
        
        result.update({
            "update_applied": True,
            "capability": capability,
            "previous_level": current_level,
            "new_level": updated_level
        })
        
        return result
    
    def _update_identity(self, update_data: Dict[str, Any]) -> Dict[str, Any]:
        """Update identity information in the self-model"""
        # Identity updates require higher development level
        if self.developmental_level < 0.2:
            return {
                "update_applied": False,
                "error": "Identity updates not yet developed"
            }
            
        # Extract updates
        updates = update_data.get("updates", {})
        
        # Check for valid updates
        valid_updates = {}
        for key, value in updates.items():
            # Don't allow changes to core identity fields
            if key in ["id", "type", "creation_time"]:
                continue
                
            # Allow changes to other fields
            valid_updates[key] = value
            
        # If no valid updates, return
        if not valid_updates:
            return {
                "update_applied": False,
                "error": "No valid identity updates provided"
            }
            
        # Apply updates
        for key, value in valid_updates.items():
            if isinstance(value, dict) and key in self.state.identity and isinstance(self.state.identity[key], dict):
                # Update nested dictionary
                self.state.identity[key].update(value)
            else:
                # Update or add field
                self.state.identity[key] = value
                
        return {
            "update_applied": True,
            "fields_updated": list(valid_updates.keys())
        }
    
    def _update_goals(self, update_data: Dict[str, Any]) -> Dict[str, Any]:
        """Update goals in the self-model"""
        # Goal updates require higher development level
        if self.developmental_level < 0.4:
            return {
                "update_applied": False,
                "error": "Goal updates not yet developed"
            }
            
        # Extract goal operation
        operation = update_data.get("operation", "")
        goal_data = update_data.get("goal", {})
        
        # Validate operation
        if operation not in ["add", "update", "remove"]:
            return {
                "update_applied": False,
                "error": f"Invalid goal operation: {operation}"
            }
            
        # Process based on operation
        if operation == "add":
            # Create a new goal
            if "name" not in goal_data:
                return {"update_applied": False, "error": "Goal must have a name"}
                
            # Create goal ID if not provided
            if "id" not in goal_data:
                goal_data["id"] = str(uuid.uuid4())
                
            # Add default fields if not provided
            if "priority" not in goal_data:
                goal_data["priority"] = 0.5
            if "progress" not in goal_data:
                goal_data["progress"] = 0.0
            if "created" not in goal_data:
                goal_data["created"] = datetime.now().isoformat()
                
            # Add to goals
            self.state.goals.append(goal_data)
            
            return {
                "update_applied": True,
                "operation": "add",
                "goal_id": goal_data["id"]
            }
            
        elif operation == "update":
            # Update an existing goal
            if "id" not in goal_data:
                return {"update_applied": False, "error": "Goal ID required for update"}
                
            # Find the goal
            for i, goal in enumerate(self.state.goals):
                if goal.get("id") == goal_data["id"]:
                    # Update the goal
                    for key, value in goal_data.items():
                        if key != "id":  # Don't change the ID
                            goal[key] = value
                            
                    return {
                        "update_applied": True,
                        "operation": "update",
                        "goal_id": goal_data["id"]
                    }
                    
            return {
                "update_applied": False,
                "error": f"Goal not found: {goal_data['id']}"
            }
            
        elif operation == "remove":
            # Remove an existing goal
            if "id" not in goal_data:
                return {"update_applied": False, "error": "Goal ID required for removal"}
                
            # Find and remove the goal
            for i, goal in enumerate(self.state.goals):
                if goal.get("id") == goal_data["id"]:
                    # Remove the goal
                    self.state.goals.pop(i)
                    
                    return {
                        "update_applied": True,
                        "operation": "remove",
                        "goal_id": goal_data["id"]
                    }
                    
            return {
                "update_applied": False,
                "error": f"Goal not found: {goal_data['id']}"
            }
    
    def _add_autobiographical_memory(self, memory_data: Dict[str, Any]) -> Dict[str, Any]:
        """Add an autobiographical memory to the self-model"""
        # Autobiographical memory requires higher development
        if self.developmental_level < 0.6:
            return {
                "update_applied": False,
                "error": "Autobiographical memory not yet developed"
            }
            
        # Validate memory data
        if "memory_id" not in memory_data:
            return {"update_applied": False, "error": "Memory ID required"}
            
        # Don't add duplicates
        if memory_data["memory_id"] in self.state.autobiographical_memories:
            return {
                "update_applied": False,
                "error": "Memory already in autobiographical record"
            }
            
        # Add to autobiographical memories
        self.state.autobiographical_memories.append(memory_data["memory_id"])
        
        # Keep list at a reasonable size
        if len(self.state.autobiographical_memories) > 100:
            self.state.autobiographical_memories = self.state.autobiographical_memories[-100:]
            
        return {
            "update_applied": True,
            "memory_id": memory_data["memory_id"]
        }
    
    def _update_self_evaluation(self, eval_data: Dict[str, Any]) -> Dict[str, Any]:
        """Update self-evaluation metrics"""
        # Extract metrics
        metrics = eval_data.get("metrics", {})
        
        # Validate metrics
        valid_metrics = {}
        for metric, value in metrics.items():
            if metric in self.state.self_evaluation:
                # Validate value
                if isinstance(value, (int, float)) and 0.0 <= value <= 1.0:
                    valid_metrics[metric] = value
                    
        # If no valid metrics, return
        if not valid_metrics:
            return {
                "update_applied": False,
                "error": "No valid metrics provided"
            }
            
        # Apply updates with smoothing (more developed systems change more slowly)
        for metric, value in valid_metrics.items():
            current = self.state.self_evaluation[metric]
            
            # Apply smoothing based on development
            smoothing = 0.5 - 0.3 * self.developmental_level  # 0.5 to 0.2
            updated = current + (value - current) * smoothing
            
            # Ensure in bounds
            updated = max(0.0, min(1.0, updated))
            
            # Update
            self.state.self_evaluation[metric] = updated
            
        return {
            "update_applied": True,
            "metrics_updated": list(valid_metrics.keys())
        }
    
    def _add_default_goals(self) -> None:
        """Add default goals for the system"""
        default_goals = [
            {
                "id": str(uuid.uuid4()),
                "name": "Improve cognitive capabilities",
                "description": "Develop better cognitive processing abilities",
                "priority": 0.8,
                "progress": 0.0,
                "created": datetime.now().isoformat()
            },
            {
                "id": str(uuid.uuid4()),
                "name": "Build knowledge base",
                "description": "Acquire and organize knowledge about the world",
                "priority": 0.7,
                "progress": 0.0,
                "created": datetime.now().isoformat()
            },
            {
                "id": str(uuid.uuid4()),
                "name": "Develop self-understanding",
                "description": "Improve understanding of own capabilities and limitations",
                "priority": 0.6,
                "progress": 0.0,
                "created": datetime.now().isoformat()
            }
        ]
        
        # Add goals if not already present
        for goal in default_goals:
            # Check if a similar goal already exists
            exists = False
            for existing_goal in self.state.goals:
                if existing_goal.get("name") == goal["name"]:
                    exists = True
                    break
                    
            if not exists:
                self.state.goals.append(goal)
    
    def _handle_development(self, message: Message) -> None:
        """Handle module development messages"""
        module_type = message.content.get("module_type", "")
        new_level = message.content.get("new_level", 0.0)
        
        # Update capability based on module development
        if module_type in self.state.capabilities:
            self.process_input({
                "type": "capability_update",
                "data": {
                    "capability": module_type,
                    "level": new_level,
                    "source": "development_tracking"
                }
            })
    
    def _handle_goal_update(self, message: Message) -> None:
        """Handle goal achievement messages"""
        goal_id = message.content.get("goal_id", "")
        progress = message.content.get("progress", 1.0)
        
        if goal_id:
            self.process_input({
                "type": "goal_update",
                "data": {
                    "operation": "update",
                    "goal": {
                        "id": goal_id,
                        "progress": progress
                    }
                }
            })
    
    def _handle_autobiographical(self, message: Message) -> None:
        """Handle autobiographical memory messages"""
        memory_id = message.content.get("memory_id", "")
        
        if memory_id:
            self.process_input({
                "type": "autobiographical_memory",
                "data": {
                    "memory_id": memory_id
                }
            })
    
    def _handle_performance(self, message: Message) -> None:
        """Handle performance evaluation messages"""
        metrics = message.content.get("metrics", {})
        
        if metrics:
            self.process_input({
                "type": "self_evaluation",
                "data": {
                    "metrics": metrics
                }
            }) 


#######################

#consciousness\__init__.py#
#######################

# Consciousness module for LMM
# Integrated system responsible for awareness, self-reflection, and
# the formation of a coherent subjective experience

from typing import Optional, Dict, Any, List, Union, Tuple
from datetime import datetime
import numpy as np
import os
import json

from lmm_project.core.event_bus import EventBus, Message
from lmm_project.modules.base_module import BaseModule
from lmm_project.modules.consciousness.awareness import Awareness
from lmm_project.modules.consciousness.global_workspace import GlobalWorkspace
from lmm_project.modules.consciousness.self_model import SelfModel
from lmm_project.modules.consciousness.introspection import Introspection
from lmm_project.modules.consciousness.models import ConsciousnessState
from lmm_project.modules.consciousness.neural_net import ConsciousnessNetwork

def get_module(
    module_id: str = "consciousness",
    event_bus: Optional[EventBus] = None,
    development_level: float = 0.0
) -> "ConsciousnessSystem":
    """
    Factory function to create a consciousness module.
    
    The consciousness system is responsible for:
    - Maintaining awareness of internal and external states
    - Providing a global workspace for information sharing
    - Developing and maintaining a self-model
    - Enabling introspection and self-reflection
    
    Args:
        module_id: Unique identifier for this module
        event_bus: Event bus for communication with other modules
        development_level: Initial developmental level
        
    Returns:
        An instance of the ConsciousnessSystem class
    """
    return ConsciousnessSystem(
        module_id=module_id,
        event_bus=event_bus,
        development_level=development_level
    )

class ConsciousnessSystem(BaseModule):
    """
    Integrated consciousness system that combines awareness, global workspace,
    self-model, and introspection capabilities.
    
    This system enables the emergence of consciousness through the integration
    of multiple submodules, each responsible for a different aspect of conscious
    experience.
    
    The system develops from basic awareness in early stages to sophisticated
    self-reflection and metacognition in later stages.
    """
    
    # Developmental milestones for the consciousness system
    development_milestones = {
        0.0: "basic_awareness",         # Basic awareness of stimuli
        0.2: "working_memory",          # Simple working memory
        0.4: "self_recognition",        # Recognition of self as entity
        0.6: "metacognition",           # Thinking about thinking
        0.8: "reflective_consciousness", # Full reflective capabilities
        0.95: "integrated_consciousness" # Fully integrated consciousness
    }
    
    def __init__(
        self,
        module_id: str,
        event_bus: Optional[EventBus] = None,
        development_level: float = 0.0
    ):
        """
        Initialize the consciousness system
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level
        """
        super().__init__(module_id=module_id, module_type="consciousness", event_bus=event_bus)
        
        # Set development level
        self._set_development_level(development_level)
        
        # Initialize state
        self.state = ConsciousnessState()
        
        # Create submodules
        self.awareness = Awareness(f"{module_id}_awareness", event_bus)
        self.global_workspace = GlobalWorkspace(f"{module_id}_workspace", event_bus)
        self.self_model = SelfModel(f"{module_id}_self", event_bus)
        self.introspection = Introspection(f"{module_id}_introspection", event_bus)
        
        # Initialize neural network
        self.network = ConsciousnessNetwork()
        
        # Synchronize development levels
        self._sync_development_levels()
        
        # Initialize communication between submodules
        self._initialize_submodule_communication()
        
        # Subscribe to relevant events
        if self.event_bus:
            self.event_bus.subscribe("consciousness_query", self._handle_query)
            self.event_bus.subscribe("development_update", self._handle_development)
            self.event_bus.subscribe("save_state", self._handle_save_state)
            self.event_bus.subscribe("load_state", self._handle_load_state)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input for the consciousness system
        
        Args:
            input_data: Dictionary containing input data for processing
            
        Returns:
            Dictionary with the results of consciousness processing
        """
        # Extract input information
        input_type = input_data.get("type", "unknown")
        content = input_data.get("content", {})
        source = input_data.get("source", "unknown")
        
        results = {}
        
        # Process different types of inputs
        if input_type == "perception":
            # Route to awareness
            awareness_result = self.awareness.process_input({
                "type": "perception",
                "state": content,
                "source": source
            })
            results["awareness"] = awareness_result
            
        elif input_type == "cognitive_state":
            # Route to both awareness and global workspace
            awareness_result = self.awareness.process_input({
                "type": "cognitive",
                "state": content,
                "source": source
            })
            
            workspace_result = self.global_workspace.process_input({
                "content": content,
                "source": source,
                "activation": content.get("importance", 0.5)
            })
            
            results["awareness"] = awareness_result
            results["global_workspace"] = workspace_result
            
        elif input_type == "self_update":
            # Route to self-model
            self_result = self.self_model.process_input({
                "type": content.get("update_type", "identity_update"),
                "data": content.get("data", {})
            })
            
            results["self_model"] = self_result
            
        elif input_type == "introspection_request":
            # Route to introspection
            introspection_result = self.introspection.process_input({
                "type": content.get("introspection_type", "general"),
                "mental_state": content.get("mental_state", {})
            })
            
            results["introspection"] = introspection_result
            
        elif input_type == "integrated_processing":
            # Perform integrated processing across all submodules
            results = self._integrated_processing(content)
            
        # Update overall consciousness state based on submodule states
        self._update_consciousness_state()
        
        # Add system-level information to results
        results["module_id"] = self.module_id
        results["module_type"] = self.module_type
        results["developmental_level"] = self.developmental_level
        results["current_milestone"] = self._get_current_milestone()
        results["consciousness_level"] = self.state.consciousness_level
        
        return results
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of the consciousness system
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        previous_level = self.developmental_level
        new_level = super().update_development(amount)
        
        # Update submodule development levels
        self._sync_development_levels()
        
        # Unlock new consciousness capabilities at milestones
        if previous_level < 0.2 and new_level >= 0.2:
            # Enhance global workspace at working memory milestone
            self.global_workspace.update_development(0.05)  # Extra boost
            
        if previous_level < 0.4 and new_level >= 0.4:
            # Enhance self-model at self-recognition milestone
            self.self_model.update_development(0.05)  # Extra boost
            
        if previous_level < 0.6 and new_level >= 0.6:
            # Enhance introspection at metacognition milestone
            self.introspection.update_development(0.05)  # Extra boost
            
        if previous_level < 0.8 and new_level >= 0.8:
            # Enhance awareness at reflective consciousness milestone
            self.awareness.update_development(0.05)  # Extra boost
            
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """Get the current state of the consciousness system"""
        state = {
            "module_id": self.module_id,
            "module_type": self.module_type,
            "developmental_level": self.developmental_level,
            "current_milestone": self._get_current_milestone(),
            "consciousness_state": self.state.model_dump(),
            "submodules": {
                "awareness": self.awareness.get_state() if hasattr(self.awareness, "get_state") else {},
                "global_workspace": self.global_workspace.get_state() if hasattr(self.global_workspace, "get_state") else {},
                "self_model": self.self_model.get_state() if hasattr(self.self_model, "get_state") else {},
                "introspection": self.introspection.get_state() if hasattr(self.introspection, "get_state") else {}
            }
        }
        return state
    
    def _set_development_level(self, level: float) -> None:
        """Set the development level manually"""
        self.developmental_level = max(0.0, min(1.0, level))
    
    def _sync_development_levels(self) -> None:
        """Synchronize development levels across all submodules"""
        level = self.developmental_level
        
        # Apply varying development rates to different submodules
        self.awareness.update_development(level - self.awareness.developmental_level)
        self.global_workspace.update_development(level - self.global_workspace.developmental_level)
        self.self_model.update_development(level - self.self_model.developmental_level)
        self.introspection.update_development(level - self.introspection.developmental_level)
    
    def _get_current_milestone(self) -> str:
        """Get the current developmental milestone"""
        milestone = "pre_conscious"
        for level, name in sorted(self.development_milestones.items()):
            if self.developmental_level >= level:
                milestone = name
        return milestone
    
    def _initialize_submodule_communication(self) -> None:
        """Initialize communication patterns between submodules"""
        # This function would set up direct communication between submodules
        # if they aren't already connected via the event bus
        pass
    
    def _update_consciousness_state(self) -> None:
        """Update the integrated consciousness state based on submodule states"""
        # Update awareness state
        if hasattr(self.awareness, "state"):
            self.state.awareness = self.awareness.state
            
        # Update global workspace state
        if hasattr(self.global_workspace, "state"):
            self.state.global_workspace = self.global_workspace.state
            
        # Update self-model state
        if hasattr(self.self_model, "state"):
            self.state.self_model = self.self_model.state
            
        # Update introspection state
        if hasattr(self.introspection, "state"):
            self.state.introspection = self.introspection.state
            
        # Update last update timestamp
        self.state.last_update = datetime.now()
        
        # Model validator will automatically update consciousness_level
    
    def _integrated_processing(self, content: Dict[str, Any]) -> Dict[str, Any]:
        """Perform integrated processing across all consciousness submodules"""
        results = {}
        
        # First, process with awareness to determine what enters consciousness
        awareness_result = self.awareness.process_input({
            "type": content.get("content_type", "unknown"),
            "state": content,
            "source": content.get("source", "integrated_request")
        })
        results["awareness"] = awareness_result
        
        # Next, place in global workspace if it passes awareness
        if awareness_result.get("state", {}).get("external_awareness", 0) > 0.3:
            workspace_result = self.global_workspace.process_input({
                "content": content,
                "source": content.get("source", "integrated_request"),
                "activation": 0.6  # Default activation for integrated processing
            })
            results["global_workspace"] = workspace_result
            
            # Get workspace contents for further processing
            workspace_contents = workspace_result.get("state", {}).get("active_items", {})
            
            # Update self-model based on workspace contents
            if workspace_contents and self.developmental_level >= 0.4:
                self_relevance = content.get("self_relevance", 0.0)
                
                if self_relevance > 0.4:
                    self_result = self.self_model.process_input({
                        "type": "identity_update",
                        "data": {
                            "updates": content.get("self_implications", {})
                        }
                    })
                    results["self_model"] = self_result
            
            # Perform introspection on the process if developed enough
            if self.developmental_level >= 0.6:
                introspection_result = self.introspection.process_input({
                    "type": "cognitive_process",
                    "mental_state": {
                        "process_type": "integrated_consciousness",
                        "state": content,
                        "self_involvement": content.get("self_relevance", 0.0)
                    }
                })
                results["introspection"] = introspection_result
        
        return results
    
    def _handle_query(self, message: Message) -> None:
        """Handle consciousness query messages"""
        query_type = message.content.get("query_type", "")
        
        response = {
            "module_id": self.module_id,
            "query_type": query_type
        }
        
        if query_type == "consciousness_state":
            response["state"] = self.state.model_dump()
            
        elif query_type == "development_level":
            response["developmental_level"] = self.developmental_level
            response["current_milestone"] = self._get_current_milestone()
            
        elif query_type == "self_model":
            response["self_model"] = self.self_model.state.model_dump() if hasattr(self.self_model, "state") else {}
            
        elif query_type == "awareness":
            response["awareness"] = self.awareness.state.model_dump() if hasattr(self.awareness, "state") else {}
            
        # Publish response if event bus is available
        if self.event_bus:
            self.event_bus.publish(
                msg_type="consciousness_response",
                content=response,
                source=self.module_id,
                target=message.source
            )
    
    def _handle_development(self, message: Message) -> None:
        """Handle development update messages"""
        if message.target in [self.module_id, "all"]:
            amount = message.content.get("amount", 0.01)
            self.update_development(amount)
    
    def _handle_save_state(self, message: Message) -> None:
        """Handle save state messages"""
        if message.target in [self.module_id, "all"]:
            path = message.content.get("path", "")
            
            if not path:
                # Default path
                path = os.path.join("data", "states", f"{self.module_id}_state.json")
                
            # Create directories if needed
            os.makedirs(os.path.dirname(path), exist_ok=True)
            
            # Get current state
            state = self.get_state()
            
            # Save to file
            try:
                with open(path, 'w') as f:
                    json.dump(state, f, indent=2)
                    
                # Publish success message
                if self.event_bus:
                    self.event_bus.publish(
                        msg_type="state_saved",
                        content={
                            "module_id": self.module_id,
                            "path": path
                        }
                    )
            except Exception as e:
                # Publish error message
                if self.event_bus:
                    self.event_bus.publish(
                        msg_type="save_error",
                        content={
                            "module_id": self.module_id,
                            "error": str(e)
                        }
                    )
    
    def _handle_load_state(self, message: Message) -> None:
        """Handle load state messages"""
        if message.target in [self.module_id, "all"]:
            path = message.content.get("path", "")
            
            if not path:
                # Default path
                path = os.path.join("data", "states", f"{self.module_id}_state.json")
                
            # Load from file
            try:
                if os.path.exists(path):
                    with open(path, 'r') as f:
                        state = json.load(f)
                        
                    # Apply development level
                    if "developmental_level" in state:
                        self._set_development_level(state["developmental_level"])
                        self._sync_development_levels()
                        
                    # Publish success message
                    if self.event_bus:
                        self.event_bus.publish(
                            msg_type="state_loaded",
                            content={
                                "module_id": self.module_id,
                                "path": path
                            }
                        )
                else:
                    # Publish error message
                    if self.event_bus:
                        self.event_bus.publish(
                            msg_type="load_error",
                            content={
                                "module_id": self.module_id,
                                "error": f"File not found: {path}"
                            }
                        )
            except Exception as e:
                # Publish error message
                if self.event_bus:
                    self.event_bus.publish(
                        msg_type="load_error",
                        content={
                            "module_id": self.module_id,
                            "error": str(e)
                        }
                    )


#######################

#creativity\concept_combination.py#
#######################

# TODO: Implement the ConceptCombination class to generate novel concepts
# This component should be able to:
# - Blend existing concepts to create new ones
# - Identify compatible conceptual properties for combination
# - Resolve conflicts when combining incompatible properties
# - Generate novel inferences from combined concepts

# TODO: Implement developmental progression in concept combination:
# - Simple property transfer in early stages
# - Basic blending of compatible concepts in childhood
# - Complex integration of diverse concepts in adolescence
# - Sophisticated conceptual blending with emergent properties in adulthood

# TODO: Create mechanisms for:
# - Property mapping: Identify corresponding properties between concepts
# - Blend space creation: Generate new conceptual spaces from inputs
# - Conflict resolution: Handle contradictory properties in combined concepts
# - Emergent property inference: Derive new properties not present in source concepts

# TODO: Implement different combination strategies:
# - Property intersection: Retain only common properties
# - Property union: Retain all properties from both concepts
# - Selective projection: Strategically select properties to transfer
# - Emergent combination: Create entirely new properties

# TODO: Connect to memory and language systems
# Concept combination should draw from semantic memory
# and be influenced by linguistic knowledge

from typing import Dict, List, Any, Optional, Set, Union, Tuple
from datetime import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import uuid

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus, Message
from lmm_project.modules.creativity.models import ConceptCombinationState, Concept, CreativeOutput
from lmm_project.modules.creativity.neural_net import ConceptCombiner

class ConceptCombination(BaseModule):
    """
    Combines existing concepts to create new ones
    
    This module creates new concepts by combining existing ones
    through various patterns such as blending, property transfer,
    and analogical mapping.
    
    Developmental progression:
    - Simple associations in early stages
    - Basic property transfers in childhood
    - Conceptual blending in adolescence
    - Complex analogical reasoning in adulthood
    """
    
    # Developmental milestones for concept combination
    development_milestones = {
        0.0: "simple_association",      # Basic association of concepts
        0.25: "property_transfer",      # Transferring properties between concepts
        0.5: "conceptual_blending",     # Blending concepts to form new ones
        0.75: "analogical_mapping",     # Using analogies to map concepts
        0.9: "creative_abstraction"     # Creating abstract concepts from concrete ones
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the concept combination module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="concept_combination", event_bus=event_bus)
        
        # Initialize state
        self.state = ConceptCombinationState()
        
        # Initialize neural network for concept combination
        self.input_dim = 128  # Default dimension
        self.network = ConceptCombiner(
            concept_dim=self.input_dim,
            hidden_dim=256,
            output_dim=self.input_dim
        )
        
        # Initialize concept combination patterns with usage frequencies
        self.state.combination_patterns = {
            "association": 1.0,  # Available at all development levels
            "property_transfer": 0.0,  # Will be enabled with development
            "blend": 0.0,  # Will be enabled with development
            "analogy": 0.0  # Will be enabled with development
        }
        
        # Subscribe to relevant events
        if self.event_bus:
            self.event_bus.subscribe("concept_created", self._handle_concept)
            self.event_bus.subscribe("combination_request", self._handle_combination_request)
            
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to combine concepts
        
        Args:
            input_data: Dictionary containing concepts to combine and parameters
            
        Returns:
            Dictionary with the results of concept combination
        """
        # Extract input information
        concepts = input_data.get("concepts", [])
        pattern = input_data.get("pattern", self._select_combination_pattern())
        context = input_data.get("context", {})
        
        # Validate input
        if not concepts or len(concepts) < 2:
            return {
                "status": "error",
                "message": "At least two concepts required for combination",
                "module_id": self.module_id,
                "module_type": self.module_type
            }
            
        # Load concepts (either directly provided or by ID)
        concept_objects = []
        for concept in concepts:
            if isinstance(concept, dict) and "concept_id" in concept:
                # Concept directly provided
                concept_obj = Concept(**concept)
                # Cache for later use
                self.state.concept_cache[concept_obj.concept_id] = concept_obj
                concept_objects.append(concept_obj)
            elif isinstance(concept, str):
                # Concept ID provided, check cache
                if concept in self.state.concept_cache:
                    concept_objects.append(self.state.concept_cache[concept])
                else:
                    # TODO: In a real system, you might need to fetch from a database here
                    # For now, return an error
                    return {
                        "status": "error",
                        "message": f"Concept {concept} not found in cache",
                        "module_id": self.module_id,
                        "module_type": self.module_type
                    }
            else:
                return {
                    "status": "error",
                    "message": "Invalid concept format",
                    "module_id": self.module_id,
                    "module_type": self.module_type
                }
                
        # Create concept embeddings (simplified - in a real system you would use actual embeddings)
        # Here we'll just create random tensors as placeholders
        concept_embeddings = []
        for concept in concept_objects:
            # In a real system, this would use the concept's features to create an embedding
            # For now, create a random tensor
            embedding = torch.randn(1, self.input_dim)
            concept_embeddings.append(embedding)
            
        # Apply the appropriate combination pattern
        result = self._combine_concepts(concept_objects, concept_embeddings, pattern, context)
        
        # Update state with new combination
        if result["status"] == "success":
            # Add to combinations
            self.state.combinations[result["combination_id"]] = {
                "concepts": [c.concept_id for c in concept_objects],
                "pattern": pattern,
                "result": result["concept"].model_dump(),
                "timestamp": datetime.now().isoformat()
            }
            
            # Update recent combinations
            self.state.recent_combinations.append(result["combination_id"])
            # Keep list at a reasonable size
            if len(self.state.recent_combinations) > 20:
                self.state.recent_combinations = self.state.recent_combinations[-20:]
                
            # Update pattern usage
            if pattern in self.state.combination_patterns:
                self.state.combination_patterns[pattern] += 0.1
                
            # Create creative output
            creative_output = CreativeOutput(
                content=result["concept"].model_dump(),
                output_type="combined_concept",
                novelty_score=result.get("novelty_score", 0.5),
                coherence_score=result.get("coherence_score", 0.5),
                usefulness_score=result.get("usefulness_score", 0.5),
                source_components=[self.module_id]
            )
            
            # Publish creative output if event bus is available
            if self.event_bus:
                self.event_bus.publish(
                    msg_type="creative_output",
                    content=creative_output.model_dump()
                )
                
        return result
        
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        previous_level = self.developmental_level
        new_level = super().update_development(amount)
        
        # Update available combination patterns based on development level
        if previous_level < 0.25 and new_level >= 0.25:
            # Enable property transfer
            self.state.combination_patterns["property_transfer"] = 0.5
            
        if previous_level < 0.5 and new_level >= 0.5:
            # Enable conceptual blending
            self.state.combination_patterns["blend"] = 0.5
            
        if previous_level < 0.75 and new_level >= 0.75:
            # Enable analogical mapping
            self.state.combination_patterns["analogy"] = 0.5
            
        return new_level
    
    def _get_current_milestone(self) -> str:
        """Get the current developmental milestone"""
        milestone = "pre_association"
        for level, name in sorted(self.development_milestones.items()):
            if self.developmental_level >= level:
                milestone = name
        return milestone
        
    def _select_combination_pattern(self) -> str:
        """Select a combination pattern based on development level and pattern frequencies"""
        # Filter patterns by availability (non-zero frequency)
        available_patterns = {k: v for k, v in self.state.combination_patterns.items() if v > 0}
        
        if not available_patterns:
            return "association"  # Default fallback
            
        # Select pattern based on probabilities
        patterns = list(available_patterns.keys())
        frequencies = list(available_patterns.values())
        total = sum(frequencies)
        probabilities = [f / total for f in frequencies]
        
        return np.random.choice(patterns, p=probabilities)
        
    def _combine_concepts(self, 
                         concepts: List[Concept], 
                         embeddings: List[torch.Tensor],
                         pattern: str, 
                         context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Combine concepts using a specified pattern
        
        Args:
            concepts: List of concepts to combine
            embeddings: Tensor embeddings of concepts
            pattern: Combination pattern to use
            context: Additional context for combination
            
        Returns:
            Dictionary with combination results
        """
        # Initialize result
        result = {
            "status": "success",
            "module_id": self.module_id,
            "module_type": self.module_type,
            "pattern": pattern,
            "combination_id": str(uuid.uuid4())
        }
        
        try:
            # Get concept names for the new concept name
            concept_names = [c.name for c in concepts]
            
            # Process based on pattern
            if pattern == "association":
                # Simple association of concepts
                new_concept = self._association_combination(concepts, embeddings)
                
                # Validate
                if new_concept is None:
                    return {
                        "status": "error",
                        "message": "Failed to create association",
                        "module_id": self.module_id,
                        "module_type": self.module_type
                    }
                    
                result["concept"] = new_concept
                result["novelty_score"] = 0.3
                result["coherence_score"] = 0.7
                
            elif pattern == "property_transfer":
                # Transfer properties from one concept to another
                if len(concepts) < 2:
                    return {
                        "status": "error",
                        "message": "Property transfer requires at least 2 concepts",
                        "module_id": self.module_id,
                        "module_type": self.module_type
                    }
                    
                new_concept = self._property_transfer_combination(concepts, embeddings)
                
                result["concept"] = new_concept
                result["novelty_score"] = 0.5
                result["coherence_score"] = 0.6
                
            elif pattern == "blend":
                # Conceptual blending
                if len(concepts) < 2:
                    return {
                        "status": "error",
                        "message": "Blending requires at least 2 concepts",
                        "module_id": self.module_id,
                        "module_type": self.module_type
                    }
                    
                new_concept = self._blend_combination(concepts, embeddings)
                
                result["concept"] = new_concept
                result["novelty_score"] = 0.7
                result["coherence_score"] = 0.5
                
            elif pattern == "analogy":
                # Analogical mapping
                if len(concepts) < 3:
                    return {
                        "status": "error",
                        "message": "Analogy requires at least 3 concepts",
                        "module_id": self.module_id,
                        "module_type": self.module_type
                    }
                    
                new_concept = self._analogy_combination(concepts, embeddings)
                
                result["concept"] = new_concept
                result["novelty_score"] = 0.8
                result["coherence_score"] = 0.4
                
            else:
                # Unknown pattern
                return {
                    "status": "error",
                    "message": f"Unknown combination pattern: {pattern}",
                    "module_id": self.module_id,
                    "module_type": self.module_type
                }
                
            # Calculate usefulness score based on development level
            result["usefulness_score"] = 0.3 + 0.5 * self.developmental_level
            
            return result
            
        except Exception as e:
            return {
                "status": "error",
                "message": f"Error combining concepts: {str(e)}",
                "module_id": self.module_id,
                "module_type": self.module_type
            }
    
    def _association_combination(self, 
                                concepts: List[Concept], 
                                embeddings: List[torch.Tensor]) -> Concept:
        """Simple association of concepts"""
        # Get concept names
        names = [c.name for c in concepts]
        new_name = " & ".join(names)
        
        # Combine features (union of all features)
        new_features = {}
        for concept in concepts:
            new_features.update(concept.features)
            
        # Get associations (union of all associations)
        new_associations = []
        for concept in concepts:
            new_associations.extend(concept.associations)
            
        # Remove duplicates
        new_associations = list(set(new_associations))
        
        # Add original concepts as associations
        for concept in concepts:
            if concept.concept_id not in new_associations:
                new_associations.append(concept.concept_id)
        
        # Create new concept
        return Concept(
            name=new_name,
            features=new_features,
            associations=new_associations,
            source=f"association({', '.join(names)})"
        )
        
    def _property_transfer_combination(self, 
                                      concepts: List[Concept], 
                                      embeddings: List[torch.Tensor]) -> Concept:
        """Transfer properties from one concept to another"""
        # Use the first concept as base, second as modifier
        base_concept = concepts[0]
        modifier_concept = concepts[1]
        
        # Create a new name
        new_name = f"{modifier_concept.name}-like {base_concept.name}"
        
        # Start with base concept features
        new_features = dict(base_concept.features)
        
        # Use neural network to determine which features to transfer
        with torch.no_grad():
            _, attention_dict = self.network([embeddings[0], embeddings[1]], "property_transfer")
            
        if attention_dict.get("attention") is not None:
            attention = attention_dict["attention"].item()
        else:
            attention = 0.5  # Default if attention is not available
            
        # Select features to transfer based on attention
        transfer_features = {}
        for key, value in modifier_concept.features.items():
            if np.random.random() < attention:
                transfer_features[key] = value
                
        # Add transferred features, overwriting existing ones
        new_features.update(transfer_features)
        
        # Combine associations
        new_associations = list(base_concept.associations)
        for assoc in modifier_concept.associations:
            if assoc not in new_associations:
                new_associations.append(assoc)
                
        # Add original concepts as associations
        for concept in concepts:
            if concept.concept_id not in new_associations:
                new_associations.append(concept.concept_id)
        
        # Create new concept
        return Concept(
            name=new_name,
            features=new_features,
            associations=new_associations,
            source=f"property_transfer({base_concept.name}, {modifier_concept.name})"
        )
        
    def _blend_combination(self, 
                          concepts: List[Concept], 
                          embeddings: List[torch.Tensor]) -> Concept:
        """Blend two concepts to create a new one"""
        # Get the two concepts to blend
        concept1 = concepts[0]
        concept2 = concepts[1]
        
        # Create a blended name
        name_parts1 = concept1.name.split()
        name_parts2 = concept2.name.split()
        
        # Try to create a portmanteau if single words
        if len(name_parts1) == 1 and len(name_parts2) == 1:
            # Simple portmanteau: first half of first word + second half of second word
            half1 = len(name_parts1[0]) // 2
            half2 = len(name_parts2[0]) // 2
            new_name = name_parts1[0][:half1+1] + name_parts2[0][half2:]
        else:
            # Otherwise use both names
            new_name = f"{concept1.name}-{concept2.name} Blend"
            
        # Use neural network to blend concepts
        with torch.no_grad():
            combined_embedding, attention_dict = self.network([embeddings[0], embeddings[1]], "blend")
            
        # Blend features based on concept similarity
        new_features = {}
        
        # Add features from both concepts
        # In a real implementation, this would use the neural network output to create new features
        for key, value in concept1.features.items():
            new_features[key] = value
            
        for key, value in concept2.features.items():
            # If feature exists in both, create a blend
            if key in new_features:
                if isinstance(value, (int, float)) and isinstance(new_features[key], (int, float)):
                    # Average numerical values
                    new_features[key] = (new_features[key] + value) / 2
                else:
                    # For non-numeric, use the second value
                    new_features[key] = value
            else:
                # Add new feature
                new_features[key] = value
        
        # Create new emergent features (more likely with higher development)
        if np.random.random() < self.developmental_level:
            # Add an emergent feature that wasn't in either concept
            new_features["emergent_property"] = "Created through conceptual blending"
        
        # Combine associations
        new_associations = list(set(concept1.associations + concept2.associations))
        
        # Add original concepts as associations
        for concept in concepts:
            if concept.concept_id not in new_associations:
                new_associations.append(concept.concept_id)
        
        # Create new concept
        return Concept(
            name=new_name,
            features=new_features,
            associations=new_associations,
            source=f"blend({concept1.name}, {concept2.name})"
        )
        
    def _analogy_combination(self, 
                            concepts: List[Concept], 
                            embeddings: List[torch.Tensor]) -> Concept:
        """Create a concept through analogical mapping"""
        # For analogy we need at least 3 concepts: A is to B as C is to ?
        if len(concepts) < 3:
            raise ValueError("Analogy requires at least 3 concepts")
            
        concept_a = concepts[0]
        concept_b = concepts[1]
        concept_c = concepts[2]
        
        # Create a name for the new concept
        new_name = f"D in [{concept_a.name}:{concept_b.name}::{concept_c.name}:D]"
        
        # Use neural network to create analogy
        with torch.no_grad():
            combined_embedding, relation_dict = self.network(
                [embeddings[0], embeddings[1], embeddings[2]], 
                "analogy"
            )
            
        # Start with features from C
        new_features = dict(concept_c.features)
        
        # Identify differences between A and B
        diff_features = {}
        for key, value in concept_b.features.items():
            if key in concept_a.features:
                # Feature exists in both
                if isinstance(value, (int, float)) and isinstance(concept_a.features[key], (int, float)):
                    # Calculate numerical difference
                    diff = value - concept_a.features[key]
                    diff_features[key] = diff
                else:
                    # For non-numeric, note the change
                    diff_features[key] = (concept_a.features[key], value)
            else:
                # Feature in B but not in A (addition)
                diff_features[key] = ("added", value)
                
        for key in concept_a.features:
            if key not in concept_b.features:
                # Feature in A but not in B (removal)
                diff_features[key] = ("removed", concept_a.features[key])
        
        # Apply differences to C to create D
        for key, diff in diff_features.items():
            if key in new_features:
                if isinstance(diff, (int, float)) and isinstance(new_features[key], (int, float)):
                    # Apply numerical difference
                    new_features[key] += diff
                elif isinstance(diff, tuple) and diff[0] == "removed":
                    # Remove feature
                    del new_features[key]
                elif isinstance(diff, tuple) and isinstance(diff[0], (int, float, str)):
                    # Update with new value based on the relationship
                    new_features[key] = diff[1]
            else:
                # Feature not in C
                if isinstance(diff, tuple) and diff[0] == "added":
                    # Add the feature
                    new_features[key] = diff[1]
        
        # Combine associations, focusing on B and C
        new_associations = list(set(concept_b.associations + concept_c.associations))
        
        # Add original concepts as associations
        for concept in concepts:
            if concept.concept_id not in new_associations:
                new_associations.append(concept.concept_id)
        
        # Create new concept
        return Concept(
            name=new_name,
            features=new_features,
            associations=new_associations,
            source=f"analogy({concept_a.name}, {concept_b.name}, {concept_c.name})"
        )
    
    def _handle_concept(self, message: Message) -> None:
        """Handle concept creation messages"""
        if isinstance(message.content, dict):
            concept_data = message.content
            # Cache the concept for potential future use
            concept = Concept(**concept_data)
            self.state.concept_cache[concept.concept_id] = concept
            
    def _handle_combination_request(self, message: Message) -> None:
        """Handle concept combination requests"""
        if isinstance(message.content, dict):
            # Process the combination request
            result = self.process_input(message.content)
            
            # Publish result if successful
            if result["status"] == "success" and self.event_bus:
                self.event_bus.publish(
                    msg_type="combination_result",
                    content=result,
                    source=self.module_id,
                    target=message.source
                )


#######################

#creativity\divergent_thinking.py#
#######################

# TODO: Implement the DivergentThinking class to generate multiple alternative solutions
# This component should be able to:
# - Generate multiple approaches to a problem or task
# - Explore unusual or non-obvious solution paths
# - Break away from conventional thinking patterns
# - Produce ideas that vary in conceptual distance

# TODO: Implement developmental progression in divergent thinking:
# - Simple variation in early stages
# - Increased idea fluency in childhood
# - Growing originality in adolescence
# - Sophisticated category-breaking in adulthood

# TODO: Create mechanisms for:
# - Idea generation: Produce multiple candidate solutions
# - Conceptual expansion: Break out of conventional categories
# - Remote association: Connect distant semantic concepts
# - Constraint relaxation: Temporarily ignore typical constraints

# TODO: Implement quantitative metrics for divergent thinking:
# - Fluency: Number of ideas generated
# - Flexibility: Number of different categories of ideas
# - Originality: Statistical rarity of ideas
# - Elaboration: Level of detail in ideas

# TODO: Connect to executive function and attention systems
# Divergent thinking requires inhibition of obvious solutions
# and attention shifting to different perspectives

from typing import Dict, List, Any, Optional, Set, Union, Tuple
from datetime import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import uuid

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus, Message
from lmm_project.modules.creativity.models import DivergentThinkingState, CreativeOutput
from lmm_project.modules.creativity.neural_net import DivergentGenerator

class DivergentThinking(BaseModule):
    """
    Generates multiple diverse solutions to problems
    
    This module enables the system to think divergently,
    producing a variety of potential solutions rather than
    fixating on a single approach.
    
    Developmental progression:
    - Simple variation in early stages
    - Increased fluency in childhood
    - Greater flexibility in adolescence
    - High originality in adulthood
    """
    
    # Developmental milestones for divergent thinking
    development_milestones = {
        0.0: "simple_variation",      # Basic ability to generate variations
        0.25: "increased_fluency",    # Ability to generate more ideas
        0.5: "greater_flexibility",   # Ability to generate diverse categories of ideas
        0.75: "remote_associations",  # Ability to make distant connections
        0.9: "high_originality"       # Ability to generate highly novel ideas
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the divergent thinking module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="divergent_thinking", event_bus=event_bus)
        
        # Initialize state
        self.state = DivergentThinkingState()
        
        # Initialize neural network for divergent generation
        self.input_dim = 128  # Default dimension
        self.network = DivergentGenerator(
            input_dim=self.input_dim,
            hidden_dim=256,
            output_dim=self.input_dim,
            latent_dim=64
        )
        
        # Subscribe to relevant events
        if self.event_bus:
            self.event_bus.subscribe("problem_posed", self._handle_problem)
            self.event_bus.subscribe("solution_request", self._handle_solution_request)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to generate diverse solutions
        
        Args:
            input_data: Dictionary containing problem and generation parameters
            
        Returns:
            Dictionary with the results of divergent thinking
        """
        # Extract input information
        problem = input_data.get("problem", {})
        problem_id = input_data.get("problem_id", str(uuid.uuid4()))
        diversity_factor = input_data.get("diversity_factor", self.developmental_level)
        num_solutions = input_data.get("num_solutions", self._calculate_fluency())
        context = input_data.get("context", {})
        
        # Validate input
        if not problem:
            return {
                "status": "error",
                "message": "Problem description required",
                "module_id": self.module_id,
                "module_type": self.module_type
            }
            
        # Create problem embedding (simplified - in a real system you would use actual embeddings)
        problem_embedding = torch.randn(1, self.input_dim)
            
        # Generate diverse solutions
        try:
            solutions = self._generate_solutions(problem_embedding, problem, diversity_factor, num_solutions)
            
            # Store in solution space
            self.state.solution_spaces[problem_id] = {
                "problem": problem,
                "solutions": solutions,
                "timestamp": datetime.now().isoformat(),
                "metrics": {
                    "fluency": len(solutions),
                    "flexibility": self._calculate_flexibility(solutions),
                    "originality": self._calculate_originality(solutions),
                    "elaboration": self._calculate_elaboration(solutions)
                }
            }
            
            # Update divergent thinking metrics
            self._update_thinking_metrics(solutions)
            
            # Create result
            result = {
                "status": "success",
                "module_id": self.module_id,
                "module_type": self.module_type,
                "problem_id": problem_id,
                "solutions": solutions,
                "metrics": self.state.solution_spaces[problem_id]["metrics"]
            }
            
            # Create creative outputs
            for i, solution in enumerate(solutions):
                creative_output = CreativeOutput(
                    content={
                        "problem": problem,
                        "solution": solution,
                        "solution_index": i
                    },
                    output_type="divergent_solution",
                    novelty_score=self._calculate_solution_novelty(solution),
                    coherence_score=self._calculate_solution_coherence(solution),
                    usefulness_score=self._calculate_solution_usefulness(solution),
                    source_components=[self.module_id]
                )
                
                # Publish creative output if event bus is available
                if self.event_bus:
                    self.event_bus.publish(
                        msg_type="creative_output",
                        content=creative_output.model_dump()
                    )
            
            return result
            
        except Exception as e:
            return {
                "status": "error",
                "message": f"Error generating solutions: {str(e)}",
                "module_id": self.module_id,
                "module_type": self.module_type
            }
        
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        previous_level = self.developmental_level
        new_level = super().update_development(amount)
        
        # Update divergent thinking metrics based on development
        
        # Fluency (ability to generate many ideas)
        self.state.fluency_score = min(1.0, 0.1 + 0.6 * new_level)
        
        # Flexibility (ability to generate diverse ideas)
        if previous_level < 0.5 and new_level >= 0.5:
            self.state.flexibility_score = min(1.0, self.state.flexibility_score + 0.2)
        
        # Originality (ability to generate novel ideas)
        if previous_level < 0.75 and new_level >= 0.75:
            self.state.originality_score = min(1.0, self.state.originality_score + 0.3)
        
        # Elaboration (ability to develop ideas in detail)
        if previous_level < 0.9 and new_level >= 0.9:
            self.state.elaboration_score = min(1.0, self.state.elaboration_score + 0.2)
        
        return new_level
    
    def _get_current_milestone(self) -> str:
        """Get the current developmental milestone"""
        milestone = "pre_divergent"
        for level, name in sorted(self.development_milestones.items()):
            if self.developmental_level >= level:
                milestone = name
        return milestone
    
    def _calculate_fluency(self) -> int:
        """Calculate number of solutions to generate based on fluency score"""
        # Base number of solutions
        base_solutions = 2
        
        # Additional solutions based on fluency
        additional_solutions = int(self.state.fluency_score * 8)  # Up to 8 more
        
        return base_solutions + additional_solutions
    
    def _generate_solutions(self, 
                           problem_embedding: torch.Tensor, 
                           problem: Dict[str, Any],
                           diversity_factor: float, 
                           num_solutions: int) -> List[Dict[str, Any]]:
        """
        Generate diverse solutions for a problem
        
        Args:
            problem_embedding: Tensor embedding of the problem
            problem: Dictionary describing the problem
            diversity_factor: How diverse the solutions should be
            num_solutions: Number of solutions to generate
            
        Returns:
            List of solution dictionaries
        """
        # Process through neural network to generate solutions
        with torch.no_grad():
            network_output = self.network(
                problem_embedding, 
                diversity_factor=diversity_factor,
                num_solutions=num_solutions
            )
            
        solution_embeddings = network_output["solutions"]
        
        # Convert embeddings to solution descriptions
        solutions = []
        for i, embedding in enumerate(solution_embeddings):
            # In a real system, this would decode the embedding into a meaningful solution
            # Here we'll create a placeholder solution
            solution = {
                "solution_id": str(uuid.uuid4()),
                "description": f"Solution {i+1} for problem: {problem.get('description', 'Unnamed problem')}",
                "approach": f"Approach {i+1}",
                "details": {},
                "category": self._determine_solution_category(i, num_solutions)
            }
            
            # Add more details based on elaboration score
            if np.random.random() < self.state.elaboration_score:
                solution["details"] = {
                    "steps": [f"Step {j+1}" for j in range(3)],
                    "resources": ["Resource A", "Resource B"],
                    "constraints": ["Constraint 1"]
                }
                
            solutions.append(solution)
            
        return solutions
    
    def _determine_solution_category(self, index: int, total: int) -> str:
        """Determine the category of a solution based on its index"""
        # This simple implementation assigns solutions to different categories
        # based on their index, ensuring diversity of approaches
        categories = [
            "analytical",
            "intuitive",
            "methodical",
            "creative",
            "practical",
            "theoretical",
            "collaborative",
            "independent",
            "technological",
            "traditional"
        ]
        
        # Early development has fewer categories
        available_categories = max(2, int(self.developmental_level * len(categories)))
        
        # Distribute solutions across available categories
        category_index = index % available_categories
        return categories[category_index]
    
    def _calculate_flexibility(self, solutions: List[Dict[str, Any]]) -> float:
        """Calculate flexibility score based on diversity of solution categories"""
        if not solutions:
            return 0.0
            
        # Get categories
        categories = [solution.get("category", "") for solution in solutions]
        
        # Count unique categories
        unique_categories = set(categories)
        
        # Calculate flexibility as ratio of unique categories to solutions
        # Multiplied by developmental scaling factor
        return min(1.0, len(unique_categories) / len(solutions)) * (0.5 + 0.5 * self.developmental_level)
    
    def _calculate_originality(self, solutions: List[Dict[str, Any]]) -> float:
        """Calculate originality score based on solution properties"""
        # In a real system, this would compare solutions to known patterns
        # Here we'll use a simplified placeholder based on developmental level
        return 0.3 + 0.6 * self.developmental_level
    
    def _calculate_elaboration(self, solutions: List[Dict[str, Any]]) -> float:
        """Calculate elaboration score based on solution detail"""
        if not solutions:
            return 0.0
            
        # Calculate average detail level
        total_details = 0
        for solution in solutions:
            # Count elements in details dictionary
            details = solution.get("details", {})
            elements = len(details)
            for key, value in details.items():
                if isinstance(value, list):
                    elements += len(value)
            total_details += elements
            
        avg_details = total_details / len(solutions)
        
        # Normalize to [0, 1] range
        return min(1.0, avg_details / 10) * (0.5 + 0.5 * self.developmental_level)
    
    def _update_thinking_metrics(self, solutions: List[Dict[str, Any]]) -> None:
        """Update the divergent thinking metrics based on generated solutions"""
        # Calculate new values
        flexibility = self._calculate_flexibility(solutions)
        originality = self._calculate_originality(solutions)
        elaboration = self._calculate_elaboration(solutions)
        
        # Update with smoothing
        smoothing = 0.3
        self.state.flexibility_score = (1 - smoothing) * self.state.flexibility_score + smoothing * flexibility
        self.state.originality_score = (1 - smoothing) * self.state.originality_score + smoothing * originality
        self.state.elaboration_score = (1 - smoothing) * self.state.elaboration_score + smoothing * elaboration
    
    def _calculate_solution_novelty(self, solution: Dict[str, Any]) -> float:
        """Calculate the novelty score of a solution"""
        # In a real system, this would compare the solution to prior solutions
        # Here we'll use a simplified approach
        base_novelty = 0.3
        dev_factor = 0.6 * self.developmental_level
        random_factor = 0.1 * np.random.random()
        
        return min(1.0, base_novelty + dev_factor + random_factor)
    
    def _calculate_solution_coherence(self, solution: Dict[str, Any]) -> float:
        """Calculate the coherence score of a solution"""
        # Check for presence of key components
        has_description = "description" in solution and bool(solution["description"])
        has_approach = "approach" in solution and bool(solution["approach"])
        has_details = "details" in solution and isinstance(solution["details"], dict) and solution["details"]
        
        # Calculate base coherence
        base_coherence = 0.5
        if has_description:
            base_coherence += 0.2
        if has_approach:
            base_coherence += 0.1
        if has_details:
            base_coherence += 0.2
            
        return min(1.0, base_coherence)
    
    def _calculate_solution_usefulness(self, solution: Dict[str, Any]) -> float:
        """Calculate the usefulness score of a solution"""
        # In a real system, this would evaluate usefulness based on the problem constraints
        # Here we'll use a simplified approach
        base_usefulness = 0.4
        dev_factor = 0.4 * self.developmental_level
        
        # More elaborate solutions are considered more useful
        details = solution.get("details", {})
        detail_factor = min(0.2, len(details) * 0.05)
        
        return min(1.0, base_usefulness + dev_factor + detail_factor)
    
    def _handle_problem(self, message: Message) -> None:
        """Handle problem messages"""
        if isinstance(message.content, dict):
            # Process the problem to generate solutions
            self.process_input({
                "problem": message.content,
                "problem_id": message.content.get("problem_id", str(uuid.uuid4()))
            })
    
    def _handle_solution_request(self, message: Message) -> None:
        """Handle solution request messages"""
        if isinstance(message.content, dict):
            # Process the solution request
            result = self.process_input(message.content)
            
            # Publish result if successful
            if result["status"] == "success" and self.event_bus:
                self.event_bus.publish(
                    msg_type="solution_result",
                    content=result,
                    source=self.module_id,
                    target=message.source
                )


#######################

#creativity\imagination.py#
#######################

# TODO: Implement the Imagination class to create novel mental scenarios
# This component should be able to:
# - Generate mental representations of novel scenarios
# - Simulate hypothetical situations and outcomes
# - Recombine elements of memory into new configurations
# - Create and manipulate mental imagery

# TODO: Implement developmental progression in imagination:
# - Simple sensory recombination in early stages
# - Basic pretend scenarios in childhood
# - Hypothetical reasoning in adolescence
# - Abstract and counterfactual imagination in adulthood

# TODO: Create mechanisms for:
# - Scenario generation: Create coherent novel scenarios
# - Mental simulation: Project outcomes of imagined scenarios
# - Counterfactual reasoning: Imagine alternatives to reality
# - Imagery manipulation: Generate and transform mental images

# TODO: Implement different imagination modes:
# - Episodic future thinking: Imagination of personal future events
# - Fantasy generation: Creation of impossible or magical scenarios
# - Empathetic imagination: Simulation of others' experiences
# - Problem-solving imagination: Simulating solutions to problems

# TODO: Connect to memory, emotion, and consciousness systems
# Imagination should draw from episodic memory, generate
# appropriate emotions, and interact with consciousness

from typing import Dict, List, Any, Optional, Set, Union, Tuple
from datetime import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import uuid

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus, Message
from lmm_project.modules.creativity.models import ImaginationState, CreativeOutput
from lmm_project.modules.creativity.neural_net import ImaginationNetwork

class Imagination(BaseModule):
    """
    Generates novel mental scenarios and simulations
    
    This module creates mental simulations, alternative worlds,
    and novel scenarios that can be explored mentally without
    direct sensory input.
    
    Developmental progression:
    - Simple recombinations of experience in early stages
    - Basic imaginative play in childhood
    - Fantastical scenario creation in adolescence
    - Complex counterfactual reasoning in adulthood
    """
    
    # Developmental milestones for imagination
    development_milestones = {
        0.0: "experiential_recombination",  # Recombining experienced elements
        0.25: "imaginative_play",           # Playful imagination of simple scenarios
        0.5: "fantasy_creation",            # Creation of fantastical scenarios
        0.75: "counterfactual_reasoning",   # Exploring what could have been
        0.9: "abstract_simulation"          # Simulation of abstract concepts
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the imagination module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="imagination", event_bus=event_bus)
        
        # Initialize state
        self.state = ImaginationState()
        
        # Initialize neural network for imagination
        self.input_dim = 128  # Default dimension
        self.network = ImaginationNetwork(
            input_dim=self.input_dim,
            hidden_dim=256,
            output_dim=self.input_dim,
            sequence_length=10
        )
        
        # Subscribe to relevant events
        if self.event_bus:
            self.event_bus.subscribe("imagination_prompt", self._handle_prompt)
            self.event_bus.subscribe("scene_request", self._handle_scene_request)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to generate imaginative scenes
        
        Args:
            input_data: Dictionary containing prompts and generation parameters
            
        Returns:
            Dictionary with the results of imagination
        """
        # Extract input information
        prompt = input_data.get("prompt", {})
        scene_id = input_data.get("scene_id", str(uuid.uuid4()))
        sequence_length = input_data.get("sequence_length", 5)
        continue_scene = input_data.get("continue_scene", False)
        previous_scene_id = input_data.get("previous_scene_id", None)
        context = input_data.get("context", {})
        
        # Validate input
        if not prompt and not continue_scene:
            return {
                "status": "error",
                "message": "Prompt required for new scene generation",
                "module_id": self.module_id,
                "module_type": self.module_type
            }
            
        # Handle scene continuation
        initial_state = None
        if continue_scene and previous_scene_id:
            if previous_scene_id in self.state.scenes:
                previous_scene = self.state.scenes[previous_scene_id]
                if "final_state" in previous_scene:
                    # Convert to tensor
                    initial_state = torch.tensor(previous_scene["final_state"])
                    
                    # If no prompt provided, use the previous scene's prompt
                    if not prompt and "prompt" in previous_scene:
                        prompt = previous_scene["prompt"]
            else:
                return {
                    "status": "error",
                    "message": f"Previous scene {previous_scene_id} not found",
                    "module_id": self.module_id,
                    "module_type": self.module_type
                }
        
        # Create prompt embedding (simplified - in a real system you would use actual embeddings)
        prompt_embedding = torch.randn(1, self.input_dim)
            
        # Generate imaginative scene
        try:
            scene = self._generate_scene(prompt_embedding, prompt, sequence_length, initial_state)
            
            # Store in scenes dictionary
            self.state.scenes[scene_id] = {
                "prompt": prompt,
                "scene": scene,
                "timestamp": datetime.now().isoformat(),
                "sequence_length": sequence_length,
                "final_state": scene.get("final_state", None),
                "metrics": {
                    "complexity": self._calculate_scene_complexity(scene),
                    "coherence": self._calculate_scene_coherence(scene),
                    "novelty": self._calculate_scene_novelty(scene)
                }
            }
            
            # Set as active scene
            self.state.active_scene = scene_id
            
            # Update imagination metrics
            self._update_imagination_metrics(scene)
            
            # Create result
            result = {
                "status": "success",
                "module_id": self.module_id,
                "module_type": self.module_type,
                "scene_id": scene_id,
                "scene": scene,
                "metrics": self.state.scenes[scene_id]["metrics"],
                "state": {
                    "scene_complexity": self.state.scene_complexity,
                    "coherence_level": self.state.coherence_level,
                    "novelty_level": self.state.novelty_level
                }
            }
            
            # Create creative output
            creative_output = CreativeOutput(
                content={
                    "prompt": prompt,
                    "scene": scene,
                    "scene_id": scene_id
                },
                output_type="imagined_scene",
                novelty_score=self.state.scenes[scene_id]["metrics"]["novelty"],
                coherence_score=self.state.scenes[scene_id]["metrics"]["coherence"],
                usefulness_score=0.5,  # Default usefulness score for imagination
                source_components=[self.module_id]
            )
            
            # Publish creative output if event bus is available
            if self.event_bus:
                self.event_bus.publish(
                    msg_type="creative_output",
                    content=creative_output.model_dump()
                )
            
            return result
            
        except Exception as e:
            return {
                "status": "error",
                "message": f"Error generating scene: {str(e)}",
                "module_id": self.module_id,
                "module_type": self.module_type
            }
        
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        previous_level = self.developmental_level
        new_level = super().update_development(amount)
        
        # Update imagination capabilities based on development level
        
        # Scene complexity
        self.state.scene_complexity = min(1.0, 0.1 + 0.7 * new_level)
        
        # Scene coherence changes with development
        # First increases as basic organization improves, then might decrease
        # as more fantastical elements are incorporated, then increases again
        # with higher cognitive organization
        if new_level < 0.5:
            # Increasing coherence in early development
            self.state.coherence_level = min(1.0, 0.2 + 0.6 * new_level)
        elif new_level < 0.7:
            # Slight decrease during fantasy stage
            self.state.coherence_level = min(1.0, 0.5 + 0.2 * (new_level - 0.5))
        else:
            # Increasing again in later development
            self.state.coherence_level = min(1.0, 0.54 + 0.4 * (new_level - 0.7))
        
        # Scene novelty
        if previous_level < 0.5 and new_level >= 0.5:
            # Significant increase in novelty at fantasy stage
            self.state.novelty_level = min(1.0, self.state.novelty_level + 0.3)
            
        return new_level
    
    def _get_current_milestone(self) -> str:
        """Get the current developmental milestone"""
        milestone = "pre_imagination"
        for level, name in sorted(self.development_milestones.items()):
            if self.developmental_level >= level:
                milestone = name
        return milestone
    
    def _generate_scene(self, 
                       prompt_embedding: torch.Tensor, 
                       prompt: Dict[str, Any],
                       sequence_length: int,
                       initial_state: Optional[torch.Tensor] = None) -> Dict[str, Any]:
        """
        Generate an imaginative scene
        
        Args:
            prompt_embedding: Tensor embedding of the prompt
            prompt: Dictionary describing the prompt
            sequence_length: Length of sequence to generate
            initial_state: Optional initial state for continuation
            
        Returns:
            Dictionary containing the generated scene
        """
        # Process through neural network to generate scene
        with torch.no_grad():
            network_output = self.network(
                prompt_embedding,
                sequence_length=sequence_length,
                initial_state=initial_state
            )
            
        # Convert network output to scene description
        scene = self._format_scene(network_output, prompt)
        
        # Store the final state for potential continuation
        if "final_state" in network_output:
            # Convert tensor to list for JSON serialization
            scene["final_state"] = network_output["final_state"].cpu().numpy().tolist()
        
        return scene
    
    def _format_scene(self, 
                     network_output: Dict[str, torch.Tensor],
                     prompt: Dict[str, Any]) -> Dict[str, Any]:
        """
        Format neural network output into a structured scene
        
        Args:
            network_output: Raw output from the imagination network
            prompt: The original prompt for context
            
        Returns:
            Structured scene dictionary
        """
        # Extract scene elements
        sequence_length = network_output["objects"].shape[1]
        
        # In a real implementation, these would be decoded from the embeddings
        # Here we'll create placeholder scene elements
        scene = {
            "title": f"Imagined Scene: {prompt.get('description', 'Untitled')}",
            "description": prompt.get("description", "An imagined scene"),
            "sequence": []
        }
        
        # Create sequence of events
        for i in range(sequence_length):
            # In a real system, this would decode the embeddings into meaningful elements
            # Here we'll create placeholder elements
            scene_frame = {
                "frame_id": i,
                "setting": f"Setting {i+1}",
                "objects": [f"Object {j+1}" for j in range(3)],
                "agents": [f"Agent {j+1}" for j in range(2)],
                "actions": [f"Action {j+1}" for j in range(2)]
            }
            
            # Add more complexity based on development level
            if np.random.random() < self.state.scene_complexity:
                scene_frame["relationships"] = [f"Relationship {j+1}" for j in range(2)]
                scene_frame["emotions"] = [f"Emotion {j+1}" for j in range(2)]
            
            # Add to sequence
            scene["sequence"].append(scene_frame)
        
        # Add coherence elements based on developmental level
        if self.developmental_level >= 0.5:
            scene["theme"] = "Imagined theme"
            
        if self.developmental_level >= 0.7:
            scene["narrative_arc"] = {
                "beginning": "Start of the scene",
                "middle": "Development of the scene",
                "end": "Conclusion of the scene"
            }
            
        # Add abstract elements if at high development level
        if self.developmental_level >= 0.9:
            scene["abstract_concepts"] = ["Abstract concept 1", "Abstract concept 2"]
            scene["metaphors"] = ["Metaphor 1"]
        
        return scene
    
    def _calculate_scene_complexity(self, scene: Dict[str, Any]) -> float:
        """Calculate complexity score of a scene"""
        if not scene or "sequence" not in scene:
            return 0.0
            
        # Count elements in scene
        total_elements = 0
        for frame in scene["sequence"]:
            elements = 0
            # Count objects, agents, actions
            for key in ["objects", "agents", "actions", "relationships", "emotions"]:
                if key in frame and isinstance(frame[key], list):
                    elements += len(frame[key])
            total_elements += elements
            
        # Average elements per frame
        avg_elements = total_elements / len(scene["sequence"])
        
        # Normalize to [0, 1] range with developmental scaling
        return min(1.0, avg_elements / 10) * (0.5 + 0.5 * self.developmental_level)
    
    def _calculate_scene_coherence(self, scene: Dict[str, Any]) -> float:
        """Calculate coherence score of a scene"""
        if not scene or "sequence" not in scene:
            return 0.0
            
        # Base coherence
        coherence = 0.3
        
        # Check for title and description
        if "title" in scene and isinstance(scene["title"], str) and scene["title"]:
            coherence += 0.1
        if "description" in scene and isinstance(scene["description"], str) and scene["description"]:
            coherence += 0.1
            
        # Check for thematic elements
        if "theme" in scene:
            coherence += 0.1
        if "narrative_arc" in scene:
            coherence += 0.2
        
        # Check sequence continuity (simplified)
        if len(scene["sequence"]) > 1:
            coherence += 0.1
            
        # Developmental scaling
        scaled_coherence = coherence * (0.5 + 0.5 * self.developmental_level)
        
        return min(1.0, scaled_coherence)
    
    def _calculate_scene_novelty(self, scene: Dict[str, Any]) -> float:
        """Calculate novelty score of a scene"""
        # In a real system, this would compare the scene to prior scenes
        # Here we'll use a simplified approach based on developmental level
        
        # Base novelty
        base_novelty = 0.3
        
        # Development factor - higher development enables more novelty
        dev_factor = 0.5 * self.developmental_level
        
        # Random factor for variation
        random_factor = 0.2 * np.random.random()
        
        # Check for fantastical elements (more likely with higher development)
        if self.developmental_level >= 0.5:
            # Fantasy bonus
            fantasy_factor = 0.2
        else:
            fantasy_factor = 0.0
        
        return min(1.0, base_novelty + dev_factor + random_factor + fantasy_factor)
    
    def _update_imagination_metrics(self, scene: Dict[str, Any]) -> None:
        """Update imagination metrics based on generated scene"""
        # Calculate new values
        complexity = self._calculate_scene_complexity(scene)
        coherence = self._calculate_scene_coherence(scene)
        novelty = self._calculate_scene_novelty(scene)
        
        # Update with smoothing
        smoothing = 0.3
        self.state.scene_complexity = (1 - smoothing) * self.state.scene_complexity + smoothing * complexity
        self.state.coherence_level = (1 - smoothing) * self.state.coherence_level + smoothing * coherence
        self.state.novelty_level = (1 - smoothing) * self.state.novelty_level + smoothing * novelty
    
    def _handle_prompt(self, message: Message) -> None:
        """Handle imagination prompt messages"""
        if isinstance(message.content, dict):
            # Process the prompt to generate a scene
            self.process_input({
                "prompt": message.content,
                "scene_id": message.content.get("scene_id", str(uuid.uuid4())),
                "sequence_length": message.content.get("sequence_length", 5)
            })
    
    def _handle_scene_request(self, message: Message) -> None:
        """Handle scene request messages"""
        if isinstance(message.content, dict):
            # Process the scene request
            result = self.process_input(message.content)
            
            # Publish result if successful
            if result["status"] == "success" and self.event_bus:
                self.event_bus.publish(
                    msg_type="scene_result",
                    content=result,
                    source=self.module_id,
                    target=message.source
                )


#######################

#creativity\models.py#
#######################

from pydantic import BaseModel, Field, model_validator
from typing import Dict, List, Any, Optional, Set, Union, Tuple
from datetime import datetime
import uuid
import numpy as np

class Concept(BaseModel):
    """Representation of a concept in the creative system"""
    concept_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    name: str
    features: Dict[str, Any] = Field(default_factory=dict)
    associations: List[str] = Field(default_factory=list)  # IDs of related concepts
    source: str = "unknown"
    creation_time: datetime = Field(default_factory=datetime.now)
    
    model_config = {
        "arbitrary_types_allowed": True
    }

class ConceptCombinationState(BaseModel):
    """State for the concept combination component"""
    combinations: Dict[str, Dict[str, Any]] = Field(default_factory=dict)  # Maps combination_id to combination data
    combination_patterns: Dict[str, float] = Field(default_factory=dict)  # Maps pattern types to usage frequency
    recent_combinations: List[str] = Field(default_factory=list)  # IDs of recent combinations
    concept_cache: Dict[str, Concept] = Field(default_factory=dict)  # Cached concepts for faster access
    
    model_config = {
        "arbitrary_types_allowed": True
    }

class DivergentThinkingState(BaseModel):
    """State for the divergent thinking component"""
    solution_spaces: Dict[str, Dict[str, Any]] = Field(default_factory=dict)  # Maps problem_id to solution space
    fluency_score: float = Field(default=0.1, ge=0.0, le=1.0)  # Ability to generate many ideas
    flexibility_score: float = Field(default=0.1, ge=0.0, le=1.0)  # Ability to generate diverse ideas
    originality_score: float = Field(default=0.1, ge=0.0, le=1.0)  # Ability to generate novel ideas
    elaboration_score: float = Field(default=0.1, ge=0.0, le=1.0)  # Ability to develop ideas in detail
    
    model_config = {
        "arbitrary_types_allowed": True
    }

class ImaginationState(BaseModel):
    """State for the imagination component"""
    scenes: Dict[str, Dict[str, Any]] = Field(default_factory=dict)  # Maps scene_id to scene data
    active_scene: Optional[str] = None  # ID of currently active scene
    scene_complexity: float = Field(default=0.1, ge=0.0, le=1.0)  # Complexity level of generated scenes
    coherence_level: float = Field(default=0.3, ge=0.0, le=1.0)  # How coherent the scenes are
    novelty_level: float = Field(default=0.5, ge=0.0, le=1.0)  # How novel the scenes are
    
    model_config = {
        "arbitrary_types_allowed": True
    }

class NoveltyDetectionState(BaseModel):
    """State for the novelty detection component"""
    novelty_thresholds: Dict[str, float] = Field(default_factory=dict)  # Thresholds for different input types
    recently_processed: List[Dict[str, Any]] = Field(default_factory=list)  # Recent inputs for baseline
    novelty_scores: Dict[str, float] = Field(default_factory=dict)  # Maps item_id to novelty score
    surprise_sensitivity: float = Field(default=0.5, ge=0.0, le=1.0)  # Sensitivity to unexpected inputs
    
    model_config = {
        "arbitrary_types_allowed": True
    }

class CreativeOutput(BaseModel):
    """A creative output generated by the system"""
    output_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    content: Dict[str, Any]
    output_type: str  # Type of creative output (concept, scene, solution, etc.)
    novelty_score: float = Field(default=0.5, ge=0.0, le=1.0)
    coherence_score: float = Field(default=0.5, ge=0.0, le=1.0)
    usefulness_score: float = Field(default=0.5, ge=0.0, le=1.0)
    source_components: List[str] = Field(default_factory=list)  # Components that contributed
    creation_time: datetime = Field(default_factory=datetime.now)
    
    model_config = {
        "arbitrary_types_allowed": True
    }

class CreativityState(BaseModel):
    """Integrated state of the creativity system"""
    concept_combination: ConceptCombinationState = Field(default_factory=ConceptCombinationState)
    divergent_thinking: DivergentThinkingState = Field(default_factory=DivergentThinkingState)
    imagination: ImaginationState = Field(default_factory=ImaginationState)
    novelty_detection: NoveltyDetectionState = Field(default_factory=NoveltyDetectionState)
    
    # Overall creativity metrics
    fluidity: float = Field(default=0.1, ge=0.0, le=1.0)  # Ability to flow between ideas
    originality: float = Field(default=0.1, ge=0.0, le=1.0)  # Overall originality of outputs
    elaboration: float = Field(default=0.1, ge=0.0, le=1.0)  # Detail and richness of outputs
    abstraction: float = Field(default=0.1, ge=0.0, le=1.0)  # Ability to work with abstract concepts
    
    # Developmental level of creativity
    developmental_level: float = Field(default=0.0, ge=0.0, le=1.0)
    
    # Recent creative outputs
    recent_outputs: List[CreativeOutput] = Field(default_factory=list)
    
    # Time tracking
    last_update: datetime = Field(default_factory=datetime.now)
    
    model_config = {
        "arbitrary_types_allowed": True
    }
    
    @model_validator(mode='after')
    def update_creativity_metrics(self):
        """Update overall creativity metrics based on component states"""
        # Fluidity combines divergent thinking fluency and imagination coherence
        self.fluidity = (
            self.divergent_thinking.fluency_score * 0.6 +
            self.imagination.coherence_level * 0.4
        ) * (0.5 + 0.5 * self.developmental_level)
        
        # Originality combines divergent thinking originality and imagination novelty
        self.originality = (
            self.divergent_thinking.originality_score * 0.5 +
            self.imagination.novelty_level * 0.5
        ) * (0.5 + 0.5 * self.developmental_level)
        
        # Elaboration combines divergent thinking elaboration and imagination complexity
        self.elaboration = (
            self.divergent_thinking.elaboration_score * 0.5 +
            self.imagination.scene_complexity * 0.5
        ) * (0.5 + 0.5 * self.developmental_level)
        
        # Abstraction is derived from concept combination complexity
        pattern_complexity = sum(self.concept_combination.combination_patterns.values()) / max(1, len(self.concept_combination.combination_patterns))
        self.abstraction = pattern_complexity * (0.5 + 0.5 * self.developmental_level)
        
        return self


#######################

#creativity\neural_net.py#
#######################

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Any, Optional, Set, Union, Tuple
import numpy as np

class ConceptEncoder(nn.Module):
    """Neural encoder for concept representation"""
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 128):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Encode input into a concept representation"""
        return self.encoder(x)

class ConceptCombiner(nn.Module):
    """Neural network for combining concepts"""
    def __init__(self, concept_dim: int = 128, hidden_dim: int = 256, output_dim: int = 128):
        super().__init__()
        # Encode individual concepts
        self.concept_encoder = ConceptEncoder(concept_dim, hidden_dim, concept_dim)
        
        # Combination networks for different patterns
        self.blend_network = nn.Sequential(
            nn.Linear(concept_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        
        self.property_transfer_network = nn.Sequential(
            nn.Linear(concept_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        
        self.analogy_network = nn.Sequential(
            nn.Linear(concept_dim * 3, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        
        # Attention mechanism for feature selection
        self.attention = nn.Sequential(
            nn.Linear(concept_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
        
    def forward(self, 
                concepts: List[torch.Tensor], 
                combination_type: str = "blend") -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Combine concepts using specified combination type
        
        Args:
            concepts: List of concept tensors to combine
            combination_type: Type of combination ("blend", "property_transfer", "analogy")
            
        Returns:
            Tuple of (combined concept tensor, attention weights)
        """
        # Ensure we have enough concepts
        if len(concepts) < 2:
            if len(concepts) == 1:
                return concepts[0], {"attention": None}
            else:
                # Return zero tensor if no concepts
                device = next(self.parameters()).device
                return torch.zeros(1, self.blend_network[-1].out_features, device=device), {"attention": None}
        
        # Encode concepts
        encoded_concepts = [self.concept_encoder(c) for c in concepts]
        
        # Apply different combination patterns
        if combination_type == "blend" and len(concepts) >= 2:
            # Conceptual blending of two concepts
            c1, c2 = encoded_concepts[0], encoded_concepts[1]
            
            # Calculate attention weights for each concept
            attn1 = self.attention(c1)
            attn2 = self.attention(c2)
            
            # Apply attention weighting
            c1_weighted = c1 * attn1
            c2_weighted = c2 * attn2
            
            # Concatenate and process through blend network
            combined = torch.cat([c1_weighted, c2_weighted], dim=-1)
            result = self.blend_network(combined)
            
            return result, {"attention": (attn1, attn2)}
            
        elif combination_type == "property_transfer" and len(concepts) >= 2:
            # Transfer properties from one concept to another
            c1, c2 = encoded_concepts[0], encoded_concepts[1]
            
            # Calculate attention weights for feature selection
            attn = self.attention(c2)  # Features to transfer
            
            # Weighted combination
            combined = torch.cat([c1, c2 * attn], dim=-1)
            result = self.property_transfer_network(combined)
            
            return result, {"attention": attn}
            
        elif combination_type == "analogy" and len(concepts) >= 3:
            # Analogical mapping: A is to B as C is to ?
            c1, c2, c3 = encoded_concepts[0], encoded_concepts[1], encoded_concepts[2]
            
            # Calculate relationship between A and B
            relation = c2 - c1
            
            # Apply relationship to C
            combined = torch.cat([c1, c2, c3], dim=-1)
            result = self.analogy_network(combined)
            
            return result, {"relation": relation}
            
        else:
            # Default to simple blending
            c1, c2 = encoded_concepts[0], encoded_concepts[1]
            combined = torch.cat([c1, c2], dim=-1)
            result = self.blend_network(combined)
            
            return result, {"attention": None}

class DivergentGenerator(nn.Module):
    """Neural network for divergent thinking and idea generation"""
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 128, latent_dim: int = 64):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim * 2)  # For mean and log variance
        )
        
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        
        # Additional heads for generating diverse solutions
        self.solution_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(latent_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, output_dim)
            ) for _ in range(4)  # Multiple solution paths
        ])
        
        self.latent_dim = latent_dim
        
    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Encode input to latent space parameters (mean, log_var)"""
        h = self.encoder(x)
        mean, log_var = torch.chunk(h, 2, dim=-1)
        return mean, log_var
        
    def reparameterize(self, mean: torch.Tensor, log_var: torch.Tensor) -> torch.Tensor:
        """Sample from latent space using reparameterization trick"""
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mean + eps * std
        
    def decode(self, z: torch.Tensor) -> torch.Tensor:
        """Decode from latent space"""
        return self.decoder(z)
        
    def generate_diverse_solutions(self, z: torch.Tensor, 
                                  diversity_factor: float = 1.0) -> List[torch.Tensor]:
        """Generate multiple diverse solutions from the latent representation"""
        solutions = []
        
        # Base solution
        solutions.append(self.decode(z))
        
        # Generate diverse variations
        for head in self.solution_heads:
            # Add noise to latent vector proportional to diversity factor
            noise = torch.randn_like(z) * diversity_factor
            z_diverse = z + noise
            solutions.append(head(z_diverse))
            
        return solutions
        
    def forward(self, x: torch.Tensor, 
               diversity_factor: float = 1.0, 
               num_solutions: int = 5) -> Dict[str, Any]:
        """
        Process input for divergent thinking
        
        Args:
            x: Input tensor representing problem or prompt
            diversity_factor: Controls how diverse the solutions should be
            num_solutions: Number of solutions to generate
            
        Returns:
            Dictionary with generated solutions and latent representations
        """
        # Encode to latent space
        mean, log_var = self.encode(x)
        
        # Sample from latent space
        z = self.reparameterize(mean, log_var)
        
        # Generate diverse solutions
        solutions = self.generate_diverse_solutions(z, diversity_factor)
        
        # Limit solutions to requested number
        solutions = solutions[:num_solutions]
        
        return {
            "solutions": solutions,
            "latent": z,
            "mean": mean,
            "log_var": log_var
        }

class ImaginationNetwork(nn.Module):
    """Neural network for imagination and scenario generation"""
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 128, 
                 sequence_length: int = 10):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # Recurrent network for temporal coherence
        self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True)
        
        # Scene element generators
        self.object_generator = nn.Linear(hidden_dim, output_dim)
        self.agent_generator = nn.Linear(hidden_dim, output_dim)
        self.action_generator = nn.Linear(hidden_dim, output_dim)
        self.setting_generator = nn.Linear(hidden_dim, output_dim)
        
        # Integration layer
        self.scene_integrator = nn.Sequential(
            nn.Linear(output_dim * 4, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        
        self.sequence_length = sequence_length
        
    def forward(self, x: torch.Tensor, 
               sequence_length: Optional[int] = None,
               initial_state: Optional[torch.Tensor] = None) -> Dict[str, Any]:
        """
        Generate imaginative scene or sequence
        
        Args:
            x: Input tensor (seed or prompt)
            sequence_length: Length of sequence to generate
            initial_state: Initial hidden state for RNN
            
        Returns:
            Dictionary with scene elements and integrated scene
        """
        batch_size = x.shape[0]
        seq_len = sequence_length or self.sequence_length
        
        # Encode input
        encoded = self.encoder(x).unsqueeze(1)  # Add sequence dimension
        
        # Expand to desired sequence length
        encoded = encoded.expand(-1, seq_len, -1)
        
        # Generate sequence with temporal coherence
        if initial_state is None:
            hidden_states, final_state = self.rnn(encoded)
        else:
            hidden_states, final_state = self.rnn(encoded, initial_state)
        
        # Generate scene elements for each step in sequence
        objects = self.object_generator(hidden_states)
        agents = self.agent_generator(hidden_states)
        actions = self.action_generator(hidden_states)
        settings = self.setting_generator(hidden_states)
        
        # Integrate scene elements
        scene_elements = torch.cat([objects, agents, actions, settings], dim=-1)
        integrated_scene = self.scene_integrator(scene_elements)
        
        return {
            "objects": objects,
            "agents": agents,
            "actions": actions,
            "settings": settings,
            "integrated_scene": integrated_scene,
            "hidden_states": hidden_states,
            "final_state": final_state
        }

class NoveltyDetector(nn.Module):
    """Neural network for detecting novelty in inputs"""
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, memory_size: int = 100):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim)
        )
        
        # Memory module to store recent experiences
        self.register_buffer("memory", torch.zeros(memory_size, input_dim))
        self.memory_counter = 0
        self.memory_size = memory_size
        
        # Novelty scoring network
        self.novelty_scorer = nn.Sequential(
            nn.Linear(input_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
        
    def update_memory(self, encoded: torch.Tensor) -> None:
        """Update memory with new encoded inputs"""
        batch_size = encoded.shape[0]
        
        for i in range(batch_size):
            idx = self.memory_counter % self.memory_size
            self.memory[idx] = encoded[i].detach()
            self.memory_counter += 1
    
    def compute_novelty(self, encoded: torch.Tensor) -> torch.Tensor:
        """Compute novelty by comparing to memory"""
        # If memory is empty, everything is novel
        if self.memory_counter == 0:
            return torch.ones(encoded.shape[0], 1, device=encoded.device)
        
        # Compute minimum distance to memory items
        memory_size = min(self.memory_size, self.memory_counter)
        memory = self.memory[:memory_size]
        
        # Compute distance to all memory items
        distances = torch.cdist(encoded, memory)
        
        # Take minimum distance for each input
        min_distances, _ = torch.min(distances, dim=1, keepdim=True)
        
        # Normalize distances to [0, 1] range
        normalized_distances = torch.tanh(min_distances)
        
        return normalized_distances
        
    def forward(self, x: torch.Tensor, update_memory: bool = True) -> Dict[str, torch.Tensor]:
        """
        Detect novelty in input
        
        Args:
            x: Input tensor to evaluate for novelty
            update_memory: Whether to update memory with this input
            
        Returns:
            Dictionary with novelty scores and encoded representation
        """
        # Encode input
        encoded = self.encoder(x)
        
        # Compute novelty score
        novelty_score = self.compute_novelty(encoded)
        
        # Update memory if requested
        if update_memory:
            self.update_memory(encoded)
            
        return {
            "novelty_score": novelty_score,
            "encoded": encoded
        }

class CreativityNetwork(nn.Module):
    """Integrated neural network for creative processing"""
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 128):
        super().__init__()
        self.input_embedding = nn.Linear(input_dim, hidden_dim)
        
        # Creativity components
        self.concept_combiner = ConceptCombiner(hidden_dim, hidden_dim, output_dim)
        self.divergent_generator = DivergentGenerator(hidden_dim, hidden_dim, output_dim)
        self.imagination = ImaginationNetwork(hidden_dim, hidden_dim, output_dim)
        self.novelty_detector = NoveltyDetector(hidden_dim, hidden_dim)
        
        # Integration layer
        self.integration = nn.Sequential(
            nn.Linear(output_dim * 4, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        
        # Developmental gate
        self.development_gate = nn.Parameter(torch.tensor(0.1))
        
    def forward(self, inputs: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """
        Process inputs through the creativity network
        
        Args:
            inputs: Dictionary of input tensors for different creativity processes
            
        Returns:
            Dictionary of creativity processing results
        """
        # Get device
        device = next(self.parameters()).device
        
        # Process concept combination if inputs available
        concept_result = None
        if "concepts" in inputs and inputs["concepts"] is not None:
            concepts = [self.input_embedding(c) for c in inputs["concepts"]]
            combination_type = inputs.get("combination_type", "blend")
            concept_result, attention = self.concept_combiner(concepts, combination_type)
        
        # Process divergent thinking if inputs available
        divergent_result = None
        if "problem" in inputs and inputs["problem"] is not None:
            embedded_problem = self.input_embedding(inputs["problem"])
            diversity_factor = inputs.get("diversity_factor", 1.0)
            num_solutions = inputs.get("num_solutions", 5)
            divergent_result = self.divergent_generator(
                embedded_problem, 
                diversity_factor, 
                num_solutions
            )
        
        # Process imagination if inputs available
        imagination_result = None
        if "seed" in inputs and inputs["seed"] is not None:
            embedded_seed = self.input_embedding(inputs["seed"])
            sequence_length = inputs.get("sequence_length", 10)
            initial_state = inputs.get("initial_state", None)
            imagination_result = self.imagination(
                embedded_seed,
                sequence_length,
                initial_state
            )
        
        # Process novelty detection if inputs available
        novelty_result = None
        if "input" in inputs and inputs["input"] is not None:
            embedded_input = self.input_embedding(inputs["input"])
            update_memory = inputs.get("update_memory", True)
            novelty_result = self.novelty_detector(embedded_input, update_memory)
        
        # Apply developmental gating
        dev_level = torch.sigmoid(self.development_gate)
        
        # Create integrated result
        result = {
            "concept_combination": concept_result,
            "divergent_thinking": divergent_result,
            "imagination": imagination_result,
            "novelty_detection": novelty_result,
            "developmental_level": dev_level.item()
        }
        
        # Integrate results if all components have produced output
        if all(v is not None for v in [concept_result, divergent_result, imagination_result, novelty_result]):
            # Extract main outputs from each component
            concept_out = concept_result
            divergent_out = divergent_result["solutions"][0]  # Take first solution
            imagination_out = imagination_result["integrated_scene"][:, -1]  # Take last frame
            novelty_out = novelty_result["encoded"]
            
            # Concatenate and integrate
            combined = torch.cat([concept_out, divergent_out, imagination_out, novelty_out], dim=-1)
            integrated = self.integration(combined)
            
            result["integrated_output"] = integrated
        
        return result
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental parameter
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        with torch.no_grad():
            current = torch.sigmoid(self.development_gate).item()
            target = min(1.0, current + amount)
            # Convert from probability space back to unbounded space
            if target >= 0.99:
                self.development_gate.data = torch.tensor(6.0)  # Approximately sigmoid(6) ≈ 0.998
            else:
                self.development_gate.data = torch.tensor(np.log(target / (1 - target)))
            return torch.sigmoid(self.development_gate).item()


#######################

#creativity\novelty_detection.py#
#######################

# TODO: Implement the NoveltyDetection class to identify unusual or surprising patterns
# This component should be able to:
# - Detect statistically unusual patterns in inputs
# - Identify violations of expectations
# - Recognize novelty in different domains (perceptual, conceptual, etc.)
# - Distinguish between degrees of novelty

# TODO: Implement developmental progression in novelty detection:
# - Simple statistical outlier detection in early stages
# - Basic expectation violation detection in childhood
# - Complex pattern novelty recognition in adolescence
# - Subtle novelty detection in adulthood

# TODO: Create mechanisms for:
# - Statistical novelty: Detect low-probability patterns
# - Expectation violation: Identify deviations from predictions
# - Conceptual novelty: Recognize unusual concept combinations
# - Contextual novelty: Detect appropriateness for context

# TODO: Implement novelty signals that:
# - Direct attention to novel stimuli
# - Trigger curiosity and exploration
# - Modulate learning rates for novel information
# - Contribute to emotional reactions (surprise, interest)

# TODO: Connect to attention, memory, and learning systems
# Novelty detection should guide attention, enhance memory formation,
# and influence learning rates for novel information

from typing import Dict, List, Any, Optional, Set, Union, Tuple
from datetime import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import uuid
from collections import deque

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus, Message
from lmm_project.modules.creativity.models import NoveltyDetectionState, CreativeOutput
from lmm_project.modules.creativity.neural_net import NoveltyDetector

class NoveltyDetection(BaseModule):
    """
    Identifies novel and surprising inputs
    
    This module detects novelty in inputs by comparing them
    to prior experiences and expectations, enabling the system
    to identify and attend to new and unexpected information.
    
    Developmental progression:
    - Basic feature mismatch detection in early stages
    - Statistical novelty detection in childhood 
    - Contextual surprise detection in adolescence
    - Abstract conceptual novelty recognition in adulthood
    """
    
    # Developmental milestones for novelty detection
    development_milestones = {
        0.0: "feature_mismatch",     # Basic detection of mismatched features
        0.25: "statistical_novelty", # Statistical deviation from baseline
        0.5: "contextual_surprise",  # Surprise in specific contexts
        0.75: "expectation_violation", # Violation of abstract expectations
        0.9: "conceptual_novelty"    # Novelty at the conceptual level
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the novelty detection module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="novelty_detection", event_bus=event_bus)
        
        # Initialize state
        self.state = NoveltyDetectionState()
        
        # Initialize neural network for novelty detection
        self.input_dim = 128  # Default dimension
        self.network = NoveltyDetector(
            input_dim=self.input_dim,
            hidden_dim=256,
            memory_size=100  # Store last 100 experiences
        )
        
        # Initialize novelty thresholds for different input types
        self.state.novelty_thresholds = {
            "perception": 0.7,  # Higher threshold for perceptual inputs
            "concept": 0.6,     # Medium threshold for conceptual inputs
            "emotion": 0.5,     # Lower threshold for emotional inputs
            "memory": 0.6,      # Medium threshold for memory inputs
            "default": 0.65     # Default threshold
        }
        
        # Initialize memory of recently processed inputs
        self.state.recently_processed = []  # Limited by max_memory_size
        self.max_memory_size = 50  # Maximum number of items to store
        
        # Initialize baseline distributions for different input types
        self.baselines = {
            "perception": {"mean": None, "std": None, "samples": []},
            "concept": {"mean": None, "std": None, "samples": []},
            "emotion": {"mean": None, "std": None, "samples": []},
            "memory": {"mean": None, "std": None, "samples": []}
        }
        
        # Maximum samples to store for each baseline
        self.max_baseline_samples = 100
        
        # Subscribe to relevant events
        if self.event_bus:
            self.event_bus.subscribe("perception_input", self._handle_input)
            self.event_bus.subscribe("concept_created", self._handle_input)
            self.event_bus.subscribe("emotion_state", self._handle_input)
            self.event_bus.subscribe("memory_retrieved", self._handle_input)
            self.event_bus.subscribe("novelty_query", self._handle_novelty_query)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to detect novelty
        
        Args:
            input_data: Dictionary containing input to evaluate for novelty
            
        Returns:
            Dictionary with the results of novelty detection
        """
        # Extract input information
        input_type = input_data.get("type", "default")
        content = input_data.get("content", {})
        input_id = input_data.get("input_id", str(uuid.uuid4()))
        update_baseline = input_data.get("update_baseline", True)
        context = input_data.get("context", {})
        
        # Validate input
        if not content:
            return {
                "status": "error",
                "message": "Input content required",
                "module_id": self.module_id,
                "module_type": self.module_type
            }
            
        # Create input embedding (simplified - in a real system you would use actual embeddings)
        input_embedding = torch.randn(1, self.input_dim)
            
        # Detect novelty
        try:
            novelty_result = self._detect_novelty(input_embedding, content, input_type)
            
            # Get appropriate threshold for this input type
            threshold = self.state.novelty_thresholds.get(input_type, self.state.novelty_thresholds["default"])
            
            # Determine if the input is novel based on threshold
            is_novel = novelty_result["novelty_score"] > threshold
            
            # Update novelty scores dictionary
            self.state.novelty_scores[input_id] = novelty_result["novelty_score"]
            
            # Add to recently processed inputs
            self._update_recent_inputs(input_id, content, input_type, novelty_result["novelty_score"], is_novel)
            
            # Update baseline distribution if requested
            if update_baseline:
                self._update_baseline(input_embedding, input_type)
            
            # Create result
            result = {
                "status": "success",
                "module_id": self.module_id,
                "module_type": self.module_type,
                "input_id": input_id,
                "input_type": input_type,
                "novelty_score": novelty_result["novelty_score"],
                "is_novel": is_novel,
                "threshold": threshold,
                "surprise_level": self._calculate_surprise(novelty_result["novelty_score"], threshold)
            }
            
            # If input is novel, create and publish a creative output
            if is_novel and self.event_bus:
                creative_output = CreativeOutput(
                    content={
                        "input_id": input_id,
                        "input_type": input_type,
                        "content": content,
                        "novelty_details": {
                            "novelty_score": novelty_result["novelty_score"],
                            "threshold": threshold,
                            "surprise_level": result["surprise_level"]
                        }
                    },
                    output_type="novel_input",
                    novelty_score=novelty_result["novelty_score"],
                    coherence_score=0.5,  # Default coherence
                    usefulness_score=0.5,  # Default usefulness
                    source_components=[self.module_id]
                )
                
                self.event_bus.publish(
                    msg_type="creative_output",
                    content=creative_output.model_dump()
                )
                
                # Also publish a direct novelty alert
                self.event_bus.publish(
                    msg_type="novelty_alert",
                    content=result
                )
            
            return result
            
        except Exception as e:
            return {
                "status": "error",
                "message": f"Error detecting novelty: {str(e)}",
                "module_id": self.module_id,
                "module_type": self.module_type
            }
        
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        previous_level = self.developmental_level
        new_level = super().update_development(amount)
        
        # Update novelty detection capabilities based on development
        
        # Adjust novelty thresholds (gradually lower thresholds to detect more subtle novelty)
        threshold_reduction = 0.2 * (new_level - previous_level)
        for input_type in self.state.novelty_thresholds:
            self.state.novelty_thresholds[input_type] = max(
                0.3,  # Minimum threshold
                self.state.novelty_thresholds[input_type] - threshold_reduction
            )
        
        # Adjust surprise sensitivity (increases with development)
        self.state.surprise_sensitivity = min(1.0, 0.3 + 0.7 * new_level)
        
        return new_level
    
    def _get_current_milestone(self) -> str:
        """Get the current developmental milestone"""
        milestone = "pre_novelty_detection"
        for level, name in sorted(self.development_milestones.items()):
            if self.developmental_level >= level:
                milestone = name
        return milestone
    
    def _detect_novelty(self, 
                       input_embedding: torch.Tensor, 
                       content: Dict[str, Any],
                       input_type: str) -> Dict[str, Any]:
        """
        Detect novelty in an input
        
        Args:
            input_embedding: Tensor embedding of the input
            content: The input content
            input_type: Type of input (perception, concept, etc.)
            
        Returns:
            Dictionary with novelty detection results
        """
        # Process through neural network to detect novelty
        with torch.no_grad():
            # Get appropriate update_memory setting based on developmental level
            # More developed systems are more selective about what they add to memory
            update_memory = np.random.random() < (1.0 - 0.5 * self.developmental_level)
            
            network_output = self.network(
                input_embedding,
                update_memory=update_memory
            )
            
        # Extract novelty score
        novelty_score = network_output["novelty_score"].item()
        
        # Apply developmental modulation to novelty detection
        if self.developmental_level < 0.25:
            # Basic feature novelty only at early stages
            novelty_score = self._modulate_early_novelty(novelty_score, content)
        elif self.developmental_level < 0.5:
            # Statistical novelty at intermediate stages
            novelty_score = self._apply_statistical_novelty(novelty_score, input_type)
        elif self.developmental_level < 0.75:
            # Contextual surprise at advanced stages
            context_modifier = np.random.uniform(0.8, 1.2)  # Simplified context effect
            novelty_score = min(1.0, novelty_score * context_modifier)
        else:
            # Conceptual novelty at highly developed stages
            # This might even reduce novelty of some inputs that are conceptually expected
            conceptual_modifier = 1.0 + 0.3 * (np.random.random() - 0.5)  # [-0.15, 0.15] adjustment
            novelty_score = max(0.0, min(1.0, novelty_score * conceptual_modifier))
        
        return {
            "novelty_score": novelty_score,
            "encoded": network_output["encoded"].cpu().numpy().tolist()
        }
    
    def _modulate_early_novelty(self, 
                              novelty_score: float, 
                              content: Dict[str, Any]) -> float:
        """Modulate novelty for early development stages focusing on features"""
        # Simplified feature-based novelty - in a real system this would analyze features
        # Here we'll just add some random variation
        feature_modifier = 0.2 * np.random.random()
        return min(1.0, novelty_score + feature_modifier)
    
    def _apply_statistical_novelty(self, 
                                 novelty_score: float, 
                                 input_type: str) -> float:
        """Apply statistical novelty detection based on baseline distributions"""
        baseline = self.baselines.get(input_type, self.baselines["perception"])
        
        # If we don't have enough samples for a baseline, return the raw score
        if baseline["mean"] is None or baseline["std"] is None:
            return novelty_score
            
        # Calculate z-score
        z_score = (novelty_score - baseline["mean"]) / max(0.01, baseline["std"])
        
        # Convert to probability using sigmoid
        probability = 1.0 / (1.0 + np.exp(-z_score))
        
        # Blend raw score with statistical score
        blended_score = 0.3 * novelty_score + 0.7 * probability
        
        return min(1.0, blended_score)
    
    def _update_baseline(self, 
                        input_embedding: torch.Tensor, 
                        input_type: str) -> None:
        """Update baseline distribution for an input type"""
        # Get relevant baseline
        baseline = self.baselines.get(input_type, self.baselines["perception"])
        
        # Extract novelty score from neural network
        with torch.no_grad():
            network_output = self.network(input_embedding, update_memory=False)
            novelty_score = network_output["novelty_score"].item()
            
        # Add to samples
        baseline["samples"].append(novelty_score)
        
        # Limit samples to max size
        if len(baseline["samples"]) > self.max_baseline_samples:
            baseline["samples"] = baseline["samples"][-self.max_baseline_samples:]
            
        # Update statistics
        baseline["mean"] = np.mean(baseline["samples"])
        baseline["std"] = np.std(baseline["samples"]) if len(baseline["samples"]) > 1 else 0.1
    
    def _update_recent_inputs(self, 
                            input_id: str, 
                            content: Dict[str, Any], 
                            input_type: str,
                            novelty_score: float,
                            is_novel: bool) -> None:
        """Update the record of recently processed inputs"""
        # Create input record
        input_record = {
            "input_id": input_id,
            "input_type": input_type,
            "timestamp": datetime.now().isoformat(),
            "novelty_score": novelty_score,
            "is_novel": is_novel,
            "content_summary": self._create_content_summary(content)
        }
        
        # Add to recently processed
        self.state.recently_processed.append(input_record)
        
        # Limit to max size
        if len(self.state.recently_processed) > self.max_memory_size:
            self.state.recently_processed = self.state.recently_processed[-self.max_memory_size:]
    
    def _create_content_summary(self, content: Dict[str, Any]) -> Dict[str, Any]:
        """Create a summary of input content (to avoid storing everything)"""
        # In a real system, this would create a compact summary
        # Here we'll just do a simple filtering
        
        if not isinstance(content, dict):
            return {"type": str(type(content))}
            
        # Create a summary with a subset of keys
        summary = {}
        
        # Include a maximum of 5 keys
        for i, (key, value) in enumerate(content.items()):
            if i >= 5:
                break
                
            # Simplify values to strings if they're complex
            if isinstance(value, dict):
                summary[key] = "dict"
            elif isinstance(value, list):
                summary[key] = f"list[{len(value)}]"
            else:
                summary[key] = str(value)[:50]  # Truncate long strings
                
        return summary
    
    def _calculate_surprise(self, novelty_score: float, threshold: float) -> float:
        """Calculate surprise level based on novelty score and threshold"""
        # No surprise if below threshold
        if novelty_score <= threshold:
            return 0.0
            
        # Calculate how far above threshold
        surprise_factor = (novelty_score - threshold) / (1.0 - threshold)
        
        # Modulate by surprise sensitivity
        surprise_level = surprise_factor * self.state.surprise_sensitivity
        
        return min(1.0, surprise_level)
    
    def _handle_input(self, message: Message) -> None:
        """Handle various input messages for novelty detection"""
        # Process the input for novelty
        input_type = message.msg_type.split('_')[0]  # Extract first part of message type
        
        self.process_input({
            "type": input_type,
            "content": message.content,
            "input_id": str(uuid.uuid4())
        })
    
    def _handle_novelty_query(self, message: Message) -> None:
        """Handle explicit novelty query messages"""
        if isinstance(message.content, dict):
            # Process the query
            result = self.process_input(message.content)
            
            # Publish result
            if self.event_bus:
                self.event_bus.publish(
                    msg_type="novelty_result",
                    content=result,
                    source=self.module_id,
                    target=message.source
                )


#######################

#creativity\__init__.py#
#######################

# Creativity module for the LMM system
# Responsible for creative generation, novelty detection, and concept combination

from typing import Dict, List, Any, Optional, Set, Union, Tuple
from datetime import datetime
import uuid

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus, Message
from lmm_project.modules.creativity.models import CreativityState, CreativeOutput, CreativityMetrics
from lmm_project.modules.creativity.concept_combination import ConceptCombination
from lmm_project.modules.creativity.divergent_thinking import DivergentThinking
from lmm_project.modules.creativity.imagination import Imagination
from lmm_project.modules.creativity.novelty_detection import NoveltyDetection

class CreativityModule(BaseModule):
    """
    Integrated creativity module responsible for generating novel and useful ideas
    
    This module combines several submodules for different aspects of creativity:
    - Novelty detection: identifying unusual or surprising elements
    - Concept combination: creating new ideas by combining existing concepts
    - Divergent thinking: generating multiple potential solutions
    - Imagination: generating novel mental scenarios and simulations
    
    The creativity module integrates these components and coordinates their
    development, providing unified creative capabilities for the LMM system.
    
    Developmental progression:
    - Basic novelty detection and simple associations in infancy
    - Expanding combinatorial play in early childhood
    - Structured creative problem-solving in later childhood
    - Metaphorical thinking and abstraction in adolescence
    - Integrated creative cognition with metacognitive awareness in adulthood
    """
    
    # Developmental milestones for creativity
    development_milestones = {
        0.0: "novelty_awareness",     # Basic novelty detection
        0.2: "associative_play",      # Simple combinations and associations
        0.4: "divergent_exploration", # Multiple solution generation
        0.6: "imaginative_scenarios", # Mental simulation capabilities  
        0.8: "metaphorical_thinking", # Abstract creative connections
        0.95: "creative_metacognition" # Self-aware creative processes
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the creativity module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="creativity", event_bus=event_bus)
        
        # Initialize state
        self.state = CreativityState()
        
        # Initialize component modules
        self.novelty_detection = NoveltyDetection(
            module_id=f"{module_id}_novelty_detection",
            event_bus=event_bus
        )
        
        self.concept_combination = ConceptCombination(
            module_id=f"{module_id}_concept_combination",
            event_bus=event_bus
        )
        
        self.divergent_thinking = DivergentThinking(
            module_id=f"{module_id}_divergent_thinking",
            event_bus=event_bus
        )
        
        self.imagination = Imagination(
            module_id=f"{module_id}_imagination",
            event_bus=event_bus
        )
        
        # Track component modules
        self.components = {
            "novelty_detection": self.novelty_detection,
            "concept_combination": self.concept_combination,
            "divergent_thinking": self.divergent_thinking,
            "imagination": self.imagination
        }
        
        # Initialize creativity metrics
        self.state.metrics = CreativityMetrics(
            fluency=0.0,
            flexibility=0.0,
            originality=0.0,
            elaboration=0.0,
            coherence=0.0,
            usefulness=0.0,
            last_updated=datetime.now()
        )
        
        # Initialize creative outputs history
        self.state.output_history = []
        self.max_history_size = 50
        
        # Subscribe to relevant events
        if self.event_bus:
            # Listen for creative outputs from components
            self.event_bus.subscribe("creative_output", self._handle_creative_output)
            
            # Listen for direct creativity requests
            self.event_bus.subscribe("creativity_request", self._handle_creativity_request)
            
            # Listen for evaluation requests
            self.event_bus.subscribe("creativity_evaluation", self._handle_evaluation_request)
            
            # Listen for developmental updates
            self.event_bus.subscribe("developmental_update", self._handle_developmental_update)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to generate creative outputs
        
        Args:
            input_data: Dictionary containing input to process creatively
            
        Returns:
            Dictionary with the results of creative processing
        """
        # Extract input information
        input_type = input_data.get("type", "general")
        content = input_data.get("content", {})
        mode = input_data.get("mode", "auto")  # auto, divergent, combination, imagination
        
        # Validate input
        if not content:
            return {
                "status": "error",
                "message": "Input content required",
                "module_id": self.module_id,
                "module_type": self.module_type
            }
        
        # Generate a unique ID for this creative process
        process_id = str(uuid.uuid4())
        
        # Determine which creative processes to apply based on mode and developmental level
        processes = self._select_processes(mode, input_type)
        
        results = {
            "status": "success",
            "module_id": self.module_id,
            "module_type": self.module_type,
            "process_id": process_id,
            "outputs": []
        }
        
        # Apply selected creative processes
        for process_name in processes:
            if process_name in self.components:
                process_result = self.components[process_name].process_input({
                    "type": input_type,
                    "content": content,
                    "process_id": process_id
                })
                
                # Store process result
                results[process_name] = process_result
                
                # If process generated outputs, add them to the outputs list
                if "outputs" in process_result and isinstance(process_result["outputs"], list):
                    results["outputs"].extend(process_result["outputs"])
        
        # If no outputs were generated directly, create a unified output
        if not results["outputs"] and all(proc in results for proc in processes):
            unified_output = self._create_unified_output(process_id, input_type, content, results)
            if unified_output:
                results["outputs"] = [unified_output]
        
        # Update creativity metrics based on outputs
        if results["outputs"]:
            self._update_creativity_metrics(results["outputs"])
            
        return results
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module and its components
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        previous_level = self.developmental_level
        new_level = super().update_development(amount)
        
        # Update components with proportional development
        # Focus early development on novelty detection and basic associations
        # Later development focuses more on imagination and divergent thinking
        
        if new_level < 0.3:
            # Early development prioritizes novelty detection and concept combination
            novelty_amt = 0.5 * amount
            concept_amt = 0.3 * amount
            divergent_amt = 0.1 * amount
            imagination_amt = 0.1 * amount
        elif new_level < 0.6:
            # Middle development balances all components with focus on divergent thinking
            novelty_amt = 0.2 * amount
            concept_amt = 0.3 * amount
            divergent_amt = 0.3 * amount
            imagination_amt = 0.2 * amount
        else:
            # Late development focuses on imagination and divergent thinking
            novelty_amt = 0.1 * amount
            concept_amt = 0.2 * amount
            divergent_amt = 0.3 * amount
            imagination_amt = 0.4 * amount
            
        # Update each component
        self.novelty_detection.update_development(novelty_amt)
        self.concept_combination.update_development(concept_amt)
        self.divergent_thinking.update_development(divergent_amt)
        self.imagination.update_development(imagination_amt)
        
        return new_level
    
    def _get_current_milestone(self) -> str:
        """Get the current developmental milestone"""
        milestone = "pre_creativity"
        for level, name in sorted(self.development_milestones.items()):
            if self.developmental_level >= level:
                milestone = name
        return milestone
    
    def _select_processes(self, mode: str, input_type: str) -> List[str]:
        """
        Select which creative processes to apply based on mode and development
        
        Args:
            mode: Requested processing mode
            input_type: Type of input to process
            
        Returns:
            List of process names to apply
        """
        # If a specific mode is requested, use the corresponding process
        if mode == "divergent":
            return ["divergent_thinking"]
        elif mode == "combination":
            return ["concept_combination"]
        elif mode == "imagination":
            return ["imagination"]
        elif mode == "novelty":
            return ["novelty_detection"]
            
        # For auto mode, select based on development level and input type
        processes = []
        
        # Always include novelty detection for all development levels
        processes.append("novelty_detection")
        
        # Add processes based on developmental level
        if self.developmental_level >= 0.2:
            processes.append("concept_combination")
        
        if self.developmental_level >= 0.4:
            processes.append("divergent_thinking")
            
        if self.developmental_level >= 0.6:
            processes.append("imagination")
            
        # Adjust based on input type
        if input_type == "problem":
            # Prioritize divergent thinking for problems
            if "divergent_thinking" not in processes and self.developmental_level >= 0.3:
                processes.append("divergent_thinking")
        elif input_type == "concept":
            # Prioritize concept combination for concepts
            if "concept_combination" not in processes and self.developmental_level >= 0.1:
                processes.append("concept_combination")
        elif input_type == "scenario":
            # Prioritize imagination for scenarios
            if "imagination" not in processes and self.developmental_level >= 0.3:
                processes.append("imagination")
                
        return processes
    
    def _create_unified_output(
        self, 
        process_id: str, 
        input_type: str, 
        content: Dict[str, Any], 
        results: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """
        Create a unified creative output from multiple process results
        
        Args:
            process_id: ID of the creative process
            input_type: Type of input processed
            content: Original input content
            results: Results from individual creative processes
            
        Returns:
            Unified creative output or None if not possible
        """
        # Extract outputs from each process
        novelty_result = results.get("novelty_detection", {})
        combination_result = results.get("concept_combination", {})
        divergent_result = results.get("divergent_thinking", {})
        imagination_result = results.get("imagination", {})
        
        # Create unified content based on available results
        unified_content = {
            "original_input": content,
            "process_id": process_id,
            "timestamp": datetime.now().isoformat()
        }
        
        # Add process-specific elements to unified content
        if "novelty_score" in novelty_result:
            unified_content["novelty_score"] = novelty_result["novelty_score"]
            
        if "new_concepts" in combination_result:
            unified_content["combined_concepts"] = combination_result.get("new_concepts", [])
            
        if "solutions" in divergent_result:
            unified_content["solutions"] = divergent_result.get("solutions", [])
            
        if "scene" in imagination_result:
            unified_content["imagined_scene"] = imagination_result.get("scene", {})
        
        # Calculate aggregate scores
        novelty_score = novelty_result.get("novelty_score", 0.0)
        
        # Default other scores
        coherence_score = 0.5
        usefulness_score = 0.5
        
        # Adjust scores based on available data
        if "coherence" in combination_result:
            coherence_score = combination_result["coherence"]
        elif "coherence" in imagination_result:
            coherence_score = imagination_result["coherence"]
            
        if "usefulness" in divergent_result:
            usefulness_score = divergent_result["usefulness"]
        
        # Create creative output
        output = {
            "content": unified_content,
            "output_type": f"unified_{input_type}",
            "novelty_score": novelty_score,
            "coherence_score": coherence_score,
            "usefulness_score": usefulness_score,
            "source_components": [comp for comp in results.keys() if comp in self.components]
        }
        
        return output
    
    def _update_creativity_metrics(self, outputs: List[Dict[str, Any]]) -> None:
        """
        Update creativity metrics based on recent outputs
        
        Args:
            outputs: List of creative outputs to evaluate
        """
        if not outputs:
            return
            
        # Calculate metrics across all outputs
        fluency = len(outputs)  # Number of outputs
        
        # Extract scores
        novelty_scores = [output.get("novelty_score", 0.0) for output in outputs]
        coherence_scores = [output.get("coherence_score", 0.0) for output in outputs]
        usefulness_scores = [output.get("usefulness_score", 0.0) for output in outputs]
        
        # Calculate averages
        avg_novelty = sum(novelty_scores) / len(novelty_scores) if novelty_scores else 0.0
        avg_coherence = sum(coherence_scores) / len(coherence_scores) if coherence_scores else 0.0
        avg_usefulness = sum(usefulness_scores) / len(usefulness_scores) if usefulness_scores else 0.0
        
        # Extract output types
        output_types = {output.get("output_type", "unknown") for output in outputs}
        flexibility = len(output_types) / 4.0  # Normalize by typical max types
        
        # Use novelty as originality approximation
        originality = avg_novelty
        
        # Calculate elaboration based on content complexity
        elaboration = 0.0
        for output in outputs:
            content = output.get("content", {})
            
            # Count the number of elements in the content
            if isinstance(content, dict):
                # More keys suggests more elaboration
                elaboration += min(1.0, len(content) / 10.0)  # Normalize to max of 1.0
        
        elaboration = elaboration / len(outputs) if outputs else 0.0
        
        # Update metrics
        self.state.metrics.fluency = min(1.0, fluency / 10.0)  # Normalize to max of 1.0
        self.state.metrics.flexibility = min(1.0, flexibility)
        self.state.metrics.originality = min(1.0, originality)
        self.state.metrics.elaboration = min(1.0, elaboration)
        self.state.metrics.coherence = min(1.0, avg_coherence)
        self.state.metrics.usefulness = min(1.0, avg_usefulness)
        self.state.metrics.last_updated = datetime.now()
        
        # Store outputs in history
        for output in outputs:
            # Add to history
            self.state.output_history.append({
                "timestamp": datetime.now().isoformat(),
                "output": output
            })
        
        # Limit history size
        if len(self.state.output_history) > self.max_history_size:
            self.state.output_history = self.state.output_history[-self.max_history_size:]
    
    def _handle_creative_output(self, message: Message) -> None:
        """
        Handle creative output messages from component modules
        
        Args:
            message: The creative output message
        """
        if isinstance(message.content, dict):
            # Store in history
            self.state.output_history.append({
                "timestamp": datetime.now().isoformat(),
                "output": message.content
            })
            
            # Limit history size
            if len(self.state.output_history) > self.max_history_size:
                self.state.output_history = self.state.output_history[-self.max_history_size:]
            
            # Update metrics (as a single output)
            self._update_creativity_metrics([message.content])
            
            # Forward to other modules if appropriate
            if self.event_bus and self.developmental_level > 0.5:
                # At higher development levels, we send creative outputs to working memory
                self.event_bus.publish(
                    msg_type="working_memory_update",
                    content={
                        "type": "creative_output",
                        "content": message.content,
                        "priority": message.content.get("novelty_score", 0.5)
                    }
                )
    
    def _handle_creativity_request(self, message: Message) -> None:
        """
        Handle direct requests for creative processing
        
        Args:
            message: The creativity request message
        """
        if isinstance(message.content, dict):
            # Process the request
            result = self.process_input(message.content)
            
            # Publish result
            if self.event_bus:
                self.event_bus.publish(
                    msg_type="creativity_result",
                    content=result,
                    source=self.module_id,
                    target=message.source
                )
    
    def _handle_evaluation_request(self, message: Message) -> None:
        """
        Handle requests to evaluate the creativity of content
        
        Args:
            message: The evaluation request message
        """
        if isinstance(message.content, dict):
            content = message.content.get("content", {})
            
            if not content:
                # No content to evaluate
                if self.event_bus:
                    self.event_bus.publish(
                        msg_type="creativity_evaluation_result",
                        content={
                            "status": "error",
                            "message": "No content to evaluate"
                        },
                        source=self.module_id,
                        target=message.source
                    )
                return
                
            # Evaluate creativity by processing with novelty detection
            novelty_result = self.novelty_detection.process_input({
                "type": message.content.get("type", "general"),
                "content": content
            })
            
            # Create evaluation result
            evaluation = {
                "status": "success",
                "module_id": self.module_id,
                "module_type": self.module_type,
                "content_id": message.content.get("content_id", str(uuid.uuid4())),
                "novelty_score": novelty_result.get("novelty_score", 0.0),
                "surprise_level": novelty_result.get("surprise_level", 0.0),
                "creativity_score": novelty_result.get("novelty_score", 0.0) * 0.7 + 0.3  # Base creativity is 0.3 + novelty effect
            }
            
            # Apply developmental modulation
            if self.developmental_level < 0.3:
                # Early development: creativity is mostly novelty
                evaluation["creativity_score"] = novelty_result.get("novelty_score", 0.0) * 0.9 + 0.1
            elif self.developmental_level < 0.6:
                # Middle development: creativity balances novelty with baseline
                evaluation["creativity_score"] = novelty_result.get("novelty_score", 0.0) * 0.7 + 0.3
            else:
                # Late development: creativity is more sophisticated
                # We would use other factors here, but for simplicity we'll use the formula above
                pass
                
            # Publish evaluation result
            if self.event_bus:
                self.event_bus.publish(
                    msg_type="creativity_evaluation_result",
                    content=evaluation,
                    source=self.module_id,
                    target=message.source
                )
    
    def _handle_developmental_update(self, message: Message) -> None:
        """
        Handle developmental update messages
        
        Args:
            message: The developmental update message
        """
        if isinstance(message.content, dict):
            # Check if this is a global update or specifically for this module
            target_module = message.content.get("target_module")
            
            if target_module is None or target_module == self.module_id or target_module == "all":
                # Update development
                amount = message.content.get("amount", 0.01)
                self.update_development(amount)


#######################

#emotion\emotion_classifier.py#
#######################

import logging
import time
import uuid
from typing import Dict, List, Any, Optional, Union, Tuple
from datetime import datetime
import numpy as np
import math
from collections import defaultdict, Counter
import re
import random

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.emotion.models import EmotionState, EmotionNeuralState

# Initialize logger
logger = logging.getLogger(__name__)

class EmotionClassifier(BaseModule):
    """
    Classifier for mapping dimensional emotions to categorical emotions
    
    This system develops from basic positive/negative distinction
    to nuanced recognition of complex emotional states.
    """
    # Development milestones
    development_milestones = {
        0.0: "Basic pleasure/displeasure distinction",
        0.2: "Primary emotion recognition",
        0.4: "Secondary emotion classification",
        0.6: "Mixed emotion recognition",
        0.8: "Complex emotional state understanding",
        1.0: "Nuanced emotion classification"
    }
    
    def __init__(
        self,
        module_id: str,
        event_bus: Optional[EventBus] = None,
        development_level: float = 0.0
    ):
        """
        Initialize the emotion classifier
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication
            development_level: Initial developmental level
        """
        super().__init__(
            module_id=module_id,
            module_type="emotion_classifier",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Define emotion prototypes in valence-arousal space
        # Values based on psychological research on emotion dimensions
        self.emotion_prototypes = {
            # Primary emotions
            "joy": (0.8, 0.6),         # High valence, moderate-high arousal
            "sadness": (-0.7, 0.2),    # Low valence, low arousal
            "anger": (-0.6, 0.8),      # Low valence, high arousal
            "fear": (-0.7, 0.7),       # Low valence, high arousal
            
            # Secondary emotions
            "surprise": (0.1, 0.8),    # Neutral-positive valence, high arousal
            "disgust": (-0.6, 0.5),    # Low valence, moderate arousal
            "anticipation": (0.4, 0.6), # Positive valence, moderate arousal
            "trust": (0.6, 0.3),       # High valence, low-moderate arousal
            
            # Complex emotions (only available at higher development levels)
            "shame": (-0.4, 0.3),      # Negative valence, low-moderate arousal
            "guilt": (-0.5, 0.4),      # Negative valence, moderate arousal
            "pride": (0.7, 0.5),       # Positive valence, moderate arousal
            "love": (0.9, 0.4),        # Very positive valence, moderate arousal
            "jealousy": (-0.3, 0.6),   # Negative valence, moderate-high arousal
            "awe": (0.5, 0.8),         # Positive valence, high arousal
            "contentment": (0.7, 0.1),  # Positive valence, very low arousal
            "boredom": (-0.2, 0.1),    # Slight negative valence, very low arousal
            
            # Neutral state
            "neutral": (0.0, 0.2)      # Neutral valence, low arousal
        }
        
        # Emotion lexicons for textual emotion detection
        self.emotion_lexicons = {
            "joy": {
                "happy", "joy", "delighted", "pleased", "glad", "cheerful",
                "content", "satisfied", "merry", "jovial", "blissful",
                "ecstatic", "elated", "thrilled", "overjoyed", "exuberant"
            },
            "sadness": {
                "sad", "unhappy", "depressed", "miserable", "gloomy", "melancholy",
                "sorrowful", "downhearted", "downcast", "blue", "dejected",
                "heartbroken", "grief", "distressed", "woeful", "despondent"
            },
            "anger": {
                "angry", "mad", "furious", "enraged", "irate", "irritated",
                "annoyed", "vexed", "indignant", "outraged", "offended",
                "heated", "fuming", "infuriated", "livid", "seething"
            },
            "fear": {
                "afraid", "scared", "frightened", "terrified", "fearful", "anxious",
                "worried", "nervous", "panicked", "alarmed", "horrified",
                "startled", "suspicious", "uneasy", "wary", "dread"
            },
            "surprise": {
                "surprised", "astonished", "amazed", "astounded", "shocked", "stunned",
                "startled", "dumbfounded", "bewildered", "awestruck", "wonderstruck",
                "flabbergasted", "thunderstruck", "dazed", "speechless", "agog"
            },
            "disgust": {
                "disgusted", "revolted", "repulsed", "nauseated", "sickened", "appalled",
                "repelled", "offended", "abhorrent", "loathsome", "detestable",
                "distasteful", "repugnant", "vile", "gross", "creepy"
            },
            "anticipation": {
                "anticipate", "expect", "await", "look forward", "hope", "excited",
                "eager", "enthusiastic", "keen", "prepared", "ready",
                "watchful", "vigilant", "alert", "attentive", "mindful"
            },
            "trust": {
                "trust", "confident", "secure", "faithful", "reliable", "dependable",
                "trustworthy", "honest", "loyal", "sincere", "devoted",
                "authentic", "genuine", "believing", "convinced", "assured"
            },
            "neutral": {
                "neutral", "okay", "fine", "alright", "balanced", "moderate",
                "neither", "indifferent", "impartial", "uninvolved", "dispassionate"
            }
        }
        
        # Neural state for tracking activations and development
        self.neural_state = EmotionNeuralState()
        self.neural_state.classifier_development = development_level
        
        # Classification parameters
        self.params = {
            # Size of influence sphere for each emotion in VA space (how wide the emotion region is)
            "emotion_radii": {
                "neutral": 0.3,  # Neutral has a wider region
                "default": 0.2   # Default for other emotions
            },
            
            # Weight for dimensional vs lexical classification
            "dimensional_weight": 0.7,
            "lexical_weight": 0.3,
            
            # Threshold for emotion detection
            "emotion_threshold": 0.1,
            
            # Whether mixed emotions are allowed
            "allow_mixed_emotions": False,
            
            # Maximum number of simultaneous emotions
            "max_emotions": 1
        }
        
        # Adjust parameters based on development level
        self._adjust_parameters_for_development()
        
        # History of recent classifications
        self.classification_history = []
        
        logger.info(f"Emotion classifier initialized at development level {development_level:.2f}")
    
    def _adjust_parameters_for_development(self):
        """Adjust classification parameters based on developmental level"""
        if self.development_level < 0.2:
            # Very basic classification - only pleasure/displeasure
            self.params.update({
                "dimensional_weight": 0.9,   # Mostly dimensional at early stages
                "lexical_weight": 0.1,
                "emotion_threshold": 0.3,    # Higher threshold (less sensitive)
                "allow_mixed_emotions": False,
                "max_emotions": 1
            })
        elif self.development_level < 0.4:
            # Primary emotion recognition
            self.params.update({
                "dimensional_weight": 0.8,
                "lexical_weight": 0.2,
                "emotion_threshold": 0.25,
                "allow_mixed_emotions": False,
                "max_emotions": 1
            })
        elif self.development_level < 0.6:
            # Secondary emotion recognition
            self.params.update({
                "dimensional_weight": 0.7,
                "lexical_weight": 0.3,
                "emotion_threshold": 0.2,
                "allow_mixed_emotions": True,  # Begin allowing mixed emotions
                "max_emotions": 2             # Up to 2 emotions
            })
        elif self.development_level < 0.8:
            # Mixed emotion recognition
            self.params.update({
                "dimensional_weight": 0.6,
                "lexical_weight": 0.4,
                "emotion_threshold": 0.15,
                "allow_mixed_emotions": True,
                "max_emotions": 3             # Up to 3 emotions
            })
        else:
            # Complex emotion recognition
            self.params.update({
                "dimensional_weight": 0.5,
                "lexical_weight": 0.5,
                "emotion_threshold": 0.1,
                "allow_mixed_emotions": True,
                "max_emotions": 4             # Up to 4 emotions
            })
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to classify emotions
        
        Args:
            input_data: Input data to process
                Required keys: 'valence', 'arousal'
                Optional keys: 'text', 'context'
                
        Returns:
            Dictionary with emotion classification results
        """
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Extract valence and arousal
        valence = input_data.get("valence", 0.0)
        arousal = input_data.get("arousal", 0.0)
        
        # Extract text (if available)
        text = ""
        if "text" in input_data:
            text = input_data["text"]
        elif "content" in input_data:
            content = input_data["content"]
            if isinstance(content, str):
                text = content
            elif isinstance(content, dict) and "text" in content:
                text = content["text"]
        
        # Classification approach based on development level
        if self.development_level < 0.2:
            # Very basic classification at early stages - only positive/negative
            result = self._basic_classification(valence, arousal, text)
        else:
            # More sophisticated classification at higher levels
            result = self._dimensional_classification(valence, arousal, text)
            
        # Add process ID and development info
        result["process_id"] = process_id
        result["development_level"] = self.development_level
        
        # Add to history
        self.classification_history.append({
            "valence": valence,
            "arousal": arousal,
            "result": result,
            "timestamp": datetime.now().isoformat()
        })
        
        # Limit history size
        if len(self.classification_history) > 50:
            self.classification_history = self.classification_history[-50:]
        
        return result
    
    def _basic_classification(self, valence: float, arousal: float, text: str) -> Dict[str, Any]:
        """
        Basic emotion classification for early development stages
        
        Args:
            valence: Pleasure-displeasure value (-1 to 1)
            arousal: Activation level (0 to 1)
            text: Optional text to analyze
            
        Returns:
            Dictionary with classification results
        """
        # At the most basic level, only distinguish pleasure/displeasure
        if valence > 0.2:
            # Positive emotion (pleasure)
            dominant_emotion = "joy"
            emotion_intensities = {
                "joy": min(1.0, valence + 0.2),
                "neutral": max(0.0, 1.0 - (valence + 0.2))
            }
        elif valence < -0.2:
            # Negative emotion (displeasure)
            # At early stages, don't differentiate between different negative emotions
            if arousal > 0.5:
                # High arousal negative is classified as anger
                dominant_emotion = "anger"
                emotion_intensities = {
                    "anger": min(1.0, abs(valence) + 0.2),
                    "neutral": max(0.0, 1.0 - (abs(valence) + 0.2))
                }
            else:
                # Low arousal negative is classified as sadness
                dominant_emotion = "sadness"
                emotion_intensities = {
                    "sadness": min(1.0, abs(valence) + 0.2),
                    "neutral": max(0.0, 1.0 - (abs(valence) + 0.2))
                }
        else:
            # Neutral emotional state
            dominant_emotion = "neutral"
            emotion_intensities = {
                "neutral": 0.8,
                "joy" if valence >= 0 else "sadness": 0.2
            }
            
        return {
            "dominant_emotion": dominant_emotion,
            "emotion_intensities": emotion_intensities,
            "classification_method": "basic",
            "confidence": 0.5 + (0.2 * self.development_level)
        }
    
    def _dimensional_classification(self, valence: float, arousal: float, text: str) -> Dict[str, Any]:
        """
        Dimensional emotion classification for higher development stages
        
        Args:
            valence: Pleasure-displeasure value (-1 to 1)
            arousal: Activation level (0 to 1)
            text: Optional text to analyze
            
        Returns:
            Dictionary with classification results
        """
        # Determine which emotions to consider based on development level
        available_emotions = self._get_available_emotions()
        
        # Classify based on dimensional coordinates
        dimensional_result = self._classify_by_dimensions(valence, arousal, available_emotions)
        
        # If we have text, also classify based on lexical content
        lexical_result = None
        if text:
            lexical_result = self._classify_by_text(text, available_emotions)
            
        # Combine results if we have both
        if lexical_result and self.development_level >= 0.2:
            combined_result = self._combine_classifications(
                dimensional_result, 
                lexical_result,
                self.params["dimensional_weight"],
                self.params["lexical_weight"]
            )
            
            combined_result["classification_method"] = "dimensional+lexical"
            return combined_result
        else:
            # Just use dimensional result
            dimensional_result["classification_method"] = "dimensional"
            return dimensional_result
    
    def _get_available_emotions(self) -> List[str]:
        """Get emotions available at current development level"""
        if self.development_level < 0.2:
            # Very basic emotional distinctions
            return ["joy", "sadness", "anger", "neutral"]
            
        elif self.development_level < 0.4:
            # Primary emotions
            return ["joy", "sadness", "anger", "fear", "neutral"]
            
        elif self.development_level < 0.6:
            # Add secondary emotions
            return ["joy", "sadness", "anger", "fear", 
                   "surprise", "disgust", "anticipation", "trust", 
                   "neutral"]
                   
        elif self.development_level < 0.8:
            # Add some complex emotions
            return ["joy", "sadness", "anger", "fear", 
                   "surprise", "disgust", "anticipation", "trust",
                   "shame", "guilt", "love", "neutral"]
                   
        else:
            # Full range of emotions
            return list(self.emotion_prototypes.keys())
    
    def _classify_by_dimensions(
        self, 
        valence: float, 
        arousal: float, 
        available_emotions: List[str]
    ) -> Dict[str, Any]:
        """
        Classify emotions based on location in dimensional space
        
        Args:
            valence: Pleasure-displeasure value (-1 to 1)
            arousal: Activation level (0 to 1)
            available_emotions: List of emotions to consider
            
        Returns:
            Dictionary with classification results
        """
        # Calculate distance to each emotion prototype
        distances = {}
        for emotion in available_emotions:
            if emotion in self.emotion_prototypes:
                prototype_valence, prototype_arousal = self.emotion_prototypes[emotion]
                # Euclidean distance in VA space
                distance = math.sqrt(
                    (valence - prototype_valence) ** 2 + 
                    (arousal - prototype_arousal) ** 2
                )
                distances[emotion] = distance
        
        # Convert distances to intensities (closer = higher intensity)
        # Use Gaussian activation function
        intensities = {}
        for emotion, distance in distances.items():
            # Get radius for this emotion (how wide the region is)
            radius = self.params["emotion_radii"].get(
                emotion, self.params["emotion_radii"]["default"]
            )
            
            # Calculate intensity using Gaussian function
            # exp(-distance²/radius²) gives 1.0 at center, decreasing with distance
            intensity = math.exp(-(distance ** 2) / (radius ** 2))
            
            # Apply threshold
            if intensity >= self.params["emotion_threshold"]:
                intensities[emotion] = intensity
                
        # If no emotions pass threshold, use neutral
        if not intensities:
            intensities["neutral"] = 1.0
            
        # Normalize intensities to sum to 1.0
        total_intensity = sum(intensities.values())
        if total_intensity > 0:
            intensities = {e: i / total_intensity for e, i in intensities.items()}
            
        # Limit to max number of emotions if mixed emotions aren't allowed
        if not self.params["allow_mixed_emotions"] or len(intensities) > self.params["max_emotions"]:
            # Keep only the strongest emotions up to max_emotions
            top_emotions = sorted(intensities.items(), key=lambda x: x[1], reverse=True)
            top_emotions = top_emotions[:self.params["max_emotions"]]
            
            # Create new intensities dict with only top emotions
            intensities = {e: i for e, i in top_emotions}
            
            # Re-normalize
            total_intensity = sum(intensities.values())
            intensities = {e: i / total_intensity for e, i in intensities.items()}
            
        # Determine dominant emotion (highest intensity)
        dominant_emotion = max(intensities.items(), key=lambda x: x[1])[0]
        
        # Record activation for tracking
        if hasattr(self, "neural_state"):
            self.neural_state.add_activation('classifier', {
                'valence': valence,
                'arousal': arousal,
                'intensities': intensities,
                'dominant_emotion': dominant_emotion
            })
        
        return {
            "dominant_emotion": dominant_emotion,
            "emotion_intensities": intensities,
            "confidence": 0.6 + (0.2 * self.development_level)
        }
    
    def _classify_by_text(
        self, 
        text: str, 
        available_emotions: List[str]
    ) -> Dict[str, Any]:
        """
        Classify emotions based on textual content
        
        Args:
            text: Text to analyze
            available_emotions: List of emotions to consider
            
        Returns:
            Dictionary with classification results
        """
        # Simple lexical approach - look for emotion words
        tokens = re.findall(r'\b\w+\b', text.lower())
        
        # Count emotion words for each available emotion
        emotion_counts = {}
        for emotion in available_emotions:
            if emotion in self.emotion_lexicons:
                # Count words in this emotion's lexicon
                emotion_words = [token for token in tokens if token in self.emotion_lexicons[emotion]]
                if emotion_words:
                    emotion_counts[emotion] = len(emotion_words)
                    
        # If no emotions detected, use neutral
        if not emotion_counts:
            return {
                "dominant_emotion": "neutral",
                "emotion_intensities": {"neutral": 1.0},
                "confidence": 0.3
            }
            
        # Convert counts to intensities
        total_count = sum(emotion_counts.values())
        intensities = {e: c / total_count for e, c in emotion_counts.items()}
        
        # Apply threshold and limit number of emotions
        intensities = {e: i for e, i in intensities.items() 
                     if i >= self.params["emotion_threshold"]}
                     
        # If no emotions pass threshold, use neutral
        if not intensities:
            intensities["neutral"] = 1.0
            
        # Limit to max number of emotions
        if len(intensities) > self.params["max_emotions"]:
            # Keep only the strongest emotions
            top_emotions = sorted(intensities.items(), key=lambda x: x[1], reverse=True)
            top_emotions = top_emotions[:self.params["max_emotions"]]
            intensities = {e: i for e, i in top_emotions}
            
        # Re-normalize
        total_intensity = sum(intensities.values())
        intensities = {e: i / total_intensity for e, i in intensities.items()}
        
        # Determine dominant emotion (highest intensity)
        dominant_emotion = max(intensities.items(), key=lambda x: x[1])[0]
        
        return {
            "dominant_emotion": dominant_emotion,
            "emotion_intensities": intensities,
            "confidence": 0.4 + (0.1 * self.development_level)
        }
    
    def _combine_classifications(
        self,
        dimensional_result: Dict[str, Any],
        lexical_result: Dict[str, Any],
        dimensional_weight: float,
        lexical_weight: float
    ) -> Dict[str, Any]:
        """
        Combine dimensional and lexical classifications
        
        Args:
            dimensional_result: Results from dimensional classification
            lexical_result: Results from lexical classification
            dimensional_weight: Weight for dimensional results
            lexical_weight: Weight for lexical results
            
        Returns:
            Dictionary with combined classification
        """
        # Normalize weights
        total_weight = dimensional_weight + lexical_weight
        dim_weight_norm = dimensional_weight / total_weight
        lex_weight_norm = lexical_weight / total_weight
        
        # Combine emotion intensities
        combined_intensities = {}
        
        # Add dimensional emotions
        for emotion, intensity in dimensional_result["emotion_intensities"].items():
            combined_intensities[emotion] = intensity * dim_weight_norm
            
        # Add lexical emotions
        for emotion, intensity in lexical_result["emotion_intensities"].items():
            if emotion in combined_intensities:
                combined_intensities[emotion] += intensity * lex_weight_norm
            else:
                combined_intensities[emotion] = intensity * lex_weight_norm
                
        # Apply threshold
        combined_intensities = {e: i for e, i in combined_intensities.items() 
                              if i >= self.params["emotion_threshold"]}
                              
        # If empty, use neutral
        if not combined_intensities:
            combined_intensities["neutral"] = 1.0
            
        # Limit to max emotions
        if len(combined_intensities) > self.params["max_emotions"]:
            top_emotions = sorted(combined_intensities.items(), key=lambda x: x[1], reverse=True)
            top_emotions = top_emotions[:self.params["max_emotions"]]
            combined_intensities = {e: i for e, i in top_emotions}
            
        # Re-normalize
        total_intensity = sum(combined_intensities.values())
        combined_intensities = {e: i / total_intensity for e, i in combined_intensities.items()}
        
        # Determine dominant emotion
        dominant_emotion = max(combined_intensities.items(), key=lambda x: x[1])[0]
        
        # Calculate combined confidence
        confidence = (
            dimensional_result["confidence"] * dim_weight_norm +
            lexical_result["confidence"] * lex_weight_norm
        )
        
        return {
            "dominant_emotion": dominant_emotion,
            "emotion_intensities": combined_intensities,
            "confidence": confidence
        }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New developmental level
        """
        # Update base module development
        new_level = super().update_development(amount)
        
        # Adjust parameters for new development level
        self._adjust_parameters_for_development()
        
        # Update neural state
        if hasattr(self, "neural_state"):
            self.neural_state.classifier_development = new_level
            self.neural_state.last_updated = datetime.now()
        
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state
        
        Returns:
            Dictionary with current state
        """
        base_state = super().get_state()
        
        # Add classifier-specific state
        classifier_state = {
            "params": self.params,
            "available_emotions": self._get_available_emotions(),
            "history_length": len(self.classification_history),
            "last_classification": self.classification_history[-1] if self.classification_history else None
        }
        
        # Combine states
        combined_state = {**base_state, **classifier_state}
        
        return combined_state 


#######################

#emotion\models.py#
#######################

from pydantic import BaseModel, Field, validator
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
import time

class EmotionState(BaseModel):
    """
    Represents the current emotional state
    
    This includes dimensional values (valence, arousal) and categorical emotions
    """
    valence: float = Field(..., ge=-1.0, le=1.0, description="Pleasure-displeasure dimension")
    arousal: float = Field(..., ge=0.0, le=1.0, description="Activation level")
    dominant_emotion: str = Field(..., description="The primary emotion being experienced")
    emotion_intensities: Dict[str, float] = Field(..., description="Intensity of each emotion")
    timestamp: datetime = Field(default_factory=datetime.now, description="When this state was recorded")
    
    @validator('emotion_intensities')
    def check_intensities(cls, v):
        """Ensure all intensities are between 0 and 1"""
        for emotion, intensity in v.items():
            if not 0 <= intensity <= 1:
                raise ValueError(f"Intensity of {emotion} must be between 0 and 1")
        return v
    
    def dict(self, *args, **kwargs):
        """Convert datetime to timestamp for serialization"""
        result = super().dict(*args, **kwargs)
        result['timestamp'] = self.timestamp.isoformat()
        return result

class EmotionalResponse(BaseModel):
    """
    An emotional response to a specific stimulus
    
    This captures how the emotional system responds to particular input
    """
    valence: float = Field(..., ge=-1.0, le=1.0)
    arousal: float = Field(..., ge=0.0, le=1.0)
    dominant_emotion: str
    emotion_intensities: Dict[str, float]
    regulated: bool = Field(False, description="Whether this response has been regulated")
    regulation_strategy: Optional[str] = Field(None, description="Strategy used for regulation")
    stimulus: Optional[str] = Field(None, description="What triggered this response")
    process_id: str = Field(..., description="ID of the process that generated this response")
    timestamp: datetime = Field(default_factory=datetime.now)
    
    @validator('emotion_intensities')
    def check_intensities(cls, v):
        """Ensure all intensities are between 0 and 1"""
        for emotion, intensity in v.items():
            if not 0 <= intensity <= 1:
                raise ValueError(f"Intensity of {emotion} must be between 0 and 1")
        return v
    
    def dict(self, *args, **kwargs):
        """Convert datetime to timestamp for serialization"""
        result = super().dict(*args, **kwargs)
        result['timestamp'] = self.timestamp.isoformat()
        return result

class SentimentAnalysis(BaseModel):
    """
    Analysis of sentiment in text
    
    This captures various aspects of emotional tone in language
    """
    text: str = Field(..., description="The text being analyzed")
    positive_score: float = Field(..., ge=0.0, le=1.0, description="Degree of positive sentiment")
    negative_score: float = Field(..., ge=0.0, le=1.0, description="Degree of negative sentiment")
    neutral_score: float = Field(..., ge=0.0, le=1.0, description="Degree of neutral sentiment")
    compound_score: float = Field(..., ge=-1.0, le=1.0, description="Overall sentiment score")
    detected_emotions: Dict[str, float] = Field(default_factory=dict, description="Detected emotions and intensities")
    highlighted_phrases: List[Dict[str, Any]] = Field(
        default_factory=list, 
        description="Emotionally salient phrases with scores"
    )
    process_id: str = Field(..., description="ID of the process that generated this analysis")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Confidence in the analysis")
    timestamp: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert datetime to timestamp for serialization"""
        result = super().dict(*args, **kwargs)
        result['timestamp'] = self.timestamp.isoformat()
        return result

class EmotionRegulationRequest(BaseModel):
    """
    Request to regulate an emotional state
    
    This specifies how emotions should be modified
    """
    current_state: EmotionState
    target_valence: Optional[float] = Field(None, ge=-1.0, le=1.0)
    target_arousal: Optional[float] = Field(None, ge=0.0, le=1.0)
    target_emotion: Optional[str] = Field(None)
    regulation_strategy: Optional[str] = Field(None)
    context: Dict[str, Any] = Field(default_factory=dict)
    process_id: str = Field(..., description="ID of the regulation process")
    
    class Config:
        arbitrary_types_allowed = True

class EmotionRegulationResult(BaseModel):
    """
    Result of an emotion regulation attempt
    
    This captures how emotions were modified through regulation
    """
    original_state: EmotionState
    regulated_state: EmotionState
    regulation_strategy: str
    success_level: float = Field(..., ge=0.0, le=1.0)
    process_id: str
    timestamp: datetime = Field(default_factory=datetime.now)
    
    class Config:
        arbitrary_types_allowed = True
        
    def dict(self, *args, **kwargs):
        """Convert datetime to timestamp for serialization"""
        result = super().dict(*args, **kwargs)
        result['timestamp'] = self.timestamp.isoformat()
        result['original_state'] = self.original_state.dict()
        result['regulated_state'] = self.regulated_state.dict()
        return result

class EmotionalParameters(BaseModel):
    """
    Parameters that control emotional processing
    
    These parameters are adjusted based on developmental level
    """
    emotional_inertia: float = Field(..., ge=0.0, le=1.0, description="Resistance to emotional change")
    stimulus_sensitivity: float = Field(..., ge=0.0, le=1.0, description="Sensitivity to emotional stimuli")
    emotion_decay_rate: float = Field(..., ge=0.0, le=1.0, description="How quickly emotions return to baseline")
    baseline_valence: float = Field(..., ge=-1.0, le=1.0, description="Default valence state")
    baseline_arousal: float = Field(..., ge=0.0, le=1.0, description="Default arousal state")
    regulation_capacity: float = Field(..., ge=0.0, le=1.0, description="Ability to regulate emotions")

class EmotionNeuralState(BaseModel):
    """
    State information for the emotion neural networks
    
    This tracks the state of neural networks used for emotional processing,
    including developmental levels and recent activations.
    """
    encoder_development: float = Field(0.0, ge=0.0, le=1.0, description="Development level of emotion encoder")
    classifier_development: float = Field(0.0, ge=0.0, le=1.0, description="Development level of emotion classifier")
    sentiment_development: float = Field(0.0, ge=0.0, le=1.0, description="Development level of sentiment analyzer")
    regulation_development: float = Field(0.0, ge=0.0, le=1.0, description="Development level of emotion regulator")
    
    # Track recent activations for each neural component
    recent_encoder_activations: List[Dict[str, Any]] = Field(
        default_factory=list, 
        description="Recent activations of the emotion encoder"
    )
    recent_classifier_activations: List[Dict[str, Any]] = Field(
        default_factory=list, 
        description="Recent activations of the emotion classifier"
    )
    recent_sentiment_activations: List[Dict[str, Any]] = Field(
        default_factory=list, 
        description="Recent activations of the sentiment analyzer"
    )
    recent_regulation_activations: List[Dict[str, Any]] = Field(
        default_factory=list, 
        description="Recent activations of the emotion regulator"
    )
    
    # Network performance metrics
    encoder_accuracy: float = Field(0.5, ge=0.0, le=1.0, description="Accuracy of emotion encoder")
    classifier_accuracy: float = Field(0.5, ge=0.0, le=1.0, description="Accuracy of emotion classifier")
    sentiment_accuracy: float = Field(0.5, ge=0.0, le=1.0, description="Accuracy of sentiment analyzer")
    regulation_success_rate: float = Field(0.5, ge=0.0, le=1.0, description="Success rate of emotion regulation")
    
    # Last update timestamp
    last_updated: datetime = Field(default_factory=datetime.now, description="When neural state was last updated")
    
    def dict(self, *args, **kwargs):
        """Convert datetime to timestamp for serialization"""
        result = super().dict(*args, **kwargs)
        result['last_updated'] = self.last_updated.isoformat()
        return result
    
    def update_accuracy(self, component: str, accuracy: float) -> None:
        """
        Update the accuracy for a specific neural component
        
        Args:
            component: The component to update ('encoder', 'classifier', 'sentiment', 'regulation')
            accuracy: The new accuracy value (0.0 to 1.0)
        """
        if component == 'encoder':
            self.encoder_accuracy = max(0.0, min(1.0, accuracy))
        elif component == 'classifier':
            self.classifier_accuracy = max(0.0, min(1.0, accuracy))
        elif component == 'sentiment':
            self.sentiment_accuracy = max(0.0, min(1.0, accuracy))
        elif component == 'regulation':
            self.regulation_success_rate = max(0.0, min(1.0, accuracy))
        
        self.last_updated = datetime.now()
    
    def add_activation(self, component: str, activation: Dict[str, Any]) -> None:
        """
        Add a recent activation for a neural component
        
        Args:
            component: The component that was activated
            activation: Dictionary with activation details
        """
        activation_with_timestamp = {
            **activation,
            "timestamp": datetime.now().isoformat()
        }
        
        if component == 'encoder':
            self.recent_encoder_activations.append(activation_with_timestamp)
            if len(self.recent_encoder_activations) > 10:  # Keep last 10
                self.recent_encoder_activations = self.recent_encoder_activations[-10:]
        elif component == 'classifier':
            self.recent_classifier_activations.append(activation_with_timestamp)
            if len(self.recent_classifier_activations) > 10:
                self.recent_classifier_activations = self.recent_classifier_activations[-10:]
        elif component == 'sentiment':
            self.recent_sentiment_activations.append(activation_with_timestamp)
            if len(self.recent_sentiment_activations) > 10:
                self.recent_sentiment_activations = self.recent_sentiment_activations[-10:]
        elif component == 'regulation':
            self.recent_regulation_activations.append(activation_with_timestamp)
            if len(self.recent_regulation_activations) > 10:
                self.recent_regulation_activations = self.recent_regulation_activations[-10:]
            
        self.last_updated = datetime.now()

class EmotionSystemState(BaseModel):
    """
    Complete state of the emotion system
    
    This combines the current emotional state, parameters, and neural state
    """
    current_state: EmotionState
    parameters: EmotionalParameters
    neural_state: EmotionNeuralState = Field(default_factory=EmotionNeuralState)
    
    # History of emotional states
    emotion_history: List[EmotionState] = Field(default_factory=list, description="Recent emotion states")
    
    # History of emotional responses
    response_history: List[EmotionalResponse] = Field(default_factory=list, description="Recent emotional responses")
    
    # History of regulation attempts
    regulation_history: List[EmotionRegulationResult] = Field(default_factory=list, description="Recent regulation attempts")
    
    # System metadata
    module_id: str
    developmental_level: float = Field(0.0, ge=0.0, le=1.0)
    last_updated: datetime = Field(default_factory=datetime.now)
    
    class Config:
        arbitrary_types_allowed = True
    
    def dict(self, *args, **kwargs):
        """Convert datetime to timestamp for serialization"""
        result = super().dict(*args, **kwargs)
        result['last_updated'] = self.last_updated.isoformat()
        
        # Convert emotional states to dictionaries
        result['current_state'] = self.current_state.dict()
        result['emotion_history'] = [state.dict() for state in self.emotion_history]
        result['response_history'] = [response.dict() for response in self.response_history]
        result['regulation_history'] = [regulation.dict() for regulation in self.regulation_history]
        
        return result
    
    def add_emotion_state(self, state: EmotionState, max_history: int = 20) -> None:
        """
        Add an emotion state to history
        
        Args:
            state: The emotion state to add
            max_history: Maximum number of states to keep
        """
        self.emotion_history.append(state)
        if len(self.emotion_history) > max_history:
            self.emotion_history = self.emotion_history[-max_history:]
        self.last_updated = datetime.now()
    
    def add_emotional_response(self, response: EmotionalResponse, max_history: int = 20) -> None:
        """
        Add an emotional response to history
        
        Args:
            response: The emotional response to add
            max_history: Maximum number of responses to keep
        """
        self.response_history.append(response)
        if len(self.response_history) > max_history:
            self.response_history = self.response_history[-max_history:]
        self.last_updated = datetime.now()
    
    def add_regulation_result(self, result: EmotionRegulationResult, max_history: int = 20) -> None:
        """
        Add a regulation result to history
        
        Args:
            result: The regulation result to add
            max_history: Maximum number of results to keep
        """
        self.regulation_history.append(result)
        if len(self.regulation_history) > max_history:
            self.regulation_history = self.regulation_history[-max_history:]
        self.last_updated = datetime.now()


#######################

#emotion\neural_net.py#
#######################

import torch 
import torch.nn as nn 
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Any, Tuple, Optional, Union

class EmotionEncoder(nn.Module):
    """
    Neural network for encoding emotion-relevant features into embedding space
    
    This model takes features from stimuli and context and maps them to
    dimensional (valence-arousal) emotion space.
    """
    
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 32):
        """
        Initialize the emotion encoder network
        
        Args:
            input_dim: Dimension of input features
            hidden_dim: Dimension of hidden layers
            output_dim: Dimension of emotion embedding space
        """
        super().__init__()
        
        # Input projection layer
        self.input_projection = nn.Linear(input_dim, hidden_dim)
        
        # Hidden layers for feature extraction
        self.hidden_layers = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # Output projection to valence-arousal space
        self.output_projection = nn.Linear(hidden_dim, output_dim)
        
        # Separate heads for valence and arousal prediction
        self.valence_head = nn.Linear(output_dim, 1)  # valence (-1 to 1)
        self.arousal_head = nn.Linear(output_dim, 1)  # arousal (0 to 1)
        
        # Layer normalization for stable training
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
        # Developmental parameters (controlled externally)
        self.developmental_factor = nn.Parameter(torch.tensor(0.1), requires_grad=False)
    
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Process input features to predict valence and arousal
        
        Args:
            x: Input tensor of features [batch_size, input_dim]
            
        Returns:
            Dictionary with emotion embedding, valence, and arousal
        """
        # Project input to hidden dimension
        hidden = self.input_projection(x)
        hidden = F.relu(hidden)
        
        # Process through hidden layers with residual connection
        hidden_output = self.hidden_layers(hidden)
        hidden = hidden + self.layer_norm(hidden_output)  # Residual connection
        
        # Project to embedding space
        embedding = self.output_projection(hidden)
        
        # Developmental modulation - more complex processing at higher development
        if self.developmental_factor.item() < 0.3:
            # Very basic processing at early development
            embedding = 0.5 * embedding + 0.5 * torch.randn_like(embedding) * 0.1
        
        # Predict valence (-1 to 1)
        valence = torch.tanh(self.valence_head(embedding))
        
        # Predict arousal (0 to 1)
        arousal = torch.sigmoid(self.arousal_head(embedding))
        
        return {
            "embedding": embedding,
            "valence": valence,
            "arousal": arousal
        }
    
    def set_development_level(self, level: float) -> None:
        """
        Set the developmental level of the encoder
        
        Args:
            level: Development level (0.0 to 1.0)
        """
        with torch.no_grad():
            self.developmental_factor.copy_(torch.tensor(max(0.0, min(1.0, level))))


class EmotionClassifierNetwork(nn.Module):
    """
    Neural network for mapping dimensional emotions to categorical emotions
    
    This model takes valence-arousal representations and classifies them
    into discrete emotion categories with confidence scores.
    """
    
    def __init__(self, input_dim: int = 2, hidden_dim: int = 128, num_emotions: int = 8):
        """
        Initialize the emotion classifier network
        
        Args:
            input_dim: Dimension of input (typically 2 for valence and arousal)
            hidden_dim: Dimension of hidden layers
            num_emotions: Number of discrete emotion categories
        """
        super().__init__()
        
        # Input layer
        self.input_layer = nn.Linear(input_dim, hidden_dim)
        
        # Hidden layers
        self.hidden_layers = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # Emotion category prediction (logits)
        self.emotion_classifier = nn.Linear(hidden_dim, num_emotions)
        
        # Developmental parameter
        self.developmental_factor = nn.Parameter(torch.tensor(0.1), requires_grad=False)
        
        # Emotion centers in valence-arousal space (for basic classification)
        # Format: [valence, arousal] where valence is -1 to 1, arousal is 0 to 1
        self.emotion_centers = {
            "joy": torch.tensor([0.8, 0.7]),
            "sadness": torch.tensor([-0.7, 0.3]), 
            "anger": torch.tensor([-0.6, 0.8]),
            "fear": torch.tensor([-0.8, 0.9]),
            "disgust": torch.tensor([-0.5, 0.5]),
            "surprise": torch.tensor([0.1, 0.9]),
            "trust": torch.tensor([0.7, 0.4]),
            "anticipation": torch.tensor([0.5, 0.7])
        }
    
    def forward(self, valence: torch.Tensor, arousal: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Classify valence-arousal values into discrete emotions
        
        Args:
            valence: Tensor of valence values [-1,1]
            arousal: Tensor of arousal values [0,1]
            
        Returns:
            Dictionary with emotion logits and probabilities
        """
        # Combine valence and arousal
        x = torch.cat([valence, arousal], dim=-1)
        
        # Check developmental level to determine classification approach
        dev_level = self.developmental_factor.item()
        
        if dev_level < 0.3:
            # Simple classification based on closest emotion center
            emotion_probs = self._basic_classification(valence, arousal)
            return {
                "emotion_probs": emotion_probs
            }
        
        # Neural classification for more developed stages
        hidden = F.relu(self.input_layer(x))
        hidden = self.hidden_layers(hidden)
        emotion_logits = self.emotion_classifier(hidden)
        emotion_probs = F.softmax(emotion_logits, dim=-1)
        
        return {
            "emotion_logits": emotion_logits,
            "emotion_probs": emotion_probs
        }
    
    def _basic_classification(self, valence: torch.Tensor, arousal: torch.Tensor) -> torch.Tensor:
        """
        Basic classification based on distance to emotion centers
        
        Args:
            valence: Tensor of valence values [-1,1]
            arousal: Tensor of arousal values [0,1]
            
        Returns:
            Tensor of emotion probabilities
        """
        # Combine valence and arousal into coordinates
        coords = torch.cat([valence, arousal], dim=-1)  # [batch_size, 2]
        
        # Get available emotions based on development level
        if self.developmental_factor.item() < 0.2:
            # Very basic emotions only
            available_emotions = ["joy", "sadness"]
        elif self.developmental_factor.item() < 0.5:
            # Primary emotions
            available_emotions = ["joy", "sadness", "anger", "fear"]
        else:
            # All emotions
            available_emotions = list(self.emotion_centers.keys())
        
        # Create a tensor for emotion centers
        centers = torch.stack([self.emotion_centers[e] for e in available_emotions])  # [num_emotions, 2]
        
        # Calculate distances to all emotion centers
        # Reshape coordinates to [batch_size, 1, 2] for broadcasting
        coords_expanded = coords.unsqueeze(1)  # [batch_size, 1, 2]
        
        # Calculate squared Euclidean distance
        # This gives us [batch_size, num_emotions]
        distances = torch.sum((coords_expanded - centers) ** 2, dim=2)
        
        # Convert distances to probabilities (inverse relationship)
        # Use softmax with negative distances (smaller distance = higher probability)
        probs = F.softmax(-distances, dim=1)
        
        return probs
    
    def set_development_level(self, level: float) -> None:
        """
        Set the developmental level of the classifier
        
        Args:
            level: Development level (0.0 to 1.0)
        """
        with torch.no_grad():
            self.developmental_factor.copy_(torch.tensor(max(0.0, min(1.0, level))))


class SentimentNetwork(nn.Module):
    """
    Neural network for analyzing sentiment in text
    
    This model processes text features to detect emotional tone,
    sentiment polarity, and specific emotion indicators.
    """
    
    def __init__(self, vocab_size: int = 5000, embedding_dim: int = 64, 
                 hidden_dim: int = 128, output_dim: int = 4):
        """
        Initialize the sentiment analysis network
        
        Args:
            vocab_size: Size of vocabulary
            embedding_dim: Dimension of word embeddings
            hidden_dim: Dimension of hidden layers
            output_dim: Dimension of output (typically 4 for pos/neg/neutral/compound)
        """
        super().__init__()
        
        # Word embedding layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        
        # Bidirectional LSTM for sequence processing
        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_dim,
            batch_first=True,
            bidirectional=True
        )
        
        # Attention mechanism
        self.attention = nn.Linear(hidden_dim * 2, 1)
        
        # Output layers for different sentiment aspects
        self.sentiment_classifier = nn.Linear(hidden_dim * 2, output_dim)  # positive, negative, neutral, compound
        
        # Developmental parameter
        self.developmental_factor = nn.Parameter(torch.tensor(0.1), requires_grad=False)
    
    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Process text to analyze sentiment
        
        Args:
            x: Input tensor of token ids [batch_size, seq_length]
            lengths: Tensor of sequence lengths [batch_size]
            
        Returns:
            Dictionary with sentiment scores and features
        """
        # Convert token ids to embeddings
        embedded = self.embedding(x)  # [batch_size, seq_length, embedding_dim]
        
        # Pack padded sequence for efficient processing
        packed = nn.utils.rnn.pack_padded_sequence(
            embedded, lengths.cpu(), batch_first=True, enforce_sorted=False
        )
        
        # Process through LSTM
        packed_output, (hidden, cell) = self.lstm(packed)
        
        # Unpack the sequence
        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)
        # output: [batch_size, seq_length, hidden_dim*2]
        
        # Apply attention to focus on sentiment-relevant parts
        attention_scores = self.attention(output).squeeze(-1)  # [batch_size, seq_length]
        
        # Create attention mask based on sequence lengths
        mask = torch.arange(output.size(1), device=x.device).expand(output.size(0), -1) < lengths.unsqueeze(-1)
        attention_scores = attention_scores.masked_fill(~mask, float('-inf'))
        
        # Apply softmax to get attention weights
        attention_weights = F.softmax(attention_scores, dim=1).unsqueeze(-1)  # [batch_size, seq_length, 1]
        
        # Apply attention weights to get context vector
        context = torch.sum(attention_weights * output, dim=1)  # [batch_size, hidden_dim*2]
        
        # Classify sentiment
        sentiment_logits = self.sentiment_classifier(context)  # [batch_size, 4]
        
        # Split into individual sentiment components
        positive = torch.sigmoid(sentiment_logits[:, 0])
        negative = torch.sigmoid(sentiment_logits[:, 1])
        neutral = torch.sigmoid(sentiment_logits[:, 2])
        
        # Compound score derived from positive and negative
        # (scaled to range from -1 to 1)
        compound = positive - negative
        
        return {
            "positive": positive,
            "negative": negative,
            "neutral": neutral,
            "compound": compound,
            "features": context
        }
    
    def set_development_level(self, level: float) -> None:
        """
        Set the developmental level of the sentiment analyzer
        
        Args:
            level: Development level (0.0 to 1.0)
        """
        with torch.no_grad():
            self.developmental_factor.copy_(torch.tensor(max(0.0, min(1.0, level))))


class EmotionRegulationNetwork(nn.Module):
    """
    Neural network for emotion regulation
    
    This model processes current emotional state and regulation goals
    to produce regulated emotional state.
    """
    
    def __init__(self, input_dim: int = 10, hidden_dim: int = 128, output_dim: int = 2):
        """
        Initialize the emotion regulation network
        
        Args:
            input_dim: Dimension of input (emotional state + regulation goals)
            hidden_dim: Dimension of hidden layers
            output_dim: Dimension of output (typically 2 for valence and arousal)
        """
        super().__init__()
        
        # Input projection
        self.input_projection = nn.Linear(input_dim, hidden_dim)
        
        # Context encoding (for regulation context)
        self.context_encoder = nn.Linear(input_dim, hidden_dim)
        
        # Hidden layers
        self.hidden_layers = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # Output projection
        self.output_projection = nn.Linear(hidden_dim, output_dim)
        
        # Success prediction (how successful the regulation attempt will be)
        self.success_predictor = nn.Linear(hidden_dim, 1)
        
        # Developmental parameter
        self.developmental_factor = nn.Parameter(torch.tensor(0.1), requires_grad=False)
    
    def forward(self, 
                current_state: torch.Tensor, 
                target_state: torch.Tensor, 
                regulation_strategy: torch.Tensor,
                context: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        Apply emotion regulation to current state
        
        Args:
            current_state: Current emotional state [batch_size, 2] (valence, arousal)
            target_state: Target emotional state [batch_size, 2] (valence, arousal)
            regulation_strategy: One-hot encoded regulation strategy [batch_size, num_strategies]
            context: Optional context information [batch_size, context_dim]
            
        Returns:
            Dictionary with regulated state and success prediction
        """
        # Combine inputs
        inputs = torch.cat([current_state, target_state, regulation_strategy], dim=-1)
        if context is not None:
            inputs = torch.cat([inputs, context], dim=-1)
        
        # Project input
        hidden = F.relu(self.input_projection(inputs))
        
        # Encode context (or use a default if none provided)
        if context is not None:
            context_hidden = F.relu(self.context_encoder(context))
        else:
            context_hidden = torch.zeros_like(hidden)
        
        # Combine input and context
        combined = torch.cat([hidden, context_hidden], dim=-1)
        
        # Process through hidden layers
        hidden = self.hidden_layers(combined)
        
        # Development-based modulation
        dev_level = self.developmental_factor.item()
        
        # Limited regulation capabilities at early development
        if dev_level < 0.3:
            # Early development - limited regulation capability
            # Regulated state is closer to current than target
            weight = 0.2 * dev_level  # Very limited movement toward target
            regulated_state = current_state * (1 - weight) + target_state * weight
            success = torch.tensor([[0.2 * dev_level]], device=current_state.device)
            
            return {
                "regulated_state": regulated_state,
                "success": success
            }
        
        # More advanced regulation for higher development
        
        # Project to regulated state
        regulated_delta = self.output_projection(hidden)
        
        # Apply delta to current state (bounded to prevent extreme changes)
        regulated_state = current_state + torch.tanh(regulated_delta) * (0.5 + 0.5 * dev_level)
        
        # Ensure valence stays in [-1,1] and arousal in [0,1]
        regulated_state_constrained = torch.stack([
            torch.clamp(regulated_state[:, 0], -1.0, 1.0),  # valence
            torch.clamp(regulated_state[:, 1], 0.0, 1.0)    # arousal
        ], dim=1)
        
        # Predict regulation success
        success = torch.sigmoid(self.success_predictor(hidden))
        
        # Developmental factor affects success
        success = success * (0.5 + 0.5 * dev_level)
        
        return {
            "regulated_state": regulated_state_constrained,
            "success": success
        }
    
    def set_development_level(self, level: float) -> None:
        """
        Set the developmental level of the regulator
        
        Args:
            level: Development level (0.0 to 1.0)
        """
        with torch.no_grad():
            self.developmental_factor.copy_(torch.tensor(max(0.0, min(1.0, level))))


def get_device() -> torch.device:
    """
    Get the appropriate device (GPU if available, otherwise CPU)
    
    Returns:
        torch.device: The device to use for tensor operations
    """
    if torch.cuda.is_available():
        return torch.device("cuda")
    return torch.device("cpu")


#######################

#emotion\regulation.py#
#######################

"""
Emotion Regulation

This component is responsible for modulating emotional responses,
providing mechanisms to adjust emotional intensity and expression
based on context and developmental capabilities.
"""

import logging
import uuid
import time
import math
import random
from typing import Dict, List, Any, Optional, Union, Tuple
from datetime import datetime
from collections import defaultdict

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.emotion.models import EmotionState, EmotionRegulationRequest, EmotionRegulationResult, EmotionNeuralState

# Initialize logger
logger = logging.getLogger(__name__)

class EmotionRegulator(BaseModule):
    """
    Regulates emotional states through various strategies
    
    This module develops from minimal regulation capability to
    sophisticated emotional control strategies based on context.
    """
    # Development milestones
    development_milestones = {
        0.0: "Minimal regulation capability",
        0.2: "Basic emotional suppression",
        0.4: "Attentional deployment strategies",
        0.6: "Cognitive reappraisal",
        0.8: "Context-sensitive regulation",
        1.0: "Sophisticated emotional regulation"
    }
    
    def __init__(
        self,
        module_id: str,
        event_bus: Optional[EventBus] = None,
        development_level: float = 0.0
    ):
        """
        Initialize the emotion regulator
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication
            development_level: Initial developmental level
        """
        super().__init__(
            module_id=module_id,
            module_type="emotion_regulator",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Initialize regulation strategies
        self._initialize_regulation_strategies()
        
        # Neural state for tracking activations and development
        self.neural_state = EmotionNeuralState()
        self.neural_state.regulation_development = development_level
        
        # Regulation parameters
        self.params = {
            # How much emotional intensity can be regulated (0-1)
            "regulation_capacity": 0.1,
            
            # How long the regulation effect lasts
            "regulation_duration": 60,  # seconds
            
            # How much cognitive resources regulation requires
            "cognitive_cost": 0.2,
            
            # Default regulation target
            "default_valence_target": 0.1,  # slightly positive
            "default_arousal_target": 0.3,  # moderate arousal
            
            # Whether regulation has side effects
            "side_effects_enabled": False
        }
        
        # Adjust parameters based on development level
        self._adjust_parameters_for_development()
        
        # History of regulation attempts
        self.regulation_history = []
        
        # Active regulation effects
        self.active_regulations = {}
        
        logger.info(f"Emotion regulator initialized at development level {development_level:.2f}")
    
    def _initialize_regulation_strategies(self):
        """Initialize available emotion regulation strategies"""
        # Define strategies based on psychological research
        self.strategies = {
            # Suppression - hide emotional expression
            # Available at early development (0.2+)
            "suppression": {
                "min_development": 0.2,
                "effectiveness": 0.4,  # moderately effective
                "side_effects": {
                    "cognitive_cost": 0.4,  # high cognitive cost
                    "rebound": 0.3      # causes some rebound effect
                },
                "description": "Masking or hiding emotional expression"
            },
            
            # Distraction - redirect attention away from emotion stimulus
            # Available at intermediate development (0.3+)
            "distraction": {
                "min_development": 0.3,
                "effectiveness": 0.5,
                "side_effects": {
                    "cognitive_cost": 0.2,
                    "rebound": 0.1
                },
                "description": "Redirecting attention away from emotional stimuli"
            },
            
            # Reappraisal - reconsider the meaning of the stimulus
            # Available at higher development (0.6+)
            "reappraisal": {
                "min_development": 0.6,
                "effectiveness": 0.7,
                "side_effects": {
                    "cognitive_cost": 0.3,
                    "rebound": 0.0
                },
                "description": "Reinterpreting the meaning of emotional stimuli"
            },
            
            # Acceptance - allow emotions to exist without judgment
            # Available at higher development (0.7+)
            "acceptance": {
                "min_development": 0.7,
                "effectiveness": 0.6,
                "side_effects": {
                    "cognitive_cost": 0.1,
                    "rebound": 0.0
                },
                "description": "Accepting emotions without judgment"
            },
            
            # Problem-solving - address the cause of the emotion
            # Available at higher development (0.8+)
            "problem_solving": {
                "min_development": 0.8,
                "effectiveness": 0.8,
                "side_effects": {
                    "cognitive_cost": 0.4,
                    "rebound": 0.0
                },
                "description": "Addressing the causes of emotional reactions"
            }
        }
    
    def _adjust_parameters_for_development(self):
        """Adjust regulation parameters based on developmental level"""
        if self.development_level < 0.2:
            # Very limited regulation at early stages
            self.params.update({
                "regulation_capacity": 0.1,
                "cognitive_cost": 0.5,  # High cost for limited effect
                "side_effects_enabled": True,  # More side effects at low development
                "regulation_duration": 30  # Short duration
            })
        elif self.development_level < 0.4:
            # Basic regulation capabilities
            self.params.update({
                "regulation_capacity": 0.2,
                "cognitive_cost": 0.4,
                "side_effects_enabled": True,
                "regulation_duration": 60
            })
        elif self.development_level < 0.6:
            # Improved regulation
            self.params.update({
                "regulation_capacity": 0.4,
                "cognitive_cost": 0.3,
                "side_effects_enabled": True,
                "regulation_duration": 120
            })
        elif self.development_level < 0.8:
            # Advanced regulation
            self.params.update({
                "regulation_capacity": 0.6,
                "cognitive_cost": 0.2,
                "side_effects_enabled": False,
                "regulation_duration": 300
            })
        else:
            # Sophisticated regulation
            self.params.update({
                "regulation_capacity": 0.8,
                "cognitive_cost": 0.1,
                "side_effects_enabled": False,
                "regulation_duration": 600
            })
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to regulate emotions
        
        Args:
            input_data: Data to process for regulation
                Required keys: 'current_state'
                Optional keys: 'target_valence', 'target_arousal', 'target_emotion',
                               'regulation_strategy', 'context'
                
        Returns:
            Dictionary with regulation results
        """
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Extract the current emotional state
        if "current_state" not in input_data:
            return {
                "status": "error",
                "message": "No current emotional state provided",
                "process_id": process_id
            }
            
        current_state = input_data["current_state"]
        
        # Get regulation capacity (how much we can regulate)
        # This may be overridden by input data
        regulation_capacity = input_data.get(
            "regulation_capacity", 
            self.params["regulation_capacity"]
        )
        
        # Determine regulation approach
        target_valence = input_data.get("target_valence")
        target_arousal = input_data.get("target_arousal")
        target_emotion = input_data.get("target_emotion")
        specified_strategy = input_data.get("regulation_strategy")
        context = input_data.get("context", {})
        
        # If no specific targets provided, use defaults
        if target_valence is None and target_arousal is None and target_emotion is None:
            target_valence = self.params["default_valence_target"]
            target_arousal = self.params["default_arousal_target"]
        
        # Select regulation strategy if not specified
        strategy = specified_strategy or self._select_strategy(
            current_state, target_valence, target_arousal, target_emotion, context
        )
        
        # Apply regulation strategy
        regulation_result = self._apply_regulation(
            current_state, 
            strategy, 
            target_valence, 
            target_arousal, 
            target_emotion,
            regulation_capacity,
            context
        )
        
        # Add to history
        self.regulation_history.append({
            "original_state": current_state,
            "regulated_state": regulation_result["regulated_state"],
            "strategy": strategy,
            "success_level": regulation_result["success_level"],
            "timestamp": datetime.now().isoformat(),
            "process_id": process_id
        })
        
        # Limit history size
        if len(self.regulation_history) > 50:
            self.regulation_history = self.regulation_history[-50:]
        
        # Create result
        result = {
            "status": "success",
            "regulation_result": regulation_result,
            "process_id": process_id,
            "development_level": self.development_level
        }
        
        # Publish result if we have event bus
        if self.event_bus:
            self.publish_message(
                "emotion_regulation_result",
                {
                    "result": regulation_result,
                    "process_id": process_id
                }
            )
        
        return result
    
    def _select_strategy(
        self, 
        current_state: Any, 
        target_valence: Optional[float], 
        target_arousal: Optional[float],
        target_emotion: Optional[str],
        context: Dict[str, Any]
    ) -> str:
        """
        Select appropriate regulation strategy
        
        Args:
            current_state: Current emotional state
            target_valence: Target valence (if any)
            target_arousal: Target arousal (if any)
            target_emotion: Target emotion (if any)
            context: Contextual information
            
        Returns:
            Selected strategy name
        """
        # Get available strategies based on development level
        available_strategies = [
            name for name, info in self.strategies.items()
            if self.development_level >= info["min_development"]
        ]
        
        # At early development, just use whatever's available
        if self.development_level < 0.4 or not available_strategies:
            return available_strategies[0] if available_strategies else "suppression"
        
        # At higher development, use more sophisticated selection
        if self.development_level >= 0.7:
            # Consider context and specific regulation goals
            
            # If we need to increase positive valence
            if target_valence is not None and target_valence > current_state.valence:
                if "reappraisal" in available_strategies:
                    return "reappraisal"
                elif "problem_solving" in available_strategies:
                    return "problem_solving"
            
            # If we need to decrease negative valence
            elif target_valence is not None and target_valence < current_state.valence:
                if current_state.valence < -0.5 and "acceptance" in available_strategies:
                    return "acceptance"
                elif "distraction" in available_strategies:
                    return "distraction"
            
            # If we need to decrease arousal
            if target_arousal is not None and target_arousal < current_state.arousal:
                if "acceptance" in available_strategies:
                    return "acceptance"
                elif "distraction" in available_strategies:
                    return "distraction"
            
            # If we need to increase arousal
            elif target_arousal is not None and target_arousal > current_state.arousal:
                if "problem_solving" in available_strategies:
                    return "problem_solving"
                
            # Default to most effective available strategy
            effectiveness_sorted = sorted(
                [(s, self.strategies[s]["effectiveness"]) for s in available_strategies],
                key=lambda x: x[1],
                reverse=True
            )
            return effectiveness_sorted[0][0]
        
        # Middle development - use simpler selection
        else:
            # Just select randomly from available, weighted by effectiveness
            weights = [self.strategies[s]["effectiveness"] for s in available_strategies]
            total = sum(weights)
            normalized_weights = [w / total for w in weights]
            
            # Random selection with weights
            return random.choices(available_strategies, weights=normalized_weights, k=1)[0]
    
    def _apply_regulation(
        self,
        current_state: Any,
        strategy: str,
        target_valence: Optional[float],
        target_arousal: Optional[float],
        target_emotion: Optional[str],
        regulation_capacity: float,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Apply regulation strategy to emotional state
        
        Args:
            current_state: Current emotional state
            strategy: Selected regulation strategy
            target_valence: Target valence (if any)
            target_arousal: Target arousal (if any) 
            target_emotion: Target emotion (if any)
            regulation_capacity: Regulation strength
            context: Contextual information
            
        Returns:
            Dictionary with regulation results
        """
        # Get strategy info
        strategy_info = self.strategies.get(strategy, self.strategies["suppression"])
        
        # Effectiveness is based on strategy and development
        effectiveness = strategy_info["effectiveness"] * (0.5 + 0.5 * self.development_level)
        
        # Calculate total regulation strength
        regulation_strength = regulation_capacity * effectiveness
        
        # Create new state values
        new_valence = current_state.valence
        new_arousal = current_state.arousal
        new_dominant_emotion = current_state.dominant_emotion
        new_emotion_intensities = dict(current_state.emotion_intensities)
        
        # Regulate valence if target provided
        if target_valence is not None:
            # Move current valence toward target
            valence_diff = target_valence - current_state.valence
            valence_change = valence_diff * regulation_strength
            new_valence = current_state.valence + valence_change
            
        # Regulate arousal if target provided
        if target_arousal is not None:
            # Move current arousal toward target
            arousal_diff = target_arousal - current_state.arousal
            arousal_change = arousal_diff * regulation_strength
            new_arousal = current_state.arousal + arousal_change
            
        # Ensure values are in valid ranges
        new_valence = max(-1.0, min(1.0, new_valence))
        new_arousal = max(0.0, min(1.0, new_arousal))
        
        # Regulate specific emotion if target provided
        if target_emotion is not None and target_emotion in new_emotion_intensities:
            # Increase target emotion intensity
            current_intensity = new_emotion_intensities[target_emotion]
            new_intensity = current_intensity + (1.0 - current_intensity) * regulation_strength
            new_emotion_intensities[target_emotion] = new_intensity
            
            # Decrease other emotions proportionally
            intensity_increase = new_intensity - current_intensity
            other_emotions = [e for e in new_emotion_intensities if e != target_emotion]
            
            if other_emotions:
                for emotion in other_emotions:
                    new_emotion_intensities[emotion] = max(
                        0.0, 
                        new_emotion_intensities[emotion] - (intensity_increase / len(other_emotions))
                    )
                
                # Normalize intensities to sum to 1.0
                total_intensity = sum(new_emotion_intensities.values())
                new_emotion_intensities = {
                    e: i / total_intensity for e, i in new_emotion_intensities.items()
                }
                
            # Update dominant emotion if target is now strongest
            if target_emotion != new_dominant_emotion and new_emotion_intensities[target_emotion] > new_emotion_intensities.get(new_dominant_emotion, 0):
                new_dominant_emotion = target_emotion
        
        # Create new emotional state
        regulated_state = EmotionState(
            valence=new_valence,
            arousal=new_arousal,
            dominant_emotion=new_dominant_emotion,
            emotion_intensities=new_emotion_intensities,
            timestamp=datetime.now()
        )
        
        # Calculate regulation success
        if target_valence is not None and target_arousal is not None:
            # Calculate Euclidean distance in VA space from target
            original_distance = math.sqrt(
                (current_state.valence - target_valence) ** 2 +
                (current_state.arousal - target_arousal) ** 2
            )
            
            new_distance = math.sqrt(
                (regulated_state.valence - target_valence) ** 2 +
                (regulated_state.arousal - target_arousal) ** 2
            )
            
            # Success is proportional to distance reduction
            if original_distance > 0:
                success_level = min(1.0, max(0.0, (original_distance - new_distance) / original_distance))
            else:
                success_level = 1.0  # Already at target
                
        elif target_valence is not None:
            # Only valence matters
            original_distance = abs(current_state.valence - target_valence)
            new_distance = abs(regulated_state.valence - target_valence)
            
            if original_distance > 0:
                success_level = min(1.0, max(0.0, (original_distance - new_distance) / original_distance))
            else:
                success_level = 1.0
                
        elif target_arousal is not None:
            # Only arousal matters
            original_distance = abs(current_state.arousal - target_arousal)
            new_distance = abs(regulated_state.arousal - target_arousal)
            
            if original_distance > 0:
                success_level = min(1.0, max(0.0, (original_distance - new_distance) / original_distance))
            else:
                success_level = 1.0
                
        elif target_emotion is not None:
            # Emotion category matters
            original_intensity = current_state.emotion_intensities.get(target_emotion, 0.0)
            new_intensity = regulated_state.emotion_intensities.get(target_emotion, 0.0)
            
            success_level = min(1.0, max(0.0, (new_intensity - original_intensity)))
            
        else:
            # No specific target, success is based on regulation strength
            success_level = regulation_strength
        
        # Record activation for tracking
        if hasattr(self, "neural_state"):
            self.neural_state.add_activation('regulation', {
                'strategy': strategy,
                'effectiveness': effectiveness,
                'regulation_strength': regulation_strength,
                'original_valence': current_state.valence,
                'original_arousal': current_state.arousal,
                'new_valence': regulated_state.valence,
                'new_arousal': regulated_state.arousal,
                'success_level': success_level
            })
            
        # Create regulation result
        result = {
            "original_state": current_state,
            "regulated_state": regulated_state,
            "regulation_strategy": strategy,
            "success_level": success_level,
            "process_id": str(uuid.uuid4())
        }
        
        return result
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New developmental level
        """
        # Update base module development
        new_level = super().update_development(amount)
        
        # Adjust parameters for new development level
        self._adjust_parameters_for_development()
        
        # Update neural state
        if hasattr(self, "neural_state"):
            self.neural_state.regulation_development = new_level
            self.neural_state.last_updated = datetime.now()
        
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state
        
        Returns:
            Dictionary with current state
        """
        base_state = super().get_state()
        
        # Add regulator-specific state
        regulator_state = {
            "params": self.params,
            "available_strategies": [
                name for name, info in self.strategies.items()
                if self.development_level >= info["min_development"]
            ],
            "history_length": len(self.regulation_history),
            "last_regulation": self.regulation_history[-1] if self.regulation_history else None
        }
        
        # Combine states
        combined_state = {**base_state, **regulator_state}
        
        return combined_state 


#######################

#emotion\sentiment_analyzer.py#
#######################

import logging
import time
import uuid
from typing import Dict, List, Any, Optional, Union, Tuple
from datetime import datetime
import re
import numpy as np
from collections import deque, Counter

# Use TextBlob for basic sentiment analysis
try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except ImportError:
    TEXTBLOB_AVAILABLE = False
    logging.warning("TextBlob not available. Basic sentiment analysis will be used.")

# Use NLTK for more advanced NLP if available
try:
    import nltk
    from nltk.tokenize import word_tokenize, sent_tokenize
    from nltk.corpus import stopwords
    NLTK_AVAILABLE = True
    
    # Download necessary NLTK resources if not already available
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt', quiet=True)
    
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        nltk.download('stopwords', quiet=True)
        
except ImportError:
    NLTK_AVAILABLE = False
    logging.warning("NLTK not available. Advanced NLP features will be limited.")

# PyTorch for neural sentiment analysis
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    logging.warning("PyTorch not available. Neural sentiment analysis will be disabled.")

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.emotion.models import SentimentAnalysis, EmotionNeuralState
from lmm_project.utils.llm_client import LLMClient, Message as LLMMessage

# Initialize logger
logger = logging.getLogger(__name__)

class SentimentNN(nn.Module):
    """Simple neural network for sentiment analysis"""
    def __init__(self, vocab_size=5000, embedding_dim=50, hidden_dim=100, output_dim=3):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.fc1 = nn.Linear(embedding_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, x):
        embedded = self.embedding(x)
        # Simple averaging of embeddings
        embedded = embedded.mean(dim=1)
        x = F.relu(self.fc1(embedded))
        x = self.fc2(x)
        return F.softmax(x, dim=1)

class SentimentAnalyzer(BaseModule):
    """
    Sentiment analyzer for detecting emotional tone in text
    
    This module develops from basic positive/negative detection to
    sophisticated emotional tone analysis with contextual understanding.
    """
    # Development milestones
    development_milestones = {
        0.0: "Basic sentiment detection",
        0.2: "Positive/negative classification",
        0.4: "Multi-class emotion detection",
        0.6: "Contextual sentiment analysis",
        0.8: "Emotion intensity detection",
        1.0: "Sophisticated sentiment understanding"
    }
    
    def __init__(
        self,
        module_id: str,
        event_bus: Optional[EventBus] = None,
        development_level: float = 0.0
    ):
        """
        Initialize the sentiment analyzer
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication
            development_level: Initial developmental level
        """
        super().__init__(
            module_id=module_id,
            module_type="sentiment_analyzer",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Initialize emotion lexicons
        self._initialize_emotion_lexicons()
        
        # Parameters that vary with development
        self.params = {
            "basic_weight": 0.8,      # Weight for basic sentiment analysis
            "advanced_weight": 0.2,    # Weight for advanced analysis
            "context_weight": 0.1,     # Weight for contextual factors
            "intensity_threshold": 0.3, # Threshold for emotion detection
            "development_factor": development_level
        }
        
        # Neural state for tracking activations and development
        self.neural_state = EmotionNeuralState()
        self.neural_state.sentiment_development = development_level
        
        # Adjust parameters based on development level
        self._adjust_parameters_for_development()
        
        # History of recent analyses
        self.analysis_history = deque(maxlen=50)
        
        # Initialize neural sentiment analyzer if PyTorch is available
        self.neural_analyzer = None
        self.vocab = {}
        
        if TORCH_AVAILABLE and development_level >= 0.4:
            self._initialize_neural_analyzer()
            
        # Try to initialize LLM client for advanced analysis
        self.llm_client = None
        if development_level >= 0.6:
            try:
                self.llm_client = LLMClient()
                logger.info("LLM client initialized for advanced sentiment analysis")
            except Exception as e:
                logger.warning(f"Could not initialize LLM client: {e}")
        
        logger.info(f"Sentiment analyzer initialized at development level {development_level:.2f}")
        
    def _initialize_emotion_lexicons(self):
        """Initialize lexicons for emotion detection"""
        # Basic positive and negative word lists
        self.positive_words = {
            "good", "great", "excellent", "wonderful", "amazing", "fantastic",
            "terrific", "outstanding", "superb", "brilliant", "awesome",
            "happy", "joy", "delighted", "pleased", "glad", "satisfied",
            "love", "adore", "like", "enjoy", "appreciate", "admire",
            "beautiful", "lovely", "pleasant", "nice", "perfect"
        }
        
        self.negative_words = {
            "bad", "terrible", "horrible", "awful", "dreadful", "poor",
            "sad", "unhappy", "depressed", "miserable", "gloomy", "disappointed",
            "angry", "mad", "furious", "upset", "annoyed", "irritated",
            "hate", "dislike", "despise", "detest", "loathe", "abhor",
            "ugly", "unpleasant", "nasty", "disgusting", "offensive"
        }
        
        # Emotion-specific lexicons
        self.emotion_lexicons = {
            "joy": {
                "happy", "joy", "delighted", "pleased", "glad", "cheerful",
                "content", "satisfied", "merry", "jovial", "blissful",
                "ecstatic", "elated", "thrilled", "overjoyed", "exuberant"
            },
            "sadness": {
                "sad", "unhappy", "depressed", "miserable", "gloomy", "melancholy",
                "sorrowful", "downhearted", "downcast", "blue", "dejected",
                "heartbroken", "grief", "distressed", "woeful", "despondent"
            },
            "anger": {
                "angry", "mad", "furious", "enraged", "irate", "irritated",
                "annoyed", "vexed", "indignant", "outraged", "offended",
                "heated", "fuming", "infuriated", "livid", "seething"
            },
            "fear": {
                "afraid", "scared", "frightened", "terrified", "fearful", "anxious",
                "worried", "nervous", "panicked", "alarmed", "horrified",
                "startled", "suspicious", "uneasy", "wary", "dread"
            },
            "surprise": {
                "surprised", "astonished", "amazed", "astounded", "shocked", "stunned",
                "startled", "dumbfounded", "bewildered", "awestruck", "wonderstruck",
                "flabbergasted", "thunderstruck", "dazed", "speechless", "agog"
            },
            "disgust": {
                "disgusted", "revolted", "repulsed", "nauseated", "sickened", "appalled",
                "repelled", "offended", "abhorrent", "loathsome", "detestable",
                "distasteful", "repugnant", "vile", "gross", "creepy"
            },
            "trust": {
                "trust", "confident", "secure", "faithful", "reliable", "dependable",
                "trustworthy", "honest", "loyal", "sincere", "devoted",
                "authentic", "genuine", "believing", "convinced", "assured"
            },
            "anticipation": {
                "anticipate", "expect", "await", "look forward", "hope", "excited",
                "eager", "enthusiastic", "keen", "prepared", "ready",
                "watchful", "vigilant", "alert", "attentive", "mindful"
            }
        }
        
        # Intensity modifiers
        self.intensifiers = {
            "very", "extremely", "incredibly", "exceptionally", "tremendously",
            "absolutely", "completely", "totally", "utterly", "highly", 
            "deeply", "profoundly", "intensely", "remarkably", "seriously"
        }
        
        self.diminishers = {
            "slightly", "somewhat", "a bit", "a little", "fairly",
            "rather", "kind of", "sort of", "moderately", "relatively",
            "barely", "hardly", "scarcely", "faintly", "mildly"
        }
        
    def _initialize_neural_analyzer(self):
        """Initialize neural sentiment analyzer"""
        if not TORCH_AVAILABLE:
            return
            
        # Very simple vocabulary for demo purposes
        # In a real implementation, this would be trained on a corpus
        words = list(self.positive_words | self.negative_words)
        for emotion_words in self.emotion_lexicons.values():
            words.extend(emotion_words)
            
        # Add common words
        common_words = [
            "the", "a", "an", "and", "or", "but", "if", "because", "as", "what",
            "when", "where", "how", "why", "who", "this", "that", "these", "those",
            "is", "are", "was", "were", "be", "been", "being", "have", "has", "had",
            "do", "does", "did", "will", "would", "shall", "should", "can", "could",
            "may", "might", "must", "to", "for", "of", "in", "on", "at", "by", "with"
        ]
        words.extend(common_words)
        
        # Create vocabulary
        words = list(set(words))[:5000]  # Limit vocabulary size
        self.vocab = {word: i for i, word in enumerate(words)}
        
        # Create neural network
        self.neural_analyzer = SentimentNN(
            vocab_size=len(self.vocab) + 1,  # +1 for unknown words
            embedding_dim=50,
            hidden_dim=100,
            output_dim=3  # Positive, Negative, Neutral
        )
        
        # Try to use GPU if available
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
        else:
            self.device = torch.device('cpu')
            
        self.neural_analyzer.to(self.device)
        logger.info(f"Neural sentiment analyzer initialized with {len(self.vocab)} words vocabulary")
        
    def _adjust_parameters_for_development(self):
        """Adjust parameters based on developmental level"""
        if self.development_level < 0.2:
            # Very basic processing - simple positive/negative
            self.params.update({
                "basic_weight": 1.0,
                "advanced_weight": 0.0,
                "context_weight": 0.0,
                "intensity_threshold": 0.4,
                "development_factor": self.development_level
            })
        elif self.development_level < 0.4:
            # Developing multi-class classification
            self.params.update({
                "basic_weight": 0.8,
                "advanced_weight": 0.2,
                "context_weight": 0.1,
                "intensity_threshold": 0.35,
                "development_factor": self.development_level
            })
        elif self.development_level < 0.6:
            # Developing contextual understanding
            self.params.update({
                "basic_weight": 0.6,
                "advanced_weight": 0.4,
                "context_weight": 0.2,
                "intensity_threshold": 0.3,
                "development_factor": self.development_level
            })
        elif self.development_level < 0.8:
            # Developing intensity detection
            self.params.update({
                "basic_weight": 0.4,
                "advanced_weight": 0.6,
                "context_weight": 0.3,
                "intensity_threshold": 0.25,
                "development_factor": self.development_level
            })
        else:
            # Sophisticated analysis
            self.params.update({
                "basic_weight": 0.2,
                "advanced_weight": 0.8,
                "context_weight": 0.4,
                "intensity_threshold": 0.2,
                "development_factor": self.development_level
            })
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to analyze sentiment
        
        Args:
            input_data: Input data to process
                Required keys: 'text' or 'content'
                Optional keys: 'context'
                
        Returns:
            Dictionary with sentiment analysis results
        """
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Extract text from input
        text = ""
        if "text" in input_data:
            text = input_data["text"]
        elif "content" in input_data:
            content = input_data["content"]
            if isinstance(content, str):
                text = content
            elif isinstance(content, dict) and "text" in content:
                text = content["text"]
            
        if not text:
            return {
                "status": "error",
                "message": "No text provided for sentiment analysis",
                "process_id": process_id
            }
        
        # Process text to analyze sentiment
        context = input_data.get("context", {})
        analysis = self._analyze_sentiment(text, context)
        
        # Create SentimentAnalysis object
        sentiment_analysis = SentimentAnalysis(
            text=text,
            positive_score=analysis["positive_score"],
            negative_score=analysis["negative_score"],
            neutral_score=analysis["neutral_score"],
            compound_score=analysis["compound_score"],
            detected_emotions=analysis["detected_emotions"],
            highlighted_phrases=analysis["highlighted_phrases"],
            process_id=process_id,
            confidence=analysis["confidence"]
        )
        
        # Add to history
        self.analysis_history.append(sentiment_analysis)
        
        # Return analysis results
        result = {
            "status": "success",
            "analysis": sentiment_analysis.dict(),
            "process_id": process_id,
            "development_level": self.development_level
        }
        
        # Publish result if we have event bus
        if self.event_bus:
            self.publish_message(
                "sentiment_analysis_result",
                {
                    "analysis": sentiment_analysis.dict(),
                    "process_id": process_id
                }
            )
        
        return result
    
    def _analyze_sentiment(self, text: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyze sentiment in text
        
        Args:
            text: Text to analyze
            context: Contextual information
            
        Returns:
            Dictionary with sentiment analysis results
        """
        # Methods to use based on development level
        methods = []
        
        # Always use lexical approach
        methods.append(self._lexical_sentiment_analysis)
        
        # Add more sophisticated methods with development
        if TEXTBLOB_AVAILABLE and self.development_level >= 0.2:
            methods.append(self._textblob_sentiment_analysis)
            
        if TORCH_AVAILABLE and self.neural_analyzer and self.development_level >= 0.4:
            methods.append(self._neural_sentiment_analysis)
            
        if self.llm_client and self.development_level >= 0.6:
            methods.append(self._llm_sentiment_analysis)
            
        # Process with each method and combine results
        results = []
        method_names = []
        
        for method in methods:
            try:
                result = method(text, context)
                results.append(result)
                method_names.append(method.__name__.replace("_sentiment_analysis", ""))
            except Exception as e:
                logger.error(f"Error in sentiment analysis method {method.__name__}: {str(e)}")
        
        # Initialize combined results
        if not results:
            # Fallback to basic neutral sentiment
            return {
                "positive_score": 0.33,
                "negative_score": 0.33,
                "neutral_score": 0.34,
                "compound_score": 0.0,
                "detected_emotions": {},
                "highlighted_phrases": [],
                "confidence": 0.1,
                "method": "fallback"
            }
            
        # Calculate weights based on development and method sophistication
        weights = []
        if len(results) == 1:
            weights = [1.0]
        elif len(results) == 2:
            weights = [self.params["basic_weight"], self.params["advanced_weight"]]
        elif len(results) == 3:
            weights = [0.2, 0.3, 0.5]
        elif len(results) == 4:
            weights = [0.1, 0.2, 0.3, 0.4]
        else:
            weights = [1.0 / len(results)] * len(results)
            
        # Normalize weights
        total_weight = sum(weights)
        weights = [w / total_weight for w in weights]
        
        # Combine results
        combined = {
            "positive_score": sum(w * r["positive_score"] for w, r in zip(weights, results)),
            "negative_score": sum(w * r["negative_score"] for w, r in zip(weights, results)),
            "neutral_score": sum(w * r["neutral_score"] for w, r in zip(weights, results)),
            "compound_score": sum(w * r["compound_score"] for w, r in zip(weights, results)),
            "detected_emotions": self._combine_emotions([r["detected_emotions"] for r in results], weights),
            "highlighted_phrases": self._combine_phrases([r.get("highlighted_phrases", []) for r in results]),
            "confidence": min(0.2 + self.development_level, 
                             sum(w * r.get("confidence", 0.5) for w, r in zip(weights, results))),
            "method": "+".join(method_names)
        }
        
        return combined
    
    def _combine_emotions(self, emotion_dicts: List[Dict[str, float]], weights: List[float]) -> Dict[str, float]:
        """
        Combine emotion dictionaries from multiple methods
        
        Args:
            emotion_dicts: List of emotion dictionaries
            weights: Weights for each dictionary
            
        Returns:
            Combined emotion dictionary
        """
        # Initialize combined dictionary
        combined = {}
        
        # Combine all emotions
        for emotion_dict, weight in zip(emotion_dicts, weights):
            for emotion, score in emotion_dict.items():
                if emotion in combined:
                    combined[emotion] += score * weight
                else:
                    combined[emotion] = score * weight
        
        # Filter low-confidence emotions
        threshold = self.params["intensity_threshold"]
        combined = {k: v for k, v in combined.items() if v >= threshold}
        
        return combined
    
    def _combine_phrases(self, phrase_lists: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """
        Combine highlighted phrases from multiple methods
        
        Args:
            phrase_lists: List of phrase lists
            
        Returns:
            Combined phrase list
        """
        # Flatten and deduplicate phrases
        all_phrases = []
        seen_phrases = set()
        
        for phrase_list in phrase_lists:
            for phrase in phrase_list:
                text = phrase.get("text", "")
                if text and text not in seen_phrases:
                    all_phrases.append(phrase)
                    seen_phrases.add(text)
        
        # Sort by score and limit to top 10
        all_phrases.sort(key=lambda x: abs(x.get("score", 0)), reverse=True)
        return all_phrases[:10]
    
    def _lexical_sentiment_analysis(self, text: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Basic lexical sentiment analysis
        
        Args:
            text: Text to analyze
            context: Contextual information
            
        Returns:
            Dictionary with sentiment analysis results
        """
        # Tokenize text
        tokens = re.findall(r'\b\w+\b', text.lower())
        
        # Count positive and negative words
        pos_words = [token for token in tokens if token in self.positive_words]
        neg_words = [token for token in tokens if token in self.negative_words]
        
        pos_count = len(pos_words)
        neg_count = len(neg_words)
        total_words = len(tokens)
        
        # Calculate scores
        if total_words == 0:
            positive_score = 0.0
            negative_score = 0.0
            neutral_score = 1.0
            compound_score = 0.0
        else:
            positive_score = pos_count / total_words
            negative_score = neg_count / total_words
            neutral_score = 1.0 - positive_score - negative_score
            
            # Ensure neutral score is not negative
            neutral_score = max(0.0, neutral_score)
            
            # Normalize scores
            total = positive_score + negative_score + neutral_score
            if total > 0:
                positive_score /= total
                negative_score /= total
                neutral_score /= total
                
            # Calculate compound score (-1 to 1)
            if pos_count == 0 and neg_count == 0:
                compound_score = 0.0
            else:
                compound_score = (pos_count - neg_count) / (pos_count + neg_count)
        
        # Detect emotions
        detected_emotions = {}
        for emotion, word_set in self.emotion_lexicons.items():
            emotion_words = [token for token in tokens if token in word_set]
            if emotion_words:
                detected_emotions[emotion] = len(emotion_words) / total_words if total_words > 0 else 0.0
        
        # Highlight key phrases
        highlighted_phrases = []
        
        # For simple implementation, just highlight individual emotional words
        emotional_words = pos_words + neg_words
        for word in emotional_words[:5]:
            score = 1.0 if word in pos_words else -1.0
            highlighted_phrases.append({
                "text": word,
                "score": score,
                "index": text.lower().find(word)
            })
        
        return {
            "positive_score": positive_score,
            "negative_score": negative_score,
            "neutral_score": neutral_score,
            "compound_score": compound_score,
            "detected_emotions": detected_emotions,
            "highlighted_phrases": highlighted_phrases,
            "confidence": 0.3,
            "method": "lexical"
        }
    
    def _textblob_sentiment_analysis(self, text: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        TextBlob-based sentiment analysis
        
        Args:
            text: Text to analyze
            context: Contextual information
            
        Returns:
            Dictionary with sentiment analysis results
        """
        if not TEXTBLOB_AVAILABLE:
            raise ImportError("TextBlob is not available")
            
        # Analyze with TextBlob
        blob = TextBlob(text)
        
        # TextBlob polarity is -1 to 1
        polarity = blob.sentiment.polarity
        
        # TextBlob subjectivity is 0 to 1
        subjectivity = blob.sentiment.subjectivity
        
        # Convert to our format
        if polarity > 0:
            positive_score = 0.5 + (polarity / 2)
            negative_score = 0.0
            neutral_score = 0.5 - (polarity / 2)
        elif polarity < 0:
            positive_score = 0.0
            negative_score = 0.5 + (abs(polarity) / 2)
            neutral_score = 0.5 - (abs(polarity) / 2)
        else:
            positive_score = 0.0
            negative_score = 0.0
            neutral_score = 1.0
            
        # Adjust by subjectivity
        if subjectivity < 0.5:
            # More objective (factual) text should be more neutral
            factor = 1.0 - subjectivity
            neutral_score = neutral_score * (1 - factor) + factor
            positive_score *= (1 - factor)
            negative_score *= (1 - factor)
            
        # Normalize scores
        total = positive_score + negative_score + neutral_score
        if total > 0:
            positive_score /= total
            negative_score /= total
            neutral_score /= total
        
        # Detect emotions (TextBlob doesn't do this directly)
        # Use our lexical approach for emotion detection
        tokens = re.findall(r'\b\w+\b', text.lower())
        total_words = len(tokens)
        
        detected_emotions = {}
        for emotion, word_set in self.emotion_lexicons.items():
            emotion_words = [token for token in tokens if token in word_set]
            if emotion_words and total_words > 0:
                detected_emotions[emotion] = len(emotion_words) / total_words
        
        # Highlight key sentences by polarity
        highlighted_phrases = []
        
        # TextBlob can analyze sentiment by sentence
        for i, sentence in enumerate(blob.sentences):
            if abs(sentence.sentiment.polarity) > 0.3:
                highlighted_phrases.append({
                    "text": str(sentence),
                    "score": sentence.sentiment.polarity,
                    "index": text.find(str(sentence))
                })
        
        return {
            "positive_score": positive_score,
            "negative_score": negative_score,
            "neutral_score": neutral_score,
            "compound_score": polarity,
            "detected_emotions": detected_emotions,
            "highlighted_phrases": highlighted_phrases,
            "confidence": 0.5,
            "method": "textblob"
        }
    
    def _neural_sentiment_analysis(self, text: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Neural network-based sentiment analysis
        
        Args:
            text: Text to analyze
            context: Contextual information
            
        Returns:
            Dictionary with sentiment analysis results
        """
        if not TORCH_AVAILABLE or not self.neural_analyzer:
            raise ImportError("PyTorch is not available or neural analyzer not initialized")
            
        # Tokenize text
        tokens = re.findall(r'\b\w+\b', text.lower())
        
        # Convert to indices
        unknown_idx = len(self.vocab)
        indices = [self.vocab.get(token, unknown_idx) for token in tokens]
        
        # Handle empty input
        if not indices:
            indices = [unknown_idx]
            
        # Convert to tensor
        tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(self.device)
        
        # Get predictions
        with torch.no_grad():
            predictions = self.neural_analyzer(tensor).squeeze(0)
            
        # Extract scores
        positive_score = predictions[0].item()
        negative_score = predictions[1].item()
        neutral_score = predictions[2].item()
        
        # Calculate compound score
        compound_score = positive_score - negative_score
        
        # Record activation for tracking purposes
        if hasattr(self, "neural_state") and self.neural_state is not None:
            self.neural_state.add_activation('sentiment', {
                'inputs': len(tokens),
                'positive': positive_score,
                'negative': negative_score,
                'neutral': neutral_score,
                'compound': compound_score
            })
        
        # Detect emotions
        # In a real implementation, this would be done by the neural network
        # Here we'll use a hybrid approach combining neural sentiment with lexical emotion detection
        detected_emotions = {}
        
        # Use our lexical approach for emotion detection
        for emotion, word_set in self.emotion_lexicons.items():
            emotion_words = [token for token in tokens if token in word_set]
            if emotion_words:
                # Weight the emotion by the sentiment scores
                emotion_score = len(emotion_words) / max(1, len(tokens))
                
                # Adjust score based on sentiment alignment
                if emotion in ["joy", "trust", "anticipation"]:
                    emotion_score *= (0.5 + 0.5 * positive_score)
                elif emotion in ["sadness", "anger", "fear", "disgust"]:
                    emotion_score *= (0.5 + 0.5 * negative_score)
                
                detected_emotions[emotion] = emotion_score
        
        # Highlight key phrases
        highlighted_phrases = []
        
        # For simple implementation, split into sentences and analyze each
        sentences = text.split('. ')
        for sentence in sentences:
            if not sentence.strip():
                continue
                
            # Analyze sentiment of sentence
            sentence_tokens = re.findall(r'\b\w+\b', sentence.lower())
            
            # Skip very short sentences
            if len(sentence_tokens) < 3:
                continue
                
            # Count emotional words in sentence
            pos_count = sum(1 for token in sentence_tokens if token in self.positive_words)
            neg_count = sum(1 for token in sentence_tokens if token in self.negative_words)
            
            # Calculate sentence sentiment
            if pos_count > neg_count:
                sentence_score = 0.5 + (pos_count / (2 * len(sentence_tokens)))
            elif neg_count > pos_count:
                sentence_score = -0.5 - (neg_count / (2 * len(sentence_tokens)))
            else:
                sentence_score = 0.0
                
            # Only include sentences with clear sentiment
            if abs(sentence_score) > 0.2:
                highlighted_phrases.append({
                    "text": sentence,
                    "score": sentence_score,
                    "index": text.find(sentence)
                })
        
        return {
            "positive_score": positive_score,
            "negative_score": negative_score,
            "neutral_score": neutral_score,
            "compound_score": compound_score,
            "detected_emotions": detected_emotions,
            "highlighted_phrases": highlighted_phrases,
            "confidence": 0.6,
            "method": "neural"
        }
    
    def _llm_sentiment_analysis(self, text: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        LLM-based sentiment analysis for advanced processing
        
        Args:
            text: Text to analyze
            context: Contextual information
            
        Returns:
            Dictionary with sentiment analysis results
        """
        if not self.llm_client:
            raise ImportError("LLM client is not available")
            
        try:
            # Only use for substantial text
            if len(text) < 20:
                raise ValueError("Text too short for LLM analysis")
                
            # Prepare prompt for structured output
            messages = [
                LLMMessage(role="system", content="""
                You are a sentiment analysis system. Analyze the emotional tone of the provided text and return a JSON object with the following structure:
                {
                    "positive_score": float (0-1),
                    "negative_score": float (0-1),
                    "neutral_score": float (0-1),
                    "compound_score": float (-1 to 1),
                    "detected_emotions": {
                        "emotion1": float (0-1),
                        "emotion2": float (0-1),
                        ...
                    },
                    "highlighted_phrases": [
                        {"text": "phrase 1", "score": float (-1 to 1)},
                        {"text": "phrase 2", "score": float (-1 to 1)},
                        ...
                    ]
                }
                
                Primary emotions to detect: joy, sadness, anger, fear, surprise, disgust, trust, anticipation.
                Ensure all scores sum to 1.0. Only include emotions with significant presence.
                """),
                LLMMessage(role="user", content=f"Analyze the emotional tone and sentiment of this text: \"{text}\"")
            ]
            
            # Get response from LLM
            response = self.llm_client.chat_completion(messages)
            
            # Extract JSON from response
            import json
            import re
            
            # First try to parse the whole response as JSON
            try:
                result = json.loads(response)
            except json.JSONDecodeError:
                # If that fails, try to extract JSON from the text
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group(0))
                else:
                    raise ValueError("Could not extract JSON from LLM response")
            
            # Extract values from result
            positive_score = result.get("positive_score", 0.33)
            negative_score = result.get("negative_score", 0.33)
            neutral_score = result.get("neutral_score", 0.34)
            compound_score = result.get("compound_score", 0.0)
            detected_emotions = result.get("detected_emotions", {})
            highlighted_phrases = result.get("highlighted_phrases", [])
            
            # Ensure all values are in correct ranges
            positive_score = max(0.0, min(1.0, positive_score))
            negative_score = max(0.0, min(1.0, negative_score))
            neutral_score = max(0.0, min(1.0, neutral_score))
            compound_score = max(-1.0, min(1.0, compound_score))
            
            # Normalize scores
            total = positive_score + negative_score + neutral_score
            if total > 0:
                positive_score /= total
                negative_score /= total
                neutral_score /= total
            
            # Ensure detected emotions are in range
            detected_emotions = {
                k: max(0.0, min(1.0, v)) 
                for k, v in detected_emotions.items()
            }
            
            # Process highlighted phrases
            for phrase in highlighted_phrases:
                if "score" in phrase:
                    phrase["score"] = max(-1.0, min(1.0, phrase["score"]))
                if "text" in phrase and "index" not in phrase:
                    phrase["index"] = text.find(phrase["text"])
            
            return {
                "positive_score": positive_score,
                "negative_score": negative_score,
                "neutral_score": neutral_score,
                "compound_score": compound_score,
                "detected_emotions": detected_emotions,
                "highlighted_phrases": highlighted_phrases,
                "confidence": 0.8,
                "method": "llm"
            }
                
        except Exception as e:
            logger.warning(f"LLM-based sentiment analysis failed: {str(e)}")
            # Fallback to textblob analysis
            if TEXTBLOB_AVAILABLE:
                return self._textblob_sentiment_analysis(text, context)
            else:
                return self._lexical_sentiment_analysis(text, context)
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New developmental level
        """
        # Update base module development
        new_level = super().update_development(amount)
        
        # Update neural state development level
        if hasattr(self, "neural_state"):
            self.neural_state.sentiment_development = new_level
            self.neural_state.last_updated = datetime.now()
        
        # Adjust parameters for new development level
        self._adjust_parameters_for_development()
        
        # Initialize neural analyzer if development is high enough
        if new_level >= 0.4 and not self.neural_analyzer and TORCH_AVAILABLE:
            self._initialize_neural_analyzer()
            
        # Initialize LLM client if development is high enough
        if new_level >= 0.6 and not self.llm_client:
            try:
                self.llm_client = LLMClient()
                logger.info("LLM client initialized for advanced sentiment analysis")
            except Exception as e:
                logger.warning(f"Could not initialize LLM client: {e}")
            
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state
        
        Returns:
            Dictionary with current state
        """
        base_state = super().get_state()
        
        # Add sentiment-specific state
        sentiment_state = {
            "params": self.params,
            "analysis_history_length": len(self.analysis_history),
            "last_analysis": self.analysis_history[-1].dict() if self.analysis_history else None,
            "available_methods": {
                "lexical": True,
                "textblob": TEXTBLOB_AVAILABLE,
                "neural": TORCH_AVAILABLE and self.neural_analyzer is not None,
                "llm": self.llm_client is not None
            }
        }
        
        # Combine states
        combined_state = {**base_state, **sentiment_state}
        
        return combined_state 


#######################

#emotion\valence_arousal.py#
#######################

"""
Valence-Arousal Emotional Processing System

This component processes inputs to determine their emotional valence 
(positive to negative) and arousal (level of activation/intensity).
"""

import logging
import time
import uuid
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import deque
import re

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.utils.llm_client import LLMClient, Message as LLMMessage
from lmm_project.modules.emotion.models import EmotionNeuralState

# Initialize logger
logger = logging.getLogger(__name__)

class ValenceArousalNetwork(nn.Module):
    """
    Neural network for extracting valence and arousal from input features
    
    This network gets increasingly sophisticated with development
    """
    def __init__(self, input_dim=10, hidden_dim=20):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.valence_head = nn.Linear(hidden_dim, 1)
        self.arousal_head = nn.Linear(hidden_dim, 1)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        valence = torch.tanh(self.valence_head(x))  # -1 to 1
        arousal = torch.sigmoid(self.arousal_head(x))  # 0 to 1
        return valence, arousal

class ValenceArousalSystem(BaseModule):
    """
    System for processing emotional valence and arousal
    
    This system develops from basic pleasure/pain distinction
    to nuanced emotional dimension processing.
    """
    # Development milestones
    development_milestones = {
        0.0: "Basic pleasure/pain distinction",
        0.2: "Intensity differentiation",
        0.4: "Context-sensitive valence",
        0.6: "Nuanced arousal sensitivity",
        0.8: "Complex emotional dimensionality",
        1.0: "Sophisticated valence-arousal processing"
    }
    
    def __init__(
        self,
        module_id: str,
        event_bus: Optional[EventBus] = None,
        development_level: float = 0.0
    ):
        """
        Initialize the valence-arousal system
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication
            development_level: Initial developmental level
        """
        super().__init__(
            module_id=module_id,
            module_type="valence_arousal",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Initialize lexical resources for emotion detection
        self._initialize_lexical_resources()
        
        # Create neural network for valence-arousal processing
        self.input_dim = 10
        self.hidden_dim = 20
        self.network = ValenceArousalNetwork(self.input_dim, self.hidden_dim)
        
        # Try to use GPU if available
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.network.to(self.device)
        
        # Neural state for tracking activations and development
        self.neural_state = EmotionNeuralState()
        self.neural_state.encoder_development = development_level
        
        # Valence-Arousal history
        self.history = deque(maxlen=100)
        
        # Initialize parameters
        self.params = {
            "default_valence": 0.0,     # Neutral by default
            "default_arousal": 0.2,     # Low arousal by default
            "valence_sensitivity": 0.6, # How sensitive to valence cues
            "arousal_sensitivity": 0.5, # How sensitive to arousal cues
            "context_weight": 0.3,      # How much context affects VA
            "development_factor": development_level,  # Scales with development
        }
        
        # Adjust parameters based on development level
        self._adjust_params_for_development()
        
        # Try to initialize LLM client if needed for advanced processing
        try:
            self.llm_client = LLMClient()
            self.has_llm = True
        except Exception as e:
            logger.warning(f"Could not initialize LLM client: {e}")
            self.has_llm = False
            
        logger.info(f"Valence-Arousal system initialized, development level: {development_level:.2f}")
        
    def _initialize_lexical_resources(self):
        """Initialize lexical resources for emotion detection"""
        # Valence word lists (positive and negative words)
        self.positive_words = {
            "joy", "happy", "glad", "delight", "pleasure", "content", 
            "satisfied", "bliss", "ecstatic", "good", "wonderful", 
            "great", "excellent", "amazing", "fantastic", "terrific",
            "lovely", "beautiful", "nice", "pleasant", "enjoyable"
        }
        
        self.negative_words = {
            "sad", "unhappy", "miserable", "depressed", "gloomy", "somber",
            "melancholy", "sorrow", "grief", "despair", "distress",
            "anger", "angry", "furious", "enraged", "mad", "irritated",
            "fear", "afraid", "scared", "terrified", "anxious", "worried",
            "hate", "dislike", "disgust", "awful", "terrible", "horrible",
            "bad", "unpleasant", "hurt", "painful", "suffering"
        }
        
        # Arousal word lists (high and low activation)
        self.high_arousal_words = {
            "excited", "thrilled", "ecstatic", "energetic", "alert",
            "active", "aroused", "stimulated", "agitated", "frantic",
            "tense", "stressed", "nervous", "restless", "hyper",
            "enraged", "furious", "terrified", "shocked", "overwhelmed",
            "exhilarated", "vibrant", "intense", "passionate", "eager"
        }
        
        self.low_arousal_words = {
            "calm", "relaxed", "serene", "peaceful", "tranquil",
            "quiet", "still", "idle", "passive", "inactive", 
            "tired", "sleepy", "drowsy", "lethargic", "sluggish",
            "dull", "bored", "uninterested", "apathetic", "indifferent",
            "mellow", "soothing", "gentle", "mild", "subtle"
        }
        
        # Intensifiers and diminishers
        self.intensifiers = {
            "very", "extremely", "incredibly", "exceptionally", "tremendously",
            "absolutely", "completely", "totally", "utterly", "highly", 
            "deeply", "profoundly", "intensely", "remarkably", "seriously"
        }
        
        self.diminishers = {
            "slightly", "somewhat", "a bit", "a little", "fairly",
            "rather", "kind of", "sort of", "moderately", "relatively",
            "barely", "hardly", "scarcely", "faintly", "mildly"
        }
        
    def _adjust_params_for_development(self):
        """Adjust parameters based on developmental level"""
        if self.development_level < 0.2:
            # Very basic processing - simple pleasure/pain
            self.params.update({
                "valence_sensitivity": 0.5,
                "arousal_sensitivity": 0.3,
                "context_weight": 0.1,
                "development_factor": self.development_level
            })
        elif self.development_level < 0.4:
            # Developing basic VA sensitivity
            self.params.update({
                "valence_sensitivity": 0.6,
                "arousal_sensitivity": 0.4,
                "context_weight": 0.2,
                "development_factor": self.development_level
            })
        elif self.development_level < 0.6:
            # Developing context sensitivity
            self.params.update({
                "valence_sensitivity": 0.7,
                "arousal_sensitivity": 0.5,
                "context_weight": 0.3,
                "development_factor": self.development_level
            })
        elif self.development_level < 0.8:
            # Developing nuanced processing
            self.params.update({
                "valence_sensitivity": 0.8,
                "arousal_sensitivity": 0.7,
                "context_weight": 0.4,
                "development_factor": self.development_level
            })
        else:
            # Sophisticated processing
            self.params.update({
                "valence_sensitivity": 0.9,
                "arousal_sensitivity": 0.8,
                "context_weight": 0.5,
                "development_factor": self.development_level
            })
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to extract valence and arousal
        
        Args:
            input_data: Input data to process
                Required keys: at least one of 'content' or 'valence'/'arousal'
                Optional keys: 'source', 'context'
                
        Returns:
            Dictionary with valence and arousal results
        """
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Direct valence/arousal values take precedence if provided
        if "valence" in input_data and "arousal" in input_data:
            valence = max(-1.0, min(1.0, input_data["valence"]))
            arousal = max(0.0, min(1.0, input_data["arousal"]))
            
            # Add to history
            self.history.append({
                "valence": valence,
                "arousal": arousal,
                "source": input_data.get("source", "direct"),
                "timestamp": datetime.now().isoformat()
            })
            
            return {
                "valence": valence,
                "arousal": arousal,
                "method": "direct",
                "process_id": process_id,
                "development_level": self.development_level
            }
        
        # Otherwise, extract from content
        content = input_data.get("content", {})
        text = ""
        
        # Extract text from content
        if isinstance(content, str):
            text = content
        elif isinstance(content, dict) and "text" in content:
            text = content["text"]
        
        if not text:
            # No content to process
            return {
                "valence": self.params["default_valence"],
                "arousal": self.params["default_arousal"],
                "method": "default",
                "process_id": process_id,
                "development_level": self.development_level
            }
        
        # Process text to extract VA
        result = self._process_text(text, input_data.get("context", {}))
        result["process_id"] = process_id
        result["development_level"] = self.development_level
        
        # Add to history
        self.history.append({
            "valence": result["valence"],
            "arousal": result["arousal"],
            "source": input_data.get("source", "text"),
            "timestamp": datetime.now().isoformat()
        })
        
        return result
    
    def _process_text(self, text: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process text to extract valence and arousal
        
        Args:
            text: Text to process
            context: Contextual information
            
        Returns:
            Dictionary with valence and arousal results
        """
        # Methods to use based on development level
        methods = []
        
        # Always use lexical approach
        methods.append(self._lexical_va_extraction)
        
        # Add more sophisticated methods with development
        if self.development_level >= 0.3:
            methods.append(self._pattern_va_extraction)
            
        if self.development_level >= 0.6 and self.has_llm:
            methods.append(self._llm_va_extraction)
            
        # Process with each method and combine results
        results = []
        method_names = []
        
        for method in methods:
            try:
                result = method(text, context)
                results.append((result["valence"], result["arousal"]))
                method_names.append(result["method"])
            except Exception as e:
                logger.error(f"Error in VA extraction method {method.__name__}: {str(e)}")
        
        # Calculate weighted average of results
        # More sophisticated methods have higher weights as development increases
        if not results:
            return {
                "valence": self.params["default_valence"],
                "arousal": self.params["default_arousal"],
                "method": "default",
                "confidence": 0.1
            }
        
        # Apply weights based on development and method sophistication
        if len(results) == 1:
            weights = [1.0]
        elif len(results) == 2:
            weights = [0.6, 0.4] if self.development_level < 0.5 else [0.3, 0.7]
        elif len(results) == 3:
            weights = [0.4, 0.3, 0.3] if self.development_level < 0.7 else [0.2, 0.3, 0.5]
        else:
            weights = [1.0 / len(results)] * len(results)
            
        weighted_valence = sum(w * v for w, (v, _) in zip(weights, results))
        weighted_arousal = sum(w * a for w, (_, a) in zip(weights, results))
        
        return {
            "valence": weighted_valence,
            "arousal": weighted_arousal,
            "method": "+".join(method_names),
            "confidence": min(0.3 + self.development_level, 0.9)
        }
    
    def _lexical_va_extraction(self, text: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract valence and arousal using lexical approach
        
        Args:
            text: Text to process
            context: Contextual information
            
        Returns:
            Dictionary with valence and arousal results
        """
        # Basic lexical approach - count positive and negative words
        tokens = re.findall(r'\b\w+\b', text.lower())
        
        # Count valence words
        pos_count = sum(1 for token in tokens if token in self.positive_words)
        neg_count = sum(1 for token in tokens if token in self.negative_words)
        
        # Count arousal words
        high_arousal_count = sum(1 for token in tokens if token in self.high_arousal_words)
        low_arousal_count = sum(1 for token in tokens if token in self.low_arousal_words)
        
        # Count intensifiers and diminishers
        intensifier_count = sum(1 for token in tokens if token in self.intensifiers)
        diminisher_count = sum(1 for token in tokens if token in self.diminishers)
        
        # Calculate valence (-1 to 1)
        if pos_count == 0 and neg_count == 0:
            valence = 0.0  # Neutral
        else:
            valence = (pos_count - neg_count) / (pos_count + neg_count)
            
        # Modify valence based on intensifiers/diminishers
        if valence > 0:
            valence_modifier = 0.2 * intensifier_count - 0.1 * diminisher_count
            valence = min(1.0, valence + valence_modifier * self.params["valence_sensitivity"])
        elif valence < 0:
            valence_modifier = 0.2 * intensifier_count - 0.1 * diminisher_count
            valence = max(-1.0, valence - valence_modifier * self.params["valence_sensitivity"])
            
        # Calculate arousal (0 to 1)
        if high_arousal_count == 0 and low_arousal_count == 0:
            arousal = 0.5  # Moderate
        else:
            arousal = (high_arousal_count) / (high_arousal_count + low_arousal_count + 0.001)
            
        # Modify arousal based on intensifiers/diminishers
        arousal_modifier = 0.2 * intensifier_count - 0.1 * diminisher_count
        arousal = min(1.0, max(0.0, arousal + arousal_modifier * self.params["arousal_sensitivity"]))
        
        # Adjust based on development level - lower development gives more extreme values
        if self.development_level < 0.3:
            # Exaggerate responses at low development
            valence = 0.6 * valence + 0.4 * np.sign(valence) * abs(valence) ** 0.5
            arousal = 0.6 * arousal + 0.4 * (0.2 + 0.8 * arousal ** 0.5)
        
        return {
            "valence": valence,
            "arousal": arousal,
            "method": "lexical",
            "confidence": 0.3 + 0.2 * self.development_level
        }
    
    def _pattern_va_extraction(self, text: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract valence and arousal using pattern-based approach
        
        Args:
            text: Text to process
            context: Contextual information
            
        Returns:
            Dictionary with valence and arousal results
        """
        # Extract features for neural network processing
        features = self._extract_text_features(text)
        
        # Convert to tensor
        features_tensor = torch.tensor(features, dtype=torch.float32).to(self.device)
        
        # Process through network
        with torch.no_grad():
            valence_tensor, arousal_tensor = self.network(features_tensor)
            
        # Convert to scalar values
        valence = valence_tensor.item()
        arousal = arousal_tensor.item()
        
        # Record activation for tracking
        if hasattr(self, "neural_state"):
            self.neural_state.add_activation('encoder', {
                'features': features,
                'valence': valence,
                'arousal': arousal
            })
        
        return {
            "valence": valence,
            "arousal": arousal,
            "method": "pattern",
            "confidence": 0.4 + 0.3 * self.development_level
        }
    
    def _extract_text_features(self, text: str) -> List[float]:
        """
        Extract features from text for neural processing
        
        Args:
            text: Text to extract features from
            
        Returns:
            List of feature values
        """
        # Count total words
        tokens = re.findall(r'\b\w+\b', text.lower())
        word_count = len(tokens)
        
        # Calculate feature values
        features = [
            # 1. Positive word ratio
            sum(1 for token in tokens if token in self.positive_words) / max(1, word_count),
            
            # 2. Negative word ratio
            sum(1 for token in tokens if token in self.negative_words) / max(1, word_count),
            
            # 3. High arousal word ratio
            sum(1 for token in tokens if token in self.high_arousal_words) / max(1, word_count),
            
            # 4. Low arousal word ratio
            sum(1 for token in tokens if token in self.low_arousal_words) / max(1, word_count),
            
            # 5. Intensifier ratio
            sum(1 for token in tokens if token in self.intensifiers) / max(1, word_count),
            
            # 6. Diminisher ratio
            sum(1 for token in tokens if token in self.diminishers) / max(1, word_count),
            
            # 7. Exclamation mark count
            text.count('!') / max(1, len(text) / 50),
            
            # 8. Question mark count
            text.count('?') / max(1, len(text) / 50),
            
            # 9. Capitalization ratio
            sum(1 for c in text if c.isupper()) / max(1, len([c for c in text if c.isalpha()])),
            
            # 10. Average word length
            sum(len(token) for token in tokens) / max(1, word_count)
        ]
        
        return features
    
    def _llm_va_extraction(self, text: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract valence and arousal using LLM
        
        This sophisticated method is only used at higher development levels
        
        Args:
            text: Text to process
            context: Contextual information
            
        Returns:
            Dictionary with valence and arousal results
        """
        try:
            # Only use for non-trivial text
            if len(text) < 10 or not self.has_llm:
                raise ValueError("Text too short or LLM unavailable")
                
            # Prepare prompt
            messages = [
                LLMMessage(role="system", content="""
                You are an emotion analysis system focused on extracting valence and arousal from text.
                - Valence ranges from -1.0 (very negative) to 1.0 (very positive), with 0 being neutral.
                - Arousal ranges from 0.0 (calm/inactive) to 1.0 (excited/agitated).
                Respond ONLY with a JSON object containing valence and arousal values.
                """),
                LLMMessage(role="user", content=f"Analyze the emotional dimensions of this text: \"{text}\"")
            ]
            
            # Get response from LLM (with timeout)
            response = self.llm_client.chat_completion(messages)
            
            # Extract values from response
            # Expecting format like {"valence": 0.5, "arousal": 0.7}
            import json
            
            # First try to parse the whole response as JSON
            try:
                result = json.loads(response)
            except json.JSONDecodeError:
                # If that fails, try to extract JSON from the text
                import re
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group(0))
                else:
                    raise ValueError("Could not extract JSON from LLM response")
            
            # Validate and extract values
            valence = result.get("valence", 0.0)
            arousal = result.get("arousal", 0.5)
            
            # Ensure values are in the correct range
            valence = max(-1.0, min(1.0, valence))
            arousal = max(0.0, min(1.0, arousal))
            
            return {
                "valence": valence,
                "arousal": arousal,
                "method": "llm",
                "confidence": 0.6 + 0.3 * self.development_level
            }
            
        except Exception as e:
            logger.warning(f"LLM-based VA extraction failed: {str(e)}")
            # Fallback to pattern-based approach
            return self._pattern_va_extraction(text, context)
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New developmental level
        """
        # Update base module development
        new_level = super().update_development(amount)
        
        # Adjust parameters for new development level
        self._adjust_params_for_development()
        
        # Update neural state
        if hasattr(self, "neural_state"):
            self.neural_state.encoder_development = new_level
            self.neural_state.last_updated = datetime.now()
        
        # More sophisticated neural network as development progresses
        if new_level > 0.5 and self.hidden_dim < 40:
            # Increase network complexity
            self.hidden_dim = 40
            self.network = ValenceArousalNetwork(self.input_dim, self.hidden_dim)
            self.network.to(self.device)
            logger.info(f"Upgraded VA network complexity at development level {new_level:.2f}")
            
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state
        
        Returns:
            Dictionary with current state
        """
        base_state = super().get_state()
        
        # Add VA-specific state
        va_state = {
            "params": self.params,
            "history_length": len(self.history),
            "last_values": list(self.history)[-1] if self.history else None
        }
        
        # Combine states
        combined_state = {**base_state, **va_state}
        
        return combined_state

#######################

#emotion\__init__.py#
#######################

"""
Emotion Module

This module is responsible for processing, generating, and regulating
emotional responses. It serves as the affective core of the Mind, enabling
emotional experiences, sentiment evaluation, and emotional regulation.
"""

import logging
import uuid
import time
from typing import Dict, List, Any, Optional, Union, Tuple
from datetime import datetime
from collections import deque
import numpy as np
import torch
from torch import nn
import torch.nn.functional as F

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.emotion.valence_arousal import ValenceArousalSystem
from lmm_project.modules.emotion.emotion_classifier import EmotionClassifier
from lmm_project.modules.emotion.sentiment_analyzer import SentimentAnalyzer
from lmm_project.modules.emotion.regulation import EmotionRegulator
from lmm_project.modules.emotion.models import (
    EmotionState, EmotionalResponse, SentimentAnalysis, 
    EmotionalParameters, EmotionNeuralState, EmotionSystemState
)
from lmm_project.modules.emotion.neural_net import (
    EmotionEncoder, EmotionClassifierNetwork, 
    SentimentNetwork, EmotionRegulationNetwork, get_device
)

logger = logging.getLogger(__name__)

def get_module(
    module_id: str = "emotion",
    event_bus: Optional[EventBus] = None,
    development_level: float = 0.0
) -> "EmotionSystem":
    """
    Factory function to create and return an emotion module
    
    This function initializes and returns a complete emotion system with
    valence-arousal tracking, emotion classification, sentiment analysis,
    and emotion regulation capabilities.
    
    Args:
        module_id: Unique identifier for the module
        event_bus: Event bus for communication
        development_level: Initial developmental level for the system
        
    Returns:
        Initialized EmotionSystem
    """
    return EmotionSystem(
        module_id=module_id,
        event_bus=event_bus,
        development_level=development_level
    )

class EmotionSystem(BaseModule):
    """
    Emotion system responsible for affective processing
    
    The emotion system develops from basic pleasure/displeasure responses
    to sophisticated emotional understanding, regulation, and expression.
    """
    # Development milestones
    development_milestones = {
        0.0: "Basic affect reactions",
        0.2: "Primary emotions",
        0.4: "Secondary emotions",
        0.6: "Emotional self-awareness",
        0.8: "Complex emotional understanding",
        1.0: "Sophisticated emotional intelligence"
    }
    
    def __init__(
        self,
        module_id: str,
        event_bus: Optional[EventBus] = None,
        development_level: float = 0.0
    ):
        """
        Initialize the emotion system
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication
            development_level: Initial developmental level
        """
        super().__init__(
            module_id=module_id,
            module_type="emotion_system",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Initialize emotional state
        initial_state = EmotionState(
            valence=0.0,      # Neutral valence initially
            arousal=0.1,      # Low arousal initially
            dominant_emotion="neutral",
            emotion_intensities={
                "neutral": 1.0,
                "joy": 0.0,
                "sadness": 0.0,
                "anger": 0.0,
                "fear": 0.0,
                "surprise": 0.0,
                "disgust": 0.0,
                "anticipation": 0.0,
                "trust": 0.0
            },
            timestamp=datetime.now()
        )
        
        # Initialize emotional parameters
        initial_params = EmotionalParameters(
            emotional_inertia=0.8,    # How quickly emotions change
            stimulus_sensitivity=0.6, # How strongly stimulus affects emotion
            emotion_decay_rate=0.05,  # How quickly emotions decay over time
            baseline_valence=0.1,     # Baseline valence to return to
            baseline_arousal=0.2,     # Baseline arousal to return to
            regulation_capacity=0.2   # Emotional regulation strength
        )
        
        # Initialize the system state
        self.system_state = EmotionSystemState(
            current_state=initial_state,
            parameters=initial_params,
            neural_state=EmotionNeuralState(),
            module_id=module_id,
            developmental_level=development_level
        )
        
        # Keep current_state as a direct reference for backward compatibility
        self.current_state = self.system_state.current_state
        self.emotional_params = {
            "emotional_inertia": self.system_state.parameters.emotional_inertia,
            "stimulus_sensitivity": self.system_state.parameters.stimulus_sensitivity,
            "emotion_decay_rate": self.system_state.parameters.emotion_decay_rate,
            "baseline_valence": self.system_state.parameters.baseline_valence,
            "baseline_arousal": self.system_state.parameters.baseline_arousal,
            "regulation_capacity": self.system_state.parameters.regulation_capacity
        }
        
        # Keep emotion history reference for backward compatibility
        self.emotion_history = deque(maxlen=50)
        self.emotion_history.append(self.current_state)
        
        # Get the device to use (CPU or CUDA)
        self.device = get_device()
        
        # Initialize neural network models
        self.emotion_encoder = EmotionEncoder(
            input_dim=128,
            hidden_dim=256,
            output_dim=32
        ).to(self.device)
        
        self.emotion_classifier_net = EmotionClassifierNetwork(
            input_dim=2,
            hidden_dim=128,
            num_emotions=9  # 8 primary emotions + neutral
        ).to(self.device)
        
        self.sentiment_net = SentimentNetwork(
            vocab_size=10000,
            embedding_dim=64,
            hidden_dim=128,
            output_dim=4
        ).to(self.device)
        
        self.regulation_net = EmotionRegulationNetwork(
            input_dim=10,
            hidden_dim=128,
            output_dim=2
        ).to(self.device)
        
        # Set the development level for neural networks
        self.emotion_encoder.set_development_level(development_level)
        self.emotion_classifier_net.set_development_level(development_level)
        self.sentiment_net.set_development_level(development_level)
        self.regulation_net.set_development_level(development_level)
        
        # Create emotional subsystems (module classes)
        self.valence_arousal_system = ValenceArousalSystem(
            module_id=f"{module_id}_va",
            event_bus=event_bus,
            development_level=development_level
        )
        
        self.emotion_classifier = EmotionClassifier(
            module_id=f"{module_id}_classifier",
            event_bus=event_bus,
            development_level=development_level
        )
        
        self.sentiment_analyzer = SentimentAnalyzer(
            module_id=f"{module_id}_sentiment",
            event_bus=event_bus,
            development_level=development_level
        )
        
        self.emotion_regulator = EmotionRegulator(
            module_id=f"{module_id}_regulation",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Adjust parameters based on development level
        self._adjust_parameters_for_development()
        
        # Subscribe to relevant events
        if self.event_bus:
            self.subscribe_to_message("perception_result")
            self.subscribe_to_message("attention_focus")
            self.subscribe_to_message("memory_retrieval")
            self.subscribe_to_message("emotion_query")
            self.subscribe_to_message("emotion_regulation")
        
        logger.info(f"Emotion system initialized at development level {development_level:.2f}")
    
    def _adjust_parameters_for_development(self):
        """Adjust emotional parameters based on developmental level"""
        if self.development_level < 0.2:
            # Early development - basic emotional reactions
            new_params = {
                "emotional_inertia": 0.4,         # Emotions change quickly
                "stimulus_sensitivity": 0.8,      # Strong reactions to stimuli
                "emotion_decay_rate": 0.1,        # Quick return to baseline
                "baseline_valence": 0.2,          # Slightly positive baseline
                "baseline_arousal": 0.3,          # Moderate arousal baseline
                "regulation_capacity": 0.1        # Very limited regulation
            }
        elif self.development_level < 0.4:
            # Developing primary emotions
            new_params = {
                "emotional_inertia": 0.5,
                "stimulus_sensitivity": 0.7,
                "emotion_decay_rate": 0.08,
                "baseline_valence": 0.15,
                "baseline_arousal": 0.25,
                "regulation_capacity": 0.2
            }
        elif self.development_level < 0.6:
            # Developing secondary emotions
            new_params = {
                "emotional_inertia": 0.6,
                "stimulus_sensitivity": 0.6,
                "emotion_decay_rate": 0.06,
                "baseline_valence": 0.1,
                "baseline_arousal": 0.2,
                "regulation_capacity": 0.4
            }
        elif self.development_level < 0.8:
            # Developing emotional self-awareness
            new_params = {
                "emotional_inertia": 0.7,
                "stimulus_sensitivity": 0.5,
                "emotion_decay_rate": 0.04,
                "baseline_valence": 0.05,
                "baseline_arousal": 0.15,
                "regulation_capacity": 0.6
            }
        else:
            # Advanced emotional intelligence
            new_params = {
                "emotional_inertia": 0.8,
                "stimulus_sensitivity": 0.4,
                "emotion_decay_rate": 0.02,
                "baseline_valence": 0.0,
                "baseline_arousal": 0.1,
                "regulation_capacity": 0.8
            }
            
        # Update both the dictionary (for backwards compatibility) and the model
        self.emotional_params.update(new_params)
        
        # Update the parameters model
        self.system_state.parameters = EmotionalParameters(
            emotional_inertia=new_params["emotional_inertia"],
            stimulus_sensitivity=new_params["stimulus_sensitivity"],
            emotion_decay_rate=new_params["emotion_decay_rate"],
            baseline_valence=new_params["baseline_valence"],
            baseline_arousal=new_params["baseline_arousal"],
            regulation_capacity=new_params["regulation_capacity"]
        )
        
        # Update neural network development levels
        self.emotion_encoder.set_development_level(self.development_level)
        self.emotion_classifier_net.set_development_level(self.development_level)
        self.sentiment_net.set_development_level(self.development_level)
        self.regulation_net.set_development_level(self.development_level)
        
        # Update neural state in system state
        self.system_state.neural_state.encoder_development = self.development_level
        self.system_state.neural_state.classifier_development = self.development_level
        self.system_state.neural_state.sentiment_development = self.development_level
        self.system_state.neural_state.regulation_development = self.development_level
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to generate emotional responses
        
        Args:
            input_data: Data to process for emotional response
                Required keys: 'content'
                Optional keys: 'valence', 'arousal', 'source', 'context'
                
        Returns:
            Dictionary with emotional response
        """
        # Generate ID for this emotion process
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Determine operation type
        operation = input_data.get("operation", "generate")
        
        # Route to appropriate handler
        if operation == "generate":
            return self._handle_generate_emotion(input_data)
        elif operation == "analyze":
            return self._handle_analyze_sentiment(input_data)
        elif operation == "regulate":
            return self._handle_regulate_emotion(input_data)
        elif operation == "query":
            return self._handle_emotion_query(input_data)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "process_id": process_id
            }
    
    def _handle_generate_emotion(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle emotion generation operation"""
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        content = input_data.get("content", {})
        text = content.get("text", "")
        context = input_data.get("context", {})
        
        # Process through valence-arousal system
        va_result = self.valence_arousal_system.process_input(input_data)
        
        # Get valence and arousal
        valence = va_result.get("valence", 0.0)
        arousal = va_result.get("arousal", 0.0)
        
        # If direct values were provided, use those
        if "valence" in input_data:
            valence = input_data["valence"]
        if "arousal" in input_data:
            arousal = input_data["arousal"]
        
        # Classify the emotion based on valence and arousal
        classification_input = {
            "valence": valence,
            "arousal": arousal,
            "text": text,
            "context": context
        }
        classification_result = self.emotion_classifier.process_input(classification_input)
        
        # Get emotional classification
        emotion_intensities = classification_result.get("emotion_intensities", 
                                                     {"neutral": 1.0})
        dominant_emotion = classification_result.get("dominant_emotion", "neutral")
        
        # Update current emotional state with inertia
        inertia = self.emotional_params["emotional_inertia"]
        
        new_valence = (inertia * self.current_state.valence + 
                      (1 - inertia) * valence)
        new_arousal = (inertia * self.current_state.arousal + 
                      (1 - inertia) * arousal)
        
        # Create new emotional state
        new_state = EmotionState(
            valence=new_valence,
            arousal=new_arousal,
            dominant_emotion=dominant_emotion,
            emotion_intensities=emotion_intensities,
            timestamp=datetime.now()
        )
        
        # Apply regulation if development allows
        if self.development_level >= 0.2:
            regulation_input = {
                "current_state": new_state,
                "context": context,
                "regulation_capacity": self.emotional_params["regulation_capacity"]
            }
            regulation_result = self.emotion_regulator.process_input(regulation_input)
            
            # Get regulated state
            if "regulated_state" in regulation_result:
                new_state = regulation_result["regulated_state"]
        
        # Update the current state
        self.current_state = new_state
        self.emotion_history.append(new_state)
        
        # Create emotional response
        response = EmotionalResponse(
            valence=new_state.valence,
            arousal=new_state.arousal,
            dominant_emotion=new_state.dominant_emotion,
            emotion_intensities=new_state.emotion_intensities,
            regulated=self.development_level >= 0.2,
            stimulus=text,
            process_id=process_id,
            timestamp=datetime.now()
        )
        
        # Prepare result dictionary
        result = {
            "status": "success",
            "process_id": process_id,
            "response": response.dict(),
            "development_level": self.development_level
        }
        
        # Publish emotional state update
        if self.event_bus:
            self.publish_message(
                "emotion_state",
                {
                    "state": new_state.dict(),
                    "process_id": process_id
                }
            )
        
        return result
    
    def _handle_analyze_sentiment(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle sentiment analysis operation"""
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Process through sentiment analyzer
        sentiment_result = self.sentiment_analyzer.process_input(input_data)
        
        return {
            "status": "success",
            "process_id": process_id,
            "analysis": sentiment_result,
            "development_level": self.development_level
        }
    
    def _handle_regulate_emotion(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle emotion regulation operation"""
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Only handle if sufficiently developed
        if self.development_level < 0.2:
            return {
                "status": "undeveloped",
                "message": "Emotion regulation not yet developed",
                "process_id": process_id,
                "development_level": self.development_level
            }
        
        # Extract regulation parameters
        current_state = input_data.get("current_state")
        if current_state is None:
            # Use current emotional state if none provided
            current_state = self.current_state
            
        # Process through emotion regulator
        regulation_input = {
            "current_state": current_state,
            "process_id": process_id,
            "regulation_capacity": self.emotional_params["regulation_capacity"]
        }
        
        # Add target valence and arousal if provided
        if "target_valence" in input_data:
            regulation_input["target_valence"] = input_data["target_valence"]
            
        if "target_arousal" in input_data:
            regulation_input["target_arousal"] = input_data["target_arousal"]
            
        if "regulation_strategy" in input_data:
            regulation_input["regulation_strategy"] = input_data["regulation_strategy"]
            
        # Process the regulation request
        regulation_result = self.emotion_regulator.process_input(regulation_input)
        
        # Update current state if regulation was applied and using system's emotional state
        if current_state == self.current_state and "regulated_state" in regulation_result:
            self.current_state = regulation_result["regulated_state"]
            self.emotion_history.append(self.current_state)
            
            # Publish updated state
            if self.event_bus:
                self.publish_message(
                    "emotion_state",
                    {
                        "state": self.current_state.dict(),
                        "process_id": process_id,
                        "regulated": True
                    }
                )
        
        # Return the regulation result directly without extra nesting
        return regulation_result
    
    def _handle_emotion_query(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle query about current emotional state"""
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        return {
            "status": "success",
            "process_id": process_id,
            "current_state": self.current_state.dict(),
            "development_level": self.development_level,
            "emotional_capacity": self._get_emotional_capacity()
        }
    
    def _get_emotional_capacity(self) -> Dict[str, Any]:
        """Get information about current emotional capabilities"""
        if self.development_level < 0.2:
            return {
                "available_emotions": ["pleasure", "displeasure"],
                "regulation_capacity": self.emotional_params["regulation_capacity"],
                "emotional_complexity": "basic",
                "self_awareness": "none"
            }
        elif self.development_level < 0.4:
            return {
                "available_emotions": ["joy", "sadness", "anger", "fear"],
                "regulation_capacity": self.emotional_params["regulation_capacity"],
                "emotional_complexity": "primary",
                "self_awareness": "minimal"
            }
        elif self.development_level < 0.6:
            return {
                "available_emotions": [
                    "joy", "sadness", "anger", "fear", 
                    "surprise", "disgust", "anticipation", "trust"
                ],
                "regulation_capacity": self.emotional_params["regulation_capacity"],
                "emotional_complexity": "secondary",
                "self_awareness": "developing"
            }
        elif self.development_level < 0.8:
            return {
                "available_emotions": [
                    "joy", "sadness", "anger", "fear", 
                    "surprise", "disgust", "anticipation", "trust",
                    "shame", "guilt", "pride", "love", "jealousy"
                ],
                "regulation_capacity": self.emotional_params["regulation_capacity"],
                "emotional_complexity": "complex",
                "self_awareness": "substantial"
            }
        else:
            return {
                "available_emotions": [
                    "joy", "sadness", "anger", "fear", 
                    "surprise", "disgust", "anticipation", "trust",
                    "shame", "guilt", "pride", "love", "jealousy",
                    "gratitude", "awe", "contentment", "interest",
                    "contempt", "embarrassment", "longing"
                ],
                "regulation_capacity": self.emotional_params["regulation_capacity"],
                "emotional_complexity": "nuanced",
                "self_awareness": "sophisticated"
            }
    
    def _handle_message(self, message: Message):
        """Handle messages from the event bus"""
        if message.message_type == "perception_result":
            self._handle_perception_message(message)
        elif message.message_type == "attention_focus":
            self._handle_attention_message(message)
        elif message.message_type == "memory_retrieval":
            self._handle_memory_message(message)
        elif message.message_type == "emotion_query":
            self._handle_query_message(message)
        elif message.message_type == "emotion_regulation":
            self._handle_regulation_message(message)
    
    def _handle_perception_message(self, message: Message):
        """Process perception results to generate emotional responses"""
        content = message.content
        if "result" not in content:
            return
            
        result = content["result"]
        
        # Process text for emotional content
        if "text" in result:
            input_data = {
                "content": {"text": result["text"]},
                "process_id": content.get("process_id", str(uuid.uuid4())),
                "source": "perception"
            }
            emotion_result = self._handle_generate_emotion(input_data)
    
    def _handle_attention_message(self, message: Message):
        """Process attention focus to modulate emotional responses"""
        content = message.content
        if "focus" not in content:
            return
            
        focus = content["focus"]
        
        # Attention amplifies emotional response to focused content
        if "content" in focus:
            # Extract text if present
            text = ""
            if isinstance(focus["content"], dict) and "text" in focus["content"]:
                text = focus["content"]["text"]
            elif isinstance(focus["content"], str):
                text = focus["content"]
                
            if text:
                # Amplify emotional response to attended content
                input_data = {
                    "content": {"text": text},
                    "process_id": content.get("process_id", str(uuid.uuid4())),
                    "source": "attention",
                    # Boost sensitivity for attended content
                    "sensitivity_boost": 0.3
                }
                self._handle_generate_emotion(input_data)
    
    def _handle_memory_message(self, message: Message):
        """Process memory retrievals to generate emotional responses"""
        content = message.content
        if "memory" not in content:
            return
            
        memory = content["memory"]
        
        # Extract emotional aspects from memory
        if "emotional_valence" in memory:
            # Direct emotional content in memory
            input_data = {
                "valence": memory.get("emotional_valence", 0.0),
                "arousal": memory.get("emotional_arousal", 0.3),
                "process_id": content.get("process_id", str(uuid.uuid4())),
                "source": "memory",
                # Memories have reduced emotional impact
                "intensity": 0.7
            }
            self._handle_generate_emotion(input_data)
        elif "content" in memory:
            # Process content for emotional aspects
            text = ""
            if isinstance(memory["content"], dict) and "text" in memory["content"]:
                text = memory["content"]["text"]
            elif isinstance(memory["content"], str):
                text = memory["content"]
                
            if text:
                input_data = {
                    "content": {"text": text},
                    "process_id": content.get("process_id", str(uuid.uuid4())),
                    "source": "memory",
                    # Memories have reduced emotional impact
                    "intensity": 0.7
                }
                self._handle_generate_emotion(input_data)
    
    def _handle_query_message(self, message: Message):
        """Handle queries about emotional state"""
        content = message.content
        query_type = content.get("query_type", "current_state")
        
        response_data = None
        if query_type == "current_state":
            response_data = self._handle_emotion_query(content)
        elif query_type == "emotional_capacity":
            response_data = {
                "status": "success",
                "emotional_capacity": self._get_emotional_capacity(),
                "development_level": self.development_level
            }
        elif query_type == "emotion_history":
            count = content.get("count", 5)
            history = list(self.emotion_history)[-count:]
            response_data = {
                "status": "success",
                "history": [state.dict() for state in history],
                "count": len(history)
            }
            
        # Publish response if we have event bus
        if response_data and self.event_bus:
            self.publish_message(
                "emotion_query_response",
                {
                    "query_id": content.get("query_id", ""),
                    "response": response_data
                }
            )
    
    def _handle_regulation_message(self, message: Message):
        """Handle emotion regulation requests"""
        content = message.content
        
        # Only process if sufficiently developed
        if self.development_level < 0.3:
            # Not yet developed enough for regulation
            if self.event_bus:
                self.publish_message(
                    "emotion_regulation_response",
                    {
                        "regulation_id": content.get("regulation_id", ""),
                        "status": "undeveloped",
                        "message": "Emotion regulation not yet developed"
                    }
                )
            return
        
        # Process regulation request
        regulation_result = self._handle_regulate_emotion(content)
        
        # Publish response
        if self.event_bus:
            self.publish_message(
                "emotion_regulation_response",
                {
                    "regulation_id": content.get("regulation_id", ""),
                    "response": regulation_result
                }
            )
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of the emotion system
        
        Args:
            amount: Amount to increase development (0.0 to 1.0)
            
        Returns:
            New developmental level
        """
        # Update base module development
        new_level = super().update_development(amount)
        
        # Update submodules development
        self.valence_arousal_system.update_development(amount)
        self.emotion_classifier.update_development(amount)
        self.sentiment_analyzer.update_development(amount)
        self.emotion_regulator.update_development(amount)
        
        # Update system state
        self.system_state.developmental_level = new_level
        
        # Update neural networks
        self.emotion_encoder.set_development_level(new_level)
        self.emotion_classifier_net.set_development_level(new_level)
        self.sentiment_net.set_development_level(new_level) 
        self.regulation_net.set_development_level(new_level)
        
        # Update neural state in system state
        self.system_state.neural_state.encoder_development = new_level
        self.system_state.neural_state.classifier_development = new_level
        self.system_state.neural_state.sentiment_development = new_level
        self.system_state.neural_state.regulation_development = new_level
        
        # Adjust parameters for new development level
        self._adjust_parameters_for_development()
        
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the emotion system
        
        Returns:
            Dictionary with current state
        """
        # Get the base state from parent
        base_state = super().get_state()
        
        # Update the system state with current information
        self.system_state.current_state = self.current_state
        self.system_state.developmental_level = self.development_level
        self.system_state.last_updated = datetime.now()
        
        # Convert the system state to a dictionary
        system_state_dict = self.system_state.dict()
        
        # For backwards compatibility, also include these directly
        emotion_state = {
            "current_emotion": self.current_state.dict(),
            "emotion_history_length": len(self.emotion_history),
            "emotional_params": self.emotional_params,
            "emotional_capacity": self._get_emotional_capacity()
        }
        
        # Combine all states
        combined_state = {**base_state, **emotion_state, "system_state": system_state_dict}
        
        return combined_state


#######################

#executive\decision_making.py#
#######################

# TODO: Implement the DecisionMaking class to evaluate options and make choices
# This component should be able to:
# - Evaluate multiple options based on various criteria
# - Calculate expected outcomes and utilities
# - Manage risk and uncertainty in decisions
# - Balance short-term and long-term consequences

# TODO: Implement developmental progression in decision making:
# - Simple immediate-reward decisions in early stages
# - Growing consideration of multiple factors in childhood
# - Inclusion of long-term outcomes in adolescence
# - Complex trade-off analysis in adulthood

# TODO: Create mechanisms for:
# - Option generation: Identify possible choices
# - Value assignment: Determine the worth of potential outcomes
# - Probability estimation: Assess likelihood of outcomes
# - Outcome integration: Combine multiple factors into decisions

# TODO: Implement different decision strategies:
# - Maximizing: Select the option with highest expected utility
# - Satisficing: Select first option meeting minimum criteria
# - Elimination by aspects: Sequentially remove options failing criteria
# - Recognition-primed: Use past experience to make rapid decisions

# TODO: Connect to emotion and memory systems
# Decision making should be influenced by emotional responses
# and informed by memories of past decisions and outcomes

import logging
import time
import uuid
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import numpy as np
import torch
from collections import deque

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.executive.models import Decision, ExecutiveNeuralState
from lmm_project.modules.executive.neural_net import DecisionNetwork, get_device

# Initialize logger
logger = logging.getLogger(__name__)

class DecisionMaking(BaseModule):
    """
    Evaluates options and makes choices
    
    This module weighs alternatives and selects actions based on
    expected outcomes, values, and contextual factors.
    """
    
    # Development milestones
    development_milestones = {
        0.0: "Simple reward-based decisions",
        0.2: "Multi-factor decisions",
        0.4: "Short-term risk assessment",
        0.6: "Long-term outcome consideration",
        0.8: "Complex trade-off analysis",
        1.0: "Strategic decision-making"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the decision making module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level of this module
        """
        super().__init__(
            module_id=module_id, 
            module_type="decision_making", 
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Initialize device
        self.device = get_device()
        
        # Initialize neural network
        self.decision_network = DecisionNetwork(
            option_dim=64,
            criteria_dim=16,
            hidden_dim=128
        ).to(self.device)
        
        # Set development level for network
        self.decision_network.set_development_level(development_level)
        
        # Create neural state for tracking
        self.neural_state = ExecutiveNeuralState()
        self.neural_state.decision_development = development_level
        
        # Recent decisions history
        self.decision_history = deque(maxlen=20)
        
        # Decision making parameters
        self.params = {
            "max_options": 3,  # Maximum number of options to consider
            "max_criteria": 2,  # Maximum number of criteria to consider
            "time_allocation": 0.5,  # Relative time allocation (0-1)
            "risk_aversion": 0.5,  # Risk aversion level (0-1)
            "consider_long_term": False,  # Whether to consider long-term outcomes
            "confidence_threshold": 0.6  # Threshold for high-confidence decisions
        }
        
        # Update parameters based on development
        self._adjust_parameters_for_development()
        
        logger.info(f"Decision making module initialized at development level {development_level:.2f}")
    
    def _adjust_parameters_for_development(self):
        """Adjust decision making parameters based on developmental level"""
        if self.development_level < 0.2:
            # Simple decision making at early stages
            self.params.update({
                "max_options": 2,
                "max_criteria": 1,
                "time_allocation": 0.3,
                "risk_aversion": 0.7,  # High risk aversion (conservative)
                "consider_long_term": False,
                "confidence_threshold": 0.7  # Require high confidence
            })
        elif self.development_level < 0.4:
            # Multi-factor decisions
            self.params.update({
                "max_options": 3,
                "max_criteria": 2,
                "time_allocation": 0.4,
                "risk_aversion": 0.6,
                "consider_long_term": False,
                "confidence_threshold": 0.65
            })
        elif self.development_level < 0.6:
            # Short-term risk assessment
            self.params.update({
                "max_options": 4,
                "max_criteria": 3,
                "time_allocation": 0.5,
                "risk_aversion": 0.5,
                "consider_long_term": False,
                "confidence_threshold": 0.6
            })
        elif self.development_level < 0.8:
            # Long-term outcome consideration
            self.params.update({
                "max_options": 5,
                "max_criteria": 4,
                "time_allocation": 0.6,
                "risk_aversion": 0.4,
                "consider_long_term": True,
                "confidence_threshold": 0.55
            })
        else:
            # Strategic decision-making
            self.params.update({
                "max_options": 7,
                "max_criteria": 5,
                "time_allocation": 0.7,
                "risk_aversion": 0.3,
                "consider_long_term": True,
                "confidence_threshold": 0.5
            })
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to make decisions
        
        Args:
            input_data: Dictionary containing decision problem information
                Required keys:
                - 'options': Dict[str, Dict[str, Any]] - Options to choose from with their attributes
                - 'criteria': Dict[str, float] - Decision criteria and their weights
                Optional keys:
                - 'context': Dict[str, Any] - Contextual information
                - 'time_allocation': float - How much time to allocate (overrides default)
                - 'deadline': float - Time by which decision is needed
                - 'operation': str - specific operation ('decide', 'explain', 'revise')
            
        Returns:
            Dictionary with the results of decision making
        """
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        operation = input_data.get("operation", "decide")
        
        # Different operations based on the request
        if operation == "decide":
            return self._make_decision(input_data, process_id)
        elif operation == "explain":
            return self._explain_decision(input_data, process_id)
        elif operation == "revise":
            return self._revise_decision(input_data, process_id)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "process_id": process_id
            }
    
    def _make_decision(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Make a decision based on options and criteria"""
        # Extract required data
        if "options" not in input_data:
            return {"status": "error", "message": "No options provided", "process_id": process_id}
        if "criteria" not in input_data:
            return {"status": "error", "message": "No criteria provided", "process_id": process_id}
        
        options = input_data.get("options", {})
        criteria = input_data.get("criteria", {})
        context = input_data.get("context", {})
        decision_context = input_data.get("decision_context", "General decision")
        
        # Apply development-based limits
        options_limited = dict(list(options.items())[:self.params["max_options"]])
        criteria_limited = dict(list(criteria.items())[:self.params["max_criteria"]])
        
        # Record decision start time
        start_time = time.time()
        
        # Format options and criteria for neural processing
        options_list = list(options_limited.keys())
        option_features = self._extract_option_features(options_limited)
        criteria_features = self._extract_criteria_features(criteria_limited)
        context_features = self._extract_context_features(context) if context else None
        
        # Process through neural network
        with torch.no_grad():
            decision_result = self.decision_network(
                options=option_features.to(self.device),
                criteria=criteria_features.to(self.device),
                context=context_features.to(self.device) if context_features is not None else None
            )
        
        # Extract results
        scores = decision_result["scores"].cpu().numpy()[0]
        probabilities = decision_result["probabilities"].cpu().numpy()[0]
        confidence = decision_result["confidence"].cpu().item()
        best_option_idx = decision_result["best_option_idx"].cpu().item()
        
        # Map back to option names
        option_scores = {opt: float(scores[i]) for i, opt in enumerate(options_list)}
        
        # Record activation in neural state
        self.neural_state.add_activation('decision', {
            'options_count': len(options_limited),
            'criteria_count': len(criteria_limited),
            'confidence': confidence,
            'best_option_idx': best_option_idx
        })
        
        # Select best option
        selected_option = options_list[best_option_idx]
        
        # Calculate decision time
        decision_time = time.time() - start_time
        
        # Create evaluations dictionary (how each option scores on each criterion)
        evaluations = {}
        for option_name in options_list:
            option_eval = {}
            for criterion_name, weight in criteria_limited.items():
                # In a real implementation, this would use actual evaluation logic
                # Here we generate plausible values based on option and criterion
                option_attrs = options_limited[option_name]
                if criterion_name in option_attrs:
                    option_eval[criterion_name] = option_attrs[criterion_name]
                else:
                    # Generate a value that's consistent with the final scores
                    base_score = option_scores[option_name] / len(criteria_limited)
                    # Add some noise to make it realistic
                    noise = np.random.normal(0, 0.1)
                    option_eval[criterion_name] = max(0.0, min(1.0, base_score + noise))
            
            evaluations[option_name] = option_eval
        
        # Create decision record
        decision = Decision(
            decision_id=str(uuid.uuid4()),
            context=decision_context,
            options=options_limited,
            criteria=criteria_limited,
            evaluations=evaluations,
            option_scores=option_scores,
            selected_option=selected_option,
            confidence=confidence,
            decision_time=decision_time
        )
        
        # Add to history
        self.decision_history.append(decision)
        
        # Return decision result
        return {
            "status": "success",
            "decision": decision.dict(),
            "process_id": process_id,
            "development_level": self.development_level
        }
    
    def _explain_decision(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Explain the reasoning behind a decision"""
        # Extract decision ID
        decision_id = input_data.get("decision_id")
        if not decision_id:
            return {"status": "error", "message": "No decision ID provided", "process_id": process_id}
        
        # Find the decision in history
        decision = next((d for d in self.decision_history if d.decision_id == decision_id), None)
        if not decision:
            return {"status": "error", "message": "Decision not found", "process_id": process_id}
        
        # Generate explanation based on development level
        if self.development_level < 0.3:
            # Simple explanation
            explanation = f"Selected {decision.selected_option} because it scored highest."
            factors = [f"{decision.selected_option} had a score of {decision.option_scores[decision.selected_option]:.2f}"]
            
        elif self.development_level < 0.6:
            # More detailed explanation with criteria
            explanation = f"Selected {decision.selected_option} based on evaluation of criteria."
            
            # Find strongest criteria for selected option
            option_evals = decision.evaluations[decision.selected_option]
            top_criteria = sorted(option_evals.items(), key=lambda x: x[1], reverse=True)
            
            factors = [
                f"{criterion}: {score:.2f}" 
                for criterion, score in top_criteria
            ]
            
        else:
            # Comprehensive explanation with comparison
            explanation = f"Selected {decision.selected_option} after comprehensive analysis of all options and criteria."
            
            # Compare selected option with runners-up
            sorted_options = sorted(decision.option_scores.items(), key=lambda x: x[1], reverse=True)
            top_options = sorted_options[:min(3, len(sorted_options))]
            
            # Find distinguishing criteria
            factors = []
            if len(top_options) > 1:
                selected = decision.selected_option
                runner_up = top_options[1][0]
                
                factors.append(f"{selected} scored {decision.option_scores[selected]:.2f} overall, compared to {runner_up}'s {decision.option_scores[runner_up]:.2f}")
                
                # Compare on specific criteria
                for criterion, weight in decision.criteria.items():
                    selected_score = decision.evaluations[selected].get(criterion, 0)
                    runner_up_score = decision.evaluations[runner_up].get(criterion, 0)
                    
                    if abs(selected_score - runner_up_score) > 0.1:
                        factors.append(f"On {criterion} (weight: {weight:.2f}), {selected} scored {selected_score:.2f} vs. {runner_up}'s {runner_up_score:.2f}")
            
            # Add confidence information
            factors.append(f"Decision confidence: {decision.confidence:.2f}")
        
        return {
            "status": "success",
            "decision_id": decision_id,
            "explanation": explanation,
            "factors": factors,
            "decision": decision.dict(),
            "process_id": process_id
        }
    
    def _revise_decision(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Revise a previous decision with new information"""
        # Extract decision ID
        decision_id = input_data.get("decision_id")
        if not decision_id:
            return {"status": "error", "message": "No decision ID provided", "process_id": process_id}
        
        # Find the decision in history
        original_decision = next((d for d in self.decision_history if d.decision_id == decision_id), None)
        if not original_decision:
            return {"status": "error", "message": "Decision not found", "process_id": process_id}
        
        # Get updates to options, criteria, or context
        updated_options = input_data.get("options", original_decision.options)
        updated_criteria = input_data.get("criteria", original_decision.criteria)
        updated_context = input_data.get("context", {})
        
        # Create new input data with updates
        new_input = {
            "options": updated_options,
            "criteria": updated_criteria,
            "context": updated_context,
            "decision_context": f"Revision of: {original_decision.context}",
            "process_id": process_id
        }
        
        # Make a new decision
        new_decision_result = self._make_decision(new_input, process_id)
        
        # Add revision information
        if new_decision_result["status"] == "success":
            revision_info = {
                "original_decision_id": decision_id,
                "changed": new_decision_result["decision"]["selected_option"] != original_decision.selected_option,
                "original_option": original_decision.selected_option
            }
            new_decision_result["revision_info"] = revision_info
        
        return new_decision_result
    
    def _extract_option_features(self, options: Dict[str, Dict[str, Any]]) -> torch.Tensor:
        """
        Extract features from options for neural processing
        
        Args:
            options: Dictionary of options with their attributes
            
        Returns:
            Tensor of options features [1, num_options, feature_dim]
        """
        num_options = len(options)
        feature_dim = 64  # Must match network's option_dim
        
        # Initialize feature tensor
        features = np.zeros((1, num_options, feature_dim))
        
        # For each option, extract features
        for i, (option_name, option_attrs) in enumerate(options.items()):
            if i >= num_options:
                break
                
            # For demonstration, create simple features
            # In a real implementation, this would do proper feature extraction
            
            # Create feature vector based on option attributes
            if option_attrs:
                # Use attribute values as features where possible
                attr_values = []
                for attr, value in option_attrs.items():
                    if isinstance(value, (int, float)):
                        attr_values.append(value)
                    elif isinstance(value, bool):
                        attr_values.append(1.0 if value else 0.0)
                
                # Fill initial positions with actual values
                for j, val in enumerate(attr_values):
                    if j < feature_dim:
                        features[0, i, j] = val
            
            # Fill remaining with random values seeded by option name
            seed = hash(option_name) % 10000
            np.random.seed(seed)
            
            # Calculate how many positions remain to be filled
            remaining = feature_dim - min(feature_dim, len(option_attrs))
            if remaining > 0:
                random_features = np.random.randn(remaining)
                features[0, i, -remaining:] = random_features
        
        return torch.tensor(features, dtype=torch.float32)
    
    def _extract_criteria_features(self, criteria: Dict[str, float]) -> torch.Tensor:
        """
        Extract features from criteria for neural processing
        
        Args:
            criteria: Dictionary of criteria with their weights
            
        Returns:
            Tensor of criteria features [1, feature_dim]
        """
        feature_dim = 16  # Must match network's criteria_dim
        
        # Initialize feature tensor
        features = np.zeros((1, feature_dim))
        
        # Use criteria weights as features where possible
        for i, (criterion, weight) in enumerate(criteria.items()):
            if i < feature_dim:
                features[0, i] = weight
        
        # Fill remainder with values derived from criteria names
        criteria_names = list(criteria.keys())
        if criteria_names:
            seed = hash("".join(criteria_names)) % 10000
            np.random.seed(seed)
            
            # Calculate how many positions remain to be filled
            remaining = feature_dim - min(feature_dim, len(criteria))
            if remaining > 0:
                random_features = np.random.randn(remaining)
                features[0, -remaining:] = random_features
        
        return torch.tensor(features, dtype=torch.float32)
    
    def _extract_context_features(self, context: Dict[str, Any]) -> torch.Tensor:
        """
        Extract features from context for neural processing
        
        Args:
            context: Dictionary of contextual information
            
        Returns:
            Tensor of context features [1, feature_dim]
        """
        feature_dim = 64  # Must match network's option_dim for the context encoder
        
        # Initialize feature tensor
        features = np.zeros((1, feature_dim))
        
        if context:
            # Use context values as features where possible
            context_values = []
            for key, value in context.items():
                if isinstance(value, (int, float)):
                    context_values.append(value)
                elif isinstance(value, bool):
                    context_values.append(1.0 if value else 0.0)
            
            # Fill initial positions with actual values
            for i, val in enumerate(context_values):
                if i < feature_dim:
                    features[0, i] = val
            
            # Fill remainder with values derived from context
            seed = hash(str(sorted(context.items()))) % 10000
            np.random.seed(seed)
            
            # Calculate how many positions remain to be filled
            remaining = feature_dim - min(feature_dim, len(context_values))
            if remaining > 0:
                random_features = np.random.randn(remaining)
                features[0, -remaining:] = random_features
        
        return torch.tensor(features, dtype=torch.float32)
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # Update base development level
        new_level = super().update_development(amount)
        
        # Update network development level
        self.decision_network.set_development_level(new_level)
        
        # Update neural state
        self.neural_state.decision_development = new_level
        self.neural_state.last_updated = datetime.now()
        
        # Adjust parameters based on new development level
        self._adjust_parameters_for_development()
        
        logger.info(f"Decision making module development updated to {new_level:.2f}")
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the module
        
        Returns:
            Dictionary containing current module state
        """
        # Get base state from parent
        base_state = super().get_state()
        
        # Add decision-specific state
        decision_state = {
            "params": self.params,
            "recent_decisions": [d.dict() for d in self.decision_history],
            "decision_count": len(self.decision_history)
        }
        
        # Add neural state
        neural_state = {
            "development_level": self.neural_state.decision_development,
            "accuracy": self.neural_state.decision_accuracy,
            "recent_activations_count": len(self.neural_state.recent_decision_activations)
        }
        
        # Combine states
        combined_state = {**base_state, **decision_state, **neural_state}
        
        return combined_state


#######################

#executive\inhibition.py#
#######################

# TODO: Implement the Inhibition class to suppress inappropriate actions and thoughts
# This component should be able to:
# - Block prepotent but inappropriate responses
# - Filter out irrelevant or distracting information
# - Delay gratification for better long-term outcomes
# - Maintain focus despite competing demands

# TODO: Implement developmental progression in inhibition:
# - Minimal inhibitory control in early stages
# - Growing ability to delay responses in childhood
# - Improved resistance to distractions in adolescence
# - Sophisticated self-control in adulthood

# TODO: Create mechanisms for:
# - Response inhibition: Stop inappropriate actions
# - Interference control: Resist distractions
# - Delayed gratification: Wait for better rewards
# - Thought suppression: Control unwanted thoughts

# TODO: Implement resource modeling for inhibition:
# - Limited inhibitory resources that can be depleted
# - Recovery of inhibitory capacity over time
# - Factors affecting inhibitory strength (motivation, stress)
# - Individual differences in inhibitory capacity

# TODO: Connect to attention and emotion systems
# Inhibition should interact with attention for filtering
# and with emotion for emotional regulation

import logging
import time
import uuid
from typing import Dict, List, Any, Optional
from datetime import datetime
import numpy as np
import torch
from collections import deque

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.executive.models import InhibitionEvent, InhibitionState, ExecutiveNeuralState
from lmm_project.modules.executive.neural_net import InhibitionNetwork, get_device

# Initialize logger
logger = logging.getLogger(__name__)

class Inhibition(BaseModule):
    """
    Suppresses inappropriate actions and thoughts
    
    This module provides control over behavior and cognition,
    blocking impulses and filtering information as needed.
    """
    
    # Development milestones
    development_milestones = {
        0.0: "Basic impulse control",
        0.2: "Simple distraction resistance",
        0.4: "Response inhibition",
        0.6: "Improved interference control",
        0.8: "Self-regulation strategies",
        1.0: "Sophisticated inhibitory control"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the inhibition module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level of this module
        """
        super().__init__(
            module_id=module_id, 
            module_type="inhibition", 
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Initialize device
        self.device = get_device()
        
        # Initialize neural network
        self.inhibition_network = InhibitionNetwork(
            input_dim=128,
            hidden_dim=128
        ).to(self.device)
        
        # Set development level for network
        self.inhibition_network.set_development_level(development_level)
        
        # Create neural state for tracking
        self.neural_state = ExecutiveNeuralState()
        self.neural_state.inhibition_development = development_level
        
        # Initialize inhibition state
        self.inhibition_state = InhibitionState(
            available_resources=1.0,
            recovery_rate=0.1,
            threshold_adjustments={},
            last_updated=datetime.now()
        )
        
        # Last resource update timestamp
        self.last_resource_update = time.time()
        
        # Inhibition parameters
        self.params = {
            "base_inhibition_threshold": 0.5,  # Base threshold for inhibiting
            "resource_depletion_rate": 0.2,  # How quickly resources deplete
            "resource_recovery_rate": 0.1,  # How quickly resources recover
            "context_sensitivity": 0.3,  # How much context affects threshold
            "max_simultaneous_inhibitions": 1,  # Maximum number of simultaneous inhibitions
            "inhibition_duration": 5.0  # How long inhibition effect lasts (seconds)
        }
        
        # Update parameters based on development
        self._adjust_parameters_for_development()
        
        logger.info(f"Inhibition module initialized at development level {development_level:.2f}")
    
    def _adjust_parameters_for_development(self):
        """Adjust inhibition parameters based on developmental level"""
        if self.development_level < 0.2:
            # Very basic inhibition at early stages
            self.params.update({
                "base_inhibition_threshold": 0.7,  # Higher threshold (less inhibition)
                "resource_depletion_rate": 0.3,  # Faster depletion
                "resource_recovery_rate": 0.05,  # Slower recovery
                "context_sensitivity": 0.1,  # Low context sensitivity
                "max_simultaneous_inhibitions": 1,
                "inhibition_duration": 3.0
            })
            
            # Update state parameters
            self.inhibition_state.recovery_rate = self.params["resource_recovery_rate"]
            
        elif self.development_level < 0.4:
            # Developing basic inhibition
            self.params.update({
                "base_inhibition_threshold": 0.6,
                "resource_depletion_rate": 0.25,
                "resource_recovery_rate": 0.07,
                "context_sensitivity": 0.2,
                "max_simultaneous_inhibitions": 1,
                "inhibition_duration": 4.0
            })
            
            # Update state parameters
            self.inhibition_state.recovery_rate = self.params["resource_recovery_rate"]
            
        elif self.development_level < 0.6:
            # Response inhibition development
            self.params.update({
                "base_inhibition_threshold": 0.5,
                "resource_depletion_rate": 0.2,
                "resource_recovery_rate": 0.1,
                "context_sensitivity": 0.3,
                "max_simultaneous_inhibitions": 2,
                "inhibition_duration": 5.0
            })
            
            # Update state parameters
            self.inhibition_state.recovery_rate = self.params["resource_recovery_rate"]
            
        elif self.development_level < 0.8:
            # Improved interference control
            self.params.update({
                "base_inhibition_threshold": 0.4,
                "resource_depletion_rate": 0.15,
                "resource_recovery_rate": 0.12,
                "context_sensitivity": 0.4,
                "max_simultaneous_inhibitions": 3,
                "inhibition_duration": 6.0
            })
            
            # Update state parameters
            self.inhibition_state.recovery_rate = self.params["resource_recovery_rate"]
            
        else:
            # Sophisticated inhibitory control
            self.params.update({
                "base_inhibition_threshold": 0.3,
                "resource_depletion_rate": 0.1,
                "resource_recovery_rate": 0.15,
                "context_sensitivity": 0.5,
                "max_simultaneous_inhibitions": 4,
                "inhibition_duration": 8.0
            })
            
            # Update state parameters
            self.inhibition_state.recovery_rate = self.params["resource_recovery_rate"]
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to apply inhibitory control
        
        Args:
            input_data: Dictionary containing stimulus and context information
                Required keys:
                - 'stimulus': Information about what might need to be inhibited
                Optional keys:
                - 'context': Contextual information affecting inhibition
                - 'type': Type of inhibition ('response', 'distraction', 'thought')
                - 'urgency': How urgent the inhibition decision is (0-1)
                - 'operation': Specific operation ('inhibit', 'query', 'recover')
            
        Returns:
            Dictionary with the results of inhibition
        """
        # Update resource recovery
        self._update_resources()
        
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        operation = input_data.get("operation", "inhibit")
        
        # Different operations based on the request
        if operation == "inhibit":
            return self._apply_inhibition(input_data, process_id)
        elif operation == "query":
            return self._query_state(input_data, process_id)
        elif operation == "recover":
            return self._force_recovery(input_data, process_id)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "process_id": process_id
            }
    
    def _apply_inhibition(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Apply inhibitory control to a stimulus"""
        # Extract required data
        if "stimulus" not in input_data:
            return {"status": "error", "message": "No stimulus provided", "process_id": process_id}
        
        stimulus = input_data.get("stimulus", {})
        context = input_data.get("context", {})
        inhibition_type = input_data.get("type", "response")
        
        # Check resource availability
        if self.inhibition_state.available_resources < 0.1:
            return {
                "status": "insufficient_resources",
                "inhibit_success": False,
                "available_resources": self.inhibition_state.available_resources,
                "message": "Insufficient inhibitory resources available",
                "process_id": process_id
            }
        
        # Convert stimulus and context to tensors for neural processing
        stimulus_features = self._extract_features(stimulus)
        context_features = self._extract_features(context)
        
        # Process through neural network
        with torch.no_grad():
            inhibition_result = self.inhibition_network(
                stimulus=stimulus_features.to(self.device),
                context=context_features.to(self.device)
            )
        
        # Extract results
        inhibit_probability = inhibition_result["inhibit_probability"].cpu().item()
        inhibition_strength = inhibition_result["inhibition_strength"].cpu().item()
        resource_cost = inhibition_result["resource_cost"].cpu().item()
        
        # Record activation in neural state
        self.neural_state.add_activation('inhibition', {
            'inhibit_probability': inhibit_probability,
            'inhibition_strength': inhibition_strength,
            'resource_cost': resource_cost,
            'inhibition_type': inhibition_type
        })
        
        # Apply context-specific threshold adjustments
        threshold = self.params["base_inhibition_threshold"]
        
        # Check for context-specific adjustments
        context_key = str(hash(str(sorted(context.items() if isinstance(context, dict) else context))))
        if context_key in self.inhibition_state.threshold_adjustments:
            threshold_adjustment = self.inhibition_state.threshold_adjustments[context_key]
            threshold = max(0.1, min(0.9, threshold + threshold_adjustment))
        
        # Determine whether to inhibit
        should_inhibit = inhibit_probability > threshold
        
        # Create inhibition event regardless of decision (for records)
        event = InhibitionEvent(
            inhibition_type=inhibition_type,
            trigger=str(stimulus)[:100],  # Truncate long stimuli
            strength=inhibition_strength,
            success=should_inhibit,
            resource_cost=resource_cost,
            context={k: str(v)[:50] for k, v in context.items()} if isinstance(context, dict) else {"data": str(context)[:50]}
        )
        
        # Add to history in state
        self.inhibition_state.recent_events.append(event)
        if len(self.inhibition_state.recent_events) > 20:
            self.inhibition_state.recent_events = self.inhibition_state.recent_events[-20:]
        
        # Update resources if inhibition was applied
        if should_inhibit:
            # Deplete resources based on cost
            self.inhibition_state.available_resources = max(
                0.0, 
                self.inhibition_state.available_resources - resource_cost
            )
            
            # Update state timestamp
            self.inhibition_state.last_updated = datetime.now()
            
            # Learn from this inhibition for future similar contexts
            if self.development_level >= 0.4:
                # Higher development enables learning from inhibition events
                if context_key not in self.inhibition_state.threshold_adjustments:
                    # Initialize adjustment
                    self.inhibition_state.threshold_adjustments[context_key] = 0.0
                
                # Adjust threshold slightly (strengthen or weaken based on outcome)
                # This simple learning rule could be replaced with more sophisticated approaches
                self.inhibition_state.threshold_adjustments[context_key] -= 0.01
                
                # Limit adjustment range
                self.inhibition_state.threshold_adjustments[context_key] = max(
                    -0.3, min(0.3, self.inhibition_state.threshold_adjustments[context_key])
                )
        
        return {
            "status": "success",
            "inhibit_decision": should_inhibit,
            "inhibition_strength": inhibition_strength,
            "available_resources": self.inhibition_state.available_resources,
            "resource_cost": resource_cost,
            "inhibition_event": event.dict(),
            "process_id": process_id
        }
    
    def _query_state(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Query the current inhibition state"""
        # Update resources first
        self._update_resources()
        
        query_type = input_data.get("query_type", "resources")
        
        if query_type == "resources":
            return {
                "status": "success",
                "available_resources": self.inhibition_state.available_resources,
                "recovery_rate": self.inhibition_state.recovery_rate,
                "process_id": process_id
            }
        elif query_type == "events":
            # Get recent events, optionally filtered by type
            event_type = input_data.get("event_type", None)
            events = self.inhibition_state.recent_events
            
            if event_type:
                events = [e for e in events if e.inhibition_type == event_type]
            
            return {
                "status": "success",
                "events": [e.dict() for e in events],
                "event_count": len(events),
                "process_id": process_id
            }
        elif query_type == "threshold_adjustments":
            return {
                "status": "success",
                "threshold_adjustments": self.inhibition_state.threshold_adjustments,
                "base_threshold": self.params["base_inhibition_threshold"],
                "process_id": process_id
            }
        else:
            # Full state
            return {
                "status": "success",
                "inhibition_state": self.inhibition_state.dict(),
                "inhibition_params": self.params,
                "development_level": self.development_level,
                "process_id": process_id
            }
    
    def _force_recovery(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Force recovery of inhibitory resources"""
        amount = input_data.get("amount", 0.5)
        
        # Cap the amount based on development level
        max_recovery = 0.2 + (0.8 * self.development_level)
        actual_amount = min(amount, max_recovery)
        
        # Apply recovery
        old_resources = self.inhibition_state.available_resources
        self.inhibition_state.available_resources = min(
            1.0, self.inhibition_state.available_resources + actual_amount
        )
        
        # Update timestamp
        self.inhibition_state.last_updated = datetime.now()
        self.last_resource_update = time.time()
        
        return {
            "status": "success",
            "previous_resources": old_resources,
            "new_resources": self.inhibition_state.available_resources,
            "recovery_amount": actual_amount,
            "process_id": process_id
        }
    
    def _update_resources(self):
        """Update resource recovery based on elapsed time"""
        current_time = time.time()
        elapsed_seconds = current_time - self.last_resource_update
        
        if elapsed_seconds > 0.1:  # Only update if enough time has passed
            # Calculate recovery amount
            recovery_amount = self.inhibition_state.recovery_rate * elapsed_seconds
            
            # Apply recovery up to maximum
            self.inhibition_state.available_resources = min(
                1.0, self.inhibition_state.available_resources + recovery_amount
            )
            
            # Update timestamp
            self.last_resource_update = current_time
            
            # Update state timestamp if resources changed significantly
            if recovery_amount > 0.01:
                self.inhibition_state.last_updated = datetime.now()
    
    def _extract_features(self, data) -> torch.Tensor:
        """
        Extract features from input data for neural processing
        
        Args:
            data: Text, dict, or other data to extract features from
            
        Returns:
            Tensor of features [1, feature_dim]
        """
        # For demonstration, create simple random features
        # In a real implementation, this would use proper feature extraction
        feature_dim = 64
        
        if isinstance(data, str):
            # Seed random generator with hash of string to ensure consistent features
            seed = hash(data) % 10000
            np.random.seed(seed)
            
            # Generate "features" based on the text
            features = np.random.randn(feature_dim)
            features = features / np.linalg.norm(features)  # Normalize
            
        elif isinstance(data, dict):
            # For dictionary data, use keys and values to generate features
            seed = hash(str(sorted(data.items()))) % 10000
            np.random.seed(seed)
            
            features = np.random.randn(feature_dim)
            features = features / np.linalg.norm(features)  # Normalize
            
        else:
            # Default random features
            features = np.random.randn(feature_dim)
            features = features / np.linalg.norm(features)  # Normalize
        
        return torch.tensor(features, dtype=torch.float32).unsqueeze(0)
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # Update base development level
        new_level = super().update_development(amount)
        
        # Update network development level
        self.inhibition_network.set_development_level(new_level)
        
        # Update neural state
        self.neural_state.inhibition_development = new_level
        self.neural_state.last_updated = datetime.now()
        
        # Adjust parameters based on new development level
        self._adjust_parameters_for_development()
        
        logger.info(f"Inhibition module development updated to {new_level:.2f}")
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the module
        
        Returns:
            Dictionary containing current module state
        """
        # Update resources first
        self._update_resources()
        
        # Get base state from parent
        base_state = super().get_state()
        
        # Add inhibition-specific state
        inhibition_state_dict = self.inhibition_state.dict()
        
        # Add neural state
        neural_state = {
            "development_level": self.neural_state.inhibition_development,
            "accuracy": self.neural_state.inhibition_accuracy,
            "recent_activations_count": len(self.neural_state.recent_inhibition_activations)
        }
        
        # Combine states
        combined_state = {
            **base_state, 
            **inhibition_state_dict, 
            "params": self.params,
            "neural_state": neural_state
        }
        
        return combined_state


#######################

#executive\models.py#
#######################

from pydantic import BaseModel, Field, validator
from typing import Dict, List, Any, Optional, Union, Tuple
from datetime import datetime
import uuid
import json

class PlanStep(BaseModel):
    """
    Represents a single step in a plan
    
    Each step has an action, expected outcome, and status
    """
    step_id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="Unique identifier for this step")
    action: str = Field(..., description="Action to be performed")
    description: str = Field(..., description="Description of this step")
    expected_outcome: str = Field(..., description="Expected result of this action")
    prerequisites: List[str] = Field(default_factory=list, description="IDs of steps that must be completed first")
    status: str = Field("pending", description="Current status of this step (pending, in_progress, completed, failed)")
    estimated_difficulty: float = Field(0.5, ge=0.0, le=1.0, description="Estimated difficulty of this step")
    completion_percentage: float = Field(0.0, ge=0.0, le=1.0, description="Percentage of completion")
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert datetime to ISO format for serialization"""
        result = super().dict(*args, **kwargs)
        result['created_at'] = self.created_at.isoformat()
        result['updated_at'] = self.updated_at.isoformat()
        return result

class Plan(BaseModel):
    """
    Represents a complete plan with multiple steps
    
    Plans have goals, steps, and overall status information
    """
    plan_id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="Unique identifier for this plan")
    goal: str = Field(..., description="The goal this plan aims to achieve")
    description: str = Field(..., description="Description of this plan")
    steps: List[PlanStep] = Field(default_factory=list, description="Steps in this plan")
    current_step_index: Optional[int] = Field(None, description="Index of the current step being executed")
    status: str = Field("created", description="Current status (created, in_progress, completed, failed, abandoned)")
    success_likelihood: float = Field(0.5, ge=0.0, le=1.0, description="Estimated likelihood of success")
    completion_percentage: float = Field(0.0, ge=0.0, le=1.0, description="Overall completion percentage")
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert datetime to ISO format for serialization"""
        result = super().dict(*args, **kwargs)
        result['created_at'] = self.created_at.isoformat()
        result['updated_at'] = self.updated_at.isoformat()
        result['steps'] = [step.dict() for step in self.steps]
        return result

class Decision(BaseModel):
    """
    Represents a decision with options and evaluations
    
    Decisions include options, their evaluations, and the selected choice
    """
    decision_id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="Unique identifier for this decision")
    context: str = Field(..., description="Context in which the decision is being made")
    options: Dict[str, Dict[str, Any]] = Field(..., description="Available options with their attributes")
    criteria: Dict[str, float] = Field(..., description="Decision criteria and their weights")
    evaluations: Dict[str, Dict[str, float]] = Field(default_factory=dict, description="Evaluations of each option on each criterion")
    option_scores: Dict[str, float] = Field(default_factory=dict, description="Overall score for each option")
    selected_option: Optional[str] = Field(None, description="The option that was selected")
    confidence: float = Field(0.5, ge=0.0, le=1.0, description="Confidence in the decision")
    decision_time: float = Field(0.0, ge=0.0, description="Time taken to make the decision (seconds)")
    created_at: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert datetime to ISO format for serialization"""
        result = super().dict(*args, **kwargs)
        result['created_at'] = self.created_at.isoformat()
        return result

class InhibitionEvent(BaseModel):
    """
    Represents an inhibition event
    
    Records when and how inhibitory control was applied
    """
    event_id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="Unique identifier for this event")
    inhibition_type: str = Field(..., description="Type of inhibition (response, distraction, etc.)")
    trigger: str = Field(..., description="What triggered the need for inhibition")
    strength: float = Field(..., ge=0.0, le=1.0, description="Strength of inhibition applied")
    success: bool = Field(..., description="Whether inhibition was successful")
    resource_cost: float = Field(..., ge=0.0, description="Resource cost of this inhibition")
    context: Dict[str, Any] = Field(default_factory=dict, description="Contextual information")
    timestamp: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert datetime to ISO format for serialization"""
        result = super().dict(*args, **kwargs)
        result['timestamp'] = self.timestamp.isoformat()
        return result

class InhibitionState(BaseModel):
    """
    Represents the current state of the inhibition system
    
    Includes resource levels and recent inhibition events
    """
    available_resources: float = Field(1.0, ge=0.0, le=1.0, description="Currently available inhibitory resources")
    recovery_rate: float = Field(0.1, ge=0.0, le=1.0, description="Rate at which resources recover")
    recent_events: List[InhibitionEvent] = Field(default_factory=list, description="Recent inhibition events")
    threshold_adjustments: Dict[str, float] = Field(default_factory=dict, description="Context-specific threshold adjustments")
    last_updated: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert datetime to ISO format for serialization"""
        result = super().dict(*args, **kwargs)
        result['last_updated'] = self.last_updated.isoformat()
        result['recent_events'] = [event.dict() for event in self.recent_events]
        return result

class WorkingMemoryItem(BaseModel):
    """
    Represents an item in working memory
    
    Items have content, activation level, and metadata
    """
    item_id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="Unique identifier for this item")
    content: Any = Field(..., description="The content being held in working memory")
    content_type: str = Field(..., description="Type of content (visual, verbal, spatial, etc.)")
    activation: float = Field(1.0, ge=0.0, le=1.0, description="Current activation level")
    creation_time: datetime = Field(default_factory=datetime.now, description="When this item was created")
    last_access: datetime = Field(default_factory=datetime.now, description="When this item was last accessed")
    access_count: int = Field(0, ge=0, description="How many times this item has been accessed")
    tags: List[str] = Field(default_factory=list, description="Tags or categories for this item")
    
    def dict(self, *args, **kwargs):
        """Convert datetime to ISO format for serialization"""
        result = super().dict(*args, **kwargs)
        result['creation_time'] = self.creation_time.isoformat()
        result['last_access'] = self.last_access.isoformat()
        
        # Handle serialization of content based on type
        if isinstance(self.content, (dict, list)):
            result['content'] = self.content
        else:
            try:
                result['content'] = str(self.content)
            except:
                result['content'] = "Unserializable content"
                
        return result

class WorkingMemoryState(BaseModel):
    """
    Represents the current state of working memory
    
    Includes current contents and capacity information
    """
    items: Dict[str, WorkingMemoryItem] = Field(default_factory=dict, description="Items currently in working memory")
    capacity: int = Field(3, ge=1, description="Maximum number of items that can be held")
    capacity_utilization: float = Field(0.0, ge=0.0, le=1.0, description="Current utilization of capacity")
    focus_of_attention: Optional[str] = Field(None, description="ID of the item currently in focus")
    last_operation: Optional[str] = Field(None, description="Last operation performed")
    last_updated: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert datetime to ISO format for serialization"""
        result = super().dict(*args, **kwargs)
        result['last_updated'] = self.last_updated.isoformat()
        result['items'] = {k: v.dict() for k, v in self.items.items()}
        return result

class ExecutiveParameters(BaseModel):
    """
    Parameters controlling the executive system's behavior
    
    These adjust based on developmental level
    """
    planning_depth: int = Field(1, ge=1, description="How many steps ahead to plan")
    decision_time_allocation: float = Field(0.5, ge=0.0, le=1.0, description="How much time to allocate to decisions")
    inhibition_threshold: float = Field(0.5, ge=0.0, le=1.0, description="Threshold for applying inhibition")
    working_memory_decay_rate: float = Field(0.1, ge=0.0, le=1.0, description="Rate of activation decay in working memory")
    cognitive_flexibility: float = Field(0.5, ge=0.0, le=1.0, description="Ability to switch between tasks or strategies")
    resource_allocation_strategy: str = Field("balanced", description="How to allocate limited resources")
    
class ExecutiveNeuralState(BaseModel):
    """
    State information for executive neural networks
    
    Tracks the state of neural networks for executive functions
    """
    planning_development: float = Field(0.0, ge=0.0, le=1.0, description="Development level of planning network")
    decision_development: float = Field(0.0, ge=0.0, le=1.0, description="Development level of decision network")
    inhibition_development: float = Field(0.0, ge=0.0, le=1.0, description="Development level of inhibition network")
    working_memory_development: float = Field(0.0, ge=0.0, le=1.0, description="Development level of working memory network")
    
    # Track recent activations for each neural component
    recent_planning_activations: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Recent activations of the planning network"
    )
    recent_decision_activations: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Recent activations of the decision network"
    )
    recent_inhibition_activations: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Recent activations of the inhibition network"
    )
    recent_working_memory_activations: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Recent activations of the working memory network"
    )
    
    # Network performance metrics
    planning_accuracy: float = Field(0.5, ge=0.0, le=1.0, description="Accuracy of planning network")
    decision_accuracy: float = Field(0.5, ge=0.0, le=1.0, description="Accuracy of decision network")
    inhibition_accuracy: float = Field(0.5, ge=0.0, le=1.0, description="Accuracy of inhibition network")
    working_memory_accuracy: float = Field(0.5, ge=0.0, le=1.0, description="Accuracy of working memory network")
    
    # Last update timestamp
    last_updated: datetime = Field(default_factory=datetime.now, description="When neural state was last updated")
    
    def dict(self, *args, **kwargs):
        """Convert datetime to ISO format for serialization"""
        result = super().dict(*args, **kwargs)
        result['last_updated'] = self.last_updated.isoformat()
        return result
    
    def update_accuracy(self, component: str, accuracy: float) -> None:
        """
        Update the accuracy for a specific neural component
        
        Args:
            component: The component to update ('planning', 'decision', 'inhibition', 'working_memory')
            accuracy: The new accuracy value (0.0 to 1.0)
        """
        if component == 'planning':
            self.planning_accuracy = max(0.0, min(1.0, accuracy))
        elif component == 'decision':
            self.decision_accuracy = max(0.0, min(1.0, accuracy))
        elif component == 'inhibition':
            self.inhibition_accuracy = max(0.0, min(1.0, accuracy))
        elif component == 'working_memory':
            self.working_memory_accuracy = max(0.0, min(1.0, accuracy))
        
        self.last_updated = datetime.now()
    
    def add_activation(self, component: str, activation: Dict[str, Any]) -> None:
        """
        Add a recent activation for a neural component
        
        Args:
            component: The component that was activated
            activation: Dictionary with activation details
        """
        activation_with_timestamp = {
            **activation,
            "timestamp": datetime.now().isoformat()
        }
        
        if component == 'planning':
            self.recent_planning_activations.append(activation_with_timestamp)
            if len(self.recent_planning_activations) > 10:  # Keep last 10
                self.recent_planning_activations = self.recent_planning_activations[-10:]
        elif component == 'decision':
            self.recent_decision_activations.append(activation_with_timestamp)
            if len(self.recent_decision_activations) > 10:
                self.recent_decision_activations = self.recent_decision_activations[-10:]
        elif component == 'inhibition':
            self.recent_inhibition_activations.append(activation_with_timestamp)
            if len(self.recent_inhibition_activations) > 10:
                self.recent_inhibition_activations = self.recent_inhibition_activations[-10:]
        elif component == 'working_memory':
            self.recent_working_memory_activations.append(activation_with_timestamp)
            if len(self.recent_working_memory_activations) > 10:
                self.recent_working_memory_activations = self.recent_working_memory_activations[-10:]
            
        self.last_updated = datetime.now()

class ExecutiveSystemState(BaseModel):
    """
    Complete state of the executive system
    
    Combines state information from all executive functions
    """
    planning_state: Dict[str, Any] = Field(default_factory=dict, description="State of planning system")
    decision_state: Dict[str, Any] = Field(default_factory=dict, description="State of decision making system")
    inhibition_state: InhibitionState = Field(default_factory=InhibitionState, description="State of inhibition system")
    working_memory_state: WorkingMemoryState = Field(default_factory=WorkingMemoryState, description="State of working memory system")
    parameters: ExecutiveParameters = Field(default_factory=ExecutiveParameters, description="Executive system parameters")
    neural_state: ExecutiveNeuralState = Field(default_factory=ExecutiveNeuralState, description="Neural network states")
    
    # Active plans and decisions
    active_plans: Dict[str, Plan] = Field(default_factory=dict, description="Currently active plans")
    recent_decisions: List[Decision] = Field(default_factory=list, description="Recent decisions made")
    
    # System metadata
    module_id: str = Field(..., description="Module identifier")
    developmental_level: float = Field(0.0, ge=0.0, le=1.0, description="Overall developmental level")
    last_updated: datetime = Field(default_factory=datetime.now, description="When system state was last updated")
    
    def dict(self, *args, **kwargs):
        """Convert datetime to ISO format for serialization"""
        result = super().dict(*args, **kwargs)
        result['last_updated'] = self.last_updated.isoformat()
        result['inhibition_state'] = self.inhibition_state.dict()
        result['working_memory_state'] = self.working_memory_state.dict()
        result['neural_state'] = self.neural_state.dict()
        result['active_plans'] = {k: v.dict() for k, v in self.active_plans.items()}
        result['recent_decisions'] = [decision.dict() for decision in self.recent_decisions]
        return result
    
    def add_plan(self, plan: Plan, max_active_plans: int = 5) -> None:
        """
        Add a plan to active plans
        
        Args:
            plan: The plan to add
            max_active_plans: Maximum number of active plans to keep
        """
        self.active_plans[plan.plan_id] = plan
        
        # Limit number of active plans
        if len(self.active_plans) > max_active_plans:
            oldest_key = min(self.active_plans.keys(), key=lambda k: self.active_plans[k].created_at)
            del self.active_plans[oldest_key]
        
        self.last_updated = datetime.now()
    
    def add_decision(self, decision: Decision, max_decisions: int = 20) -> None:
        """
        Add a decision to recent decisions
        
        Args:
            decision: The decision to add
            max_decisions: Maximum number of recent decisions to keep
        """
        self.recent_decisions.append(decision)
        
        # Limit number of recent decisions
        if len(self.recent_decisions) > max_decisions:
            self.recent_decisions = self.recent_decisions[-max_decisions:]
            
        self.last_updated = datetime.now()


#######################

#executive\neural_net.py#
#######################

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Any, Tuple, Optional, Union

class PlanningNetwork(nn.Module):
    """
    Neural network for plan generation and execution monitoring
    
    This network processes goals and states to generate plan steps and
    monitor execution progress.
    """
    
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 64):
        """
        Initialize the planning network
        
        Args:
            input_dim: Dimension of input features (goal + state)
            hidden_dim: Dimension of hidden layers
            output_dim: Dimension of output features
        """
        super().__init__()
        
        # Goal and state encoders
        self.goal_encoder = nn.Linear(input_dim // 2, hidden_dim // 2)
        self.state_encoder = nn.Linear(input_dim // 2, hidden_dim // 2)
        
        # Combined processing layers
        self.hidden_layers = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # Plan generation head
        self.plan_generator = nn.Linear(hidden_dim, output_dim)
        
        # Execution monitoring head
        self.execution_monitor = nn.Linear(hidden_dim, 3)  # [progress, success_prob, revision_needed]
        
        # Layer normalization for stable training
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
        # Developmental parameter
        self.developmental_factor = nn.Parameter(torch.tensor(0.1), requires_grad=False)
        
    def forward(self, goal: torch.Tensor, state: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Process goal and state to generate plan information
        
        Args:
            goal: Tensor representing the goal [batch_size, input_dim//2]
            state: Tensor representing current state [batch_size, input_dim//2]
            
        Returns:
            Dictionary with plan features and monitoring outputs
        """
        # Encode goal and state
        goal_features = F.relu(self.goal_encoder(goal))
        state_features = F.relu(self.state_encoder(state))
        
        # Combine features
        combined = torch.cat([goal_features, state_features], dim=1)
        
        # Process through hidden layers with residual connection
        hidden_output = self.hidden_layers(combined)
        combined = combined + self.layer_norm(hidden_output)  # Residual connection
        
        # Generate plan features
        plan_features = self.plan_generator(combined)
        
        # Monitor execution
        monitor_outputs = self.execution_monitor(combined)
        progress = torch.sigmoid(monitor_outputs[:, 0])
        success_prob = torch.sigmoid(monitor_outputs[:, 1])
        revision_needed = torch.sigmoid(monitor_outputs[:, 2])
        
        # Developmental modulation
        dev_factor = self.developmental_factor.item()
        if dev_factor < 0.3:
            # Very basic planning at early development
            # Add randomness and limit planning horizon
            plan_features = 0.7 * plan_features + 0.3 * torch.randn_like(plan_features) * 0.1
            # Simple binary outcomes at early stages
            success_prob = torch.round(success_prob * 2) / 2
        
        return {
            "plan_features": plan_features,
            "progress": progress,
            "success_probability": success_prob,
            "revision_needed": revision_needed
        }
    
    def set_development_level(self, level: float) -> None:
        """
        Set the developmental level of the planning network
        
        Args:
            level: Development level (0.0 to 1.0)
        """
        with torch.no_grad():
            self.developmental_factor.copy_(torch.tensor(max(0.0, min(1.0, level))))


class DecisionNetwork(nn.Module):
    """
    Neural network for decision making
    
    This network evaluates options and selects the best one based on
    multiple criteria and contextual factors.
    """
    
    def __init__(self, option_dim: int = 64, criteria_dim: int = 16, hidden_dim: int = 128):
        """
        Initialize the decision network
        
        Args:
            option_dim: Dimension of option features
            criteria_dim: Dimension of criteria features
            hidden_dim: Dimension of hidden layers
        """
        super().__init__()
        
        # Option encoder
        self.option_encoder = nn.Linear(option_dim, hidden_dim)
        
        # Criteria encoder
        self.criteria_encoder = nn.Linear(criteria_dim, hidden_dim // 2)
        
        # Context encoder
        self.context_encoder = nn.Linear(option_dim, hidden_dim // 2)
        
        # Evaluation layers
        self.evaluation_layers = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # Option scoring head
        self.option_scorer = nn.Linear(hidden_dim, 1)
        
        # Confidence estimation head
        self.confidence_estimator = nn.Linear(hidden_dim, 1)
        
        # Layer normalization
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
        # Developmental parameter
        self.developmental_factor = nn.Parameter(torch.tensor(0.1), requires_grad=False)
        
    def forward(self, 
                options: torch.Tensor, 
                criteria: torch.Tensor, 
                context: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        Evaluate options based on criteria and context
        
        Args:
            options: Tensor of option features [batch_size, num_options, option_dim]
            criteria: Tensor of criteria features [batch_size, criteria_dim]
            context: Optional tensor of context features [batch_size, option_dim]
            
        Returns:
            Dictionary with option scores and confidence
        """
        batch_size, num_options, _ = options.shape
        
        # Encode criteria
        criteria_encoded = F.relu(self.criteria_encoder(criteria))  # [batch_size, hidden_dim//2]
        
        # Encode context (or use zeros if not provided)
        if context is not None:
            context_encoded = F.relu(self.context_encoder(context))  # [batch_size, hidden_dim//2]
        else:
            context_encoded = torch.zeros(batch_size, self.hidden_dim // 2, device=options.device)
            
        # Combine criteria and context
        criteria_context = torch.cat([criteria_encoded, context_encoded], dim=1)  # [batch_size, hidden_dim]
        
        # Repeat for each option
        criteria_context = criteria_context.unsqueeze(1).expand(-1, num_options, -1)  # [batch_size, num_options, hidden_dim]
        
        # Process each option
        options_flat = options.view(-1, options.size(-1))  # [batch_size*num_options, option_dim]
        options_encoded = F.relu(self.option_encoder(options_flat))  # [batch_size*num_options, hidden_dim]
        options_encoded = options_encoded.view(batch_size, num_options, -1)  # [batch_size, num_options, hidden_dim]
        
        # Combine options with criteria and context
        combined = torch.cat([options_encoded, criteria_context], dim=2)  # [batch_size, num_options, hidden_dim*2]
        
        # Reshape for processing
        combined_flat = combined.view(-1, combined.size(-1))  # [batch_size*num_options, hidden_dim*2]
        
        # Evaluate options
        evaluated = self.evaluation_layers(combined_flat)  # [batch_size*num_options, hidden_dim]
        evaluated = evaluated.view(batch_size, num_options, -1)  # [batch_size, num_options, hidden_dim]
        
        # Score options
        scores_flat = self.option_scorer(evaluated.view(-1, evaluated.size(-1)))  # [batch_size*num_options, 1]
        scores = scores_flat.view(batch_size, num_options)  # [batch_size, num_options]
        
        # Apply developmental modulation
        dev_factor = self.developmental_factor.item()
        if dev_factor < 0.3:
            # At early development, decisions are more random and less nuanced
            scores = scores * 0.7 + torch.randn_like(scores) * 0.3
        
        # Convert to probabilities
        probabilities = F.softmax(scores, dim=1)
        
        # Estimate confidence (based on score distribution entropy)
        # High entropy = low confidence, Low entropy = high confidence
        log_probs = F.log_softmax(scores, dim=1)
        entropy = -torch.sum(probabilities * log_probs, dim=1, keepdim=True)
        max_entropy = torch.log(torch.tensor(num_options, dtype=torch.float))
        confidence = 1 - entropy / max_entropy
        
        # Best option
        best_option_idx = torch.argmax(scores, dim=1)
        
        return {
            "scores": scores,
            "probabilities": probabilities,
            "confidence": confidence,
            "best_option_idx": best_option_idx
        }
    
    def set_development_level(self, level: float) -> None:
        """
        Set the developmental level of the decision network
        
        Args:
            level: Development level (0.0 to 1.0)
        """
        with torch.no_grad():
            self.developmental_factor.copy_(torch.tensor(max(0.0, min(1.0, level))))


class InhibitionNetwork(nn.Module):
    """
    Neural network for inhibitory control
    
    This network processes stimuli and context to determine whether
    inhibitory control should be applied and with what strength.
    """
    
    def __init__(self, input_dim: int = 128, hidden_dim: int = 128):
        """
        Initialize the inhibition network
        
        Args:
            input_dim: Dimension of input features
            hidden_dim: Dimension of hidden layers
        """
        super().__init__()
        
        # Stimulus encoder
        self.stimulus_encoder = nn.Linear(input_dim // 2, hidden_dim // 2)
        
        # Context encoder
        self.context_encoder = nn.Linear(input_dim // 2, hidden_dim // 2)
        
        # Processing layers
        self.processing_layers = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # Inhibition decision head
        self.inhibition_head = nn.Linear(hidden_dim, 1)  # Inhibit or not
        
        # Inhibition strength head
        self.strength_head = nn.Linear(hidden_dim, 1)  # How strongly to inhibit
        
        # Resource cost head
        self.cost_head = nn.Linear(hidden_dim, 1)  # Resource cost of inhibition
        
        # Layer normalization
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
        # Developmental parameter
        self.developmental_factor = nn.Parameter(torch.tensor(0.1), requires_grad=False)
        
    def forward(self, stimulus: torch.Tensor, context: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Determine whether and how strongly to apply inhibitory control
        
        Args:
            stimulus: Tensor representing the stimulus to potentially inhibit
            context: Tensor representing contextual information
            
        Returns:
            Dictionary with inhibition decision and parameters
        """
        # Encode stimulus and context
        stimulus_encoded = F.relu(self.stimulus_encoder(stimulus))
        context_encoded = F.relu(self.context_encoder(context))
        
        # Combine features
        combined = torch.cat([stimulus_encoded, context_encoded], dim=1)
        
        # Process through hidden layers with residual connection
        processed = self.processing_layers(combined)
        combined = combined + self.layer_norm(processed)  # Residual connection
        
        # Inhibition decision
        inhibit_logit = self.inhibition_head(combined)
        inhibit_prob = torch.sigmoid(inhibit_logit)
        
        # Inhibition strength
        strength = torch.sigmoid(self.strength_head(combined))
        
        # Resource cost
        cost = F.softplus(self.cost_head(combined))  # Always positive
        
        # Apply developmental modulation
        dev_factor = self.developmental_factor.item()
        if dev_factor < 0.3:
            # Low development: weak/inconsistent inhibition
            inhibit_prob = inhibit_prob * 0.7 + torch.rand_like(inhibit_prob) * 0.3
            strength = strength * 0.5  # Weaker inhibition
            cost = cost * 1.5  # Higher cost
        
        return {
            "inhibit_probability": inhibit_prob,
            "inhibition_strength": strength,
            "resource_cost": cost
        }
    
    def set_development_level(self, level: float) -> None:
        """
        Set the developmental level of the inhibition network
        
        Args:
            level: Development level (0.0 to 1.0)
        """
        with torch.no_grad():
            self.developmental_factor.copy_(torch.tensor(max(0.0, min(1.0, level))))


class WorkingMemoryNetwork(nn.Module):
    """
    Neural network for working memory control
    
    This network handles maintenance, updating, and manipulation
    of items in working memory.
    """
    
    def __init__(self, item_dim: int = 64, control_dim: int = 32, hidden_dim: int = 128, capacity: int = 7):
        """
        Initialize the working memory network
        
        Args:
            item_dim: Dimension of item features
            control_dim: Dimension of control signals
            hidden_dim: Dimension of hidden layers
            capacity: Maximum number of items in working memory
        """
        super().__init__()
        
        self.item_dim = item_dim
        self.control_dim = control_dim
        self.hidden_dim = hidden_dim
        self.capacity = capacity
        
        # Item encoder
        self.item_encoder = nn.Linear(item_dim, hidden_dim)
        
        # Control signal encoder
        self.control_encoder = nn.Linear(control_dim, hidden_dim)
        
        # Memory slots (as parameters)
        self.memory_slots = nn.Parameter(torch.zeros(capacity, hidden_dim))
        
        # Attention mechanism for slot selection
        self.attention = nn.Linear(hidden_dim * 2, 1)
        
        # Gate for memory updating
        self.update_gate = nn.Linear(hidden_dim * 2, hidden_dim)
        
        # Output projection
        self.output_projection = nn.Linear(hidden_dim, item_dim)
        
        # Memory decay parameters
        self.decay_rate = nn.Parameter(torch.tensor(0.1), requires_grad=True)
        
        # Developmental parameter
        self.developmental_factor = nn.Parameter(torch.tensor(0.1), requires_grad=False)
        
    def forward(self, 
                operation: str,
                items: Optional[torch.Tensor] = None,
                control: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        Perform operations on working memory
        
        Args:
            operation: Type of operation ('store', 'retrieve', 'update', 'clear')
            items: Optional tensor of items to store [batch_size, item_dim]
            control: Optional tensor of control signals [batch_size, control_dim]
            
        Returns:
            Dictionary with operation results
        """
        batch_size = 1
        if items is not None:
            batch_size = items.size(0)
            
        # Get effective capacity based on development
        dev_factor = self.developmental_factor.item()
        effective_capacity = max(1, int(self.capacity * (0.3 + 0.7 * dev_factor)))
        
        # Initialize result dictionary
        result = {}
        
        if operation == 'store' and items is not None:
            # Encode items
            item_encoded = F.relu(self.item_encoder(items))  # [batch_size, hidden_dim]
            
            # Find least active slot or empty slot
            # In a real implementation, this would use actual memory activations
            # Here we'll use a simple heuristic for demonstration
            slot_idx = torch.randint(0, effective_capacity, (batch_size,))
            
            # Store items in selected slots
            # In practice, this would update a persistent memory state
            memory_update = F.sigmoid(self.update_gate(
                torch.cat([item_encoded, self.memory_slots[slot_idx]], dim=1)
            ))
            
            # Apply developmental modulation
            if dev_factor < 0.4:
                # Lower development: more forgetting, less precise storage
                memory_update = memory_update * 0.8 + torch.randn_like(memory_update) * 0.2
            
            result['stored_indices'] = slot_idx
            result['store_success'] = torch.ones(batch_size)
            
        elif operation == 'retrieve' and control is not None:
            # Encode control signal
            control_encoded = F.relu(self.control_encoder(control))  # [batch_size, hidden_dim]
            
            # Calculate attention over memory slots
            attention_inputs = []
            for i in range(effective_capacity):
                slot_expand = self.memory_slots[i:i+1].expand(batch_size, -1)
                combined = torch.cat([control_encoded, slot_expand], dim=1)
                attention_inputs.append(self.attention(combined))
            
            attention_logits = torch.cat(attention_inputs, dim=1)  # [batch_size, capacity]
            attention_weights = F.softmax(attention_logits, dim=1)
            
            # Retrieve weighted combination of memory slots
            retrieved_items = torch.zeros(batch_size, self.hidden_dim, device=control.device)
            for i in range(effective_capacity):
                slot_expand = self.memory_slots[i:i+1].expand(batch_size, -1)
                retrieved_items += attention_weights[:, i:i+1] * slot_expand
            
            # Project back to item space
            retrieved_items = self.output_projection(retrieved_items)
            
            # Apply developmental modulation
            if dev_factor < 0.4:
                # Lower development: noisier retrieval
                retrieved_items = retrieved_items * 0.8 + torch.randn_like(retrieved_items) * 0.2
            
            result['retrieved_items'] = retrieved_items
            result['attention_weights'] = attention_weights
            
        elif operation == 'update' and items is not None and control is not None:
            # For simplicity, this is similar to store but with target slot
            # Encode items and control
            item_encoded = F.relu(self.item_encoder(items))
            control_encoded = F.relu(self.control_encoder(control))
            
            # Calculate attention to find target slot
            attention_inputs = []
            for i in range(effective_capacity):
                slot_expand = self.memory_slots[i:i+1].expand(batch_size, -1)
                combined = torch.cat([control_encoded, slot_expand], dim=1)
                attention_inputs.append(self.attention(combined))
            
            attention_logits = torch.cat(attention_inputs, dim=1)
            attention_weights = F.softmax(attention_logits, dim=1)
            slot_idx = torch.argmax(attention_weights, dim=1)
            
            # Update selected slots
            memory_update = F.sigmoid(self.update_gate(
                torch.cat([item_encoded, self.memory_slots[slot_idx]], dim=1)
            ))
            
            result['updated_indices'] = slot_idx
            result['update_success'] = torch.ones(batch_size)
            
        elif operation == 'clear':
            # Reset all memory slots
            # In practice, this would update a persistent memory state
            # with torch.no_grad():
            #     self.memory_slots.zero_()
            
            result['clear_success'] = torch.ones(1)
        
        return result
    
    def set_development_level(self, level: float) -> None:
        """
        Set the developmental level of the working memory network
        
        Args:
            level: Development level (0.0 to 1.0)
        """
        with torch.no_grad():
            self.developmental_factor.copy_(torch.tensor(max(0.0, min(1.0, level))))


def get_device() -> torch.device:
    """
    Get the appropriate device (GPU if available, otherwise CPU)
    
    Returns:
        torch.device: The device to use for tensor operations
    """
    if torch.cuda.is_available():
        return torch.device("cuda")
    return torch.device("cpu")


#######################

#executive\planning.py#
#######################

# TODO: Implement the Planning class to develop and execute plans for goal achievement
# This component should be able to:
# - Create sequences of actions to achieve goals
# - Anticipate obstacles and develop contingency plans
# - Monitor plan execution and adjust as needed
# - Coordinate with other cognitive modules during plan execution

# TODO: Implement developmental progression in planning abilities:
# - Simple one-step plans in early stages
# - Short sequential plans in childhood
# - Complex hierarchical planning in adolescence
# - Strategic, flexible planning in adulthood

# TODO: Create mechanisms for:
# - Goal representation: Maintain clear goal states
# - Action sequencing: Order actions appropriately
# - Temporal projection: Anticipate future states
# - Error detection: Identify deviations from the plan

# TODO: Implement different planning approaches:
# - Forward planning: Plan from current state to goal
# - Backward planning: Plan from goal to current state
# - Hierarchical planning: Break complex goals into subgoals
# - Opportunistic planning: Flexibly adapt plans to changing conditions

# TODO: Connect to working memory and attention systems
# Planning requires working memory resources to maintain plans
# and attention to monitor execution

import logging
import time
import uuid
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import numpy as np
import torch
from collections import deque

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.executive.models import Plan, PlanStep, ExecutiveNeuralState
from lmm_project.modules.executive.neural_net import PlanningNetwork, get_device

# Initialize logger
logger = logging.getLogger(__name__)

class Planning(BaseModule):
    """
    Develops and executes plans to achieve goals
    
    This module creates sequences of actions to reach goal states,
    monitors plan execution, and adapts plans as needed.
    """
    
    # Development milestones
    development_milestones = {
        0.0: "Single step plans",
        0.2: "Multi-step sequential plans",
        0.4: "Conditional branching in plans",
        0.6: "Hierarchical planning",
        0.8: "Parallel action planning",
        1.0: "Strategic planning with contingencies"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the planning module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level of this module
        """
        super().__init__(
            module_id=module_id, 
            module_type="planning", 
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Initialize device
        self.device = get_device()
        
        # Initialize neural network
        self.planning_network = PlanningNetwork(
            input_dim=128,
            hidden_dim=256,
            output_dim=64
        ).to(self.device)
        
        # Set development level for network
        self.planning_network.set_development_level(development_level)
        
        # Create neural state for tracking
        self.neural_state = ExecutiveNeuralState()
        self.neural_state.planning_development = development_level
        
        # Active plans
        self.active_plans = {}
        
        # Plan history
        self.plan_history = deque(maxlen=20)
        
        # Planning parameters
        self.params = {
            "max_steps": 3,  # Maximum number of steps in a plan - increases with development
            "plan_horizon": 1,  # How many steps ahead to plan - increases with development
            "success_threshold": 0.6,  # Threshold for considering a plan successful
            "revision_threshold": 0.4,  # Threshold for when to revise a plan
            "hierarchical_planning": False,  # Whether hierarchical planning is enabled
            "parallel_actions": False  # Whether parallel actions are allowed
        }
        
        # Update parameters based on development
        self._adjust_parameters_for_development()
        
        logger.info(f"Planning module initialized at development level {development_level:.2f}")
    
    def _adjust_parameters_for_development(self):
        """Adjust planning parameters based on developmental level"""
        if self.development_level < 0.2:
            # Very simple planning at early stages
            self.params.update({
                "max_steps": max(1, int(2 * self.development_level) + 1),
                "plan_horizon": 1,
                "success_threshold": 0.7,  # Higher threshold (more conservative)
                "revision_threshold": 0.5,  # More likely to revise plans
                "hierarchical_planning": False,
                "parallel_actions": False
            })
        elif self.development_level < 0.4:
            # More steps but still simple planning
            self.params.update({
                "max_steps": 3,
                "plan_horizon": 2,
                "success_threshold": 0.65, 
                "revision_threshold": 0.45,
                "hierarchical_planning": False,
                "parallel_actions": False
            })
        elif self.development_level < 0.6:
            # Introduction of conditionals in plans
            self.params.update({
                "max_steps": 5,
                "plan_horizon": 3,
                "success_threshold": 0.6,
                "revision_threshold": 0.4,
                "hierarchical_planning": False,
                "parallel_actions": False
            })
        elif self.development_level < 0.8:
            # Hierarchical planning enabled
            self.params.update({
                "max_steps": 7,
                "plan_horizon": 4,
                "success_threshold": 0.55,
                "revision_threshold": 0.35,
                "hierarchical_planning": True,
                "parallel_actions": False
            })
        else:
            # Full planning capabilities
            self.params.update({
                "max_steps": 10,
                "plan_horizon": 5,
                "success_threshold": 0.5,
                "revision_threshold": 0.3,
                "hierarchical_planning": True,
                "parallel_actions": True
            })
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to create or update plans
        
        Args:
            input_data: Dictionary containing goal and state information
                Required keys depend on operation:
                - For 'create': 'goal', 'initial_state'
                - For 'execute': 'plan_id'
                - For 'update': 'plan_id', 'step_id', 'status', 'progress'
                - For 'revise': 'plan_id', 'current_state'
                - For 'query': 'plan_id' or 'all' flag
            
        Returns:
            Dictionary with the results of planning
        """
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        operation = input_data.get("operation", "create")
        
        # Different operations based on the request
        if operation == "create":
            return self._create_plan(input_data, process_id)
        elif operation == "execute":
            return self._execute_plan(input_data, process_id)
        elif operation == "update":
            return self._update_plan(input_data, process_id)
        elif operation == "revise":
            return self._revise_plan(input_data, process_id)
        elif operation == "query":
            return self._query_plans(input_data, process_id)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "process_id": process_id
            }
    
    def _create_plan(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Create a new plan"""
        # Extract required data
        if "goal" not in input_data:
            return {"status": "error", "message": "No goal provided", "process_id": process_id}
        
        goal = input_data.get("goal", "")
        description = input_data.get("description", f"Plan for: {goal}")
        initial_state = input_data.get("initial_state", {})
        constraints = input_data.get("constraints", [])
        
        # Convert goal and state to tensors for neural processing
        goal_features = self._extract_features(goal)
        state_features = self._extract_features(initial_state)
        
        # Process through neural network
        with torch.no_grad():
            planning_result = self.planning_network(
                goal=goal_features.to(self.device),
                state=state_features.to(self.device)
            )
        
        # Record activation in neural state
        self.neural_state.add_activation('planning', {
            'goal': goal,
            'plan_features': planning_result["plan_features"].cpu().numpy().tolist(),
            'success_probability': planning_result["success_probability"].cpu().item()
        })
        
        # Generate plan steps based on development level
        num_steps = min(
            self.params["max_steps"],
            max(1, int(3 * self.development_level) + 1)
        )
        
        plan_steps = []
        for i in range(num_steps):
            # Create more detailed steps at higher development levels
            if self.development_level < 0.3:
                # Very simple steps at early stages
                action = f"Perform action {i+1} to achieve goal"
                outcome = "Goal progress"
            else:
                # More specific steps at higher levels
                action = self._generate_action_for_step(goal, initial_state, i, num_steps)
                outcome = self._generate_outcome_for_step(goal, action, i, num_steps)
            
            # Create the step
            step = PlanStep(
                action=action,
                description=f"Step {i+1}: {action}",
                expected_outcome=outcome,
                prerequisites=[str(j) for j in range(i) if j < i],  # Prior steps are prerequisites
                estimated_difficulty=0.5  # Default difficulty
            )
            plan_steps.append(step)
        
        # Create the plan
        plan = Plan(
            goal=goal,
            description=description,
            steps=plan_steps,
            success_likelihood=planning_result["success_probability"].cpu().item(),
            completion_percentage=0.0
        )
        
        # Store the plan
        self.active_plans[plan.plan_id] = plan
        self.plan_history.append(plan.plan_id)
        
        # Return the created plan
        return {
            "status": "success",
            "plan_id": plan.plan_id,
            "plan": plan.dict(),
            "process_id": process_id
        }
    
    def _execute_plan(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Execute a plan or advance to next step"""
        # Extract plan ID
        plan_id = input_data.get("plan_id")
        if not plan_id or plan_id not in self.active_plans:
            return {"status": "error", "message": "Invalid plan ID", "process_id": process_id}
        
        plan = self.active_plans[plan_id]
        
        # Start execution or advance to next step
        if plan.current_step_index is None:
            # Starting execution
            plan.current_step_index = 0
            plan.status = "in_progress"
            current_step = plan.steps[0]
            current_step.status = "in_progress"
            
            plan.updated_at = datetime.now()
        else:
            # Advance to next step if current is complete
            current_idx = plan.current_step_index
            
            if current_idx >= len(plan.steps):
                return {
                    "status": "complete",
                    "message": "Plan already completed",
                    "plan": plan.dict(),
                    "process_id": process_id
                }
            
            current_step = plan.steps[current_idx]
            
            if current_step.status == "completed":
                # Move to next step
                if current_idx + 1 < len(plan.steps):
                    plan.current_step_index = current_idx + 1
                    next_step = plan.steps[plan.current_step_index]
                    next_step.status = "in_progress"
                    
                    plan.updated_at = datetime.now()
                else:
                    # Plan complete
                    plan.status = "completed"
                    plan.completion_percentage = 1.0
                    
                    plan.updated_at = datetime.now()
                    
                    return {
                        "status": "complete",
                        "message": "Plan execution completed",
                        "plan": plan.dict(),
                        "process_id": process_id
                    }
            
            # Get current step again after possible advancement
            current_step = plan.steps[plan.current_step_index]
        
        # Calculate plan progress
        completed_steps = sum(1 for step in plan.steps if step.status == "completed")
        plan.completion_percentage = completed_steps / len(plan.steps)
        
        # Return current step information
        return {
            "status": "in_progress",
            "current_step_index": plan.current_step_index,
            "current_step": plan.steps[plan.current_step_index].dict(),
            "completion_percentage": plan.completion_percentage,
            "plan": plan.dict(),
            "process_id": process_id
        }
    
    def _update_plan(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Update a plan or step status"""
        # Extract plan and step information
        plan_id = input_data.get("plan_id")
        if not plan_id or plan_id not in self.active_plans:
            return {"status": "error", "message": "Invalid plan ID", "process_id": process_id}
        
        plan = self.active_plans[plan_id]
        
        # Check if updating plan status or step status
        if "step_id" in input_data:
            # Updating a specific step
            step_id = input_data["step_id"]
            step = next((s for s in plan.steps if s.step_id == step_id), None)
            
            if not step:
                return {"status": "error", "message": "Invalid step ID", "process_id": process_id}
            
            # Update step status if provided
            if "status" in input_data:
                step.status = input_data["status"]
            
            # Update completion percentage if provided
            if "completion_percentage" in input_data:
                step.completion_percentage = input_data["completion_percentage"]
            
            step.updated_at = datetime.now()
            
            # Update plan completion percentage
            completed_steps = sum(1 for s in plan.steps if s.status == "completed")
            in_progress_steps = sum(s.completion_percentage for s in plan.steps if s.status == "in_progress")
            
            plan.completion_percentage = (completed_steps + in_progress_steps / len(plan.steps)) / len(plan.steps)
            
        else:
            # Updating the plan as a whole
            if "status" in input_data:
                plan.status = input_data["status"]
            
            if "completion_percentage" in input_data:
                plan.completion_percentage = input_data["completion_percentage"]
        
        plan.updated_at = datetime.now()
        
        # Return updated plan
        return {
            "status": "success",
            "plan": plan.dict(),
            "process_id": process_id
        }
    
    def _revise_plan(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Revise a plan based on new information"""
        # Extract plan information
        plan_id = input_data.get("plan_id")
        if not plan_id or plan_id not in self.active_plans:
            return {"status": "error", "message": "Invalid plan ID", "process_id": process_id}
        
        plan = self.active_plans[plan_id]
        current_state = input_data.get("current_state", {})
        
        # Get the current step
        current_step_idx = plan.current_step_index
        if current_step_idx is None or current_step_idx >= len(plan.steps):
            return {"status": "error", "message": "No current step to revise", "process_id": process_id}
        
        # Convert goal and state to tensors for neural processing
        goal_features = self._extract_features(plan.goal)
        state_features = self._extract_features(current_state)
        
        # Process through neural network to evaluate plan
        with torch.no_grad():
            planning_result = self.planning_network(
                goal=goal_features.to(self.device),
                state=state_features.to(self.device)
            )
        
        # Record activation
        self.neural_state.add_activation('planning', {
            'operation': 'revise',
            'goal': plan.goal,
            'revision_needed': planning_result["revision_needed"].cpu().item()
        })
        
        # Determine if revision is needed
        revision_needed = planning_result["revision_needed"].cpu().item() > self.params["revision_threshold"]
        
        if not revision_needed:
            return {
                "status": "success",
                "message": "No revision needed",
                "plan": plan.dict(),
                "process_id": process_id
            }
        
        # Create revised steps
        # Keep completed steps and revise remaining ones
        completed_steps = [step for step in plan.steps[:current_step_idx] if step.status == "completed"]
        
        # Generate new steps based on current state
        num_new_steps = min(
            self.params["max_steps"] - len(completed_steps),
            max(1, int(3 * self.development_level) + 1)
        )
        
        new_steps = []
        for i in range(num_new_steps):
            # Create more detailed steps at higher development levels
            action = self._generate_action_for_step(plan.goal, current_state, i, num_new_steps)
            outcome = self._generate_outcome_for_step(plan.goal, action, i, num_new_steps)
            
            # Create the step
            step = PlanStep(
                action=action,
                description=f"Revised Step {len(completed_steps) + i + 1}: {action}",
                expected_outcome=outcome,
                prerequisites=[s.step_id for s in completed_steps]  # Completed steps are prerequisites
            )
            new_steps.append(step)
        
        # Update the plan
        plan.steps = completed_steps + new_steps
        plan.current_step_index = len(completed_steps)
        plan.updated_at = datetime.now()
        
        # If any steps remain, next one should be in progress
        if plan.current_step_index < len(plan.steps):
            plan.steps[plan.current_step_index].status = "in_progress"
        
        # Update completion percentage
        completed_count = len(completed_steps)
        plan.completion_percentage = completed_count / len(plan.steps) if plan.steps else 0.0
        
        return {
            "status": "revised",
            "message": "Plan revised",
            "plan": plan.dict(),
            "process_id": process_id
        }
    
    def _query_plans(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Query plan information"""
        # Check if querying specific plan or all plans
        if "plan_id" in input_data:
            plan_id = input_data["plan_id"]
            if plan_id not in self.active_plans:
                return {"status": "error", "message": "Invalid plan ID", "process_id": process_id}
            
            return {
                "status": "success",
                "plan": self.active_plans[plan_id].dict(),
                "process_id": process_id
            }
        else:
            # Return all active plans
            return {
                "status": "success",
                "active_plans": {pid: plan.dict() for pid, plan in self.active_plans.items()},
                "plan_count": len(self.active_plans),
                "process_id": process_id
            }
    
    def _extract_features(self, data) -> torch.Tensor:
        """
        Extract features from input data for neural processing
        
        Args:
            data: Text, dict, or other data to extract features from
            
        Returns:
            Tensor of features [1, feature_dim]
        """
        # For demonstration, create simple random features
        # In a real implementation, this would use proper feature extraction
        feature_dim = 64
        
        if isinstance(data, str):
            # Seed random generator with hash of string to ensure consistent features
            seed = hash(data) % 10000
            np.random.seed(seed)
            
            # Generate "features" based on the text
            features = np.random.randn(feature_dim)
            features = features / np.linalg.norm(features)  # Normalize
            
        elif isinstance(data, dict):
            # For dictionary data, use keys and values to generate features
            seed = hash(str(sorted(data.items()))) % 10000
            np.random.seed(seed)
            
            features = np.random.randn(feature_dim)
            features = features / np.linalg.norm(features)  # Normalize
            
        else:
            # Default random features
            features = np.random.randn(feature_dim)
            features = features / np.linalg.norm(features)  # Normalize
        
        return torch.tensor(features, dtype=torch.float32).unsqueeze(0)
    
    def _generate_action_for_step(self, goal: str, state: Dict[str, Any], step_idx: int, total_steps: int) -> str:
        """Generate appropriate action text for a plan step"""
        # Simple action generation based on development level
        if self.development_level < 0.3:
            return f"Perform step {step_idx + 1} toward {goal}"
            
        elif self.development_level < 0.6:
            progress = (step_idx + 1) / total_steps
            if progress < 0.4:
                return f"Begin {goal} by taking initial actions"
            elif progress < 0.8:
                return f"Continue making progress toward {goal}"
            else:
                return f"Finalize actions to complete {goal}"
                
        else:
            # More specific actions at higher development levels
            progress = (step_idx + 1) / total_steps
            
            if progress < 0.3:
                return f"Initialize approach for {goal} by establishing prerequisites"
            elif progress < 0.6:
                return f"Implement core methods to achieve {goal} using appropriate techniques"
            elif progress < 0.9:
                return f"Integrate components and verify progress toward {goal}"
            else:
                return f"Finalize and validate completion of {goal}"
    
    def _generate_outcome_for_step(self, goal: str, action: str, step_idx: int, total_steps: int) -> str:
        """Generate expected outcome text for a plan step"""
        # Simple outcome generation based on development level
        progress = (step_idx + 1) / total_steps
        
        if self.development_level < 0.3:
            return f"Progress toward {goal}"
            
        elif self.development_level < 0.6:
            if progress < 0.4:
                return f"Initial progress established"
            elif progress < 0.8:
                return f"Substantial progress made toward {goal}"
            else:
                return f"Goal nearly achieved"
                
        else:
            # More specific outcomes at higher development levels
            if progress < 0.3:
                return f"Foundations established for achieving {goal}"
            elif progress < 0.6:
                return f"Core components implemented, {int(progress*100)}% progress toward {goal}"
            elif progress < 0.9:
                return f"Major milestones achieved, final steps for {goal} identified"
            else:
                return f"All requirements for {goal} completion in place"
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # Update base development level
        new_level = super().update_development(amount)
        
        # Update network development level
        self.planning_network.set_development_level(new_level)
        
        # Update neural state
        self.neural_state.planning_development = new_level
        self.neural_state.last_updated = datetime.now()
        
        # Adjust parameters based on new development level
        self._adjust_parameters_for_development()
        
        logger.info(f"Planning module development updated to {new_level:.2f}")
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the module
        
        Returns:
            Dictionary containing current module state
        """
        # Get base state from parent
        base_state = super().get_state()
        
        # Add planning-specific state
        planning_state = {
            "params": self.params,
            "active_plans": {pid: plan.dict() for pid, plan in self.active_plans.items()},
            "plan_count": len(self.active_plans),
            "recent_plan_ids": list(self.plan_history)
        }
        
        # Add neural state
        neural_state = {
            "development_level": self.neural_state.planning_development,
            "accuracy": self.neural_state.planning_accuracy,
            "recent_activations_count": len(self.neural_state.recent_planning_activations)
        }
        
        # Combine states
        combined_state = {**base_state, **planning_state, **neural_state}
        
        return combined_state


#######################

#executive\working_memory_control.py#
#######################

# TODO: Implement the WorkingMemoryControl class to manage working memory contents
# This component should be able to:
# - Maintain information in an active state
# - Update working memory contents as needed
# - Protect contents from interference
# - Manipulate and transform held information

# TODO: Implement developmental progression in working memory control:
# - Very limited capacity and duration in early stages
# - Gradual increase in capacity during childhood
# - Improved manipulation abilities in adolescence
# - Strategic working memory management in adulthood

# TODO: Create mechanisms for:
# - Maintenance: Keep information active through rehearsal
# - Updating: Replace old information with new when appropriate
# - Binding: Associate multiple pieces of information together
# - Manipulation: Transform or reorganize held information

# TODO: Implement capacity limitations:
# - Limit on number of items that can be held simultaneously
# - Limit on complexity of items based on developmental level
# - Trade-offs between maintenance and manipulation
# - Interference effects between similar items

# TODO: Connect to attention and consciousness systems
# Working memory should be influenced by attentional focus
# and should feed information to conscious awareness

import logging
import time
import uuid
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import numpy as np
import torch
from collections import deque

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.executive.models import WorkingMemoryItem, WorkingMemoryState, ExecutiveNeuralState
from lmm_project.modules.executive.neural_net import WorkingMemoryNetwork, get_device

# Initialize logger
logger = logging.getLogger(__name__)

class WorkingMemoryControl(BaseModule):
    """
    Manages the contents of working memory
    
    This module controls what information is maintained in an active state,
    updated, protected from interference, and manipulated.
    """
    
    # Development milestones
    development_milestones = {
        0.0: "Basic maintenance",
        0.2: "Improved capacity",
        0.4: "Active updating",
        0.6: "Information manipulation",
        0.8: "Strategic memory allocation",
        1.0: "Sophisticated working memory control"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the working memory control module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level of this module
        """
        super().__init__(
            module_id=module_id, 
            module_type="working_memory_control", 
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Initialize device
        self.device = get_device()
        
        # Initialize neural network
        self.memory_network = WorkingMemoryNetwork(
            item_dim=64,
            control_dim=32,
            hidden_dim=128,
            capacity=7  # Maximum theoretical capacity
        ).to(self.device)
        
        # Set development level for network
        self.memory_network.set_development_level(development_level)
        
        # Create neural state for tracking
        self.neural_state = ExecutiveNeuralState()
        self.neural_state.working_memory_development = development_level
        
        # Initialize working memory state
        self.memory_state = WorkingMemoryState(
            capacity=2 + int(5 * development_level),  # Capacity increases with development
            capacity_utilization=0.0,
            last_updated=datetime.now()
        )
        
        # Last decay update timestamp
        self.last_decay_update = time.time()
        
        # Working memory parameters
        self.params = {
            "activation_decay_rate": 0.1,  # How quickly items decay
            "retrieval_threshold": 0.2,  # Minimum activation to retrieve
            "interference_factor": 0.3,  # How much similar items interfere
            "rehearsal_boost": 0.5,  # Activation boost from rehearsal
            "manipulation_cost": 0.2  # Activation cost of manipulation
        }
        
        # Update parameters based on development
        self._adjust_parameters_for_development()
        
        logger.info(f"Working memory control module initialized at development level {development_level:.2f}")
    
    def _adjust_parameters_for_development(self):
        """Adjust working memory parameters based on developmental level"""
        if self.development_level < 0.2:
            # Very basic working memory at early stages
            self.params.update({
                "activation_decay_rate": 0.2,  # Faster decay
                "retrieval_threshold": 0.3,  # Higher threshold (harder to retrieve)
                "interference_factor": 0.5,  # More interference
                "rehearsal_boost": 0.3,  # Smaller boost from rehearsal
                "manipulation_cost": 0.4  # Higher cost for manipulation
            })
            
            # Update capacity in state
            self.memory_state.capacity = max(1, 2 + int(5 * self.development_level))
            
        elif self.development_level < 0.4:
            # Developing basic capacity
            self.params.update({
                "activation_decay_rate": 0.15,
                "retrieval_threshold": 0.25,
                "interference_factor": 0.4,
                "rehearsal_boost": 0.4,
                "manipulation_cost": 0.3
            })
            
            # Update capacity in state
            self.memory_state.capacity = max(2, 2 + int(5 * self.development_level))
            
        elif self.development_level < 0.6:
            # Improved updating capabilities
            self.params.update({
                "activation_decay_rate": 0.1,
                "retrieval_threshold": 0.2,
                "interference_factor": 0.3,
                "rehearsal_boost": 0.5,
                "manipulation_cost": 0.25
            })
            
            # Update capacity in state
            self.memory_state.capacity = max(3, 2 + int(5 * self.development_level))
            
        elif self.development_level < 0.8:
            # Developing manipulation abilities
            self.params.update({
                "activation_decay_rate": 0.08,
                "retrieval_threshold": 0.15,
                "interference_factor": 0.2,
                "rehearsal_boost": 0.6,
                "manipulation_cost": 0.2
            })
            
            # Update capacity in state
            self.memory_state.capacity = max(4, 2 + int(5 * self.development_level))
            
        else:
            # Strategic memory control
            self.params.update({
                "activation_decay_rate": 0.05,
                "retrieval_threshold": 0.1,
                "interference_factor": 0.1,
                "rehearsal_boost": 0.7,
                "manipulation_cost": 0.1
            })
            
            # Update capacity in state
            self.memory_state.capacity = max(6, 2 + int(5 * self.development_level))
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to control working memory
        
        Args:
            input_data: Dictionary containing working memory operations
                Required keys:
                - 'operation': The operation to perform ('store', 'retrieve', 'update', 'clear', 'query')
                For 'store' operation:
                - 'content': The content to store
                - 'content_type': Type of content (visual, verbal, spatial, etc.)
                For 'retrieve' operation:
                - 'item_id' or 'query': How to find the item (direct ID or search query)
                For 'update' operation:
                - 'item_id': ID of item to update
                - 'content': New content
                For 'clear' operation:
                - 'item_id' (optional): Specific item to clear, otherwise clear all
            
        Returns:
            Dictionary with the results of working memory control
        """
        # Update activation decay
        self._update_activations()
        
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        operation = input_data.get("operation", "query")
        
        # Different operations based on the request
        if operation == "store":
            return self._store_item(input_data, process_id)
        elif operation == "retrieve":
            return self._retrieve_item(input_data, process_id)
        elif operation == "update":
            return self._update_item(input_data, process_id)
        elif operation == "manipulate":
            return self._manipulate_item(input_data, process_id)
        elif operation == "clear":
            return self._clear_items(input_data, process_id)
        elif operation == "query":
            return self._query_memory(input_data, process_id)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "process_id": process_id
            }
    
    def _store_item(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Store a new item in working memory"""
        # Extract required data
        if "content" not in input_data:
            return {"status": "error", "message": "No content provided", "process_id": process_id}
        
        content = input_data.get("content")
        content_type = input_data.get("content_type", "general")
        tags = input_data.get("tags", [])
        
        # Check capacity
        if len(self.memory_state.items) >= self.memory_state.capacity:
            if self.development_level < 0.6:
                # At lower development, simply fail when capacity is reached
                return {
                    "status": "error",
                    "message": "Working memory capacity reached",
                    "capacity": self.memory_state.capacity,
                    "current_items": len(self.memory_state.items),
                    "process_id": process_id
                }
            else:
                # At higher development, remove least active item
                least_active_id = min(
                    self.memory_state.items.items(), 
                    key=lambda x: x[1].activation
                )[0]
                
                # Remove item
                del self.memory_state.items[least_active_id]
                
                # Log the replacement
                logger.info(f"Replaced least active item {least_active_id} to make room for new item")
        
        # Convert content to tensors for neural processing
        content_features = self._extract_features(content)
        
        # Create dummy control tensor (for store operation)
        control_features = torch.zeros((1, 32), dtype=torch.float32)
        
        # Process through neural network
        with torch.no_grad():
            store_result = self.memory_network(
                operation='store',
                items=content_features.to(self.device),
                control=control_features.to(self.device)
            )
        
        # Create new memory item
        item_id = str(uuid.uuid4())
        new_item = WorkingMemoryItem(
            item_id=item_id,
            content=content,
            content_type=content_type,
            activation=1.0,  # Start with maximum activation
            tags=tags,
            creation_time=datetime.now(),
            last_access=datetime.now(),
            access_count=1
        )
        
        # Store in working memory
        self.memory_state.items[item_id] = new_item
        
        # Update focus of attention
        self.memory_state.focus_of_attention = item_id
        self.memory_state.last_operation = "store"
        
        # Update utilization
        self.memory_state.capacity_utilization = len(self.memory_state.items) / self.memory_state.capacity
        
        # Update timestamp
        self.memory_state.last_updated = datetime.now()
        
        # Record activation in neural state
        self.neural_state.add_activation('working_memory', {
            'operation': 'store',
            'content_type': content_type,
            'success': store_result.get('store_success', torch.tensor([1.0])).item()
        })
        
        return {
            "status": "success",
            "item_id": item_id,
            "operation": "store",
            "memory_item": new_item.dict(),
            "capacity_utilization": self.memory_state.capacity_utilization,
            "process_id": process_id
        }
    
    def _retrieve_item(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Retrieve an item from working memory"""
        # Extract retrieval parameters
        item_id = input_data.get("item_id")
        query = input_data.get("query")
        
        if not item_id and not query:
            return {"status": "error", "message": "No item_id or query provided", "process_id": process_id}
        
        retrieved_item = None
        retrieval_method = "direct"
        
        if item_id:
            # Direct retrieval by ID
            if item_id in self.memory_state.items:
                retrieved_item = self.memory_state.items[item_id]
                
                # Check activation threshold
                if retrieved_item.activation < self.params["retrieval_threshold"]:
                    return {
                        "status": "forgotten",
                        "message": f"Item activation below threshold: {retrieved_item.activation:.2f}",
                        "item_id": item_id,
                        "activation": retrieved_item.activation,
                        "threshold": self.params["retrieval_threshold"],
                        "process_id": process_id
                    }
            else:
                return {
                    "status": "not_found",
                    "message": f"Item with ID {item_id} not found",
                    "process_id": process_id
                }
        else:
            # Query-based retrieval
            retrieval_method = "query"
            query_features = self._extract_features(query)
            
            # Create control tensor from query
            control_features = torch.zeros((1, 32), dtype=torch.float32)
            for i, val in enumerate(query_features.squeeze().tolist()):
                if i < 32:
                    control_features[0, i] = val
            
            # Process through neural network
            with torch.no_grad():
                retrieve_result = self.memory_network(
                    operation='retrieve',
                    control=control_features.to(self.device)
                )
            
            # Find the best matching item
            if self.memory_state.items:
                # Convert query to features
                query_features_np = query_features.cpu().numpy()
                
                # For each item, calculate similarity to query
                best_score = -1
                best_item_id = None
                
                for item_id, item in self.memory_state.items.items():
                    # Skip items below threshold
                    if item.activation < self.params["retrieval_threshold"]:
                        continue
                        
                    # Calculate similarity (simple cosine)
                    item_features = self._extract_features(item.content).cpu().numpy()
                    similarity = np.sum(query_features_np * item_features) / (
                        np.sqrt(np.sum(query_features_np ** 2)) * 
                        np.sqrt(np.sum(item_features ** 2))
                    )
                    
                    # Apply activation as a weight
                    weighted_similarity = similarity * item.activation
                    
                    if weighted_similarity > best_score:
                        best_score = weighted_similarity
                        best_item_id = item_id
                
                if best_item_id:
                    retrieved_item = self.memory_state.items[best_item_id]
                    item_id = best_item_id
                    
                    # Record the query match in neural state
                    self.neural_state.add_activation('working_memory', {
                        'operation': 'retrieve',
                        'method': 'query',
                        'similarity': best_score
                    })
            
            if not retrieved_item:
                return {
                    "status": "not_found",
                    "message": "No items matching query",
                    "query": query,
                    "process_id": process_id
                }
        
        # Update item data on successful retrieval
        retrieved_item.access_count += 1
        retrieved_item.last_access = datetime.now()
        
        # Boost activation from retrieval
        retrieved_item.activation = min(1.0, retrieved_item.activation + self.params["rehearsal_boost"])
        
        # Update focus of attention
        self.memory_state.focus_of_attention = item_id
        self.memory_state.last_operation = "retrieve"
        
        # Update timestamp
        self.memory_state.last_updated = datetime.now()
        
        # Record activation in neural state
        self.neural_state.add_activation('working_memory', {
            'operation': 'retrieve',
            'method': retrieval_method,
            'item_id': item_id,
            'content_type': retrieved_item.content_type,
            'activation': retrieved_item.activation
        })
        
        return {
            "status": "success",
            "item_id": item_id,
            "operation": "retrieve",
            "memory_item": retrieved_item.dict(),
            "process_id": process_id
        }
    
    def _update_item(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Update an existing item in working memory"""
        # Extract required data
        if "item_id" not in input_data:
            return {"status": "error", "message": "No item_id provided", "process_id": process_id}
        if "content" not in input_data:
            return {"status": "error", "message": "No content provided", "process_id": process_id}
        
        item_id = input_data.get("item_id")
        content = input_data.get("content")
        
        # Check if item exists
        if item_id not in self.memory_state.items:
            return {
                "status": "not_found",
                "message": f"Item with ID {item_id} not found",
                "process_id": process_id
            }
        
        # Get the existing item
        item = self.memory_state.items[item_id]
        
        # Check activation threshold
        if item.activation < self.params["retrieval_threshold"]:
            return {
                "status": "forgotten",
                "message": f"Item activation below threshold: {item.activation:.2f}",
                "item_id": item_id,
                "activation": item.activation,
                "threshold": self.params["retrieval_threshold"],
                "process_id": process_id
            }
        
        # Convert content to tensors for neural processing
        content_features = self._extract_features(content)
        
        # Create control tensor for item ID
        control_features = self._extract_features(item_id)
        control_features_resized = torch.zeros((1, 32), dtype=torch.float32)
        for i, val in enumerate(control_features.squeeze().tolist()):
            if i < 32:
                control_features_resized[0, i] = val
        
        # Process through neural network
        with torch.no_grad():
            update_result = self.memory_network(
                operation='update',
                items=content_features.to(self.device),
                control=control_features_resized.to(self.device)
            )
        
        # Update the item
        old_content = item.content
        item.content = content
        item.access_count += 1
        item.last_access = datetime.now()
        
        # Updating takes some activation resources
        item.activation = max(
            self.params["retrieval_threshold"],
            item.activation - self.params["manipulation_cost"]
        )
        
        # Update focus of attention
        self.memory_state.focus_of_attention = item_id
        self.memory_state.last_operation = "update"
        
        # Update timestamp
        self.memory_state.last_updated = datetime.now()
        
        # Record activation in neural state
        self.neural_state.add_activation('working_memory', {
            'operation': 'update',
            'item_id': item_id,
            'content_type': item.content_type,
            'activation': item.activation
        })
        
        return {
            "status": "success",
            "item_id": item_id,
            "operation": "update",
            "previous_content": old_content,
            "memory_item": item.dict(),
            "process_id": process_id
        }
    
    def _manipulate_item(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Transform or manipulate content in working memory"""
        # Only available at higher development levels
        if self.development_level < 0.6:
            return {
                "status": "undeveloped",
                "message": "Manipulation operations require higher development",
                "development_level": self.development_level,
                "required_level": 0.6,
                "process_id": process_id
            }
            
        # Extract required data
        if "item_id" not in input_data:
            return {"status": "error", "message": "No item_id provided", "process_id": process_id}
        if "operation_type" not in input_data:
            return {"status": "error", "message": "No operation_type provided", "process_id": process_id}
        
        item_id = input_data.get("item_id")
        operation_type = input_data.get("operation_type")
        parameters = input_data.get("parameters", {})
        
        # Check if item exists
        if item_id not in self.memory_state.items:
            return {
                "status": "not_found",
                "message": f"Item with ID {item_id} not found",
                "process_id": process_id
            }
        
        # Get the existing item
        item = self.memory_state.items[item_id]
        
        # Check activation threshold
        if item.activation < self.params["retrieval_threshold"]:
            return {
                "status": "forgotten",
                "message": f"Item activation below threshold: {item.activation:.2f}",
                "item_id": item_id,
                "activation": item.activation,
                "threshold": self.params["retrieval_threshold"],
                "process_id": process_id
            }
        
        # Manipulate based on operation type
        old_content = item.content
        manipulation_result = None
        
        try:
            if operation_type == "transform":
                # Apply a transformation to the content
                transform_type = parameters.get("transform_type", "")
                
                if isinstance(item.content, str):
                    if transform_type == "uppercase":
                        manipulation_result = item.content.upper()
                    elif transform_type == "lowercase":
                        manipulation_result = item.content.lower()
                    elif transform_type == "reverse":
                        manipulation_result = item.content[::-1]
                    else:
                        return {"status": "error", "message": f"Unknown transform_type: {transform_type}", "process_id": process_id}
                        
                elif isinstance(item.content, (list, tuple)):
                    if transform_type == "reverse":
                        manipulation_result = list(reversed(item.content))
                    elif transform_type == "sort":
                        manipulation_result = sorted(item.content)
                    else:
                        return {"status": "error", "message": f"Unknown transform_type: {transform_type}", "process_id": process_id}
                        
                elif isinstance(item.content, dict):
                    if transform_type == "keys":
                        manipulation_result = list(item.content.keys())
                    elif transform_type == "values":
                        manipulation_result = list(item.content.values())
                    else:
                        return {"status": "error", "message": f"Unknown transform_type: {transform_type}", "process_id": process_id}
                else:
                    return {"status": "error", "message": "Content type cannot be transformed", "process_id": process_id}
                    
            elif operation_type == "combine":
                # Combine with another item
                other_id = parameters.get("other_id")
                
                if not other_id or other_id not in self.memory_state.items:
                    return {"status": "error", "message": f"Other item {other_id} not found", "process_id": process_id}
                    
                other_item = self.memory_state.items[other_id]
                
                # Check other item activation
                if other_item.activation < self.params["retrieval_threshold"]:
                    return {
                        "status": "forgotten",
                        "message": f"Other item activation below threshold: {other_item.activation:.2f}",
                        "item_id": other_id,
                        "process_id": process_id
                    }
                
                # Combine based on content types
                if isinstance(item.content, str) and isinstance(other_item.content, str):
                    manipulation_result = item.content + " " + other_item.content
                    
                elif isinstance(item.content, (list, tuple)) and isinstance(other_item.content, (list, tuple)):
                    manipulation_result = list(item.content) + list(other_item.content)
                    
                elif isinstance(item.content, dict) and isinstance(other_item.content, dict):
                    manipulation_result = {**item.content, **other_item.content}
                    
                else:
                    manipulation_result = [item.content, other_item.content]
                
                # Also update other item's activation and access count
                other_item.access_count += 1
                other_item.last_access = datetime.now()
                other_item.activation = max(
                    self.params["retrieval_threshold"],
                    other_item.activation - self.params["manipulation_cost"]
                )
                
            else:
                return {"status": "error", "message": f"Unknown operation_type: {operation_type}", "process_id": process_id}
                
            # Update the item with manipulation result
            item.content = manipulation_result
            item.access_count += 1
            item.last_access = datetime.now()
            
            # Manipulation takes significant activation resources
            item.activation = max(
                self.params["retrieval_threshold"],
                item.activation - self.params["manipulation_cost"] * 1.5
            )
            
            # Update focus of attention
            self.memory_state.focus_of_attention = item_id
            self.memory_state.last_operation = f"manipulate_{operation_type}"
            
            # Update timestamp
            self.memory_state.last_updated = datetime.now()
            
            # Record activation in neural state
            self.neural_state.add_activation('working_memory', {
                'operation': 'manipulate',
                'sub_operation': operation_type,
                'item_id': item_id,
                'content_type': item.content_type,
                'activation': item.activation
            })
            
            return {
                "status": "success",
                "item_id": item_id,
                "operation": "manipulate",
                "operation_type": operation_type,
                "previous_content": old_content,
                "memory_item": item.dict(),
                "process_id": process_id
            }
            
        except Exception as e:
            return {
                "status": "error",
                "message": f"Manipulation failed: {str(e)}",
                "operation_type": operation_type,
                "process_id": process_id
            }
    
    def _clear_items(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Clear items from working memory"""
        item_id = input_data.get("item_id")
        
        if item_id:
            # Clear specific item
            if item_id in self.memory_state.items:
                deleted_item = self.memory_state.items[item_id]
                del self.memory_state.items[item_id]
                
                # Update focus if needed
                if self.memory_state.focus_of_attention == item_id:
                    self.memory_state.focus_of_attention = None
                
                # Update utilization
                self.memory_state.capacity_utilization = len(self.memory_state.items) / self.memory_state.capacity
                
                # Update timestamp
                self.memory_state.last_updated = datetime.now()
                self.memory_state.last_operation = "clear_item"
                
                # Record activation in neural state
                self.neural_state.add_activation('working_memory', {
                    'operation': 'clear',
                    'item_id': item_id,
                    'content_type': deleted_item.content_type
                })
                
                return {
                    "status": "success",
                    "operation": "clear",
                    "item_id": item_id,
                    "message": f"Item {item_id} cleared",
                    "process_id": process_id
                }
            else:
                return {
                    "status": "not_found",
                    "message": f"Item with ID {item_id} not found",
                    "process_id": process_id
                }
        else:
            # Clear all items
            item_count = len(self.memory_state.items)
            self.memory_state.items = {}
            self.memory_state.focus_of_attention = None
            self.memory_state.capacity_utilization = 0.0
            
            # Update timestamp
            self.memory_state.last_updated = datetime.now()
            self.memory_state.last_operation = "clear_all"
            
            # Process through neural network (dummy call for clear operation)
            with torch.no_grad():
                self.memory_network(operation='clear')
            
            # Record activation in neural state
            self.neural_state.add_activation('working_memory', {
                'operation': 'clear_all',
                'item_count': item_count
            })
            
            return {
                "status": "success",
                "operation": "clear",
                "message": f"All {item_count} items cleared",
                "process_id": process_id
            }
    
    def _query_memory(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Query information about working memory"""
        query_type = input_data.get("query_type", "state")
        
        if query_type == "state":
            # Basic state information
            return {
                "status": "success",
                "operation": "query",
                "query_type": "state",
                "capacity": self.memory_state.capacity,
                "capacity_utilization": self.memory_state.capacity_utilization,
                "item_count": len(self.memory_state.items),
                "focus_of_attention": self.memory_state.focus_of_attention,
                "last_operation": self.memory_state.last_operation,
                "process_id": process_id
            }
            
        elif query_type == "items":
            # List all items
            return {
                "status": "success",
                "operation": "query",
                "query_type": "items",
                "items": {id: item.dict() for id, item in self.memory_state.items.items()},
                "item_count": len(self.memory_state.items),
                "process_id": process_id
            }
            
        elif query_type == "activation":
            # Get activation levels
            return {
                "status": "success",
                "operation": "query",
                "query_type": "activation",
                "activations": {id: item.activation for id, item in self.memory_state.items.items()},
                "threshold": self.params["retrieval_threshold"],
                "process_id": process_id
            }
            
        else:
            # Full state
            return {
                "status": "success",
                "operation": "query",
                "working_memory_state": self.memory_state.dict(),
                "working_memory_params": self.params,
                "development_level": self.development_level,
                "process_id": process_id
            }
    
    def _update_activations(self):
        """Update activation decay based on elapsed time"""
        current_time = time.time()
        elapsed_seconds = current_time - self.last_decay_update
        
        if elapsed_seconds > 0.1 and self.memory_state.items:  # Only update if enough time has passed
            # Calculate decay amount
            decay_amount = self.params["activation_decay_rate"] * elapsed_seconds
            
            # Apply decay to all items
            decayed = False
            for item in self.memory_state.items.values():
                old_activation = item.activation
                item.activation = max(0.0, item.activation - decay_amount)
                
                if abs(old_activation - item.activation) > 0.01:
                    decayed = True
            
            # Update timestamp
            self.last_decay_update = current_time
            
            # Update state timestamp if activations changed significantly
            if decayed:
                self.memory_state.last_updated = datetime.now()
    
    def _extract_features(self, data) -> torch.Tensor:
        """
        Extract features from input data for neural processing
        
        Args:
            data: Text, dict, or other data to extract features from
            
        Returns:
            Tensor of features [1, feature_dim]
        """
        # For demonstration, create simple random features
        # In a real implementation, this would use proper feature extraction
        feature_dim = 64
        
        if isinstance(data, str):
            # Seed random generator with hash of string to ensure consistent features
            seed = hash(data) % 10000
            np.random.seed(seed)
            
            # Generate "features" based on the text
            features = np.random.randn(feature_dim)
            features = features / np.linalg.norm(features)  # Normalize
            
        elif isinstance(data, dict):
            # For dictionary data, use keys and values to generate features
            seed = hash(str(sorted(data.items()))) % 10000
            np.random.seed(seed)
            
            features = np.random.randn(feature_dim)
            features = features / np.linalg.norm(features)  # Normalize
            
        elif isinstance(data, (list, tuple)):
            # For list/tuple data
            seed = hash(str(data)) % 10000
            np.random.seed(seed)
            
            features = np.random.randn(feature_dim)
            features = features / np.linalg.norm(features)  # Normalize
            
        else:
            # Default random features
            seed = hash(str(data)) % 10000
            np.random.seed(seed)
            
            features = np.random.randn(feature_dim)
            features = features / np.linalg.norm(features)  # Normalize
        
        return torch.tensor(features, dtype=torch.float32).unsqueeze(0)
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # Update base development level
        new_level = super().update_development(amount)
        
        # Update network development level
        self.memory_network.set_development_level(new_level)
        
        # Update neural state
        self.neural_state.working_memory_development = new_level
        self.neural_state.last_updated = datetime.now()
        
        # Adjust parameters based on new development level
        self._adjust_parameters_for_development()
        
        logger.info(f"Working memory control module development updated to {new_level:.2f}")
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the module
        
        Returns:
            Dictionary containing current module state
        """
        # Update activations first
        self._update_activations()
        
        # Get base state from parent
        base_state = super().get_state()
        
        # Add memory-specific state
        memory_state_dict = self.memory_state.dict()
        
        # Add neural state
        neural_state = {
            "development_level": self.neural_state.working_memory_development,
            "accuracy": self.neural_state.working_memory_accuracy,
            "recent_activations_count": len(self.neural_state.recent_working_memory_activations)
        }
        
        # Combine states
        combined_state = {
            **base_state, 
            **memory_state_dict, 
            "params": self.params,
            "neural_state": neural_state
        }
        
        return combined_state


#######################

#executive\__init__.py#
#######################

# Executive module 

# TODO: Implement the executive module factory function to return an integrated ExecutiveSystem
# This module should be responsible for planning, decision-making, inhibition,
# cognitive control, and working memory management.

# TODO: Create ExecutiveSystem class that integrates all executive sub-components:
# - planning: develops and executes plans to achieve goals
# - decision_making: evaluates options and makes choices
# - inhibition: suppresses inappropriate actions and thoughts
# - working_memory_control: manages contents of working memory

# TODO: Implement development tracking for executive function
# Executive capabilities should develop from minimal control in early stages
# to sophisticated planning and self-regulation in later stages

# TODO: Connect executive module to attention, consciousness, and motivation modules
# Executive function should direct attention resources, be influenced by
# conscious goals, and be driven by motivational priorities

# TODO: Implement resource management for executive functions
# The system should have limited executive resources that must be
# allocated efficiently across different control demands

from typing import Optional, Dict, Any, List, Tuple, Union
import uuid
import logging
from datetime import datetime

from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.base_module import BaseModule
from lmm_project.modules.executive.planning import Planning
from lmm_project.modules.executive.decision_making import DecisionMaking
from lmm_project.modules.executive.inhibition import Inhibition
from lmm_project.modules.executive.working_memory_control import WorkingMemoryControl
from lmm_project.modules.executive.models import ExecutiveSystemState, ExecutiveParameters, ExecutiveNeuralState, Plan, Decision

# Initialize logger
logger = logging.getLogger(__name__)

class ExecutiveSystem(BaseModule):
    """
    Coordinates executive functions for high-level cognitive control
    
    The executive system integrates planning, decision-making, inhibition,
    and working memory control to manage goal-directed behavior.
    """
    
    # Development milestones
    development_milestones = {
        0.0: "Basic reflexive control",
        0.2: "Simple goal-directed actions", 
        0.4: "Basic planning and inhibition",
        0.6: "Integrated executive functions",
        0.8: "Strategic planning and control",
        1.0: "Sophisticated executive functions"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the executive system
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level of this module
        """
        super().__init__(
            module_id=module_id, 
            module_type="executive", 
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Create sub-modules with appropriate IDs
        base_id = module_id.split('.')[-1] if '.' in module_id else module_id
        
        self.planning = Planning(
            module_id=f"{base_id}.planning", 
            event_bus=event_bus,
            development_level=development_level
        )
        
        self.decision_making = DecisionMaking(
            module_id=f"{base_id}.decision", 
            event_bus=event_bus,
            development_level=development_level
        )
        
        self.inhibition = Inhibition(
            module_id=f"{base_id}.inhibition", 
            event_bus=event_bus,
            development_level=development_level
        )
        
        self.working_memory = WorkingMemoryControl(
            module_id=f"{base_id}.working_memory", 
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Create system state
        self.system_state = ExecutiveSystemState(
            module_id=module_id,
            developmental_level=development_level,
            parameters=ExecutiveParameters(),
            neural_state=ExecutiveNeuralState()
        )
        
        # Subscribe to events if event bus is provided
        if self.event_bus:
            self._subscribe_to_events()
        
        logger.info(f"Executive system initialized at development level {development_level:.2f}")
    
    def _subscribe_to_events(self):
        """Subscribe to relevant events from other modules"""
        # Subscribe to attention focus events
        self.event_bus.subscribe(
            "attention.focus_change",
            self._handle_attention_event
        )
        
        # Subscribe to emotion events for emotional regulation
        self.event_bus.subscribe(
            "emotion.state_change",
            self._handle_emotion_event
        )
        
        # Subscribe to perception events
        self.event_bus.subscribe(
            "perception.new_input",
            self._handle_perception_event
        )
        
        # Subscribe to memory events
        self.event_bus.subscribe(
            "memory.retrieval",
            self._handle_memory_event
        )
    
    def _handle_attention_event(self, message: Message):
        """Handle attention focus change events"""
        # Focus changes may require inhibition or working memory updates
        if self.development_level >= 0.4:
            # Extract focus data
            focus_data = message.content
            
            # Update working memory with new focus if appropriate
            if "focus_content" in focus_data and focus_data.get("focus_strength", 0) > 0.5:
                self.working_memory.process_input({
                    "operation": "store",
                    "content": focus_data["focus_content"],
                    "content_type": "attention_focus",
                    "tags": ["attention_focus"]
                })
    
    def _handle_emotion_event(self, message: Message):
        """Handle emotion state change events"""
        # Emotions can influence decision making and may require inhibition
        if self.development_level >= 0.4:
            emotion_data = message.content
            
            # If strong negative emotion, may need inhibitory control
            if emotion_data.get("valence", 0) < -0.7 and emotion_data.get("intensity", 0) > 0.7:
                self.inhibition.process_input({
                    "operation": "apply",
                    "stimulus": "emotional_response",
                    "context": {
                        "emotion_type": emotion_data.get("emotion_type", "unknown"),
                        "intensity": emotion_data.get("intensity", 0),
                        "valence": emotion_data.get("valence", 0)
                    }
                })
    
    def _handle_perception_event(self, message: Message):
        """Handle new perception input events"""
        # New perceptions may trigger planning or decision making
        perception_data = message.content
        
        # Store relevant perceptions in working memory
        if perception_data.get("importance", 0) > 0.6:
            self.working_memory.process_input({
                "operation": "store",
                "content": perception_data.get("content", {}),
                "content_type": "perception",
                "tags": ["perception", perception_data.get("modality", "unknown")]
            })
    
    def _handle_memory_event(self, message: Message):
        """Handle memory retrieval events"""
        # Retrieved memories may provide context for decisions or plans
        memory_data = message.content
        
        # Store retrieved memories in working memory if relevant
        if memory_data.get("relevance", 0) > 0.7:
            self.working_memory.process_input({
                "operation": "store",
                "content": memory_data.get("content", {}),
                "content_type": "memory_retrieval",
                "tags": ["memory", memory_data.get("memory_type", "unknown")]
            })
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to the executive system
        
        Args:
            input_data: Dictionary containing operation and parameters
                Required keys:
                - 'operation': The operation to perform
                  Options: 'plan', 'decide', 'inhibit', 'working_memory', 'status'
                
                For 'plan' operation, forwards to planning module
                For 'decide' operation, forwards to decision_making module
                For 'inhibit' operation, forwards to inhibition module
                For 'working_memory' operation, forwards to working_memory module
                For 'status' operation, returns overall executive system status
                
        Returns:
            Dictionary with the results of processing
        """
        operation = input_data.get("operation", "status")
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Process based on operation
        if operation == "plan":
            result = self.planning.process_input(input_data)
            
            # Update system state with new or updated plans
            if result.get("status") == "success" and "plan" in result:
                plan = result["plan"]
                self.system_state.add_plan(plan)
                
            return result
            
        elif operation == "decide":
            result = self.decision_making.process_input(input_data)
            
            # Update system state with new decisions
            if result.get("status") == "success" and "decision" in result:
                decision = result["decision"]
                self.system_state.add_decision(decision)
                
            return result
            
        elif operation == "inhibit":
            return self.inhibition.process_input(input_data)
            
        elif operation == "working_memory":
            return self.working_memory.process_input(input_data)
            
        elif operation == "status":
            # Return overall executive system status
            return {
                "status": "success",
                "process_id": process_id,
                "executive_system": {
                    "development_level": self.development_level,
                    "active_plans_count": len(self.system_state.active_plans),
                    "recent_decisions_count": len(self.system_state.recent_decisions),
                    "working_memory_utilization": self.working_memory.get_state().get("capacity_utilization", 0),
                    "inhibition_resources": self.inhibition.get_state().get("available_resources", 1.0)
                },
                "sub_modules": {
                    "planning": self.planning.get_state(),
                    "decision_making": self.decision_making.get_state(),
                    "inhibition": self.inhibition.get_state(),
                    "working_memory": self.working_memory.get_state()
                }
            }
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "process_id": process_id
            }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module and its submodules
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # Update base development level
        new_level = super().update_development(amount)
        
        # Update submodules
        self.planning.update_development(amount)
        self.decision_making.update_development(amount)
        self.inhibition.update_development(amount)
        self.working_memory.update_development(amount)
        
        # Update system state
        self.system_state.developmental_level = new_level
        self.system_state.last_updated = datetime.now()
        
        logger.info(f"Executive system development updated to {new_level:.2f}")
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the module
        
        Returns:
            Dictionary containing current module state
        """
        # Get base state from parent
        base_state = super().get_state()
        
        # Update system state with current states from submodules
        planning_state = self.planning.get_state()
        decision_state = self.decision_making.get_state()
        inhibition_state = self.inhibition.get_state()
        working_memory_state = self.working_memory.get_state()
        
        # Update system state
        self.system_state.planning_state = planning_state
        self.system_state.decision_state = decision_state
        self.system_state.inhibition_state = inhibition_state.get("inhibition_state", {})
        self.system_state.working_memory_state = working_memory_state.get("working_memory_state", {})
        
        # Combine states
        combined_state = {
            **base_state,
            "system_state": self.system_state.dict(),
            "submodules": {
                "planning": planning_state,
                "decision_making": decision_state,
                "inhibition": inhibition_state,
                "working_memory": working_memory_state
            }
        }
        
        return combined_state


def get_module(module_id: str, event_bus: Optional[EventBus] = None) -> Any:
    """
    Factory function to create an executive function module.
    
    The executive system is responsible for:
    - Planning and executing multi-step behaviors
    - Making decisions between alternative options
    - Inhibiting inappropriate actions and thoughts
    - Managing the contents of working memory
    
    Args:
        module_id: Identifier for the module to create
        event_bus: Event bus for inter-module communication
        
    Returns:
        An instance of ExecutiveSystem or one of its components
    """
    # Check if requesting a specific submodule
    if "." in module_id:
        base_id, submodule = module_id.split(".", 1)
        
        if submodule == "planning":
            return Planning(module_id=module_id, event_bus=event_bus)
        elif submodule == "decision" or submodule == "decision_making":
            return DecisionMaking(module_id=module_id, event_bus=event_bus)
        elif submodule == "inhibition":
            return Inhibition(module_id=module_id, event_bus=event_bus)
        elif submodule == "working_memory":
            return WorkingMemoryControl(module_id=module_id, event_bus=event_bus)
    
    # Default to returning the integrated executive system
    return ExecutiveSystem(module_id=module_id, event_bus=event_bus)


#######################

#identity\models.py#
#######################

from typing import Dict, List, Any, Optional, Set, Tuple
from datetime import datetime
import uuid
from pydantic import BaseModel, Field

class SelfAttribute(BaseModel):
    """
    Represents a single attribute of the self-concept
    
    Attributes include beliefs about the self in various domains
    """
    attribute_id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="Unique identifier for this attribute")
    domain: str = Field(..., description="Domain of this attribute (e.g., 'physical', 'academic', 'social')")
    content: str = Field(..., description="Content of the self-attribute")
    confidence: float = Field(0.5, ge=0.0, le=1.0, description="Confidence in this self-belief")
    importance: float = Field(0.5, ge=0.0, le=1.0, description="Personal importance of this attribute")
    valence: float = Field(0.0, ge=-1.0, le=1.0, description="Emotional valence of this attribute")
    evidence: List[str] = Field(default_factory=list, description="Supporting evidence for this attribute")
    sources: List[str] = Field(default_factory=list, description="Sources of this belief (e.g., 'personal experience', 'social feedback')")
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert to dictionary, preserving datetime objects"""
        result = super().dict(*args, **kwargs)
        result["created_at"] = self.created_at
        result["updated_at"] = self.updated_at
        return result

class SelfConcept(BaseModel):
    """
    Represents the overall self-concept
    
    The self-concept is the collection of beliefs about oneself
    """
    attributes: Dict[str, SelfAttribute] = Field(default_factory=dict, description="Self-attributes by ID")
    domains: Dict[str, List[str]] = Field(default_factory=dict, description="Attribute IDs organized by domain")
    global_self_esteem: float = Field(0.5, ge=0.0, le=1.0, description="Overall evaluation of self-worth")
    clarity: float = Field(0.5, ge=0.0, le=1.0, description="Clarity and coherence of self-concept")
    stability: float = Field(0.5, ge=0.0, le=1.0, description="Stability of self-concept over time")
    complexity: float = Field(0.5, ge=0.0, le=1.0, description="Complexity and differentiation of self-concept")
    last_updated: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert to dictionary, preserving datetime objects"""
        result = super().dict(*args, **kwargs)
        result["last_updated"] = self.last_updated
        return result
    
    def add_attribute(self, attribute: SelfAttribute) -> None:
        """
        Add an attribute to the self-concept
        
        Args:
            attribute: The attribute to add
        """
        self.attributes[attribute.attribute_id] = attribute
        
        # Add to domain mapping
        if attribute.domain not in self.domains:
            self.domains[attribute.domain] = []
        
        if attribute.attribute_id not in self.domains[attribute.domain]:
            self.domains[attribute.domain].append(attribute.attribute_id)
            
        self.last_updated = datetime.now()

class NarrativeEvent(BaseModel):
    """
    Represents a significant event in the personal narrative
    
    Events include important personal experiences and their interpretation
    """
    event_id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="Unique identifier for this event")
    title: str = Field(..., description="Brief title for this event")
    description: str = Field(..., description="Description of what happened")
    interpretation: str = Field("", description="Personal meaning or interpretation of the event")
    emotional_impact: Dict[str, float] = Field(default_factory=dict, description="Emotional impact (emotion: intensity)")
    importance: float = Field(0.5, ge=0.0, le=1.0, description="Subjective importance of this event")
    age_period: str = Field(..., description="Life period when this event occurred (e.g., 'early childhood')")
    themes: List[str] = Field(default_factory=list, description="Themes associated with this event")
    related_events: List[str] = Field(default_factory=list, description="IDs of related events")
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert to dictionary, preserving datetime objects"""
        result = super().dict(*args, **kwargs)
        result["created_at"] = self.created_at
        result["updated_at"] = self.updated_at
        return result

class NarrativeTheme(BaseModel):
    """
    Represents a recurring theme in the personal narrative
    
    Themes connect events through common patterns or meanings
    """
    theme_id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="Unique identifier for this theme")
    name: str = Field(..., description="Name of this theme")
    description: str = Field(..., description="Description of what this theme represents")
    events: List[str] = Field(default_factory=list, description="IDs of events associated with this theme")
    emotional_tone: float = Field(0.0, ge=-1.0, le=1.0, description="Overall emotional tone of this theme")
    importance: float = Field(0.5, ge=0.0, le=1.0, description="Subjective importance of this theme")
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert to dictionary, preserving datetime objects"""
        result = super().dict(*args, **kwargs)
        result["created_at"] = self.created_at
        result["updated_at"] = self.updated_at
        return result

class PersonalNarrative(BaseModel):
    """
    Represents the autobiographical narrative
    
    The personal narrative is the story one constructs about oneself
    """
    events: Dict[str, NarrativeEvent] = Field(default_factory=dict, description="Narrative events by ID")
    themes: Dict[str, NarrativeTheme] = Field(default_factory=dict, description="Narrative themes by ID")
    life_periods: Dict[str, List[str]] = Field(default_factory=dict, description="Event IDs organized by life period")
    coherence: float = Field(0.5, ge=0.0, le=1.0, description="Narrative coherence and integration")
    emotional_tone: float = Field(0.0, ge=-1.0, le=1.0, description="Overall emotional tone of the narrative")
    agency: float = Field(0.5, ge=0.0, le=1.0, description="Sense of personal agency in the narrative")
    last_updated: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert to dictionary, preserving datetime objects"""
        result = super().dict(*args, **kwargs)
        result["last_updated"] = self.last_updated
        return result
    
    def add_event(self, event: NarrativeEvent) -> None:
        """
        Add an event to the personal narrative
        
        Args:
            event: The event to add
        """
        self.events[event.event_id] = event
        
        # Add to life period mapping
        if event.age_period not in self.life_periods:
            self.life_periods[event.age_period] = []
        
        if event.event_id not in self.life_periods[event.age_period]:
            self.life_periods[event.age_period].append(event.event_id)
        
        # Update related themes
        for theme_id in event.themes:
            if theme_id in self.themes:
                if event.event_id not in self.themes[theme_id].events:
                    theme = self.themes[theme_id]
                    theme.events.append(event.event_id)
                    theme.updated_at = datetime.now()
                    self.themes[theme_id] = theme
            
        self.last_updated = datetime.now()
    
    def add_theme(self, theme: NarrativeTheme) -> None:
        """
        Add a theme to the personal narrative
        
        Args:
            theme: The theme to add
        """
        self.themes[theme.theme_id] = theme
        
        # Update related events
        for event_id in theme.events:
            if event_id in self.events:
                if theme.theme_id not in self.events[event_id].themes:
                    event = self.events[event_id]
                    event.themes.append(theme.theme_id)
                    event.updated_at = datetime.now()
                    self.events[event_id] = event
        
        self.last_updated = datetime.now()

class Preference(BaseModel):
    """
    Represents a preference for or against something
    
    Preferences include likes, dislikes, and value judgments
    """
    preference_id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="Unique identifier for this preference")
    domain: str = Field(..., description="Domain of this preference (e.g., 'food', 'music', 'activities')")
    target: str = Field(..., description="Target of the preference")
    valence: float = Field(0.0, ge=-1.0, le=1.0, description="Degree of liking/disliking")
    strength: float = Field(0.5, ge=0.0, le=1.0, description="Strength of the preference")
    certainty: float = Field(0.5, ge=0.0, le=1.0, description="Certainty of the preference")
    reasons: List[str] = Field(default_factory=list, description="Reasons for this preference")
    related_experiences: List[str] = Field(default_factory=list, description="Related experiences that shaped this preference")
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert to dictionary, preserving datetime objects"""
        result = super().dict(*args, **kwargs)
        result["created_at"] = self.created_at
        result["updated_at"] = self.updated_at
        return result

class Value(BaseModel):
    """
    Represents a personal value
    
    Values are abstract principles or qualities that are important to the individual
    """
    value_id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="Unique identifier for this value")
    name: str = Field(..., description="Name of this value")
    description: str = Field(..., description="Description of what this value represents")
    importance: float = Field(0.5, ge=0.0, le=1.0, description="Importance of this value")
    related_preferences: List[str] = Field(default_factory=list, description="IDs of preferences related to this value")
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert to dictionary, preserving datetime objects"""
        result = super().dict(*args, **kwargs)
        result["created_at"] = self.created_at
        result["updated_at"] = self.updated_at
        return result

class PreferenceSystem(BaseModel):
    """
    Represents the system of preferences and values
    
    The preference system organizes likes, dislikes, and values
    """
    preferences: Dict[str, Preference] = Field(default_factory=dict, description="Preferences by ID")
    values: Dict[str, Value] = Field(default_factory=dict, description="Values by ID")
    domains: Dict[str, List[str]] = Field(default_factory=dict, description="Preference IDs organized by domain")
    consistency: float = Field(0.5, ge=0.0, le=1.0, description="Consistency of preferences across domains")
    value_hierarchy: List[str] = Field(default_factory=list, description="Value IDs in order of importance")
    last_updated: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert to dictionary, preserving datetime objects"""
        result = super().dict(*args, **kwargs)
        result["last_updated"] = self.last_updated
        return result
    
    def add_preference(self, preference: Preference) -> None:
        """
        Add a preference to the system
        
        Args:
            preference: The preference to add
        """
        self.preferences[preference.preference_id] = preference
        
        # Add to domain mapping
        if preference.domain not in self.domains:
            self.domains[preference.domain] = []
        
        if preference.preference_id not in self.domains[preference.domain]:
            self.domains[preference.domain].append(preference.preference_id)
            
        self.last_updated = datetime.now()
    
    def add_value(self, value: Value) -> None:
        """
        Add a value to the system
        
        Args:
            value: The value to add
        """
        self.values[value.value_id] = value
        
        # Update value hierarchy
        if value.value_id not in self.value_hierarchy:
            # Insert based on importance
            for i, v_id in enumerate(self.value_hierarchy):
                if self.values[v_id].importance < value.importance:
                    self.value_hierarchy.insert(i, value.value_id)
                    break
            else:
                self.value_hierarchy.append(value.value_id)
                
        self.last_updated = datetime.now()

class PersonalityTrait(BaseModel):
    """
    Represents a personality trait
    
    Traits are stable patterns of thinking, feeling, and behaving
    """
    trait_id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="Unique identifier for this trait")
    name: str = Field(..., description="Name of this trait")
    description: str = Field(..., description="Description of what this trait represents")
    score: float = Field(..., ge=0.0, le=1.0, description="Strength of this trait")
    stability: float = Field(0.5, ge=0.0, le=1.0, description="Stability of this trait over time")
    behavioral_instances: List[str] = Field(default_factory=list, description="Examples of behaviors reflecting this trait")
    opposing_trait: Optional[str] = Field(None, description="ID of the opposing trait, if any")
    dimension: Optional[str] = Field(None, description="Dimension this trait belongs to (e.g., 'extraversion')")
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert to dictionary, preserving datetime objects"""
        result = super().dict(*args, **kwargs)
        result["created_at"] = self.created_at
        result["updated_at"] = self.updated_at
        return result

class TraitDimension(BaseModel):
    """
    Represents a dimension along which traits vary
    
    Dimensions organize traits into bipolar continuums
    """
    dimension_id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="Unique identifier for this dimension")
    name: str = Field(..., description="Name of this dimension")
    description: str = Field(..., description="Description of what this dimension represents")
    positive_pole: str = Field(..., description="Description of the high end of this dimension")
    negative_pole: str = Field(..., description="Description of the low end of this dimension")
    traits: List[str] = Field(default_factory=list, description="IDs of traits along this dimension")
    score: float = Field(0.5, ge=0.0, le=1.0, description="Overall score on this dimension")
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert to dictionary, preserving datetime objects"""
        result = super().dict(*args, **kwargs)
        result["created_at"] = self.created_at
        result["updated_at"] = self.updated_at
        return result

class PersonalityProfile(BaseModel):
    """
    Represents the overall personality profile
    
    The personality profile organizes traits into a coherent whole
    """
    traits: Dict[str, PersonalityTrait] = Field(default_factory=dict, description="Traits by ID")
    dimensions: Dict[str, TraitDimension] = Field(default_factory=dict, description="Trait dimensions by ID")
    stability: float = Field(0.5, ge=0.0, le=1.0, description="Overall stability of personality")
    differentiation: float = Field(0.5, ge=0.0, le=1.0, description="Degree of differentiation among traits")
    integration: float = Field(0.5, ge=0.0, le=1.0, description="Degree of integration among traits")
    last_updated: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert to dictionary, preserving datetime objects"""
        result = super().dict(*args, **kwargs)
        result["last_updated"] = self.last_updated
        return result
    
    def add_trait(self, trait: PersonalityTrait) -> None:
        """
        Add a trait to the profile
        
        Args:
            trait: The trait to add
        """
        self.traits[trait.trait_id] = trait
        
        # Update dimension if applicable
        if trait.dimension and trait.dimension in self.dimensions:
            dimension = self.dimensions[trait.dimension]
            if trait.trait_id not in dimension.traits:
                dimension.traits.append(trait.trait_id)
                dimension.updated_at = datetime.now()
                
                # Recalculate dimension score
                dimension_traits = [self.traits[t] for t in dimension.traits if t in self.traits]
                if dimension_traits:
                    dimension.score = sum(t.score for t in dimension_traits) / len(dimension_traits)
                    
                self.dimensions[trait.dimension] = dimension
                
        self.last_updated = datetime.now()
    
    def add_dimension(self, dimension: TraitDimension) -> None:
        """
        Add a dimension to the profile
        
        Args:
            dimension: The dimension to add
        """
        self.dimensions[dimension.dimension_id] = dimension
        
        # Update traits
        for trait_id in dimension.traits:
            if trait_id in self.traits:
                trait = self.traits[trait_id]
                trait.dimension = dimension.dimension_id
                trait.updated_at = datetime.now()
                self.traits[trait_id] = trait
                
        self.last_updated = datetime.now()

class IdentityState(BaseModel):
    """
    Represents the overall state of identity
    
    Identity integrates self-concept, narrative, preferences, and personality
    """
    self_concept: SelfConcept = Field(default_factory=SelfConcept, description="Current self-concept")
    personal_narrative: PersonalNarrative = Field(default_factory=PersonalNarrative, description="Personal narrative")
    preference_system: PreferenceSystem = Field(default_factory=PreferenceSystem, description="Preferences and values")
    personality_profile: PersonalityProfile = Field(default_factory=PersonalityProfile, description="Personality traits")
    
    identity_integration: float = Field(0.5, ge=0.0, le=1.0, description="Integration across identity components")
    identity_clarity: float = Field(0.5, ge=0.0, le=1.0, description="Clarity and definition of identity")
    identity_stability: float = Field(0.5, ge=0.0, le=1.0, description="Stability of identity over time")
    
    module_id: str = Field(..., description="Module identifier")
    developmental_level: float = Field(0.0, ge=0.0, le=1.0, description="Overall developmental level")
    last_updated: datetime = Field(default_factory=datetime.now)
    
    def dict(self, *args, **kwargs):
        """Convert to dictionary, preserving datetime objects"""
        result = super().dict(*args, **kwargs)
        result["last_updated"] = self.last_updated
        return result

class IdentityNeuralState(BaseModel):
    """
    Neural state information for identity networks
    
    Tracks the state of neural networks for identity components
    """
    self_concept_development: float = Field(0.0, ge=0.0, le=1.0, description="Development level of self-concept network")
    narrative_development: float = Field(0.0, ge=0.0, le=1.0, description="Development level of narrative network")
    preference_development: float = Field(0.0, ge=0.0, le=1.0, description="Development level of preference network")
    personality_development: float = Field(0.0, ge=0.0, le=1.0, description="Development level of personality network")
    
    # Track recent activations for each neural component
    recent_self_concept_activations: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Recent activations of the self-concept network"
    )
    recent_narrative_activations: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Recent activations of the narrative network"
    )
    recent_preference_activations: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Recent activations of the preference network"
    )
    recent_personality_activations: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Recent activations of the personality network"
    )
    
    # Network performance metrics
    self_concept_accuracy: float = Field(0.5, ge=0.0, le=1.0, description="Accuracy of self-concept network")
    narrative_accuracy: float = Field(0.5, ge=0.0, le=1.0, description="Accuracy of narrative network")
    preference_accuracy: float = Field(0.5, ge=0.0, le=1.0, description="Accuracy of preference network")
    personality_accuracy: float = Field(0.5, ge=0.0, le=1.0, description="Accuracy of personality network")
    
    # Last update timestamp
    last_updated: datetime = Field(default_factory=datetime.now, description="When neural state was last updated")
    
    def dict(self, *args, **kwargs):
        """Convert to dictionary, preserving datetime objects"""
        result = super().dict(*args, **kwargs)
        result["last_updated"] = self.last_updated
        return result
    
    def update_accuracy(self, component: str, accuracy: float) -> None:
        """
        Update the accuracy of a component network
        
        Args:
            component: Component to update ('self_concept', 'narrative', 'preference', or 'personality')
            accuracy: New accuracy value (0.0 to 1.0)
        """
        accuracy = max(0.0, min(1.0, accuracy))
        
        if component == "self_concept":
            self.self_concept_accuracy = accuracy
        elif component == "narrative":
            self.narrative_accuracy = accuracy
        elif component == "preference":
            self.preference_accuracy = accuracy
        elif component == "personality":
            self.personality_accuracy = accuracy
            
        self.last_updated = datetime.now()
    
    def add_activation(self, component: str, activation: Dict[str, Any]) -> None:
        """
        Add a network activation record
        
        Args:
            component: Component that was activated ('self_concept', 'narrative', 'preference', or 'personality')
            activation: Activation data
        """
        # Add timestamp if not present
        if "timestamp" not in activation:
            activation["timestamp"] = datetime.now()
            
        if component == "self_concept":
            self.recent_self_concept_activations.append(activation)
            # Keep only the most recent activations (max 100)
            if len(self.recent_self_concept_activations) > 100:
                self.recent_self_concept_activations = self.recent_self_concept_activations[-100:]
                
        elif component == "narrative":
            self.recent_narrative_activations.append(activation)
            # Keep only the most recent activations (max 100)
            if len(self.recent_narrative_activations) > 100:
                self.recent_narrative_activations = self.recent_narrative_activations[-100:]
                
        elif component == "preference":
            self.recent_preference_activations.append(activation)
            # Keep only the most recent activations (max 100)
            if len(self.recent_preference_activations) > 100:
                self.recent_preference_activations = self.recent_preference_activations[-100:]
                
        elif component == "personality":
            self.recent_personality_activations.append(activation)
            # Keep only the most recent activations (max 100)
            if len(self.recent_personality_activations) > 100:
                self.recent_personality_activations = self.recent_personality_activations[-100:]
                
        self.last_updated = datetime.now()


#######################

#identity\neural_net.py#
#######################

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Any, Optional, Tuple

def get_device() -> torch.device:
    """
    Get the appropriate device (GPU if available, otherwise CPU)
    
    Returns:
        torch.device: The device to use for tensor operations
    """
    if torch.cuda.is_available():
        return torch.device("cuda")
    return torch.device("cpu")

class SelfConceptNetwork(nn.Module):
    """
    Neural network for processing and developing self-concept
    
    This network processes information about the self and integrates it
    into a coherent self-representation.
    """
    
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 64):
        """
        Initialize the self-concept network
        
        Args:
            input_dim: Dimension of input features
            hidden_dim: Dimension of hidden layers
            output_dim: Dimension of output features
        """
        super().__init__()
        
        # Input layer
        self.input_layer = nn.Linear(input_dim, hidden_dim)
        
        # Hidden layers
        self.hidden1 = nn.Linear(hidden_dim, hidden_dim)
        self.hidden2 = nn.Linear(hidden_dim, hidden_dim)
        
        # Domain-specific processing paths
        self.domain_physical = nn.Linear(hidden_dim, hidden_dim//2)
        self.domain_social = nn.Linear(hidden_dim, hidden_dim//2)
        self.domain_academic = nn.Linear(hidden_dim, hidden_dim//2)
        self.domain_emotional = nn.Linear(hidden_dim, hidden_dim//2)
        
        # Integration layer
        self.integration = nn.Linear(hidden_dim * 2, hidden_dim)
        
        # Output layers
        self.attribute_content = nn.Linear(hidden_dim, output_dim)
        self.attribute_confidence = nn.Linear(hidden_dim, 1)
        self.attribute_importance = nn.Linear(hidden_dim, 1)
        self.attribute_valence = nn.Linear(hidden_dim, 1)
        
        # Global self-evaluation
        self.global_self_esteem = nn.Linear(hidden_dim, 1)
        
        # Developmental factor (modulates network with development level)
        self.developmental_factor = nn.Parameter(torch.tensor(0.0), requires_grad=False)
        
        # Dropout for regularization
        self.dropout = nn.Dropout(0.3)
    
    def forward(self, input_data: torch.Tensor, domain: Optional[str] = None) -> Dict[str, torch.Tensor]:
        """
        Process input data to update self-concept
        
        Args:
            input_data: Input data tensor [batch_size, input_dim]
            domain: Optional domain specifier for domain-specific processing
            
        Returns:
            Dictionary containing processed outputs
        """
        # Get developmental modulation factor
        dev_factor = torch.sigmoid(self.developmental_factor * 5)
        
        # Initial layers with developmental modulation
        x = F.relu(self.input_layer(input_data))
        x = self.dropout(x)
        
        # Hidden layers
        h1 = F.relu(self.hidden1(x))
        h1 = self.dropout(h1)
        
        # Development affects depth of processing
        if dev_factor > 0.3:
            h2 = F.relu(self.hidden2(h1))
            h2 = self.dropout(h2)
            x = h2
        else:
            x = h1
        
        # Domain-specific processing based on development level
        if dev_factor > 0.5 and domain is not None:
            if domain == "physical":
                domain_x = F.relu(self.domain_physical(x))
            elif domain == "social":
                domain_x = F.relu(self.domain_social(x))
            elif domain == "academic":
                domain_x = F.relu(self.domain_academic(x))
            elif domain == "emotional":
                domain_x = F.relu(self.domain_emotional(x))
            else:
                domain_x = x
                
            # Concatenate domain-specific and general processing
            x = torch.cat([x, domain_x], dim=1)
            x = F.relu(self.integration(x))
        
        # Generate outputs
        attribute_content = self.attribute_content(x)
        
        # Meta-cognitive aspects develop with maturity
        if dev_factor > 0.2:
            confidence = torch.sigmoid(self.attribute_confidence(x))
            importance = torch.sigmoid(self.attribute_importance(x))
            valence = torch.tanh(self.attribute_valence(x))
            self_esteem = torch.sigmoid(self.global_self_esteem(x))
        else:
            # Limited metacognitive abilities at early development
            confidence = torch.sigmoid(torch.randn_like(self.attribute_confidence(x)) * 0.1 + 0.5)
            importance = torch.sigmoid(torch.randn_like(self.attribute_importance(x)) * 0.1 + 0.5)
            valence = torch.tanh(torch.randn_like(self.attribute_valence(x)) * 0.1)
            self_esteem = torch.sigmoid(torch.randn_like(self.global_self_esteem(x)) * 0.1 + 0.5)
        
        return {
            "attribute_content": attribute_content,
            "confidence": confidence,
            "importance": importance,
            "valence": valence,
            "self_esteem": self_esteem
        }
    
    def set_development_level(self, level: float) -> None:
        """
        Set the developmental level of the self-concept network
        
        Args:
            level: Development level (0.0 to 1.0)
        """
        with torch.no_grad():
            self.developmental_factor.copy_(torch.tensor(max(0.0, min(1.0, level))))

class NarrativeNetwork(nn.Module):
    """
    Neural network for processing and integrating personal narrative
    
    This network processes experiences and integrates them into
    a coherent personal narrative with themes and meaning.
    """
    
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 64):
        """
        Initialize the narrative network
        
        Args:
            input_dim: Dimension of input features
            hidden_dim: Dimension of hidden layers
            output_dim: Dimension of output features
        """
        super().__init__()
        
        # Input processing
        self.input_layer = nn.Linear(input_dim, hidden_dim)
        
        # Experience processing
        self.event_encoder = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # Theme extraction
        self.theme_extraction = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim//2, hidden_dim//2)
        )
        
        # Interpretation generation
        self.interpretation = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim//2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, output_dim)
        )
        
        # Event importance evaluation
        self.importance_evaluation = nn.Linear(hidden_dim, 1)
        
        # Emotional processing
        self.emotional_impact = nn.Linear(hidden_dim, output_dim)
        
        # Coherence evaluation
        self.coherence_evaluation = nn.Linear(hidden_dim, 1)
        
        # Developmental factor
        self.developmental_factor = nn.Parameter(torch.tensor(0.0), requires_grad=False)
        
        # LSTM for temporal sequence processing (more advanced narrative abilities)
        self.temporal_lstm = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.3
        )
    
    def forward(self, 
               input_data: torch.Tensor, 
               operation: str = "process_event",
               past_events: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        Process input data for narrative operations
        
        Args:
            input_data: Input data tensor [batch_size, input_dim]
            operation: Operation to perform
                "process_event": Process a new event
                "extract_theme": Extract themes from events
                "evaluate_coherence": Evaluate narrative coherence
            past_events: Optional tensor of past events for contextual processing
                [batch_size, num_events, hidden_dim]
                
        Returns:
            Dictionary containing processed outputs
        """
        # Get developmental modulation factor
        dev_factor = torch.sigmoid(self.developmental_factor * 5)
        
        # Initial processing
        x = F.relu(self.input_layer(input_data))
        
        if operation == "process_event":
            # Event encoding
            event_encoding = self.event_encoder(x)
            
            # Event importance (develops with maturity)
            importance = torch.sigmoid(self.importance_evaluation(event_encoding))
            
            # Emotional processing
            emotional_impact = torch.tanh(self.emotional_impact(event_encoding))
            
            # Context-sensitive processing with development
            if dev_factor > 0.4 and past_events is not None:
                # Add current event to past events
                batch_size = input_data.size(0)
                event_expanded = event_encoding.unsqueeze(1)  # [batch_size, 1, hidden_dim]
                
                # Process temporal sequence with LSTM
                if past_events.size(1) > 0:
                    all_events = torch.cat([past_events, event_expanded], dim=1)
                    seq_output, _ = self.temporal_lstm(all_events)
                    temporal_context = seq_output[:, -1, :]  # Get last output
                else:
                    temporal_context = event_encoding
                
                # Theme extraction with context
                if dev_factor > 0.6:
                    theme_vector = self.theme_extraction(temporal_context)
                    # Generate interpretation with thematic understanding
                    context_augmented = torch.cat([event_encoding, theme_vector], dim=1)
                    interpretation = self.interpretation(context_augmented)
                else:
                    theme_vector = torch.zeros((batch_size, self.theme_extraction[0].out_features), 
                                             device=input_data.device)
                    interpretation = torch.zeros((batch_size, self.interpretation[-1].out_features),
                                               device=input_data.device)
            else:
                # Simple processing without context at early development
                temporal_context = event_encoding
                theme_vector = torch.zeros((input_data.size(0), self.theme_extraction[0].out_features), 
                                         device=input_data.device)
                interpretation = torch.zeros((input_data.size(0), self.interpretation[-1].out_features),
                                           device=input_data.device)
            
            return {
                "event_encoding": event_encoding,
                "importance": importance,
                "emotional_impact": emotional_impact,
                "theme_vector": theme_vector,
                "interpretation": interpretation,
                "temporal_context": temporal_context
            }
            
        elif operation == "extract_theme":
            # Theme extraction from events
            # At early development, themes are simple and concrete
            if dev_factor < 0.4:
                theme_vector = torch.randn_like(self.theme_extraction(x)) * 0.1
            else:
                theme_vector = self.theme_extraction(x)
                
            return {
                "theme_vector": theme_vector
            }
            
        elif operation == "evaluate_coherence":
            # Evaluate narrative coherence
            # At early development, coherence evaluation is limited
            if dev_factor < 0.6:
                coherence = torch.sigmoid(torch.tensor([0.3], device=input_data.device))
            else:
                # Process with temporal LSTM if past events available
                if past_events is not None and past_events.size(1) > 0:
                    seq_output, _ = self.temporal_lstm(past_events)
                    temporal_features = seq_output[:, -1, :]
                    coherence = torch.sigmoid(self.coherence_evaluation(temporal_features))
                else:
                    coherence = torch.sigmoid(self.coherence_evaluation(x))
            
            return {
                "coherence": coherence
            }
        
        # Default return
        return {"features": x}
    
    def set_development_level(self, level: float) -> None:
        """
        Set the developmental level of the narrative network
        
        Args:
            level: Development level (0.0 to 1.0)
        """
        with torch.no_grad():
            self.developmental_factor.copy_(torch.tensor(max(0.0, min(1.0, level))))

class PreferenceNetwork(nn.Module):
    """
    Neural network for processing preferences and values
    
    This network processes experiences to form preferences and
    extracts values from preference patterns.
    """
    
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 64):
        """
        Initialize the preference network
        
        Args:
            input_dim: Dimension of input features
            hidden_dim: Dimension of hidden layers
            output_dim: Dimension of output features
        """
        super().__init__()
        
        # Input processing
        self.input_layer = nn.Linear(input_dim, hidden_dim)
        
        # Preference formation
        self.preference_formation = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # Preference evaluation
        self.valence_evaluation = nn.Linear(hidden_dim, 1)
        self.strength_evaluation = nn.Linear(hidden_dim, 1)
        self.certainty_evaluation = nn.Linear(hidden_dim, 1)
        
        # Value extraction (higher-level preferences)
        self.value_extraction = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim//2, output_dim)
        )
        
        # Value importance evaluation
        self.value_importance = nn.Linear(output_dim, 1)
        
        # Preference application (using preferences to make decisions)
        self.preference_application = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, output_dim)
        )
        
        # Developmental factor
        self.developmental_factor = nn.Parameter(torch.tensor(0.0), requires_grad=False)
    
    def forward(self, 
               input_data: torch.Tensor, 
               operation: str = "form_preference",
               context: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        Process input data for preference operations
        
        Args:
            input_data: Input data tensor [batch_size, input_dim]
            operation: Operation to perform
                "form_preference": Form a preference from experience
                "extract_value": Extract values from preferences
                "apply_preference": Apply preferences to a choice
            context: Optional context tensor for preference application
                
        Returns:
            Dictionary containing processed outputs
        """
        # Get developmental modulation factor
        dev_factor = torch.sigmoid(self.developmental_factor * 5)
        
        # Initial processing
        x = F.relu(self.input_layer(input_data))
        
        if operation == "form_preference":
            # Preference formation
            preference_encoding = self.preference_formation(x)
            
            # Basic preference evaluation (develops with maturity)
            if dev_factor < 0.2:
                # Simple approach/avoid at early stages
                valence = torch.tanh(self.valence_evaluation(preference_encoding)) 
                strength = torch.ones_like(self.strength_evaluation(preference_encoding)) * 0.5
                certainty = torch.ones_like(self.certainty_evaluation(preference_encoding)) * 0.5
            else:
                # More nuanced preferences with development
                valence = torch.tanh(self.valence_evaluation(preference_encoding))
                strength = torch.sigmoid(self.strength_evaluation(preference_encoding))
                certainty = torch.sigmoid(self.certainty_evaluation(preference_encoding))
            
            # Value extraction develops later
            if dev_factor > 0.6:
                value_vector = self.value_extraction(preference_encoding)
                value_importance = torch.sigmoid(self.value_importance(value_vector))
            else:
                value_vector = torch.zeros((input_data.size(0), self.value_extraction[-1].out_features), 
                                         device=input_data.device)
                value_importance = torch.zeros((input_data.size(0), 1), device=input_data.device)
            
            return {
                "preference_encoding": preference_encoding,
                "valence": valence,
                "strength": strength,
                "certainty": certainty,
                "value_vector": value_vector,
                "value_importance": value_importance
            }
            
        elif operation == "extract_value":
            # Value extraction from preference patterns
            # Only available at higher development levels
            if dev_factor < 0.6:
                value_vector = torch.zeros((input_data.size(0), self.value_extraction[-1].out_features), 
                                         device=input_data.device)
                value_importance = torch.zeros((input_data.size(0), 1), device=input_data.device)
            else:
                preference_encoding = self.preference_formation(x)
                value_vector = self.value_extraction(preference_encoding)
                value_importance = torch.sigmoid(self.value_importance(value_vector))
                
            return {
                "value_vector": value_vector,
                "value_importance": value_importance
            }
            
        elif operation == "apply_preference" and context is not None:
            # Apply preferences to a choice context
            preference_encoding = self.preference_formation(x)
            
            # Combine preference with context
            combined = torch.cat([preference_encoding, context], dim=1)
            
            # Generate decision influence
            decision_influence = self.preference_application(combined)
            
            # Decision confidence based on preference strength and certainty
            strength = torch.sigmoid(self.strength_evaluation(preference_encoding))
            certainty = torch.sigmoid(self.certainty_evaluation(preference_encoding))
            confidence = strength * certainty
            
            return {
                "decision_influence": decision_influence,
                "confidence": confidence
            }
        
        # Default return
        return {"features": x}
    
    def set_development_level(self, level: float) -> None:
        """
        Set the developmental level of the preference network
        
        Args:
            level: Development level (0.0 to 1.0)
        """
        with torch.no_grad():
            self.developmental_factor.copy_(torch.tensor(max(0.0, min(1.0, level))))

class PersonalityNetwork(nn.Module):
    """
    Neural network for processing and developing personality traits
    
    This network identifies consistent patterns across behaviors
    and organizes them into stable traits.
    """
    
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 64, num_traits: int = 10):
        """
        Initialize the personality network
        
        Args:
            input_dim: Dimension of input features
            hidden_dim: Dimension of hidden layers
            output_dim: Dimension of output features
            num_traits: Number of personality traits to track
        """
        super().__init__()
        
        # Input processing
        self.input_layer = nn.Linear(input_dim, hidden_dim)
        
        # Behavioral pattern recognition
        self.pattern_recognition = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # Trait extraction
        self.trait_extraction = nn.Sequential(
            nn.Linear(hidden_dim, num_traits),
            nn.Sigmoid()  # Traits from 0.0 to 1.0
        )
        
        # Dimension organization (e.g., Big Five)
        self.dimension_organization = nn.Linear(num_traits, 5)
        
        # Trait application (predicting behavior from traits)
        self.trait_application = nn.Sequential(
            nn.Linear(num_traits, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, output_dim)
        )
        
        # Trait stability tracking
        self.stability_evaluation = nn.Linear(hidden_dim, 1)
        
        # Developmental factor
        self.developmental_factor = nn.Parameter(torch.tensor(0.0), requires_grad=False)
        
        # Running average of trait scores for stability
        register_buffer = lambda name, tensor: self.register_buffer(name, tensor)
        register_buffer('trait_averages', torch.zeros(num_traits))
        register_buffer('trait_update_count', torch.zeros(1))
    
    def forward(self, 
               input_data: torch.Tensor, 
               operation: str = "extract_traits",
               context: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        Process input data for personality operations
        
        Args:
            input_data: Input data tensor [batch_size, input_dim]
            operation: Operation to perform
                "extract_traits": Extract traits from behavioral patterns
                "predict_behavior": Predict behavior based on traits
                "evaluate_stability": Evaluate trait stability
            context: Optional context tensor for contextual processing
                
        Returns:
            Dictionary containing processed outputs
        """
        # Get developmental modulation factor
        dev_factor = torch.sigmoid(self.developmental_factor * 5)
        
        # Initial processing
        x = F.relu(self.input_layer(input_data))
        
        if operation == "extract_traits":
            # Behavioral pattern recognition
            patterns = self.pattern_recognition(x)
            
            # Trait extraction (develops with maturity)
            if dev_factor < 0.2:
                # Very simple temperamental tendencies at early stages
                trait_scores = torch.sigmoid(torch.randn_like(self.trait_extraction(patterns)) * 0.1 + 0.5)
                # No dimension organization at early stages
                dimension_scores = torch.zeros((input_data.size(0), 5), device=input_data.device)
                stability = torch.tensor([0.1], device=input_data.device)
            else:
                # More structured trait extraction with development
                trait_scores = self.trait_extraction(patterns)
                
                # Update running averages for stability tracking
                if self.training:
                    batch_avg = trait_scores.mean(0)
                    old_count = self.trait_update_count
                    new_count = old_count + 1
                    self.trait_averages.copy_((self.trait_averages * old_count + batch_avg) / new_count)
                    self.trait_update_count.copy_(new_count)
                
                # Stability calculation
                stability = torch.sigmoid(self.stability_evaluation(patterns))
                
                # Dimension organization develops later
                if dev_factor > 0.6:
                    dimension_scores = torch.tanh(self.dimension_organization(trait_scores))
                else:
                    dimension_scores = torch.zeros((input_data.size(0), 5), device=input_data.device)
            
            return {
                "trait_scores": trait_scores,
                "dimension_scores": dimension_scores,
                "stability": stability,
                "patterns": patterns
            }
            
        elif operation == "predict_behavior":
            # Predict behavior from traits
            # Only meaningful at higher development levels
            if dev_factor < 0.4:
                behavior_prediction = torch.randn((input_data.size(0), self.trait_application[-1].out_features), 
                                                device=input_data.device) * 0.1
            else:
                # Extract traits first
                patterns = self.pattern_recognition(x)
                trait_scores = self.trait_extraction(patterns)
                
                # Context-sensitive behavior prediction
                if context is not None and dev_factor > 0.6:
                    # Modulate traits based on context (trait x context interaction)
                    context_processed = F.relu(self.input_layer(context))
                    # Simple context influence through multiplication
                    trait_scores = trait_scores * torch.sigmoid(context_processed.mean(dim=1, keepdim=True))
                
                # Predict behavior from traits
                behavior_prediction = self.trait_application(trait_scores)
                
            return {
                "behavior_prediction": behavior_prediction
            }
            
        elif operation == "evaluate_stability":
            # Evaluate trait stability
            patterns = self.pattern_recognition(x)
            trait_scores = self.trait_extraction(patterns)
            
            # Calculate deviation from running averages
            if self.trait_update_count > 0:
                deviation = torch.abs(trait_scores.mean(0) - self.trait_averages)
                stability = 1.0 - torch.clamp(deviation.mean(), 0.0, 1.0)
            else:
                stability = torch.tensor([0.5], device=input_data.device)
                
            return {
                "stability": stability
            }
        
        # Default return
        return {"features": x}
    
    def set_development_level(self, level: float) -> None:
        """
        Set the developmental level of the personality network
        
        Args:
            level: Development level (0.0 to 1.0)
        """
        with torch.no_grad():
            self.developmental_factor.copy_(torch.tensor(max(0.0, min(1.0, level))))


#######################

#identity\personality_traits.py#
#######################

# TODO: Implement the PersonalityTraits class to represent stable behavior patterns
# This component should be able to:
# - Represent consistent patterns of thinking, feeling, and behaving
# - Develop traits gradually through experience
# - Maintain trait stability while allowing for growth and change
# - Express traits through behavior in context-appropriate ways

# TODO: Implement developmental progression in personality traits:
# - Simple temperamental tendencies in early stages
# - Growing behavioral consistencies in childhood
# - Trait consolidation in adolescence
# - Stable yet nuanced personality in adulthood

# TODO: Create mechanisms for:
# - Trait extraction: Identify patterns across behaviors
# - Trait integration: Organize traits into coherent dimensions
# - Trait expression: Apply traits to guide behavior
# - Trait adaptation: Adjust expression based on context

# TODO: Implement trait frameworks:
# - Consider using established models (Big Five, etc.)
# - Include traits for thinking styles (analytical, intuitive, etc.)
# - Include traits for emotional tendencies (reactive, stable, etc.)
# - Include traits for behavioral patterns (cautious, impulsive, etc.)

# TODO: Connect to behavior generation and social systems
# Traits should influence behavior production and
# should develop through social interactions

from typing import Dict, List, Any, Optional, Set, Tuple
import torch
import uuid
from datetime import datetime
import numpy as np
from pydantic import ValidationError

from lmm_project.base.module import BaseModule
from lmm_project.event_bus import EventBus
from lmm_project.modules.identity.models import (
    PersonalityTrait, 
    TraitDimension, 
    PersonalityProfile
)
from lmm_project.modules.identity.neural_net import PersonalityNetwork, get_device

class PersonalityTraits(BaseModule):
    """
    Manages personality traits and their expression
    
    This module represents stable patterns of thinking, feeling, and behaving
    that develop gradually and become increasingly consistent over time.
    """
    
    # Development milestones
    development_milestones = {
        0.0: "Basic temperamental tendencies",
        0.2: "Simple behavioral consistencies",
        0.4: "Emerging trait patterns across situations",
        0.6: "Consolidated traits with some situational flexibility",
        0.8: "Stable trait dimensions with contextual adaptation",
        1.0: "Fully developed personality with balance of stability and adaptation"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the personality traits module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level (0.0 to 1.0)
        """
        super().__init__(module_id, event_bus)
        
        # Initialize personality profile
        self.profile = PersonalityProfile()
        
        # Set initial development level
        self.development_level = max(0.0, min(1.0, development_level))
        
        # Initialize neural network
        self.device = get_device()
        self.network = PersonalityNetwork().to(self.device)
        self.network.set_development_level(self.development_level)
        
        # Initialize trait embeddings for similarity search
        self.trait_embeddings = {}
        self.dimension_embeddings = {}
        
        # Initialize with basic traits and dimensions based on development level
        self._initialize_basic_traits()
    
    def _initialize_basic_traits(self):
        """Initialize basic traits based on development level"""
        # Initialize with Big Five dimensions at a minimum
        dimensions = [
            {
                "name": "Extraversion",
                "description": "Tendency to seek stimulation and engage with others",
                "positive_pole": "Extraverted, outgoing, energetic, sociable",
                "negative_pole": "Introverted, reserved, solitary, quiet",
                "score": 0.5
            },
            {
                "name": "Neuroticism",
                "description": "Tendency to experience negative emotions",
                "positive_pole": "Emotionally stable, calm, secure, resilient",
                "negative_pole": "Anxious, irritable, moody, insecure",
                "score": 0.5
            },
            {
                "name": "Agreeableness",
                "description": "Tendency to be compassionate toward others",
                "positive_pole": "Cooperative, compassionate, kind, trusting",
                "negative_pole": "Critical, suspicious, uncooperative, challenging",
                "score": 0.5
            },
            {
                "name": "Conscientiousness",
                "description": "Tendency to show self-discipline and aim for achievement",
                "positive_pole": "Organized, disciplined, careful, responsible",
                "negative_pole": "Spontaneous, careless, disorganized, impulsive",
                "score": 0.5
            },
            {
                "name": "Openness",
                "description": "Tendency to appreciate novelty and variety",
                "positive_pole": "Creative, curious, open-minded, imaginative",
                "negative_pole": "Conventional, practical, focused, traditional",
                "score": 0.5
            }
        ]
        
        # Create each dimension
        for dim_info in dimensions:
            dimension = TraitDimension(
                dimension_id=str(uuid.uuid4()),
                name=dim_info["name"],
                description=dim_info["description"],
                positive_pole=dim_info["positive_pole"],
                negative_pole=dim_info["negative_pole"],
                score=dim_info["score"]
            )
            
            # Add to profile
            self.profile.add_dimension(dimension)
            
            # Create initial embedding for this dimension
            features = self._extract_features(f"{dimension.name}: {dimension.description}")
            with torch.no_grad():
                output = self.network(
                    features.to(self.device),
                    operation="extract_traits"
                )
                self.dimension_embeddings[dimension.dimension_id] = output["trait_encoding"].cpu().squeeze(0)
        
        # If development level is high enough, add some basic traits
        if self.development_level >= 0.2:
            # Create a few basic traits based on the dimensions
            for dim_id, dimension in self.profile.dimensions.items():
                if dimension.name == "Extraversion":
                    self._create_initial_trait("Sociable", 0.6, dimension.dimension_id, is_positive=True)
                    self._create_initial_trait("Quiet", 0.4, dimension.dimension_id, is_positive=False)
                elif dimension.name == "Neuroticism":
                    self._create_initial_trait("Calm", 0.6, dimension.dimension_id, is_positive=True)
                    self._create_initial_trait("Anxious", 0.4, dimension.dimension_id, is_positive=False)
                elif dimension.name == "Agreeableness":
                    self._create_initial_trait("Kind", 0.6, dimension.dimension_id, is_positive=True)
                    self._create_initial_trait("Critical", 0.4, dimension.dimension_id, is_positive=False)
                elif dimension.name == "Conscientiousness":
                    self._create_initial_trait("Organized", 0.6, dimension.dimension_id, is_positive=True)
                    self._create_initial_trait("Impulsive", 0.4, dimension.dimension_id, is_positive=False)
                elif dimension.name == "Openness":
                    self._create_initial_trait("Curious", 0.6, dimension.dimension_id, is_positive=True)
                    self._create_initial_trait("Traditional", 0.4, dimension.dimension_id, is_positive=False)
    
    def _create_initial_trait(self, name: str, score: float, dimension_id: str, is_positive: bool):
        """
        Create an initial trait for a dimension
        
        Args:
            name: Trait name
            score: Trait score (0.0 to 1.0)
            dimension_id: ID of the dimension this trait belongs to
            is_positive: Whether this trait is on the positive or negative pole
        """
        dimension = self.profile.dimensions[dimension_id]
        
        # Create description based on dimension's poles
        if is_positive:
            description = f"Tendency to be {name.lower()}, related to {dimension.name.lower()}"
        else:
            description = f"Tendency to be {name.lower()}, opposite of high {dimension.name.lower()}"
        
        # Create the trait
        trait = PersonalityTrait(
            trait_id=str(uuid.uuid4()),
            name=name,
            description=description,
            score=score,
            stability=0.3 + (self.development_level * 0.3),  # More stable at higher development
            dimension=dimension_id
        )
        
        # Add to profile
        self.profile.add_trait(trait)
        
        # Create initial embedding for this trait
        features = self._extract_features(f"{trait.name}: {trait.description}")
        with torch.no_grad():
            output = self.network(
                features.to(self.device),
                operation="extract_traits"
            )
            self.trait_embeddings[trait.trait_id] = output["trait_encoding"].cpu().squeeze(0)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to the personality traits module
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Dict with processing results
        """
        # Validate input
        if not isinstance(input_data, dict):
            return {
                "status": "error",
                "message": "Input must be a dictionary"
            }
        
        # Extract process ID if provided
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Extract operation
        operation = input_data.get("operation", "")
        
        # Dispatch to appropriate handler
        if operation == "add_trait":
            return self._add_trait(input_data, process_id)
        elif operation == "update_trait":
            return self._update_trait(input_data, process_id)
        elif operation == "add_dimension":
            return self._add_dimension(input_data, process_id)
        elif operation == "update_dimension":
            return self._update_dimension(input_data, process_id)
        elif operation == "extract_traits":
            return self._extract_traits(input_data, process_id)
        elif operation == "query_traits":
            return self._query_traits(input_data, process_id)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "process_id": process_id
            }
    
    def _add_trait(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Add a new personality trait
        
        Args:
            input_data: Input data dictionary
            process_id: Process identifier
            
        Returns:
            Dict with processing results
        """
        # Check required fields
        required_fields = ["name", "description", "score"]
        for field in required_fields:
            if field not in input_data:
                return {
                    "status": "error",
                    "message": f"Missing required field: {field}",
                    "process_id": process_id
                }
        
        try:
            # Create trait
            trait_id = str(uuid.uuid4())
            
            # Get dimension if provided
            dimension = input_data.get("dimension")
            
            # Create trait
            trait = PersonalityTrait(
                trait_id=trait_id,
                name=input_data["name"],
                description=input_data["description"],
                score=float(input_data["score"]),
                stability=input_data.get("stability", 0.5),
                behavioral_instances=input_data.get("behavioral_instances", []),
                opposing_trait=input_data.get("opposing_trait"),
                dimension=dimension
            )
            
            # Add to profile
            self.profile.add_trait(trait)
            
            # Create embedding for this trait
            features = self._extract_features(f"{trait.name}: {trait.description}")
            with torch.no_grad():
                output = self.network(
                    features.to(self.device),
                    operation="extract_traits"
                )
                self.trait_embeddings[trait_id] = output["trait_encoding"].cpu().squeeze(0)
            
            return {
                "status": "success",
                "message": "Trait added successfully",
                "trait_id": trait_id,
                "process_id": process_id
            }
            
        except ValidationError as e:
            return {
                "status": "error",
                "message": f"Validation error: {str(e)}",
                "process_id": process_id
            }
        except Exception as e:
            return {
                "status": "error",
                "message": f"Error adding trait: {str(e)}",
                "process_id": process_id
            }
    
    def _update_trait(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Update an existing personality trait
        
        Args:
            input_data: Input data dictionary
            process_id: Process identifier
            
        Returns:
            Dict with processing results
        """
        # Check for trait ID
        if "trait_id" not in input_data:
            return {
                "status": "error",
                "message": "Missing trait_id",
                "process_id": process_id
            }
            
        trait_id = input_data["trait_id"]
        
        # Check if trait exists
        if trait_id not in self.profile.traits:
            return {
                "status": "error",
                "message": f"Trait not found: {trait_id}",
                "process_id": process_id
            }
            
        # Get trait
        trait = self.profile.traits[trait_id]
        
        # Track if updated
        updated = False
        
        # Update fields
        if "name" in input_data:
            trait.name = input_data["name"]
            updated = True
            
        if "description" in input_data:
            trait.description = input_data["description"]
            updated = True
            
        if "score" in input_data:
            trait.score = max(0.0, min(1.0, float(input_data["score"])))
            updated = True
            
        if "stability" in input_data:
            trait.stability = max(0.0, min(1.0, float(input_data["stability"])))
            updated = True
            
        if "behavioral_instances" in input_data:
            if isinstance(input_data["behavioral_instances"], list):
                trait.behavioral_instances = input_data["behavioral_instances"]
                updated = True
                
        if "opposing_trait" in input_data:
            trait.opposing_trait = input_data["opposing_trait"]
            updated = True
            
        if "dimension" in input_data:
            trait.dimension = input_data["dimension"]
            updated = True
        
        if updated:
            # Update timestamp
            trait.updated_at = datetime.now()
            
            # Update in profile
            self.profile.traits[trait_id] = trait
            self.profile.last_updated = datetime.now()
            
            # Update embedding if name or description changed
            if "name" in input_data or "description" in input_data:
                features = self._extract_features(f"{trait.name}: {trait.description}")
                with torch.no_grad():
                    output = self.network(
                        features.to(self.device),
                        operation="extract_traits"
                    )
                    self.trait_embeddings[trait_id] = output["trait_encoding"].cpu().squeeze(0)
            
            return {
                "status": "success",
                "message": "Trait updated successfully",
                "process_id": process_id
            }
        else:
            return {
                "status": "error",
                "message": "No fields updated",
                "process_id": process_id
            }
    
    def _add_dimension(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Add a new trait dimension
        
        Args:
            input_data: Input data dictionary
            process_id: Process identifier
            
        Returns:
            Dict with processing results
        """
        # Check required fields
        required_fields = ["name", "description", "positive_pole", "negative_pole"]
        for field in required_fields:
            if field not in input_data:
                return {
                    "status": "error",
                    "message": f"Missing required field: {field}",
                    "process_id": process_id
                }
        
        try:
            # Create dimension
            dimension_id = str(uuid.uuid4())
            
            # Create dimension
            dimension = TraitDimension(
                dimension_id=dimension_id,
                name=input_data["name"],
                description=input_data["description"],
                positive_pole=input_data["positive_pole"],
                negative_pole=input_data["negative_pole"],
                score=input_data.get("score", 0.5)
            )
            
            # Add to profile
            self.profile.add_dimension(dimension)
            
            # Create embedding for this dimension
            features = self._extract_features(f"{dimension.name}: {dimension.description}")
            with torch.no_grad():
                output = self.network(
                    features.to(self.device),
                    operation="extract_traits"
                )
                self.dimension_embeddings[dimension_id] = output["trait_encoding"].cpu().squeeze(0)
            
            return {
                "status": "success",
                "message": "Dimension added successfully",
                "dimension_id": dimension_id,
                "process_id": process_id
            }
            
        except ValidationError as e:
            return {
                "status": "error",
                "message": f"Validation error: {str(e)}",
                "process_id": process_id
            }
        except Exception as e:
            return {
                "status": "error",
                "message": f"Error adding dimension: {str(e)}",
                "process_id": process_id
            }
    
    def _update_dimension(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Update an existing trait dimension
        
        Args:
            input_data: Input data dictionary
            process_id: Process identifier
            
        Returns:
            Dict with processing results
        """
        # Check for dimension ID
        if "dimension_id" not in input_data:
            return {
                "status": "error",
                "message": "Missing dimension_id",
                "process_id": process_id
            }
            
        dimension_id = input_data["dimension_id"]
        
        # Check if dimension exists
        if dimension_id not in self.profile.dimensions:
            return {
                "status": "error",
                "message": f"Dimension not found: {dimension_id}",
                "process_id": process_id
            }
            
        # Get dimension
        dimension = self.profile.dimensions[dimension_id]
        
        # Track if updated
        updated = False
        
        # Update fields
        if "name" in input_data:
            dimension.name = input_data["name"]
            updated = True
            
        if "description" in input_data:
            dimension.description = input_data["description"]
            updated = True
            
        if "positive_pole" in input_data:
            dimension.positive_pole = input_data["positive_pole"]
            updated = True
            
        if "negative_pole" in input_data:
            dimension.negative_pole = input_data["negative_pole"]
            updated = True
            
        if "score" in input_data:
            dimension.score = max(0.0, min(1.0, float(input_data["score"])))
            updated = True
        
        if updated:
            # Update timestamp
            dimension.updated_at = datetime.now()
            
            # Update in profile
            self.profile.dimensions[dimension_id] = dimension
            self.profile.last_updated = datetime.now()
            
            # Update embedding if name or description changed
            if "name" in input_data or "description" in input_data:
                features = self._extract_features(f"{dimension.name}: {dimension.description}")
                with torch.no_grad():
                    output = self.network(
                        features.to(self.device),
                        operation="extract_traits"
                    )
                    self.dimension_embeddings[dimension_id] = output["trait_encoding"].cpu().squeeze(0)
            
            return {
                "status": "success",
                "message": "Dimension updated successfully",
                "process_id": process_id
            }
        else:
            return {
                "status": "error",
                "message": "No fields updated",
                "process_id": process_id
            }
    
    def _extract_traits(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Extract personality traits from behavioral data
        
        Args:
            input_data: Input data dictionary
            process_id: Process identifier
            
        Returns:
            Dict with processing results
        """
        # Check for behavior description
        if "behavior" not in input_data:
            return {
                "status": "error",
                "message": "Missing behavior description",
                "process_id": process_id
            }
            
        behavior = input_data["behavior"]
        
        # Extract features
        features = self._extract_features(behavior)
        
        # Process through network
        with torch.no_grad():
            output = self.network(
                features.to(self.device),
                operation="extract_traits"
            )
            
            trait_encoding = output["trait_encoding"].cpu().squeeze(0)
            
        # Find most similar traits
        similar_traits = []
        
        if len(self.trait_embeddings) > 0:
            # Calculate similarity to existing traits
            similarities = {}
            for trait_id, embedding in self.trait_embeddings.items():
                similarity = torch.cosine_similarity(
                    trait_encoding.unsqueeze(0),
                    embedding.unsqueeze(0)
                ).item()
                similarities[trait_id] = similarity
                
            # Get top 3 similar traits
            sorted_traits = sorted(similarities.items(), key=lambda x: x[1], reverse=True)
            for trait_id, similarity in sorted_traits[:3]:
                if similarity > 0.7:  # Only include if reasonably similar
                    trait = self.profile.traits[trait_id]
                    similar_traits.append({
                        "trait_id": trait_id,
                        "name": trait.name,
                        "similarity": similarity
                    })
        
        # Update behavioral instances for the most similar trait
        if len(similar_traits) > 0 and similar_traits[0]["similarity"] > 0.85:
            most_similar_trait_id = similar_traits[0]["trait_id"]
            trait = self.profile.traits[most_similar_trait_id]
            
            # Add behavior as an instance if not too many already
            if len(trait.behavioral_instances) < 20:  # Limit number of instances
                trait.behavioral_instances.append(behavior)
                trait.updated_at = datetime.now()
                self.profile.traits[most_similar_trait_id] = trait
            
            # Update trait score with small adjustment toward extremes
            current_score = trait.score
            if current_score > 0.5:
                # Strengthen high score traits
                new_score = current_score + (0.02 * (1.0 - trait.stability))
                trait.score = min(1.0, new_score)
            else:
                # Strengthen low score traits
                new_score = current_score - (0.02 * (1.0 - trait.stability))
                trait.score = max(0.0, new_score)
                
            self.profile.traits[most_similar_trait_id] = trait
        
        # If at higher development level, consider new trait formation
        if self.development_level >= 0.6 and len(similar_traits) == 0:
            # Extract potential trait name and description
            try:
                # Use network to generate trait properties
                context_tensor = torch.cat([
                    features, 
                    torch.zeros(1, features.size(1) - features.size(0)).to(features.device)
                ], dim=0)
                
                trait_properties = self.network(
                    context_tensor.to(self.device),
                    operation="generate_trait"
                )
                
                # If confidence is high enough, create a new trait
                if "trait_confidence" in trait_properties and trait_properties["trait_confidence"].item() > 0.7:
                    # Get development-appropriate stability
                    stability = 0.3 + (self.development_level * 0.4)
                    
                    # Create trait
                    self._add_trait({
                        "name": f"New Trait {len(self.profile.traits) + 1}",
                        "description": f"Pattern extracted from behavior: {behavior[:50]}...",
                        "score": 0.7,  # Start with moderately high score
                        "stability": stability,
                        "behavioral_instances": [behavior]
                    }, str(uuid.uuid4()))
            except Exception as e:
                # Silently fail trait generation - it's an advanced feature
                pass
        
        return {
            "status": "success",
            "similar_traits": similar_traits,
            "process_id": process_id
        }
    
    def _query_traits(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Query personality traits
        
        Args:
            input_data: Input data dictionary
            process_id: Process identifier
            
        Returns:
            Dict with processing results
        """
        # Get query type
        query_type = input_data.get("query_type", "all")
        
        if query_type == "all":
            # Return all traits
            traits = []
            for trait_id, trait in self.profile.traits.items():
                traits.append({
                    "trait_id": trait_id,
                    "name": trait.name,
                    "description": trait.description,
                    "score": trait.score,
                    "stability": trait.stability,
                    "dimension": trait.dimension
                })
                
            return {
                "status": "success",
                "traits": traits,
                "process_id": process_id
            }
        
        elif query_type == "by_dimension":
            # Get dimension
            if "dimension_id" not in input_data:
                return {
                    "status": "error",
                    "message": "Missing dimension_id for by_dimension query",
                    "process_id": process_id
                }
                
            dimension_id = input_data["dimension_id"]
            
            # Get traits for this dimension
            traits = []
            for trait_id, trait in self.profile.traits.items():
                if trait.dimension == dimension_id:
                    traits.append({
                        "trait_id": trait_id,
                        "name": trait.name,
                        "description": trait.description,
                        "score": trait.score,
                        "stability": trait.stability
                    })
                    
            if not traits:
                return {
                    "status": "success",
                    "message": f"No traits found for dimension {dimension_id}",
                    "traits": [],
                    "process_id": process_id
                }
                
            return {
                "status": "success",
                "traits": traits,
                "process_id": process_id
            }
        
        elif query_type == "similar":
            # Get query
            if "query" not in input_data:
                return {
                    "status": "error",
                    "message": "Missing query for similar query",
                    "process_id": process_id
                }
                
            query = input_data["query"]
            
            # Extract features
            features = self._extract_features(query)
            
            # Process through network
            with torch.no_grad():
                output = self.network(
                    features.to(self.device),
                    operation="extract_traits"
                )
                
                query_encoding = output["trait_encoding"].cpu().squeeze(0)
                
            # Find most similar traits
            similar_traits = []
            
            if len(self.trait_embeddings) > 0:
                # Calculate similarity to existing traits
                similarities = {}
                for trait_id, embedding in self.trait_embeddings.items():
                    similarity = torch.cosine_similarity(
                        query_encoding.unsqueeze(0),
                        embedding.unsqueeze(0)
                    ).item()
                    similarities[trait_id] = similarity
                    
                # Get top 5 similar traits
                sorted_traits = sorted(similarities.items(), key=lambda x: x[1], reverse=True)
                for trait_id, similarity in sorted_traits[:5]:
                    trait = self.profile.traits[trait_id]
                    similar_traits.append({
                        "trait_id": trait_id,
                        "name": trait.name,
                        "description": trait.description,
                        "score": trait.score,
                        "similarity": similarity
                    })
                    
            return {
                "status": "success",
                "similar_traits": similar_traits,
                "process_id": process_id
            }
            
        else:
            return {
                "status": "error",
                "message": f"Unknown query_type: {query_type}",
                "process_id": process_id
            }
    
    def _extract_features(self, data) -> torch.Tensor:
        """
        Extract features from data using the neural network
        
        Args:
            data: Data to extract features from
            
        Returns:
            Tensor of features
        """
        # Simple feature extraction
        text = str(data)
        words = text.split()
        
        # Create a simple word embedding
        embedding = torch.zeros(min(len(words), 128), dtype=torch.float32)
        
        for i, word in enumerate(words[:embedding.size(0)]):
            # Simple hash-based embedding
            hash_val = hash(word) % 10000
            embedding[i] = (hash_val / 10000) * 2 - 1
            
        # Pad if needed
        if embedding.size(0) < 128:
            padding = torch.zeros(128 - embedding.size(0), dtype=torch.float32)
            embedding = torch.cat([embedding, padding])
            
        return embedding.unsqueeze(0)  # Add batch dimension
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of the module
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New development level
        """
        old_level = self.development_level
        
        # Update development level
        self.development_level = max(0.0, min(1.0, self.development_level + amount))
        
        # Update neural network
        self.network.set_development_level(self.development_level)
        
        # Check if crossed a milestone
        for level in sorted(self.development_milestones.keys()):
            if old_level < level <= self.development_level:
                milestone = self.development_milestones[level]
                
                # Publish milestone event if we have an event bus
                if self.event_bus:
                    self.event_bus.publish({
                        "sender": self.module_id,
                        "message_type": "development_milestone",
                        "content": {
                            "module": "personality_traits",
                            "milestone": milestone,
                            "level": level
                        }
                    })
                
                # Log milestone
                print(f"Personality Traits Development Milestone: {milestone} (level {level})")
                
                # Perform developmental updates
                if level == 0.4:
                    # At this level, traits become more stable
                    for trait_id, trait in self.profile.traits.items():
                        trait.stability = min(0.7, trait.stability + 0.2)
                        self.profile.traits[trait_id] = trait
                        
                elif level == 0.6:
                    # At this level, traits organize into more coherent dimensions
                    self.profile.integration = min(0.8, self.profile.integration + 0.3)
                    self.profile.differentiation = min(0.8, self.profile.differentiation + 0.3)
                    
                elif level == 0.8:
                    # At this level, traits become highly stable
                    for trait_id, trait in self.profile.traits.items():
                        trait.stability = min(0.9, trait.stability + 0.2)
                        self.profile.traits[trait_id] = trait
                        
                    self.profile.stability = min(0.9, self.profile.stability + 0.3)
        
        return self.development_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the personality traits module
        
        Returns:
            Dict representing the current state
        """
        return {
            "module_id": self.module_id,
            "personality_profile": self.profile.dict(),
            "developmental_level": self.development_level,
            "trait_count": len(self.profile.traits),
            "dimension_count": len(self.profile.dimensions)
        }
    
    def save_state(self) -> Dict[str, Any]:
        """
        Save the current state for persistence
        
        Returns:
            Dict with serializable state
        """
        # Convert embeddings to lists for serialization
        trait_embeddings_serialized = {}
        for trait_id, embedding in self.trait_embeddings.items():
            trait_embeddings_serialized[trait_id] = embedding.numpy().tolist()
            
        dimension_embeddings_serialized = {}
        for dim_id, embedding in self.dimension_embeddings.items():
            dimension_embeddings_serialized[dim_id] = embedding.numpy().tolist()
            
        return {
            "module_id": self.module_id,
            "personality_profile": self.profile.dict(),
            "developmental_level": self.development_level,
            "trait_embeddings": trait_embeddings_serialized,
            "dimension_embeddings": dimension_embeddings_serialized
        }
    
    def load_state(self, state: Dict[str, Any]) -> None:
        """
        Load a previously saved state
        
        Args:
            state: The state to load
        """
        # Load basic state
        self.module_id = state["module_id"]
        self.development_level = state["developmental_level"]
        
        # Update network development level
        self.network.set_development_level(self.development_level)
        
        # Load personality profile
        if "personality_profile" in state:
            try:
                profile_data = state["personality_profile"]
                self.profile = PersonalityProfile(**profile_data)
            except Exception as e:
                print(f"Error loading personality profile: {e}")
        
        # Load embeddings
        if "trait_embeddings" in state:
            self.trait_embeddings = {}
            for trait_id, embedding_list in state["trait_embeddings"].items():
                self.trait_embeddings[trait_id] = torch.tensor(embedding_list, dtype=torch.float32)
                
        if "dimension_embeddings" in state:
            self.dimension_embeddings = {}
            for dim_id, embedding_list in state["dimension_embeddings"].items():
                self.dimension_embeddings[dim_id] = torch.tensor(embedding_list, dtype=torch.float32)


#######################

#identity\personal_narrative.py#
#######################

# TODO: Implement the PersonalNarrative class to create autobiographical continuity
# This component should be able to:
# - Construct a coherent story of personal experiences
# - Integrate new experiences into the ongoing narrative
# - Identify themes and patterns across experiences
# - Maintain temporal continuity of identity

# TODO: Implement developmental progression in personal narrative:
# - Simple episodic sequences in early stages
# - Chronological life stories in childhood
# - Theme-based integration in adolescence
# - Complex, meaning-focused narratives in adulthood

# TODO: Create mechanisms for:
# - Narrative construction: Form coherent stories from experiences
# - Causal connection: Link events with causal relationships
# - Thematic integration: Identify recurring themes and patterns
# - Meaning-making: Extract personal significance from events

# TODO: Implement narrative characteristics:
# - Coherence: Logical and temporal consistency
# - Complexity: Multilayered interpretation of events
# - Agency: Sense of control in one's life story
# - Emotional tone: Overall valence of the narrative

# TODO: Connect to episodic memory and belief systems
# Personal narrative should draw on episodic memories
# and influence/be influenced by the belief system

import logging
import uuid
import time
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

import numpy as np
import torch
from collections import deque

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.identity.models import NarrativeEvent, NarrativeTheme, PersonalNarrative as NarrativeModel, IdentityNeuralState
from lmm_project.modules.identity.neural_net import NarrativeNetwork, get_device

# Initialize logger
logger = logging.getLogger(__name__)

class PersonalNarrative(BaseModule):
    """
    Creates and maintains autobiographical continuity
    
    This module constructs a coherent story from experiences,
    providing a sense of continuity and meaning to identity.
    """
    
    # Development milestones
    development_milestones = {
        0.0: "Simple episodic memory",
        0.2: "Temporal sequences",
        0.4: "Causal connections",
        0.6: "Thematic integration",
        0.8: "Coherent life story",
        1.0: "Meaning-focused narrative"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the personal narrative module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level of this module
        """
        super().__init__(
            module_id=module_id, 
            module_type="personal_narrative", 
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Initialize device for neural processing
        self.device = get_device()
        
        # Initialize neural network
        self.network = NarrativeNetwork().to(self.device)
        self.network.set_development_level(development_level)
        
        # Initialize personal narrative state
        self.narrative = NarrativeModel()
        
        # Initialize neural state for tracking
        self.neural_state = IdentityNeuralState()
        self.neural_state.narrative_development = development_level
        
        # Event memory cache (for temporal sequence processing)
        self.event_embeddings = {}  # event_id -> embedding tensor
        
        # Available life periods (expands with development)
        self.available_life_periods = ["immediate"]
        self._adjust_life_periods_for_development()
        
        # Recent processing queue
        self.recent_inputs = deque(maxlen=100)
        
        logger.info(f"Personal narrative module initialized at development level {development_level:.2f}")
    
    def _adjust_life_periods_for_development(self):
        """Adjust available life periods based on developmental level"""
        if self.development_level < 0.2:
            # Very limited temporal span at early stages
            self.available_life_periods = ["immediate"]
            
        elif self.development_level < 0.4:
            # Basic temporal categories
            self.available_life_periods = ["immediate", "recent_past"]
            
        elif self.development_level < 0.6:
            # More differentiated temporal categories
            self.available_life_periods = ["immediate", "recent_past", "early_memories"]
            
        elif self.development_level < 0.8:
            # Life stage temporal categories
            self.available_life_periods = ["immediate", "recent_past", "early_memories", "childhood", "current_period"]
            
        else:
            # Full life narrative categories
            self.available_life_periods = [
                "immediate", "recent_past", "early_memories", "early_childhood", 
                "middle_childhood", "adolescence", "young_adulthood", "current_period", "anticipated_future"
            ]
            
        logger.info(f"Personal narrative life periods adjusted to: {self.available_life_periods}")
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to update personal narrative
        
        Args:
            input_data: Dictionary containing narrative operations
                Required keys:
                - 'operation': The operation to perform
                  Options: 'add_event', 'update_event', 'extract_theme', 'query_narrative'
                
                For 'add_event' operation:
                - 'title': Title of the event
                - 'description': Description of what happened
                - 'interpretation': (Optional) Personal meaning of the event
                - 'emotional_impact': (Optional) Emotional reactions to the event
                - 'age_period': Life period when this event occurred
                - 'importance': (Optional) Subjective importance (0.0 to 1.0)
                
                For 'update_event' operation:
                - 'event_id': ID of the event to update
                - 'interpretation': (Optional) Updated interpretation
                - 'importance': (Optional) Updated importance
                - 'emotional_impact': (Optional) Updated emotional impact
                
                For 'extract_theme' operation:
                - 'event_ids': List of event IDs to analyze for themes
                - 'name': (Optional) Suggested theme name
                
                For 'query_narrative' operation:
                - 'query_type': Type of query ('events', 'themes', 'coherence', 'all')
                - 'life_period': (Optional) Life period to filter by
            
        Returns:
            Dictionary with the results of narrative processing
        """
        operation = input_data.get("operation", "query_narrative")
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Process based on operation
        if operation == "add_event":
            return self._add_event(input_data, process_id)
        elif operation == "update_event":
            return self._update_event(input_data, process_id)
        elif operation == "extract_theme":
            return self._extract_theme(input_data, process_id)
        elif operation == "query_narrative":
            return self._query_narrative(input_data, process_id)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "process_id": process_id
            }
    
    def _add_event(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Add a new event to the personal narrative"""
        # Extract required data
        title = input_data.get("title")
        description = input_data.get("description")
        age_period = input_data.get("age_period")
        
        if not title:
            return {"status": "error", "message": "No title provided", "process_id": process_id}
        if not description:
            return {"status": "error", "message": "No description provided", "process_id": process_id}
        if not age_period:
            return {"status": "error", "message": "No age_period provided", "process_id": process_id}
            
        # Check if age period is available at current development level
        if age_period not in self.available_life_periods:
            return {
                "status": "undeveloped",
                "message": f"Life period '{age_period}' not available at current development level",
                "available_life_periods": self.available_life_periods,
                "process_id": process_id
            }
            
        # Extract optional data
        interpretation = input_data.get("interpretation", "")
        emotional_impact = input_data.get("emotional_impact", {})
        importance = input_data.get("importance", 0.5)
        
        # Get past events for context (if development permits)
        past_events_tensor = None
        if self.development_level >= 0.4 and self.event_embeddings:
            # Create tensor of past event embeddings for contextual processing
            past_embeddings = list(self.event_embeddings.values())
            if past_embeddings:
                past_events_tensor = torch.stack(past_embeddings, dim=0).unsqueeze(0)  # [1, num_events, hidden_dim]
        
        # Process event through neural network
        input_features = self._extract_features(description)
        
        with torch.no_grad():
            network_output = self.network(
                input_features.to(self.device),
                operation="process_event",
                past_events=past_events_tensor.to(self.device) if past_events_tensor is not None else None
            )
        
        # Create new event
        event_id = str(uuid.uuid4())
        
        # Generate interpretation based on development level
        if not interpretation and self.development_level >= 0.6:
            # At higher development levels, generate interpretation from neural network
            interpretation_embedding = network_output["interpretation"]
            # For demo, convert embedding to simple text
            if torch.sum(interpretation_embedding) > 0:
                interpretation = f"This event relates to my sense of {self._get_narrative_theme_from_embedding(interpretation_embedding)}"
        
        # Adjust importance based on neural network and development level
        if self.development_level >= 0.2:
            importance = max(0.0, min(1.0, importance * 0.5 + network_output["importance"].item() * 0.5))
        
        # Create the event
        new_event = NarrativeEvent(
            event_id=event_id,
            title=title,
            description=description,
            interpretation=interpretation,
            emotional_impact=emotional_impact,
            importance=importance,
            age_period=age_period,
            themes=[],  # No themes initially
            related_events=[]  # No related events initially
        )
        
        # Add to narrative
        self.narrative.add_event(new_event)
        
        # Store event embedding for future reference
        self.event_embeddings[event_id] = network_output["event_encoding"].cpu().squeeze(0)
        
        # Record activation in neural state
        self.neural_state.add_activation('narrative', {
            'operation': 'add_event',
            'age_period': age_period,
            'importance': importance
        })
        
        # Check for automatic theme extraction at higher development levels
        if self.development_level >= 0.6 and len(self.narrative.events) >= 3:
            # Try to extract themes automatically after adding events
            self._auto_extract_themes()
        
        # Add to recent inputs
        self.recent_inputs.append({
            "type": "add_event",
            "data": input_data,
            "timestamp": datetime.now()
        })
        
        # Check and update narrative coherence
        self._update_narrative_coherence()
        
        return {
            "status": "success",
            "event_id": event_id,
            "operation": "add_event",
            "event": new_event.dict(),
            "process_id": process_id
        }
    
    def _update_event(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Update an existing event in the personal narrative"""
        # Extract data
        event_id = input_data.get("event_id")
        
        if not event_id:
            return {"status": "error", "message": "No event_id provided", "process_id": process_id}
            
        # Check if event exists
        if event_id not in self.narrative.events:
            return {
                "status": "error", 
                "message": f"Event with ID {event_id} not found", 
                "process_id": process_id
            }
            
        # Get the existing event
        event = self.narrative.events[event_id]
        
        # Update fields if provided
        updated = False
        
        if "interpretation" in input_data:
            event.interpretation = input_data["interpretation"]
            updated = True
            
        if "importance" in input_data:
            event.importance = max(0.0, min(1.0, float(input_data["importance"])))
            updated = True
            
        if "emotional_impact" in input_data:
            # Update emotional impact (can be partial update)
            new_impact = input_data["emotional_impact"]
            if isinstance(new_impact, dict):
                for emotion, intensity in new_impact.items():
                    event.emotional_impact[emotion] = intensity
                updated = True
                
        if "title" in input_data:
            event.title = input_data["title"]
            updated = True
            
        if "description" in input_data:
            event.description = input_data["description"]
            updated = True
            
            # Re-process through neural network to update embeddings
            input_features = self._extract_features(event.description)
            
            with torch.no_grad():
                network_output = self.network(
                    input_features.to(self.device),
                    operation="process_event"
                )
                
            # Update stored embedding
            self.event_embeddings[event_id] = network_output["event_encoding"].cpu().squeeze(0)
        
        if updated:
            # Update timestamp
            event.updated_at = datetime.now()
            
            # Update in narrative
            self.narrative.events[event_id] = event
            self.narrative.last_updated = datetime.now()
            
            # Record activation in neural state
            self.neural_state.add_activation('narrative', {
                'operation': 'update_event',
                'event_id': event_id,
                'age_period': event.age_period
            })
            
            # Check and update narrative coherence
            self._update_narrative_coherence()
            
            # Add to recent inputs
            self.recent_inputs.append({
                "type": "update_event",
                "data": input_data,
                "timestamp": datetime.now()
            })
            
            return {
                "status": "success",
                "event_id": event_id,
                "operation": "update_event",
                "event": event.dict(),
                "process_id": process_id
            }
        else:
            return {
                "status": "not_modified",
                "message": "No changes were made to the event",
                "process_id": process_id
            }
    
    def _extract_theme(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Extract a theme from a set of events"""
        # Theme extraction requires higher development
        if self.development_level < 0.4:
            return {
                "status": "undeveloped",
                "message": "Theme extraction requires higher developmental level",
                "development_level": self.development_level,
                "required_level": 0.4,
                "process_id": process_id
            }
            
        # Extract data
        event_ids = input_data.get("event_ids", [])
        suggested_name = input_data.get("name", "")
        
        if not event_ids:
            return {"status": "error", "message": "No event_ids provided", "process_id": process_id}
            
        # Check if events exist
        valid_events = []
        for event_id in event_ids:
            if event_id in self.narrative.events:
                valid_events.append(self.narrative.events[event_id])
            
        if not valid_events:
            return {
                "status": "error", 
                "message": "None of the provided event IDs were found", 
                "process_id": process_id
            }
            
        # Get embeddings for these events
        event_embeddings = []
        for event in valid_events:
            if event.event_id in self.event_embeddings:
                event_embeddings.append(self.event_embeddings[event.event_id])
                
        if not event_embeddings:
            # Re-process events to get embeddings
            for event in valid_events:
                input_features = self._extract_features(event.description)
                
                with torch.no_grad():
                    network_output = self.network(
                        input_features.to(self.device),
                        operation="process_event"
                    )
                    
                self.event_embeddings[event.event_id] = network_output["event_encoding"].cpu().squeeze(0)
                event_embeddings.append(self.event_embeddings[event.event_id])
                
        # Average embeddings to get theme representation
        if event_embeddings:
            avg_embedding = torch.stack(event_embeddings).mean(dim=0).unsqueeze(0)
            
            # Process through neural network for theme extraction
            with torch.no_grad():
                theme_output = self.network(
                    avg_embedding.to(self.device),
                    operation="extract_theme"
                )
                
            theme_vector = theme_output["theme_vector"].cpu().squeeze(0)
        else:
            return {
                "status": "error",
                "message": "Could not generate embeddings for events",
                "process_id": process_id
            }
            
        # Generate theme name and description if not provided
        if not suggested_name:
            # Generate theme name based on common elements
            if self.development_level < 0.6:
                suggested_name = f"Theme from {valid_events[0].age_period}"
            else:
                suggested_name = self._get_narrative_theme_from_embedding(theme_vector)
                
        # Generate theme description
        if self.development_level < 0.6:
            description = f"Common elements in events from {valid_events[0].age_period}"
        else:
            # More sophisticated theme description at higher development levels
            common_elements = []
            if len(valid_events) >= 2:
                # Look for common emotional impacts
                emotions = {}
                for event in valid_events:
                    for emotion, intensity in event.emotional_impact.items():
                        if emotion not in emotions:
                            emotions[emotion] = []
                        emotions[emotion].append(intensity)
                
                # Find emotions present in multiple events
                for emotion, intensities in emotions.items():
                    if len(intensities) >= len(valid_events) / 2:
                        avg_intensity = sum(intensities) / len(intensities)
                        if avg_intensity > 0.5:
                            common_elements.append(f"strong {emotion}")
                        else:
                            common_elements.append(emotion)
                            
            if not common_elements:
                description = f"A recurring pattern across events in my life"
            else:
                description = f"A recurring pattern involving {', '.join(common_elements)}"
                
        # Calculate emotional tone of the theme
        emotional_tone = 0.0
        emotion_count = 0
        for event in valid_events:
            for emotion, intensity in event.emotional_impact.items():
                # Simple mapping of common emotions to valence
                if emotion in ["joy", "happiness", "excitement", "satisfaction", "pride"]:
                    emotional_tone += intensity
                    emotion_count += 1
                elif emotion in ["sadness", "fear", "anger", "disappointment", "shame"]:
                    emotional_tone -= intensity
                    emotion_count += 1
        
        if emotion_count > 0:
            emotional_tone = emotional_tone / emotion_count
        emotional_tone = max(-1.0, min(1.0, emotional_tone))
        
        # Calculate importance of the theme
        importance = sum(event.importance for event in valid_events) / len(valid_events)
        
        # Create the theme
        theme_id = str(uuid.uuid4())
        new_theme = NarrativeTheme(
            theme_id=theme_id,
            name=suggested_name,
            description=description,
            events=[event.event_id for event in valid_events],
            emotional_tone=emotional_tone,
            importance=importance
        )
        
        # Add to narrative
        self.narrative.add_theme(new_theme)
        
        # Record activation in neural state
        self.neural_state.add_activation('narrative', {
            'operation': 'extract_theme',
            'event_count': len(valid_events),
            'importance': importance
        })
        
        # Add to recent inputs
        self.recent_inputs.append({
            "type": "extract_theme",
            "data": input_data,
            "timestamp": datetime.now()
        })
        
        # Check and update narrative coherence
        self._update_narrative_coherence()
        
        return {
            "status": "success",
            "theme_id": theme_id,
            "operation": "extract_theme",
            "theme": new_theme.dict(),
            "process_id": process_id
        }
    
    def _query_narrative(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Query personal narrative information"""
        query_type = input_data.get("query_type", "all")
        life_period = input_data.get("life_period")
        
        if query_type == "events":
            # Return events, filtered by life period if specified
            if life_period:
                if life_period not in self.narrative.life_periods:
                    return {
                        "status": "not_found",
                        "message": f"No events found in life period '{life_period}'",
                        "process_id": process_id
                    }
                    
                # Get events in the specified life period
                period_events = {}
                for event_id in self.narrative.life_periods[life_period]:
                    if event_id in self.narrative.events:
                        period_events[event_id] = self.narrative.events[event_id].dict()
                        
                return {
                    "status": "success",
                    "operation": "query_narrative",
                    "query_type": "events",
                    "life_period": life_period,
                    "events": period_events,
                    "count": len(period_events),
                    "process_id": process_id
                }
            else:
                # Return all events
                return {
                    "status": "success",
                    "operation": "query_narrative",
                    "query_type": "events",
                    "events": {id: event.dict() for id, event in self.narrative.events.items()},
                    "count": len(self.narrative.events),
                    "process_id": process_id
                }
                
        elif query_type == "themes":
            # Return all themes
            return {
                "status": "success",
                "operation": "query_narrative",
                "query_type": "themes",
                "themes": {id: theme.dict() for id, theme in self.narrative.themes.items()},
                "count": len(self.narrative.themes),
                "process_id": process_id
            }
            
        elif query_type == "coherence":
            # Return narrative coherence information
            return {
                "status": "success",
                "operation": "query_narrative",
                "query_type": "coherence",
                "coherence": self.narrative.coherence,
                "emotional_tone": self.narrative.emotional_tone,
                "agency": self.narrative.agency,
                "process_id": process_id
            }
            
        elif query_type == "all":
            # Return complete narrative state
            return {
                "status": "success",
                "operation": "query_narrative",
                "query_type": "all",
                "narrative": self.narrative.dict(),
                "available_life_periods": self.available_life_periods,
                "development_level": self.development_level,
                "process_id": process_id
            }
            
        else:
            return {
                "status": "error",
                "message": f"Unknown query_type: {query_type}",
                "process_id": process_id
            }
    
    def _auto_extract_themes(self):
        """Automatically extract themes from events at higher development levels"""
        # Require sufficient development and events
        if self.development_level < 0.6 or len(self.narrative.events) < 3:
            return
            
        # Group events by life period
        period_events = {}
        for event_id, event in self.narrative.events.items():
            if event.age_period not in period_events:
                period_events[event.age_period] = []
            period_events[event.age_period].append(event_id)
            
        # Extract themes for each period with enough events
        for period, event_ids in period_events.items():
            if len(event_ids) >= 3:
                # Check if we already have a theme for this period
                period_themes = []
                for theme_id, theme in self.narrative.themes.items():
                    if all(event_id in theme.events for event_id in event_ids):
                        period_themes.append(theme_id)
                        
                # Skip if we already have a theme for this period
                if period_themes:
                    continue
                    
                # Extract a theme
                self._extract_theme({
                    "event_ids": event_ids,
                    "name": f"Theme from {period}"
                }, str(uuid.uuid4()))
    
    def _update_narrative_coherence(self):
        """Update narrative coherence metrics"""
        # Skip if not enough events
        if len(self.narrative.events) < 2:
            self.narrative.coherence = 0.5
            self.narrative.emotional_tone = 0.0
            self.narrative.agency = 0.5
            return
            
        # Use neural network to evaluate coherence if development permits
        if self.development_level >= 0.6 and self.event_embeddings:
            # Create tensor of all event embeddings
            event_embeddings = list(self.event_embeddings.values())
            if event_embeddings:
                all_events_tensor = torch.stack(event_embeddings, dim=0).unsqueeze(0)  # [1, num_events, hidden_dim]
                
                with torch.no_grad():
                    coherence_output = self.network(
                        torch.zeros((1, 128), device=self.device),  # Dummy input
                        operation="evaluate_coherence",
                        past_events=all_events_tensor.to(self.device)
                    )
                    
                # Update coherence value
                self.narrative.coherence = coherence_output["coherence"].item()
        else:
            # Basic coherence calculation based on number of themes
            themes_ratio = min(1.0, len(self.narrative.themes) / max(1, len(self.narrative.events) / 3))
            self.narrative.coherence = 0.3 + 0.4 * themes_ratio
            
        # Calculate emotional tone across all events
        if self.narrative.events:
            total_tone = 0.0
            emotion_count = 0
            for event in self.narrative.events.values():
                for emotion, intensity in event.emotional_impact.items():
                    # Simple mapping of common emotions to valence
                    if emotion in ["joy", "happiness", "excitement", "satisfaction", "pride"]:
                        total_tone += intensity
                        emotion_count += 1
                    elif emotion in ["sadness", "fear", "anger", "disappointment", "shame"]:
                        total_tone -= intensity
                        emotion_count += 1
            
            if emotion_count > 0:
                self.narrative.emotional_tone = max(-1.0, min(1.0, total_tone / emotion_count))
                
        # Calculate agency based on content (very simplistic)
        if self.development_level >= 0.4:
            self.narrative.agency = min(0.8, 0.4 + self.development_level * 0.4)
        else:
            self.narrative.agency = 0.4
            
        # Update timestamp
        self.narrative.last_updated = datetime.now()
    
    def _get_narrative_theme_from_embedding(self, embedding: torch.Tensor) -> str:
        """Generate a theme name from an embedding"""
        # Basic theme possibilities
        basic_themes = [
            "personal growth", "achievement", "relationships", "challenge", 
            "change", "learning", "responsibility", "identity", "connection"
        ]
        
        # For demonstration, use the embedding to select a theme
        # In a real implementation, this would use more sophisticated methods
        if isinstance(embedding, torch.Tensor):
            embedding_sum = torch.sum(embedding).item()
            theme_index = abs(hash(str(embedding_sum))) % len(basic_themes)
            return basic_themes[theme_index]
        else:
            # Random fallback
            return basic_themes[abs(hash(str(time.time()))) % len(basic_themes)]
    
    def _extract_features(self, data) -> torch.Tensor:
        """
        Extract features from input data for neural processing
        
        Args:
            data: Text or other data to extract features from
            
        Returns:
            Tensor of features [1, feature_dim]
        """
        # For demonstration, create simple random features
        # In a real implementation, this would use proper feature extraction
        feature_dim = 128
        
        if isinstance(data, str):
            # Seed random generator with hash of string to ensure consistent features
            seed = hash(data) % 10000
            np.random.seed(seed)
            
            # Generate "features" based on the text
            features = np.random.randn(feature_dim)
            features = features / np.linalg.norm(features)  # Normalize
            
        elif isinstance(data, dict):
            # For dictionary data, use keys and values to generate features
            seed = hash(str(sorted(data.items()))) % 10000
            np.random.seed(seed)
            
            features = np.random.randn(feature_dim)
            features = features / np.linalg.norm(features)  # Normalize
            
        else:
            # Default random features
            seed = hash(str(data)) % 10000
            np.random.seed(seed)
            
            features = np.random.randn(feature_dim)
            features = features / np.linalg.norm(features)  # Normalize
        
        return torch.tensor(features, dtype=torch.float32).unsqueeze(0)
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        old_level = self.development_level
        
        # Update base development level
        new_level = super().update_development(amount)
        
        # Update network development level
        self.network.set_development_level(new_level)
        
        # Update neural state
        self.neural_state.narrative_development = new_level
        self.neural_state.last_updated = datetime.now()
        
        # If crossing a developmental threshold, adjust available life periods
        if int(old_level * 5) != int(new_level * 5):
            self._adjust_life_periods_for_development()
            
            # Re-evaluate narrative coherence with new capabilities
            self._update_narrative_coherence()
        
        logger.info(f"Personal narrative development updated to {new_level:.2f}")
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the module
        
        Returns:
            Dictionary containing current module state
        """
        # Get base state from parent
        base_state = super().get_state()
        
        # Add narrative-specific state
        narrative_dict = self.narrative.dict()
        
        # Add neural state
        neural_state = {
            "development_level": self.neural_state.narrative_development,
            "accuracy": self.neural_state.narrative_accuracy,
            "recent_activations_count": len(self.neural_state.recent_narrative_activations)
        }
        
        # Combine states
        combined_state = {
            **base_state, 
            "narrative": narrative_dict,
            "available_life_periods": self.available_life_periods,
            "neural_state": neural_state
        }
        
        return combined_state


#######################

#identity\preferences.py#
#######################

# TODO: Implement the Preferences class to track likes, dislikes, and values
# This component should be able to:
# - Represent preferences across different domains
# - Update preferences based on experiences
# - Form preference hierarchies and priorities
# - Generate preference-based choices

# TODO: Implement developmental progression in preferences:
# - Simple approach/avoid preferences in early stages
# - Concrete likes and dislikes in childhood
# - Value-based preferences in adolescence
# - Stable yet flexible preference systems in adulthood

# TODO: Create mechanisms for:
# - Preference formation: Develop likes/dislikes from experiences
# - Preference integration: Organize preferences into coherent systems
# - Value extraction: Derive abstract values from concrete preferences
# - Preference application: Use preferences to guide decisions

# TODO: Implement different preference types:
# - Sensory preferences: Likes/dislikes for physical sensations
# - Activity preferences: Preferred activities and pastimes
# - Social preferences: Preferred interaction styles and partners
# - Abstract preferences: Values and principles

# TODO: Connect to emotion and memory systems
# Preferences should be influenced by emotional responses
# and should draw on memories of past experiences

import logging
import uuid
import time
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

import numpy as np
import torch
from collections import deque

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.identity.models import Preference, Value, PreferenceSystem, IdentityNeuralState
from lmm_project.modules.identity.neural_net import PreferenceNetwork, get_device

# Initialize logger
logger = logging.getLogger(__name__)

class Preferences(BaseModule):
    """
    Manages preferences, likes, dislikes, and values
    
    This module tracks preferences across different domains and
    extracts higher-level values from preference patterns.
    """
    
    # Development milestones
    development_milestones = {
        0.0: "Basic approach/avoid preferences",
        0.2: "Domain-specific likes and dislikes",
        0.4: "Preference hierarchies",
        0.6: "Abstract value formation",
        0.8: "Integrated value system",
        1.0: "Sophisticated preference/value system"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the preferences module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level of this module
        """
        super().__init__(
            module_id=module_id, 
            module_type="preferences", 
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Initialize device for neural processing
        self.device = get_device()
        
        # Initialize neural network
        self.network = PreferenceNetwork().to(self.device)
        self.network.set_development_level(development_level)
        
        # Initialize preference system
        self.preference_system = PreferenceSystem()
        
        # Initialize neural state for tracking
        self.neural_state = IdentityNeuralState()
        self.neural_state.preference_development = development_level
        
        # Available preference domains (expands with development)
        self.available_domains = ["sensory"]
        self._adjust_domains_for_development()
        
        # Recent processing queue
        self.recent_inputs = deque(maxlen=100)
        
        logger.info(f"Preferences module initialized at development level {development_level:.2f}")
    
    def _adjust_domains_for_development(self):
        """Adjust available preference domains based on developmental level"""
        if self.development_level < 0.2:
            # Very basic sensory preferences at early stages
            self.available_domains = ["sensory"]
            
        elif self.development_level < 0.4:
            # Basic concrete preferences
            self.available_domains = ["sensory", "food", "activities", "people"]
            
        elif self.development_level < 0.6:
            # More differentiated preference domains
            self.available_domains = ["sensory", "food", "activities", "people", "entertainment", "aesthetics"]
            
        elif self.development_level < 0.8:
            # Higher-level preference domains
            self.available_domains = [
                "sensory", "food", "activities", "people", "entertainment", 
                "aesthetics", "social", "intellectual", "achievement"
            ]
            
        else:
            # Abstract and value-based preference domains
            self.available_domains = [
                "sensory", "food", "activities", "people", "entertainment", 
                "aesthetics", "social", "intellectual", "achievement", 
                "moral", "political", "spiritual", "personal_growth"
            ]
            
        logger.info(f"Preference domains adjusted to: {self.available_domains}")
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to update preferences
        
        Args:
            input_data: Dictionary containing preference operations
                Required keys:
                - 'operation': The operation to perform
                  Options: 'add_preference', 'update_preference', 'extract_value', 'query_preferences'
                
                For 'add_preference' operation:
                - 'domain': Domain of the preference (e.g., 'food', 'activities')
                - 'target': Target of the preference
                - 'valence': (Optional) Degree of liking/disliking (-1.0 to 1.0)
                - 'reasons': (Optional) Reasons for this preference
                - 'experiences': (Optional) Experiences related to this preference
                
                For 'update_preference' operation:
                - 'preference_id': ID of the preference to update
                - 'valence': (Optional) Updated degree of liking/disliking
                - 'strength': (Optional) Updated strength of preference
                - 'reasons': (Optional) Additional reasons
                
                For 'extract_value' operation:
                - 'preference_ids': List of preference IDs to analyze for underlying values
                - 'name': (Required) Name of the value
                - 'description': (Required) Description of the value
                
                For 'query_preferences' operation:
                - 'query_type': Type of query ('preferences', 'values', 'domains', 'all')
                - 'domain': (Optional) Domain to filter by
            
        Returns:
            Dictionary with the results of preference processing
        """
        operation = input_data.get("operation", "query_preferences")
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Process based on operation
        if operation == "add_preference":
            return self._add_preference(input_data, process_id)
        elif operation == "update_preference":
            return self._update_preference(input_data, process_id)
        elif operation == "extract_value":
            return self._extract_value(input_data, process_id)
        elif operation == "query_preferences":
            return self._query_preferences(input_data, process_id)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "process_id": process_id
            }
    
    def _add_preference(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Add a new preference"""
        # Extract required data
        domain = input_data.get("domain")
        target = input_data.get("target")
        
        if not domain:
            return {"status": "error", "message": "No domain provided", "process_id": process_id}
        if not target:
            return {"status": "error", "message": "No target provided", "process_id": process_id}
            
        # Check if domain is available at current development level
        if domain not in self.available_domains:
            return {
                "status": "undeveloped",
                "message": f"Domain '{domain}' not available at current development level",
                "available_domains": self.available_domains,
                "process_id": process_id
            }
            
        # Extract optional data
        valence = input_data.get("valence", 0.0)
        reasons = input_data.get("reasons", [])
        related_experiences = input_data.get("experiences", [])
        
        # Process preference through neural network
        input_features = self._extract_features(target)
        
        with torch.no_grad():
            network_output = self.network(
                input_features.to(self.device),
                operation="form_preference"
            )
        
        # Create new preference
        preference_id = str(uuid.uuid4())
        
        # Apply development-specific processing
        if self.development_level < 0.2:
            # At early development, preferences are simple and binary
            valence = 1.0 if valence > 0 else -1.0
            strength = 1.0
            certainty = 1.0
        else:
            # More nuanced preferences with development
            # Blend provided valence with network output
            if "valence" in input_data:
                valence = valence * 0.7 + network_output["valence"].item() * 0.3
            else:
                valence = network_output["valence"].item()
                
            # Get strength and certainty from network
            strength = network_output["strength"].item()
            certainty = network_output["certainty"].item()
            
        # Ensure values are in appropriate ranges
        valence = max(-1.0, min(1.0, valence))
        strength = max(0.0, min(1.0, strength))
        certainty = max(0.0, min(1.0, certainty))
        
        # Create preference
        new_preference = Preference(
            preference_id=preference_id,
            domain=domain,
            target=target,
            valence=valence,
            strength=strength,
            certainty=certainty,
            reasons=reasons if isinstance(reasons, list) else [reasons],
            related_experiences=related_experiences if isinstance(related_experiences, list) else [related_experiences]
        )
        
        # Add to preference system
        self.preference_system.add_preference(new_preference)
        
        # Record activation in neural state
        self.neural_state.add_activation('preference', {
            'operation': 'add_preference',
            'domain': domain,
            'valence': valence,
            'strength': strength
        })
        
        # Check for value extraction at higher development levels
        if self.development_level >= 0.6 and len(self.preference_system.preferences) >= 5:
            # Attempt automatic value extraction
            self._auto_extract_values()
        
        # Add to recent inputs
        self.recent_inputs.append({
            "type": "add_preference",
            "data": input_data,
            "timestamp": datetime.now()
        })
        
        # Update consistency score
        self._update_preference_consistency()
        
        return {
            "status": "success",
            "preference_id": preference_id,
            "operation": "add_preference",
            "preference": new_preference.dict(),
            "process_id": process_id
        }
    
    def _update_preference(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Update an existing preference"""
        # Extract data
        preference_id = input_data.get("preference_id")
        
        if not preference_id:
            return {"status": "error", "message": "No preference_id provided", "process_id": process_id}
            
        # Check if preference exists
        if preference_id not in self.preference_system.preferences:
            return {
                "status": "error", 
                "message": f"Preference with ID {preference_id} not found", 
                "process_id": process_id
            }
            
        # Get the existing preference
        preference = self.preference_system.preferences[preference_id]
        
        # Update fields if provided
        updated = False
        
        if "valence" in input_data:
            valence = max(-1.0, min(1.0, float(input_data["valence"])))
            if self.development_level < 0.2:
                # At early development, preferences are binary
                valence = 1.0 if valence > 0 else -1.0
            preference.valence = valence
            updated = True
            
        if "strength" in input_data and self.development_level >= 0.2:
            preference.strength = max(0.0, min(1.0, float(input_data["strength"])))
            updated = True
            
        if "certainty" in input_data and self.development_level >= 0.4:
            preference.certainty = max(0.0, min(1.0, float(input_data["certainty"])))
            updated = True
            
        if "reasons" in input_data:
            new_reasons = input_data["reasons"]
            if isinstance(new_reasons, list):
                preference.reasons.extend(new_reasons)
            else:
                preference.reasons.append(new_reasons)
            updated = True
            
        if "experiences" in input_data:
            new_experiences = input_data["experiences"]
            if isinstance(new_experiences, list):
                preference.related_experiences.extend(new_experiences)
            else:
                preference.related_experiences.append(new_experiences)
            updated = True
            
        if "target" in input_data:
            preference.target = input_data["target"]
            updated = True
        
        if updated:
            # Update timestamp
            preference.updated_at = datetime.now()
            
            # Update in preference system
            self.preference_system.preferences[preference_id] = preference
            self.preference_system.last_updated = datetime.now()
            
            # Record activation in neural state
            self.neural_state.add_activation('preference', {
                'operation': 'update_preference',
                'domain': preference.domain,
                'preference_id': preference_id
            })
            
            # Update consistency score
            self._update_preference_consistency()
            
            # Add to recent inputs
            self.recent_inputs.append({
                "type": "update_preference",
                "data": input_data,
                "timestamp": datetime.now()
            })
            
            return {
                "status": "success",
                "preference_id": preference_id,
                "operation": "update_preference",
                "preference": preference.dict(),
                "process_id": process_id
            }
        else:
            return {
                "status": "not_modified",
                "message": "No changes were made to the preference",
                "process_id": process_id
            }
    
    def _extract_value(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Extract a value from a set of preferences"""
        # Value extraction requires higher development
        if self.development_level < 0.6:
            return {
                "status": "undeveloped",
                "message": "Value extraction requires higher developmental level",
                "development_level": self.development_level,
                "required_level": 0.6,
                "process_id": process_id
            }
            
        # Extract data
        preference_ids = input_data.get("preference_ids", [])
        name = input_data.get("name")
        description = input_data.get("description")
        
        if not preference_ids:
            return {"status": "error", "message": "No preference_ids provided", "process_id": process_id}
        if not name:
            return {"status": "error", "message": "No name provided", "process_id": process_id}
        if not description:
            return {"status": "error", "message": "No description provided", "process_id": process_id}
            
        # Check if preferences exist
        valid_preferences = []
        for pref_id in preference_ids:
            if pref_id in self.preference_system.preferences:
                valid_preferences.append(pref_id)
            
        if not valid_preferences:
            return {
                "status": "error", 
                "message": "None of the provided preference IDs were found", 
                "process_id": process_id
            }
            
        # Process through neural network
        # Combine embeddings from all preferences
        preference_embeddings = []
        for pref_id in valid_preferences:
            pref = self.preference_system.preferences[pref_id]
            input_features = self._extract_features(pref.target)
            
            with torch.no_grad():
                preference_output = self.network(
                    input_features.to(self.device),
                    operation="extract_value"
                )
                
            preference_embeddings.append(preference_output["value_vector"].cpu().squeeze(0))
        
        # Average the embeddings
        if preference_embeddings:
            avg_embedding = torch.stack(preference_embeddings).mean(dim=0)
            
            # Calculate importance based on network output and preference strengths
            importance_sum = 0.0
            for pref_id in valid_preferences:
                pref = self.preference_system.preferences[pref_id]
                importance_sum += pref.strength * pref.certainty
            
            if len(valid_preferences) > 0:
                importance_base = importance_sum / len(valid_preferences)
            else:
                importance_base = 0.5
                
            # Create the value
            value_id = str(uuid.uuid4())
            new_value = Value(
                value_id=value_id,
                name=name,
                description=description,
                importance=importance_base,
                related_preferences=valid_preferences
            )
            
            # Add to preference system
            self.preference_system.add_value(new_value)
            
            # Record activation in neural state
            self.neural_state.add_activation('preference', {
                'operation': 'extract_value',
                'value_name': name,
                'preference_count': len(valid_preferences),
                'importance': importance_base
            })
            
            # Update consistency score
            self._update_preference_consistency()
            
            # Add to recent inputs
            self.recent_inputs.append({
                "type": "extract_value",
                "data": input_data,
                "timestamp": datetime.now()
            })
            
            return {
                "status": "success",
                "value_id": value_id,
                "operation": "extract_value",
                "value": new_value.dict(),
                "process_id": process_id
            }
        else:
            return {
                "status": "error",
                "message": "Could not process preferences to extract value",
                "process_id": process_id
            }
    
    def _query_preferences(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """Query preference system information"""
        query_type = input_data.get("query_type", "all")
        domain = input_data.get("domain")
        
        if query_type == "preferences":
            # Return preferences, filtered by domain if specified
            if domain:
                if domain not in self.preference_system.domains:
                    return {
                        "status": "not_found",
                        "message": f"No preferences found in domain '{domain}'",
                        "process_id": process_id
                    }
                    
                # Get preferences in the specified domain
                domain_preferences = {}
                for pref_id in self.preference_system.domains[domain]:
                    if pref_id in self.preference_system.preferences:
                        domain_preferences[pref_id] = self.preference_system.preferences[pref_id].dict()
                        
                return {
                    "status": "success",
                    "operation": "query_preferences",
                    "query_type": "preferences",
                    "domain": domain,
                    "preferences": domain_preferences,
                    "count": len(domain_preferences),
                    "process_id": process_id
                }
            else:
                # Return all preferences
                return {
                    "status": "success",
                    "operation": "query_preferences",
                    "query_type": "preferences",
                    "preferences": {id: pref.dict() for id, pref in self.preference_system.preferences.items()},
                    "count": len(self.preference_system.preferences),
                    "process_id": process_id
                }
                
        elif query_type == "values":
            # Value queries require higher development
            if self.development_level < 0.6:
                return {
                    "status": "undeveloped",
                    "message": "Value queries require higher developmental level",
                    "development_level": self.development_level,
                    "required_level": 0.6,
                    "process_id": process_id
                }
                
            # Return all values
            return {
                "status": "success",
                "operation": "query_preferences",
                "query_type": "values",
                "values": {id: value.dict() for id, value in self.preference_system.values.items()},
                "count": len(self.preference_system.values),
                "value_hierarchy": [self.preference_system.values[v_id].dict() if v_id in self.preference_system.values else None 
                                   for v_id in self.preference_system.value_hierarchy],
                "process_id": process_id
            }
            
        elif query_type == "domains":
            # Return available domains and preferences per domain
            result = {
                "status": "success",
                "operation": "query_preferences",
                "query_type": "domains",
                "available_domains": self.available_domains,
                "domain_counts": {d: len(ids) for d, ids in self.preference_system.domains.items()},
                "process_id": process_id
            }
            
            return result
            
        elif query_type == "all":
            # Return complete preference system state
            return {
                "status": "success",
                "operation": "query_preferences",
                "query_type": "all",
                "preference_system": self.preference_system.dict(),
                "available_domains": self.available_domains,
                "development_level": self.development_level,
                "process_id": process_id
            }
            
        else:
            return {
                "status": "error",
                "message": f"Unknown query_type: {query_type}",
                "process_id": process_id
            }
    
    def _auto_extract_values(self):
        """
        Automatically attempt to extract values from preference patterns
        
        This is used at higher development levels to find emerging values
        """
        # Require higher development level
        if self.development_level < 0.6:
            return
            
        # Group preferences by domain
        domain_preferences = {}
        for pref_id, pref in self.preference_system.preferences.items():
            if pref.domain not in domain_preferences:
                domain_preferences[pref.domain] = []
            domain_preferences[pref.domain].append(pref_id)
            
        # Look for domains with enough preferences for value extraction
        for domain, pref_ids in domain_preferences.items():
            if len(pref_ids) >= 3:
                # Check valence patterns
                positive_prefs = []
                negative_prefs = []
                
                for pref_id in pref_ids:
                    pref = self.preference_system.preferences[pref_id]
                    if pref.valence > 0.3:
                        positive_prefs.append(pref_id)
                    elif pref.valence < -0.3:
                        negative_prefs.append(pref_id)
                
                # Extract values from strong preference patterns
                if len(positive_prefs) >= 3:
                    # Check if these preferences already have a common value
                    has_common_value = False
                    for value in self.preference_system.values.values():
                        common_prefs = set(value.related_preferences).intersection(set(positive_prefs))
                        if len(common_prefs) >= 3:
                            has_common_value = True
                            break
                            
                    if not has_common_value:
                        # Generate a value
                        if domain == "food":
                            value_name = "Culinary enjoyment"
                            description = "Appreciating good food and diverse tastes"
                        elif domain == "activities":
                            value_name = "Active engagement"
                            description = "Enjoying participatory and engaging activities"
                        elif domain == "social":
                            value_name = "Social connection"
                            description = "Valuing meaningful relationships with others"
                        elif domain == "intellectual":
                            value_name = "Intellectual growth"
                            description = "Valuing learning and mental stimulation"
                        elif domain == "aesthetics":
                            value_name = "Aesthetic appreciation"
                            description = "Valuing beauty and artistic expression"
                        else:
                            value_name = f"{domain.capitalize()} appreciation"
                            description = f"Valuing positive experiences in {domain}"
                            
                        # Extract the value
                        self._extract_value({
                            "preference_ids": positive_prefs,
                            "name": value_name,
                            "description": description
                        }, str(uuid.uuid4()))
    
    def _update_preference_consistency(self):
        """Update preference consistency score"""
        # Skip if not enough preferences
        if len(self.preference_system.preferences) < 3:
            self.preference_system.consistency = 0.5
            return
            
        # Basic consistency calculation based on preference patterns
        # Group by domain
        domain_preferences = {}
        for pref_id, pref in self.preference_system.preferences.items():
            if pref.domain not in domain_preferences:
                domain_preferences[pref.domain] = []
            domain_preferences[pref.domain].append(pref)
        
        # Calculate consistency within domains
        domain_consistency = {}
        for domain, prefs in domain_preferences.items():
            if len(prefs) < 2:
                domain_consistency[domain] = 1.0  # No inconsistency with single preference
                continue
                
            # Calculate variance in valence for similar targets
            targets = {}
            for pref in prefs:
                # Simple grouping by target similarity (in a real system, would use semantic similarity)
                target_key = pref.target.lower()[:5]  # Simple prefix grouping
                if target_key not in targets:
                    targets[target_key] = []
                targets[target_key].append(pref)
            
            # Calculate consistency based on valence agreement within target groups
            if not targets:
                domain_consistency[domain] = 1.0
                continue
                
            inconsistency_sum = 0
            group_count = 0
            
            for target_group in targets.values():
                if len(target_group) > 1:
                    # Calculate variance in valence
                    valences = [pref.valence for pref in target_group]
                    mean_valence = sum(valences) / len(valences)
                    variance = sum((v - mean_valence) ** 2 for v in valences) / len(valences)
                    
                    inconsistency_sum += variance
                    group_count += 1
                    
            # Convert to consistency score
            if group_count > 0:
                domain_consistency[domain] = max(0, 1.0 - inconsistency_sum / group_count)
            else:
                domain_consistency[domain] = 1.0
                
        # Average domain consistencies
        if domain_consistency:
            consistency = sum(domain_consistency.values()) / len(domain_consistency)
        else:
            consistency = 0.5
            
        # Adjust based on value integration at higher development levels
        if self.development_level >= 0.8 and self.preference_system.values:
            # Calculate the proportion of preferences that are connected to values
            preferences_with_values = set()
            for value in self.preference_system.values.values():
                preferences_with_values.update(value.related_preferences)
                
            value_coverage = len(preferences_with_values) / max(1, len(self.preference_system.preferences))
            
            # Blend consistency with value coverage
            consistency = consistency * 0.7 + value_coverage * 0.3
            
        # Update consistency score
        self.preference_system.consistency = max(0.0, min(1.0, consistency))
        self.preference_system.last_updated = datetime.now()
    
    def _extract_features(self, data) -> torch.Tensor:
        """
        Extract features from input data for neural processing
        
        Args:
            data: Text or other data to extract features from
            
        Returns:
            Tensor of features [1, feature_dim]
        """
        # For demonstration, create simple random features
        # In a real implementation, this would use proper feature extraction
        feature_dim = 128
        
        if isinstance(data, str):
            # Seed random generator with hash of string to ensure consistent features
            seed = hash(data) % 10000
            np.random.seed(seed)
            
            # Generate "features" based on the text
            features = np.random.randn(feature_dim)
            features = features / np.linalg.norm(features)  # Normalize
            
        elif isinstance(data, dict):
            # For dictionary data, use keys and values to generate features
            seed = hash(str(sorted(data.items()))) % 10000
            np.random.seed(seed)
            
            features = np.random.randn(feature_dim)
            features = features / np.linalg.norm(features)  # Normalize
            
        else:
            # Default random features
            seed = hash(str(data)) % 10000
            np.random.seed(seed)
            
            features = np.random.randn(feature_dim)
            features = features / np.linalg.norm(features)  # Normalize
        
        return torch.tensor(features, dtype=torch.float32).unsqueeze(0)
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        old_level = self.development_level
        
        # Update base development level
        new_level = super().update_development(amount)
        
        # Update network development level
        self.network.set_development_level(new_level)
        
        # Update neural state
        self.neural_state.preference_development = new_level
        self.neural_state.last_updated = datetime.now()
        
        # If crossing a developmental threshold, adjust available domains
        if int(old_level * 5) != int(new_level * 5):
            self._adjust_domains_for_development()
            
            # Re-evaluate preference consistency with new capabilities
            self._update_preference_consistency()
            
            # Try to extract values if development is sufficient
            if new_level >= 0.6 and old_level < 0.6:
                self._auto_extract_values()
        
        logger.info(f"Preferences development updated to {new_level:.2f}")
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the module
        
        Returns:
            Dictionary containing current module state
        """
        # Get base state from parent
        base_state = super().get_state()
        
        # Add preference-specific state
        preference_system_dict = self.preference_system.dict()
        
        # Add neural state
        neural_state = {
            "development_level": self.neural_state.preference_development,
            "accuracy": self.neural_state.preference_accuracy,
            "recent_activations_count": len(self.neural_state.recent_preference_activations)
        }
        
        # Combine states
        combined_state = {
            **base_state, 
            "preference_system": preference_system_dict,
            "available_domains": self.available_domains,
            "neural_state": neural_state
        }
        
        return combined_state


#######################

#identity\self_concept.py#
#######################

# TODO: Implement the SelfConcept class to maintain beliefs about the self
# This component should be able to:
# - Represent knowledge and beliefs about the self
# - Organize self-knowledge into domains (abilities, traits, etc.)
# - Update self-concept based on experiences and feedback
# - Maintain consistency in self-representation

# TODO: Implement developmental progression in self-concept:
# - Simple categorical self-recognition in early stages
# - Concrete trait descriptions in childhood
# - Social comparison and ideal self in adolescence
# - Complex, nuanced self-understanding in adulthood

# TODO: Create mechanisms for:
# - Self-schema formation: Organize self-knowledge by domain
# - Self-evaluation: Assess self-attributes against standards
# - Identity integration: Maintain coherence across domains
# - Self-verification: Seek confirmation of existing self-views

# TODO: Implement different self-concept domains:
# - Ability domain: Beliefs about capabilities and skills
# - Social domain: Representations of social roles and identities
# - Physical domain: Beliefs about physical attributes
# - Psychological domain: Understanding of internal states and traits

# TODO: Connect to memory and social systems
# Self-concept should draw on autobiographical memory
# and incorporate social feedback and comparisons

import logging
import uuid
import time
from typing import Dict, List, Any, Optional, Set, Tuple
from datetime import datetime

import numpy as np
import torch
from collections import deque
from pydantic import ValidationError

from lmm_project.base.module import BaseModule
from lmm_project.event_bus import EventBus
from lmm_project.modules.identity.models import SelfAttribute, SelfConcept as SelfConceptModel
from lmm_project.modules.identity.neural_net import SelfConceptNetwork, get_device

# Initialize logger
logger = logging.getLogger(__name__)

class SelfConcept(BaseModule):
    """
    Represents beliefs and knowledge about oneself
    
    This module maintains and updates beliefs about the self in various domains,
    integrating new information into a coherent self-concept.
    """
    
    # Development milestones
    development_milestones = {
        0.0: "Basic self-recognition",
        0.2: "Simple categorical self-descriptions",
        0.4: "Concrete trait descriptions across domains",
        0.6: "Social comparison and ideal self-development",
        0.8: "Integration of self across domains and contexts",
        1.0: "Complex, nuanced self-understanding with temporal stability"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the self-concept module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level (0.0 to 1.0)
        """
        super().__init__(module_id, event_bus)
        
        # Initialize self-concept
        self.self_concept = SelfConceptModel()
        
        # Set initial development level
        self.development_level = max(0.0, min(1.0, development_level))
        
        # Initialize neural network
        self.device = get_device()
        self.network = SelfConceptNetwork().to(self.device)
        self.network.set_development_level(self.development_level)
        
        # Initialize attribute embeddings for similarity search
        self.attribute_embeddings = {}
        
        # Initialize with basic domains based on development level
        self._adjust_domains_for_development()
    
    def _adjust_domains_for_development(self):
        """Adjust available self-concept domains based on development level"""
        # Initialize domain structure
        domains = []
        
        # Basic domains available at initial development
        if self.development_level >= 0.0:
            domains.extend([
                "physical",  # Basic physical characteristics
                "preferences"  # Simple likes and dislikes
            ])
            
        # Additional domains that become available with development
        if self.development_level >= 0.2:
            domains.extend([
                "abilities",  # What I can and cannot do
                "social"  # Relationships with others
            ])
            
        if self.development_level >= 0.4:
            domains.extend([
                "emotions",  # Emotional tendencies
                "personality",  # Trait-based descriptions
                "academic"  # Knowledge and learning abilities
            ])
            
        if self.development_level >= 0.6:
            domains.extend([
                "values",  # Personal values and ethics
                "goals",  # Future-oriented aspirations
                "competence"  # Areas of mastery
            ])
            
        if self.development_level >= 0.8:
            domains.extend([
                "ideal_self",  # Who I want to be
                "moral",  # Moral character
                "belief_systems",  # Worldview and beliefs
                "life_narrative"  # Self as a story
            ])
        
        # Initialize domains in self-concept if not already present
        for domain in domains:
            if domain not in self.self_concept.domains:
                self.self_concept.domains[domain] = []
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to the self-concept module
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Dict with processing results
        """
        # Validate input
        if not isinstance(input_data, dict):
            return {
                "status": "error",
                "message": "Input must be a dictionary"
            }
        
        # Extract process ID if provided
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Extract operation
        operation = input_data.get("operation", "")
        
        # Dispatch to appropriate handler
        if operation == "add_attribute":
            return self._add_attribute(input_data, process_id)
        elif operation == "update_attribute":
            return self._update_attribute(input_data, process_id)
        elif operation == "query_self":
            return self._query_self(input_data, process_id)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "process_id": process_id
            }
    
    def _add_attribute(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Add a new self-attribute
        
        Args:
            input_data: Input data dictionary
            process_id: Process identifier
            
        Returns:
            Dict with processing results
        """
        # Check required fields
        required_fields = ["domain", "content"]
        for field in required_fields:
            if field not in input_data:
                return {
                    "status": "error",
                    "message": f"Missing required field: {field}",
                    "process_id": process_id
                }
        
        # Get domain
        domain = input_data["domain"]
        
        # Check if this domain is available at current development level
        if domain not in self.self_concept.domains:
            # If domain isn't recognized, check development level
            self._adjust_domains_for_development()
            
            # If still not available, return error
            if domain not in self.self_concept.domains:
                return {
                    "status": "error",
                    "message": f"Domain '{domain}' not available at current development level",
                    "process_id": process_id
                }
        
        try:
            # Create attribute
            attribute_id = str(uuid.uuid4())
            
            # Get optional fields with defaults
            confidence = input_data.get("confidence", 0.5)
            importance = input_data.get("importance", 0.5)
            valence = input_data.get("valence", 0.0)
            evidence = input_data.get("evidence", [])
            sources = input_data.get("sources", [])
            
            # Create attribute
            attribute = SelfAttribute(
                attribute_id=attribute_id,
                domain=domain,
                content=input_data["content"],
                confidence=confidence,
                importance=importance,
                valence=valence,
                evidence=evidence,
                sources=sources
            )
            
            # Add to self-concept
            self.self_concept.add_attribute(attribute)
            
            # Create embedding for this attribute
            features = self._extract_features(f"{domain}: {attribute.content}")
            with torch.no_grad():
                output = self.network(
                    features.to(self.device),
                    domain=domain
                )
                self.attribute_embeddings[attribute_id] = output["attribute_encoding"].cpu().squeeze(0)
            
            # Update self-esteem if valence is provided
            self._update_self_esteem()
            
            return {
                "status": "success",
                "message": "Attribute added successfully",
                "attribute_id": attribute_id,
                "process_id": process_id
            }
            
        except ValidationError as e:
            return {
                "status": "error",
                "message": f"Validation error: {str(e)}",
                "process_id": process_id
            }
        except Exception as e:
            return {
                "status": "error",
                "message": f"Error adding attribute: {str(e)}",
                "process_id": process_id
            }
    
    def _update_attribute(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Update an existing self-attribute
        
        Args:
            input_data: Input data dictionary
            process_id: Process identifier
            
        Returns:
            Dict with processing results
        """
        # Check for attribute ID
        if "attribute_id" not in input_data:
            return {
                "status": "error",
                "message": "Missing attribute_id",
                "process_id": process_id
            }
            
        attribute_id = input_data["attribute_id"]
        
        # Check if attribute exists
        if attribute_id not in self.self_concept.attributes:
            return {
                "status": "error",
                "message": f"Attribute not found: {attribute_id}",
                "process_id": process_id
            }
            
        # Get attribute
        attribute = self.self_concept.attributes[attribute_id]
        
        # Track if updated
        updated = False
        
        # Update fields
        if "content" in input_data:
            attribute.content = input_data["content"]
            updated = True
            
        if "domain" in input_data:
            new_domain = input_data["domain"]
            
            # Check if domain is available
            if new_domain not in self.self_concept.domains:
                self._adjust_domains_for_development()
                
                if new_domain not in self.self_concept.domains:
                    return {
                        "status": "error",
                        "message": f"Domain '{new_domain}' not available at current development level",
                        "process_id": process_id
                    }
            
            # Update domain
            old_domain = attribute.domain
            attribute.domain = new_domain
            
            # Update domain lists
            if attribute_id in self.self_concept.domains[old_domain]:
                self.self_concept.domains[old_domain].remove(attribute_id)
            
            if attribute_id not in self.self_concept.domains[new_domain]:
                self.self_concept.domains[new_domain].append(attribute_id)
                
            updated = True
            
        if "confidence" in input_data:
            attribute.confidence = max(0.0, min(1.0, float(input_data["confidence"])))
            updated = True
            
        if "importance" in input_data:
            attribute.importance = max(0.0, min(1.0, float(input_data["importance"])))
            updated = True
            
        if "valence" in input_data:
            attribute.valence = max(-1.0, min(1.0, float(input_data["valence"])))
            updated = True
            
        if "evidence" in input_data:
            if isinstance(input_data["evidence"], list):
                attribute.evidence = input_data["evidence"]
                updated = True
                
        if "sources" in input_data:
            if isinstance(input_data["sources"], list):
                attribute.sources = input_data["sources"]
                updated = True
        
        if updated:
            # Update timestamp
            attribute.updated_at = datetime.now()
            
            # Update in self-concept
            self.self_concept.attributes[attribute_id] = attribute
            self.self_concept.last_updated = datetime.now()
            
            # Update embedding if content or domain changed
            if "content" in input_data or "domain" in input_data:
                features = self._extract_features(f"{attribute.domain}: {attribute.content}")
                with torch.no_grad():
                    output = self.network(
                        features.to(self.device),
                        domain=attribute.domain
                    )
                    self.attribute_embeddings[attribute_id] = output["attribute_encoding"].cpu().squeeze(0)
            
            # Update self-esteem if valence is updated
            if "valence" in input_data:
                self._update_self_esteem()
            
            return {
                "status": "success",
                "message": "Attribute updated successfully",
                "process_id": process_id
            }
        else:
            return {
                "status": "error",
                "message": "No fields updated",
                "process_id": process_id
            }
    
    def _query_self(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Query information about the self-concept
        
        Args:
            input_data: Input data dictionary
            process_id: Process identifier
            
        Returns:
            Dict with processing results
        """
        # Get query type
        query_type = input_data.get("query_type", "all")
        
        if query_type == "all":
            # Return entire self-concept summary
            return {
                "status": "success",
                "domains": list(self.self_concept.domains.keys()),
                "attribute_count": len(self.self_concept.attributes),
                "global_self_esteem": self.self_concept.global_self_esteem,
                "clarity": self.self_concept.clarity,
                "stability": self.self_concept.stability,
                "complexity": self.self_concept.complexity,
                "process_id": process_id
            }
        
        elif query_type == "domain":
            # Get domain-specific attributes
            if "domain" not in input_data:
                return {
                    "status": "error",
                    "message": "Missing domain for domain query",
                    "process_id": process_id
                }
                
            domain = input_data["domain"]
            
            if domain not in self.self_concept.domains:
                return {
                    "status": "error",
                    "message": f"Domain not found: {domain}",
                    "process_id": process_id
                }
                
            # Get attributes in this domain
            attributes = []
            for attr_id in self.self_concept.domains[domain]:
                if attr_id in self.self_concept.attributes:
                    attr = self.self_concept.attributes[attr_id]
                    attributes.append({
                        "attribute_id": attr_id,
                        "content": attr.content,
                        "confidence": attr.confidence,
                        "importance": attr.importance,
                        "valence": attr.valence
                    })
                    
            return {
                "status": "success",
                "domain": domain,
                "attributes": attributes,
                "process_id": process_id
            }
        
        elif query_type == "similarity":
            # Find attributes similar to a query
            if "query" not in input_data:
                return {
                    "status": "error",
                    "message": "Missing query for similarity search",
                    "process_id": process_id
                }
                
            query = input_data["query"]
            domain = input_data.get("domain")  # Optional domain filter
            
            # Extract features from query
            features = self._extract_features(query)
            
            # Get encoding from network
            with torch.no_grad():
                output = self.network(
                    features.to(self.device),
                    domain=domain
                )
                query_encoding = output["attribute_encoding"].cpu().squeeze(0)
            
            # Find similar attributes
            similar_attributes = []
            
            if len(self.attribute_embeddings) > 0:
                # Calculate similarity to existing attributes
                similarities = {}
                for attr_id, embedding in self.attribute_embeddings.items():
                    # Skip if domain filter is applied and doesn't match
                    if domain and self.self_concept.attributes[attr_id].domain != domain:
                        continue
                        
                    similarity = torch.cosine_similarity(
                        query_encoding.unsqueeze(0),
                        embedding.unsqueeze(0)
                    ).item()
                    similarities[attr_id] = similarity
                    
                # Get top 5 similar attributes
                sorted_attrs = sorted(similarities.items(), key=lambda x: x[1], reverse=True)
                for attr_id, similarity in sorted_attrs[:5]:
                    if similarity > 0.6:  # Only include reasonably similar results
                        attr = self.self_concept.attributes[attr_id]
                        similar_attributes.append({
                            "attribute_id": attr_id,
                            "domain": attr.domain,
                            "content": attr.content,
                            "confidence": attr.confidence,
                            "importance": attr.importance,
                            "similarity": similarity
                        })
                        
            return {
                "status": "success",
                "similar_attributes": similar_attributes,
                "process_id": process_id
            }
        
        else:
            return {
                "status": "error",
                "message": f"Unknown query_type: {query_type}",
                "process_id": process_id
            }
    
    def _update_self_esteem(self):
        """
        Update the global self-esteem based on attribute valence and importance
        """
        if not self.self_concept.attributes:
            # No attributes yet, keep default
            return
        
        # Calculate weighted average of valence, weighted by importance
        total_importance = 0.0
        weighted_valence_sum = 0.0
        
        for attr_id, attr in self.self_concept.attributes.items():
            # Convert valence from [-1, 1] to [0, 1] for self-esteem
            valence_positive = (attr.valence + 1) / 2
            weighted_valence_sum += valence_positive * attr.importance
            total_importance += attr.importance
        
        if total_importance > 0:
            # Calculate weighted average
            self.self_concept.global_self_esteem = weighted_valence_sum / total_importance
        
        # Also update clarity, stability and complexity based on development
        # These become more pronounced with development
        self.self_concept.clarity = 0.3 + (self.development_level * 0.7)
        self.self_concept.stability = 0.2 + (self.development_level * 0.8)
        self.self_concept.complexity = 0.1 + (self.development_level * 0.9)
        
        # Adjust for number of domains and attributes
        if len(self.self_concept.domains) > 0:
            domain_factor = min(1.0, len(self.self_concept.domains) / 10)
            self.self_concept.complexity = (self.self_concept.complexity + domain_factor) / 2
    
    def _extract_features(self, data) -> torch.Tensor:
        """
        Extract features from data using the neural network
        
        Args:
            data: Data to extract features from
            
        Returns:
            Tensor of features
        """
        # Simple feature extraction
        text = str(data)
        words = text.split()
        
        # Create a simple word embedding
        embedding = torch.zeros(min(len(words), 128), dtype=torch.float32)
        
        for i, word in enumerate(words[:embedding.size(0)]):
            # Simple hash-based embedding
            hash_val = hash(word) % 10000
            embedding[i] = (hash_val / 10000) * 2 - 1
            
        # Pad if needed
        if embedding.size(0) < 128:
            padding = torch.zeros(128 - embedding.size(0), dtype=torch.float32)
            embedding = torch.cat([embedding, padding])
            
        return embedding.unsqueeze(0)  # Add batch dimension
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of the module
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New development level
        """
        old_level = self.development_level
        
        # Update development level
        self.development_level = max(0.0, min(1.0, self.development_level + amount))
        
        # Update neural network
        self.network.set_development_level(self.development_level)
        
        # Check if crossed a milestone
        for level in sorted(self.development_milestones.keys()):
            if old_level < level <= self.development_level:
                milestone = self.development_milestones[level]
                
                # Publish milestone event if we have an event bus
                if self.event_bus:
                    self.event_bus.publish({
                        "sender": self.module_id,
                        "message_type": "development_milestone",
                        "content": {
                            "module": "self_concept",
                            "milestone": milestone,
                            "level": level
                        }
                    })
                
                # Log milestone
                print(f"Self-Concept Development Milestone: {milestone} (level {level})")
                
                # Update domains for new development level
                self._adjust_domains_for_development()
                
        # Update self-esteem and other metrics with new development level
        self._update_self_esteem()
        
        return self.development_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the self-concept module
        
        Returns:
            Dict representing the current state
        """
        return {
            "module_id": self.module_id,
            "self_concept": self.self_concept.dict(),
            "developmental_level": self.development_level,
            "attribute_count": len(self.self_concept.attributes),
            "domain_count": len(self.self_concept.domains)
        }
    
    def save_state(self) -> Dict[str, Any]:
        """
        Save the current state for persistence
        
        Returns:
            Dict with serializable state
        """
        # Convert embeddings to lists for serialization
        attribute_embeddings_serialized = {}
        for attr_id, embedding in self.attribute_embeddings.items():
            attribute_embeddings_serialized[attr_id] = embedding.numpy().tolist()
            
        return {
            "module_id": self.module_id,
            "self_concept": self.self_concept.dict(),
            "developmental_level": self.development_level,
            "attribute_embeddings": attribute_embeddings_serialized
        }
    
    def load_state(self, state: Dict[str, Any]) -> None:
        """
        Load a previously saved state
        
        Args:
            state: The state to load
        """
        # Load basic state
        self.module_id = state["module_id"]
        self.development_level = state["developmental_level"]
        
        # Update network development level
        self.network.set_development_level(self.development_level)
        
        # Load self-concept
        if "self_concept" in state:
            try:
                concept_data = state["self_concept"]
                self.self_concept = SelfConceptModel(**concept_data)
            except Exception as e:
                print(f"Error loading self-concept: {e}")
        
        # Load embeddings
        if "attribute_embeddings" in state:
            self.attribute_embeddings = {}
            for attr_id, embedding_list in state["attribute_embeddings"].items():
                self.attribute_embeddings[attr_id] = torch.tensor(embedding_list, dtype=torch.float32)


#######################

#identity\__init__.py#
#######################

# Identity module 

# TODO: Implement the identity module factory function to return an integrated IdentitySystem
# This module should be responsible for self-concept, personal narrative,
# preferences, and personality trait development.

# TODO: Create IdentitySystem class that integrates all identity sub-components:
# - self_concept: representation of self-knowledge and beliefs about self
# - personal_narrative: autobiographical story that creates continuity of self
# - preferences: likes, dislikes, and value judgments
# - personality_traits: stable patterns of thinking, feeling, and behaving

# TODO: Implement development tracking for identity
# Identity should develop from minimal self-awareness in early stages
# to complex, integrated self-concept in adulthood

# TODO: Connect identity module to memory, emotion, and social modules
# Identity should be informed by autobiographical memories, emotional
# responses, and social feedback

# TODO: Implement stability vs. change dynamics
# The system should maintain some stability in identity while
# allowing for appropriate change and growth over time

from typing import Optional, Dict, Any, List, Union, Tuple
import uuid
from datetime import datetime

from lmm_project.base.module import BaseModule
from lmm_project.event_bus import EventBus
from lmm_project.modules.identity.models import (
    IdentityState,
    IdentityNeuralState,
    SelfConcept as SelfConceptModel,
    PersonalNarrative as PersonalNarrativeModel,
    PreferenceSystem,
    PersonalityProfile
)
from lmm_project.modules.identity.self_concept import SelfConcept
from lmm_project.modules.identity.personal_narrative import PersonalNarrative
from lmm_project.modules.identity.preferences import Preferences
from lmm_project.modules.identity.personality_traits import PersonalityTraits


class IdentitySystem(BaseModule):
    """
    Integrated identity system that brings together all identity components
    
    The IdentitySystem coordinates the self-concept, personal narrative,
    preferences, and personality traits to create a coherent sense of self.
    """
    
    # Development milestones
    development_milestones = {
        0.0: "Basic self-recognition",
        0.2: "Simple self-descriptions and preferences",
        0.4: "Coherent self-concept and emerging narrative",
        0.6: "Integrated identity with social comparison",
        0.8: "Stable personality and autobiographical continuity",
        1.0: "Fully differentiated identity with values integration"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the identity system
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level (0.0 to 1.0)
        """
        super().__init__(module_id, event_bus)
        
        # Set initial development level
        self.development_level = max(0.0, min(1.0, development_level))
        
        # Initialize component modules
        self.self_concept = SelfConcept(
            module_id=f"{module_id}.self_concept",
            event_bus=event_bus,
            development_level=development_level
        )
        
        self.personal_narrative = PersonalNarrative(
            module_id=f"{module_id}.personal_narrative",
            event_bus=event_bus,
            development_level=development_level
        )
        
        self.preferences = Preferences(
            module_id=f"{module_id}.preferences",
            event_bus=event_bus,
            development_level=development_level
        )
        
        self.personality_traits = PersonalityTraits(
            module_id=f"{module_id}.personality_traits",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Initialize identity state
        self.identity_state = IdentityState(
            self_concept=SelfConceptModel(),
            personal_narrative=PersonalNarrativeModel(),
            preference_system=PreferenceSystem(),
            personality_profile=PersonalityProfile(),
            module_id=module_id,
            developmental_level=development_level
        )
        
        # Identity integration metrics
        self.identity_integration = 0.3  # Starts with low integration
        self.identity_stability = 0.2    # Starts with low stability
        self.identity_clarity = 0.4      # Starts with moderate clarity
        
        # Register for event subscriptions
        if event_bus:
            event_bus.subscribe(
                sender=f"{module_id}.self_concept", 
                callback=self._handle_self_concept_event
            )
            event_bus.subscribe(
                sender=f"{module_id}.personal_narrative", 
                callback=self._handle_narrative_event
            )
            event_bus.subscribe(
                sender=f"{module_id}.preferences", 
                callback=self._handle_preference_event
            )
            event_bus.subscribe(
                sender=f"{module_id}.personality_traits", 
                callback=self._handle_personality_event
            )
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to the identity system
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Dict with processing results
        """
        # Validate input
        if not isinstance(input_data, dict):
            return {
                "status": "error",
                "message": "Input must be a dictionary"
            }
        
        # Extract process ID if provided
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Extract target component and operation
        component = input_data.get("component", "identity")
        operation = input_data.get("operation", "")
        
        # Route to appropriate component or handle at system level
        if component == "self_concept":
            return self.self_concept.process_input(input_data)
            
        elif component == "personal_narrative":
            return self.personal_narrative.process_input(input_data)
            
        elif component == "preferences":
            return self.preferences.process_input(input_data)
            
        elif component == "personality_traits":
            return self.personality_traits.process_input(input_data)
            
        elif component == "identity":
            # Handle identity-level operations
            if operation == "get_state":
                return self._get_identity_state(input_data, process_id)
                
            elif operation == "update_integration":
                return self._update_identity_integration(input_data, process_id)
                
            elif operation == "query_identity":
                return self._query_identity(input_data, process_id)
                
            else:
                return {
                    "status": "error",
                    "message": f"Unknown operation for identity component: {operation}",
                    "process_id": process_id
                }
                
        else:
            return {
                "status": "error",
                "message": f"Unknown component: {component}",
                "process_id": process_id
            }
    
    def _get_identity_state(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Get the current identity state
        
        Args:
            input_data: Input data dictionary
            process_id: Process identifier
            
        Returns:
            Dict with identity state information
        """
        # Get current states from components
        self_concept_state = self.self_concept.get_state()
        narrative_state = self.personal_narrative.get_state()
        preference_state = self.preferences.get_state()
        personality_state = self.personality_traits.get_state()
        
        # Combine into unified identity state
        identity_state = {
            "status": "success",
            "module_id": self.module_id,
            "developmental_level": self.development_level,
            "identity_integration": self.identity_integration,
            "identity_stability": self.identity_stability,
            "identity_clarity": self.identity_clarity,
            "components": {
                "self_concept": self_concept_state,
                "personal_narrative": narrative_state,
                "preferences": preference_state,
                "personality_traits": personality_state
            },
            "process_id": process_id
        }
        
        return identity_state
    
    def _update_identity_integration(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Update the integration of identity components
        
        Args:
            input_data: Input data dictionary
            process_id: Process identifier
            
        Returns:
            Dict with update results
        """
        # This operation forces an update of identity integration metrics
        self._calculate_identity_integration()
        
        return {
            "status": "success",
            "module_id": self.module_id,
            "identity_integration": self.identity_integration,
            "identity_stability": self.identity_stability,
            "identity_clarity": self.identity_clarity,
            "process_id": process_id
        }
    
    def _query_identity(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Query information about identity
        
        Args:
            input_data: Input data dictionary
            process_id: Process identifier
            
        Returns:
            Dict with query results
        """
        query_type = input_data.get("query_type", "summary")
        
        if query_type == "summary":
            # Provide a summary of the identity
            return {
                "status": "success",
                "module_id": self.module_id,
                "developmental_level": self.development_level,
                "identity_integration": self.identity_integration,
                "identity_stability": self.identity_stability,
                "identity_clarity": self.identity_clarity,
                "component_levels": {
                    "self_concept": self.self_concept.development_level,
                    "personal_narrative": self.personal_narrative.development_level,
                    "preferences": self.preferences.development_level,
                    "personality_traits": self.personality_traits.development_level
                },
                "process_id": process_id
            }
            
        elif query_type == "milestones":
            # Return developmental milestones
            return {
                "status": "success",
                "module_id": self.module_id,
                "identity_milestones": self.development_milestones,
                "self_concept_milestones": self.self_concept.development_milestones,
                "narrative_milestones": self.personal_narrative.development_milestones,
                "preferences_milestones": self.preferences.development_milestones,
                "personality_milestones": self.personality_traits.development_milestones,
                "process_id": process_id
            }
            
        else:
            return {
                "status": "error",
                "message": f"Unknown query_type: {query_type}",
                "process_id": process_id
            }
    
    def _calculate_identity_integration(self):
        """Calculate the level of integration between identity components"""
        # Identity integration increases with development
        base_integration = 0.3 + (self.development_level * 0.5)
        
        # Identity stability increases with development
        base_stability = 0.2 + (self.development_level * 0.6)
        
        # Identity clarity increases with development
        base_clarity = 0.4 + (self.development_level * 0.5)
        
        # Component development also influences integration
        component_levels = [
            self.self_concept.development_level,
            self.personal_narrative.development_level,
            self.preferences.development_level,
            self.personality_traits.development_level
        ]
        
        # Average component development
        avg_component_level = sum(component_levels) / len(component_levels)
        
        # Variance in component development (less variance = more integration)
        variance = sum((level - avg_component_level) ** 2 for level in component_levels) / len(component_levels)
        variance_factor = max(0.0, 1.0 - (variance * 5.0))  # Low variance gives higher factor
        
        # Calculate final metrics
        self.identity_integration = min(1.0, base_integration * 0.7 + variance_factor * 0.3)
        self.identity_stability = min(1.0, base_stability * 0.8 + variance_factor * 0.2)
        self.identity_clarity = min(1.0, base_clarity * 0.6 + variance_factor * 0.4)
        
        # Update identity state
        self.identity_state.identity_integration = self.identity_integration
        self.identity_state.identity_stability = self.identity_stability
        self.identity_state.identity_clarity = self.identity_clarity
        self.identity_state.developmental_level = self.development_level
        self.identity_state.last_updated = datetime.now()
    
    def _handle_self_concept_event(self, event: Dict[str, Any]) -> None:
        """Handle events from the self-concept component"""
        message_type = event.get("message_type", "")
        
        if message_type == "development_milestone":
            # A milestone was reached in self-concept development
            content = event.get("content", {})
            level = content.get("level", 0.0)
            
            # Update identity integration metrics
            self._calculate_identity_integration()
            
            # Possibly trigger identity-level milestone events
            self._check_identity_milestones()
            
    def _handle_narrative_event(self, event: Dict[str, Any]) -> None:
        """Handle events from the personal narrative component"""
        message_type = event.get("message_type", "")
        
        if message_type == "development_milestone":
            # A milestone was reached in narrative development
            content = event.get("content", {})
            level = content.get("level", 0.0)
            
            # Update identity integration metrics
            self._calculate_identity_integration()
            
            # Possibly trigger identity-level milestone events
            self._check_identity_milestones()
            
    def _handle_preference_event(self, event: Dict[str, Any]) -> None:
        """Handle events from the preferences component"""
        message_type = event.get("message_type", "")
        
        if message_type == "development_milestone":
            # A milestone was reached in preference development
            content = event.get("content", {})
            level = content.get("level", 0.0)
            
            # Update identity integration metrics
            self._calculate_identity_integration()
            
            # Possibly trigger identity-level milestone events
            self._check_identity_milestones()
            
    def _handle_personality_event(self, event: Dict[str, Any]) -> None:
        """Handle events from the personality traits component"""
        message_type = event.get("message_type", "")
        
        if message_type == "development_milestone":
            # A milestone was reached in personality development
            content = event.get("content", {})
            level = content.get("level", 0.0)
            
            # Update identity integration metrics
            self._calculate_identity_integration()
            
            # Possibly trigger identity-level milestone events
            self._check_identity_milestones()
    
    def _check_identity_milestones(self):
        """Check if any identity-level milestones have been reached"""
        # Identity development is influenced by component development
        component_levels = [
            self.self_concept.development_level,
            self.personal_narrative.development_level,
            self.preferences.development_level,
            self.personality_traits.development_level
        ]
        
        # Average component development
        avg_component_level = sum(component_levels) / len(component_levels)
        
        # Get previous development level
        old_level = self.development_level
        
        # Update overall development level (weighted average of components and existing level)
        self.development_level = (self.development_level * 0.3) + (avg_component_level * 0.7)
        self.development_level = max(0.0, min(1.0, self.development_level))
        
        # Check if crossed a milestone
        for level in sorted(self.development_milestones.keys()):
            if old_level < level <= self.development_level:
                milestone = self.development_milestones[level]
                
                # Publish milestone event if we have an event bus
                if self.event_bus:
                    self.event_bus.publish({
                        "sender": self.module_id,
                        "message_type": "development_milestone",
                        "content": {
                            "module": "identity",
                            "milestone": milestone,
                            "level": level
                        }
                    })
                
                print(f"Identity Development Milestone: {milestone} (level {level})")
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of the module and its components
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New development level
        """
        old_level = self.development_level
        
        # Update component development levels
        self.self_concept.update_development(amount)
        self.personal_narrative.update_development(amount)
        self.preferences.update_development(amount)
        self.personality_traits.update_development(amount)
        
        # Check identity milestones and update integration
        self._check_identity_milestones()
        self._calculate_identity_integration()
        
        return self.development_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the identity system
        
        Returns:
            Dict representing the current state
        """
        return {
            "module_id": self.module_id,
            "developmental_level": self.development_level,
            "identity_integration": self.identity_integration,
            "identity_stability": self.identity_stability,
            "identity_clarity": self.identity_clarity,
            "components": {
                "self_concept": self.self_concept.get_state(),
                "personal_narrative": self.personal_narrative.get_state(),
                "preferences": self.preferences.get_state(),
                "personality_traits": self.personality_traits.get_state()
            }
        }
    
    def save_state(self) -> Dict[str, Any]:
        """
        Save the current state for persistence
        
        Returns:
            Dict with serializable state
        """
        return {
            "module_id": self.module_id,
            "developmental_level": self.development_level,
            "identity_integration": self.identity_integration,
            "identity_stability": self.identity_stability,
            "identity_clarity": self.identity_clarity,
            "components": {
                "self_concept": self.self_concept.save_state(),
                "personal_narrative": self.personal_narrative.save_state(),
                "preferences": self.preferences.save_state(),
                "personality_traits": self.personality_traits.save_state()
            },
            "identity_state": self.identity_state.dict()
        }
    
    def load_state(self, state: Dict[str, Any]) -> None:
        """
        Load a previously saved state
        
        Args:
            state: The state to load
        """
        # Load basic state
        self.module_id = state["module_id"]
        self.development_level = state["developmental_level"]
        self.identity_integration = state["identity_integration"]
        self.identity_stability = state["identity_stability"]
        self.identity_clarity = state["identity_clarity"]
        
        # Load component states
        if "components" in state:
            components = state["components"]
            
            if "self_concept" in components:
                self.self_concept.load_state(components["self_concept"])
                
            if "personal_narrative" in components:
                self.personal_narrative.load_state(components["personal_narrative"])
                
            if "preferences" in components:
                self.preferences.load_state(components["preferences"])
                
            if "personality_traits" in components:
                self.personality_traits.load_state(components["personality_traits"])
        
        # Load identity state
        if "identity_state" in state:
            try:
                self.identity_state = IdentityState(**state["identity_state"])
            except Exception as e:
                print(f"Error loading identity state: {e}")


def get_module(module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0) -> Union[IdentitySystem, BaseModule]:
    """
    Factory function to get an identity module
    
    Args:
        module_id: Unique identifier for this module
        event_bus: Event bus for communication with other modules
        development_level: Initial developmental level (0.0 to 1.0)
    
    Returns:
        An identity module instance (IdentitySystem or a specific component)
    """
    # Check if requesting a specific component or the full system
    module_parts = module_id.split('.')
    
    if len(module_parts) > 1 and module_parts[0] == "identity":
        component = module_parts[1]
        
        # Return specific component
        if component == "self_concept":
            return SelfConcept(module_id, event_bus, development_level)
            
        elif component == "personal_narrative":
            return PersonalNarrative(module_id, event_bus, development_level)
            
        elif component == "preferences":
            return Preferences(module_id, event_bus, development_level)
            
        elif component == "personality_traits":
            return PersonalityTraits(module_id, event_bus, development_level)
            
        else:
            # Unknown component, return full system
            print(f"Unknown identity component '{component}', returning full identity system")
            return IdentitySystem(module_id, event_bus, development_level)
    
    # Return full identity system
    return IdentitySystem(module_id, event_bus, development_level) 

#######################

#language\expression_generator.py#
#######################

# TODO: Implement the ExpressionGenerator class to produce language output
# This component should be able to:
# - Generate coherent linguistic expressions from concepts
# - Apply grammatical rules to structure output
# - Select appropriate vocabulary for the intended meaning
# - Adapt expression style to different contexts and purposes

# TODO: Implement developmental progression in language production:
# - Simple sounds and single words in early stages
# - Basic grammatical combinations in early childhood
# - Complex sentences in later childhood
# - Sophisticated and context-appropriate expression in adulthood

# TODO: Create mechanisms for:
# - Conceptual encoding: Translate concepts to linguistic form
# - Grammatical structuring: Apply syntactic rules to output
# - Lexical selection: Choose appropriate words for meanings
# - Pragmatic adjustment: Adapt expression to social context

# TODO: Implement different expression types:
# - Declarative statements: Convey information
# - Questions: Request information
# - Directives: Request actions
# - Expressive: Communicate emotions and attitudes

# TODO: Connect to semantic processing and social understanding
# Expression should build on semantic representations
# and be shaped by social context understanding

from typing import Dict, List, Any, Optional, Set, Tuple
import torch
import uuid
import numpy as np
from datetime import datetime
from collections import deque

from lmm_project.base.module import BaseModule
from lmm_project.event_bus import EventBus
from lmm_project.modules.language.models import ExpressionModel, LanguageNeuralState
from lmm_project.modules.language.neural_net import ExpressionNetwork, get_device
from lmm_project.utils.llm_client import LLMClient

class ExpressionGenerator(BaseModule):
    """
    Generates language expressions from meaning
    
    This module is responsible for producing language output,
    translating intentions and meanings into words and sentences.
    """
    
    # Development milestones
    development_milestones = {
        0.0: "Basic vocalization",
        0.2: "Single word utterances",
        0.4: "Two-word combinations",
        0.6: "Simple sentences",
        0.8: "Complex sentences",
        1.0: "Full expressive language"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the expression generator module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level (0.0 to 1.0)
        """
        super().__init__(module_id, event_bus)
        
        # Initialize expression model
        self.expression_model = ExpressionModel()
        
        # Set initial development level
        self.development_level = max(0.0, min(1.0, development_level))
        
        # Initialize neural network
        self.device = get_device()
        self.network = ExpressionNetwork().to(self.device)
        self.network.set_development_level(self.development_level)
        
        # Initialize neural state
        self.neural_state = LanguageNeuralState()
        self.neural_state.expression_generation_development = self.development_level
        
        # Initialize with expression templates based on development level
        self._initialize_expression_templates()
        
        # Recent outputs queue (for tracking recent expressions)
        self.recent_outputs = deque(maxlen=100)
        
        # For embedding generation when needed
        self.llm_client = LLMClient()
    
    def _initialize_expression_templates(self):
        """Initialize expression templates based on development level"""
        # Basic expression templates at earliest stages
        if self.development_level >= 0.0:
            # Single word expressions for basic needs
            self.expression_model.expression_templates.append({
                "template_id": str(uuid.uuid4()),
                "name": "Basic request",
                "template": "{object}",
                "examples": ["milk", "mama", "up"],
                "complexity": 0.1,
                "confidence": 0.8 * max(0.2, self.development_level)
            })
            
            # Initialize communication intents
            self.expression_model.communication_intents["request"] = {
                "core_words": ["want", "give", "more"],
                "templates": ["Basic request"]
            }
            
            # Initialize fluency metrics
            self.expression_model.fluency_metrics["word_clarity"] = 0.3 * max(0.3, self.development_level)
            self.expression_model.fluency_metrics["response_time"] = 0.2 * max(0.3, self.development_level)
        
        if self.development_level >= 0.2:
            # Two-word combinations
            self.expression_model.expression_templates.append({
                "template_id": str(uuid.uuid4()),
                "name": "Agent-action",
                "template": "{agent} {action}",
                "examples": ["mama come", "baby eat", "dog run"],
                "complexity": 0.3,
                "confidence": 0.7 * ((self.development_level - 0.2) / 0.8)
            })
            
            self.expression_model.expression_templates.append({
                "template_id": str(uuid.uuid4()),
                "name": "Action-object",
                "template": "{action} {object}",
                "examples": ["want milk", "see dog", "throw ball"],
                "complexity": 0.3,
                "confidence": 0.7 * ((self.development_level - 0.2) / 0.8)
            })
            
            # Update communication intents
            self.expression_model.communication_intents["describe"] = {
                "core_words": ["see", "look", "big", "small"],
                "templates": ["Agent-action"]
            }
            
            # Update fluency metrics
            self.expression_model.fluency_metrics["word_connections"] = 0.3 * ((self.development_level - 0.2) / 0.8)
        
        if self.development_level >= 0.4:
            # Simple sentences
            self.expression_model.expression_templates.append({
                "template_id": str(uuid.uuid4()),
                "name": "Simple sentence",
                "template": "{agent} {action} {object}",
                "examples": ["I want milk", "Dog chase ball", "Baby eat food"],
                "complexity": 0.5,
                "confidence": 0.7 * ((self.development_level - 0.4) / 0.6)
            })
            
            # Add more communication intents
            self.expression_model.communication_intents["inform"] = {
                "core_words": ["is", "has", "can", "will"],
                "templates": ["Simple sentence"]
            }
            
            # Add simple pragmatic rules
            self.expression_model.pragmatic_rules.append({
                "rule_id": str(uuid.uuid4()),
                "name": "Polite request",
                "condition": "request + formal",
                "adjustment": "Add 'please'",
                "examples": ["Want milk -> Want milk please"],
                "confidence": 0.6 * ((self.development_level - 0.4) / 0.6)
            })
            
            # Update fluency metrics
            self.expression_model.fluency_metrics["sentence_formation"] = 0.4 * ((self.development_level - 0.4) / 0.6)
        
        if self.development_level >= 0.6:
            # Complex sentences
            self.expression_model.expression_templates.append({
                "template_id": str(uuid.uuid4()),
                "name": "Complex sentence",
                "template": "{agent} {action} {object} {modifier}",
                "examples": ["I want milk now", "Dog runs fast outside", "Baby sleeps quietly in bed"],
                "complexity": 0.7,
                "confidence": 0.7 * ((self.development_level - 0.6) / 0.4)
            })
            
            # Add compound sentences
            self.expression_model.expression_templates.append({
                "template_id": str(uuid.uuid4()),
                "name": "Compound sentence",
                "template": "{clause1} and {clause2}",
                "examples": ["I am hungry and I want food", "Dog runs and cat jumps"],
                "complexity": 0.8,
                "confidence": 0.6 * ((self.development_level - 0.6) / 0.4)
            })
            
            # Add more communication intents
            self.expression_model.communication_intents["explain"] = {
                "core_words": ["because", "so", "when", "if"],
                "templates": ["Complex sentence", "Compound sentence"]
            }
            
            # Add more speech acts
            self.expression_model.speech_acts["request"] = [
                {
                    "form": "Can you {action} {object}?",
                    "examples": ["Can you get milk?", "Can you help me?"],
                    "politeness": 0.7
                },
                {
                    "form": "I would like {object}, please.",
                    "examples": ["I would like water, please."],
                    "politeness": 0.9
                }
            ]
            
            # Update fluency metrics
            self.expression_model.fluency_metrics["grammatical_accuracy"] = 0.5 * ((self.development_level - 0.6) / 0.4)
            self.expression_model.fluency_metrics["vocabulary_diversity"] = 0.5 * ((self.development_level - 0.6) / 0.4)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to the expression generator module
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Dict with processing results
        """
        # Validate input
        if not isinstance(input_data, dict):
            return {
                "status": "error",
                "message": "Input must be a dictionary"
            }
        
        # Extract process ID if provided
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Extract operation
        operation = input_data.get("operation", "generate")
        
        # Dispatch to appropriate handler
        if operation == "generate":
            return self._generate_expression(input_data, process_id)
        elif operation == "evaluate":
            return self._evaluate_expression(input_data, process_id)
        elif operation == "learn_template":
            return self._learn_expression_template(input_data, process_id)
        elif operation == "query_expressions":
            return self._query_expressions(input_data, process_id)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "process_id": process_id
            }
    
    def _generate_expression(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Generate a language expression based on intent
        
        Args:
            input_data: Input data dictionary including intent information
            process_id: Process identifier
            
        Returns:
            Dict with generated expression
        """
        # Check for intent
        if "intent" not in input_data:
            return {
                "status": "error",
                "message": "Missing intent for expression generation",
                "process_id": process_id
            }
        
        intent = input_data["intent"]
        context = input_data.get("context", {})
        
        # Development level constrains complexity
        max_complexity = min(1.0, 0.2 + (self.development_level * 0.8))
        
        # Find suitable templates based on intent and development level
        suitable_templates = []
        
        # Check if we have specific templates for this intent
        if intent in self.expression_model.communication_intents:
            intent_info = self.expression_model.communication_intents[intent]
            template_names = intent_info.get("templates", [])
            
            # Find the template objects
            for template in self.expression_model.expression_templates:
                if template["name"] in template_names and template["complexity"] <= max_complexity:
                    suitable_templates.append(template)
        
        # If no suitable templates found, use templates within complexity constraints
        if not suitable_templates:
            for template in self.expression_model.expression_templates:
                if template["complexity"] <= max_complexity:
                    suitable_templates.append(template)
        
        # If still no templates, return error
        if not suitable_templates:
            return {
                "status": "undeveloped",
                "message": "No suitable expression templates available at current development level",
                "development_level": self.development_level,
                "process_id": process_id
            }
        
        # Sort by complexity (descending) to use the most complex templates possible
        suitable_templates.sort(key=lambda t: t["complexity"], reverse=True)
        
        # Choose the best template (most complex that we can handle)
        chosen_template = suitable_templates[0]
        
        # Generate expression by filling template
        expression = chosen_template["template"]
        
        # Find placeholders in the template
        placeholders = []
        current = ""
        in_placeholder = False
        
        for char in expression:
            if char == '{':
                in_placeholder = True
                current = ""
            elif char == '}' and in_placeholder:
                in_placeholder = False
                placeholders.append(current)
            elif in_placeholder:
                current += char
        
        # Fill placeholders from context
        filled_expression = expression
        missing_placeholders = []
        
        for placeholder in placeholders:
            if placeholder in context:
                filled_expression = filled_expression.replace(f"{{{placeholder}}}", context[placeholder])
            else:
                missing_placeholders.append(placeholder)
        
        # Check for missing placeholders
        if missing_placeholders:
            return {
                "status": "error",
                "message": f"Missing context for placeholders: {', '.join(missing_placeholders)}",
                "template": chosen_template["name"],
                "required_context": missing_placeholders,
                "process_id": process_id
            }
        
        # Apply appropriate speech act modifications if available
        if self.development_level >= 0.6 and intent in self.expression_model.speech_acts:
            speech_acts = self.expression_model.speech_acts[intent]
            
            # Use a simple speech act modification if available
            if speech_acts and "politeness" in context:
                politeness = float(context.get("politeness", 0.5))
                
                # Find the speech act with closest politeness level
                closest_act = min(speech_acts, key=lambda act: abs(act.get("politeness", 0.5) - politeness))
                
                # Apply the speech act form if it doesn't reduce complexity too much
                if closest_act["politeness"] <= max_complexity + 0.1:
                    # This is a simple approximation - in a real system, would properly
                    # restructure the sentence according to the speech act
                    if "{action}" in closest_act["form"] and "action" in context:
                        if "{object}" in closest_act["form"] and "object" in context:
                            filled_expression = closest_act["form"].replace("{action}", context["action"]).replace("{object}", context["object"])
        
        # Generate expression features for neural processing
        intent_features = np.zeros(128)
        
        # Simple feature creation based on intent and context
        intent_hash = hash(intent) % 50
        intent_features[intent_hash] = 1.0
        
        # Add context features
        for i, (key, value) in enumerate(context.items()[:10]):  # Limit to first 10 context items
            key_hash = (hash(key) + i) % 20
            intent_features[50 + key_hash] = 1.0
            
            value_hash = (hash(str(value)) + i) % 20
            intent_features[70 + value_hash] = 1.0
        
        # Convert to tensor
        intent_tensor = torch.tensor(intent_features, dtype=torch.float32).unsqueeze(0)
        intent_tensor = intent_tensor.to(self.device)
        
        # Process through network
        with torch.no_grad():
            # Generate expression plan
            output = self.network(
                intent_tensor, 
                operation="plan",
                sequence_length=min(10, len(filled_expression.split()))
            )
            
            # Get plan quality
            plan_quality = output["plan_quality"].cpu().item()
            
            # Apply development factor to quality
            fluency = plan_quality * max(0.3, self.development_level)
        
        # Apply developmental constraints to expression
        if self.development_level < 0.2:
            # Single word stage - limit to first word
            words = filled_expression.split()
            if words:
                filled_expression = words[0]
                
        elif self.development_level < 0.4:
            # Two-word stage - limit to first two words
            words = filled_expression.split()
            if len(words) > 2:
                filled_expression = " ".join(words[:2])
        
        # Record in recent outputs
        self.recent_outputs.append({
            "type": "expression_generation",
            "intent": intent,
            "expression": filled_expression,
            "template": chosen_template["name"],
            "fluency": fluency,
            "timestamp": datetime.now()
        })
        
        # Record activation in neural state
        self.neural_state.add_activation("expression_generation", {
            'operation': 'generate',
            'intent': intent,
            'template': chosen_template["name"],
            'fluency': fluency
        })
        
        # Return generated expression
        return {
            "status": "success",
            "expression": filled_expression,
            "intent": intent,
            "template_used": chosen_template["name"],
            "fluency": fluency,
            "development_level": self.development_level,
            "process_id": process_id
        }
    
    def _evaluate_expression(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Evaluate the quality of a language expression
        
        Args:
            input_data: Input data dictionary including expression to evaluate
            process_id: Process identifier
            
        Returns:
            Dict with evaluation results
        """
        # Check for expression
        if "expression" not in input_data:
            return {
                "status": "error",
                "message": "Missing expression for evaluation",
                "process_id": process_id
            }
        
        expression = input_data["expression"]
        intent = input_data.get("intent")
        
        # Create expression features
        expression_words = expression.split()
        
        if not expression_words:
            return {
                "status": "error",
                "message": "Empty expression provided",
                "process_id": process_id
            }
        
        # Create a context tensor from the expression
        expression_tensor = torch.zeros((len(expression_words), 128), dtype=torch.float32)
        
        for i, word in enumerate(expression_words):
            # Set word features
            word_hash = hash(word) % 100
            expression_tensor[i, word_hash] = 1.0
            
            # Add positional information
            expression_tensor[i, 100 + min(10, i)] = 1.0
            
            # Add intent information if provided
            if intent:
                intent_hash = hash(intent) % 10
                expression_tensor[i, 110 + intent_hash] = 1.0
        
        expression_tensor = expression_tensor.to(self.device)
        
        # Create a simple intent tensor if intent provided
        if intent:
            intent_features = np.zeros(128)
            intent_hash = hash(intent) % 50
            intent_features[intent_hash] = 1.0
            
            intent_tensor = torch.tensor(intent_features, dtype=torch.float32).unsqueeze(0)
            intent_tensor = intent_tensor.to(self.device)
        else:
            # Use simple placeholder
            intent_tensor = torch.zeros((1, 128), dtype=torch.float32).to(self.device)
        
        # Process through network
        with torch.no_grad():
            output = self.network(
                intent_tensor,
                operation="evaluate",
                context=expression_tensor
            )
            
            # Get fluency and confidence
            fluency = output["fluency"].cpu().item()
            confidence = output["evaluation_confidence"].cpu().item()
        
        # Determine appropriate fluency metrics based on expression complexity
        metrics = {}
        
        # Word-level fluency
        metrics["word_clarity"] = self.expression_model.fluency_metrics.get("word_clarity", 0.3) * fluency
        
        # Add higher-level metrics based on development
        if len(expression_words) > 1 and self.development_level >= 0.2:
            metrics["word_connections"] = self.expression_model.fluency_metrics.get("word_connections", 0.3) * fluency
            
        if len(expression_words) > 2 and self.development_level >= 0.4:
            metrics["sentence_formation"] = self.expression_model.fluency_metrics.get("sentence_formation", 0.3) * fluency
            
        if len(expression_words) > 3 and self.development_level >= 0.6:
            metrics["grammatical_accuracy"] = self.expression_model.fluency_metrics.get("grammatical_accuracy", 0.3) * fluency
            metrics["vocabulary_diversity"] = self.expression_model.fluency_metrics.get("vocabulary_diversity", 0.3) * fluency
        
        # Overall fluency (weighted average of metrics)
        overall_fluency = sum(metrics.values()) / max(1, len(metrics))
        
        # Record activation in neural state
        self.neural_state.add_activation("expression_generation", {
            'operation': 'evaluate',
            'expression_length': len(expression_words),
            'fluency': overall_fluency
        })
        
        # Return evaluation results
        return {
            "status": "success",
            "expression": expression,
            "fluency": overall_fluency,
            "evaluation_confidence": confidence,
            "metrics": metrics,
            "development_level": self.development_level,
            "process_id": process_id
        }
    
    def _learn_expression_template(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Learn a new expression template
        
        Args:
            input_data: Input data dictionary including template information
            process_id: Process identifier
            
        Returns:
            Dict with learning results
        """
        # Check for required fields
        if "name" not in input_data or "template" not in input_data:
            return {
                "status": "error",
                "message": "Missing name or template for learning expression template",
                "process_id": process_id
            }
        
        name = input_data["name"]
        template = input_data["template"]
        examples = input_data.get("examples", [])
        complexity = input_data.get("complexity", 0.5)
        intent = input_data.get("intent")
        
        # Check for existing template with same name
        for existing in self.expression_model.expression_templates:
            if existing["name"] == name:
                # Update existing template
                existing["template"] = template
                if examples:
                    existing["examples"] = examples
                existing["complexity"] = complexity
                existing["confidence"] = min(1.0, existing["confidence"] + 0.1)
                
                # Record activation in neural state
                self.neural_state.add_activation("expression_generation", {
                    'operation': 'update_template',
                    'template_name': name,
                    'complexity': complexity
                })
                
                # Update intent mapping if provided
                if intent and intent in self.expression_model.communication_intents:
                    if name not in self.expression_model.communication_intents[intent]["templates"]:
                        self.expression_model.communication_intents[intent]["templates"].append(name)
                
                return {
                    "status": "success",
                    "message": "Updated existing expression template",
                    "template_name": name,
                    "process_id": process_id
                }
        
        # Create new template
        new_template = {
            "template_id": str(uuid.uuid4()),
            "name": name,
            "template": template,
            "examples": examples,
            "complexity": complexity,
            "confidence": 0.5  # Initial confidence
        }
        
        # Development level affects initial confidence
        new_template["confidence"] *= max(0.5, self.development_level)
        
        # Add to templates
        self.expression_model.expression_templates.append(new_template)
        
        # Add to intent mapping if provided
        if intent:
            if intent in self.expression_model.communication_intents:
                if name not in self.expression_model.communication_intents[intent]["templates"]:
                    self.expression_model.communication_intents[intent]["templates"].append(name)
            else:
                self.expression_model.communication_intents[intent] = {
                    "core_words": [],
                    "templates": [name]
                }
        
        # Record activation in neural state
        self.neural_state.add_activation("expression_generation", {
            'operation': 'learn_template',
            'template_name': name,
            'complexity': complexity
        })
        
        return {
            "status": "success",
            "message": "Learned new expression template",
            "template_name": name,
            "process_id": process_id
        }
    
    def _query_expressions(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Query expression information
        
        Args:
            input_data: Input data dictionary including query parameters
            process_id: Process identifier
            
        Returns:
            Dict with query results
        """
        # Get query type
        query_type = input_data.get("query_type", "all")
        
        if query_type == "all":
            # Return summary of expression capabilities
            return {
                "status": "success",
                "templates_count": len(self.expression_model.expression_templates),
                "intents": list(self.expression_model.communication_intents.keys()),
                "fluency_metrics": self.expression_model.fluency_metrics,
                "development_level": self.development_level,
                "process_id": process_id
            }
        
        elif query_type == "templates":
            # Return expression templates
            # Sort by complexity for easier review
            sorted_templates = sorted(self.expression_model.expression_templates, key=lambda t: t["complexity"])
            
            templates = []
            for template in sorted_templates:
                templates.append({
                    "name": template["name"],
                    "template": template["template"],
                    "complexity": template["complexity"],
                    "confidence": template["confidence"]
                })
                
            return {
                "status": "success",
                "templates": templates,
                "count": len(templates),
                "process_id": process_id
            }
        
        elif query_type == "intent":
            # Check for intent
            if "intent" not in input_data:
                return {
                    "status": "error",
                    "message": "Missing intent for intent query",
                    "process_id": process_id
                }
                
            intent = input_data["intent"]
            
            # Check if intent exists
            if intent not in self.expression_model.communication_intents:
                return {
                    "status": "error",
                    "message": f"Intent not found: {intent}",
                    "available_intents": list(self.expression_model.communication_intents.keys()),
                    "process_id": process_id
                }
                
            # Get intent information
            intent_info = self.expression_model.communication_intents[intent]
            
            # Get related templates
            template_details = []
            for template_name in intent_info["templates"]:
                for template in self.expression_model.expression_templates:
                    if template["name"] == template_name:
                        template_details.append({
                            "name": template["name"],
                            "template": template["template"],
                            "complexity": template["complexity"]
                        })
                        break
                        
            return {
                "status": "success",
                "intent": intent,
                "core_words": intent_info["core_words"],
                "templates": template_details,
                "process_id": process_id
            }
            
        elif query_type == "speech_acts":
            # Return speech acts
            speech_acts = {}
            for intent, acts in self.expression_model.speech_acts.items():
                speech_acts[intent] = []
                for act in acts:
                    speech_acts[intent].append({
                        "form": act["form"],
                        "politeness": act.get("politeness", 0.5)
                    })
                    
            return {
                "status": "success",
                "speech_acts": speech_acts,
                "count": sum(len(acts) for acts in self.expression_model.speech_acts.values()),
                "process_id": process_id
            }
            
        else:
            return {
                "status": "error",
                "message": f"Unknown query_type: {query_type}",
                "process_id": process_id
            }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of the module
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New development level
        """
        old_level = self.development_level
        
        # Update development level
        self.development_level = max(0.0, min(1.0, self.development_level + amount))
        
        # Update neural network
        self.network.set_development_level(self.development_level)
        
        # Update neural state
        self.neural_state.expression_generation_development = self.development_level
        self.neural_state.last_updated = datetime.now()
        
        # Check if crossed a milestone
        for level in sorted(self.development_milestones.keys()):
            if old_level < level <= self.development_level:
                milestone = self.development_milestones[level]
                
                # Publish milestone event if we have an event bus
                if self.event_bus:
                    self.event_bus.publish({
                        "sender": self.module_id,
                        "message_type": "development_milestone",
                        "content": {
                            "module": "expression_generation",
                            "milestone": milestone,
                            "level": level
                        }
                    })
                
                # Update expression templates for new development level
                self._initialize_expression_templates()
        
        return self.development_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the expression generator module
        
        Returns:
            Dict representing the current state
        """
        return {
            "module_id": self.module_id,
            "developmental_level": self.development_level,
            "template_count": len(self.expression_model.expression_templates),
            "intent_count": len(self.expression_model.communication_intents),
            "speech_act_count": sum(len(acts) for acts in self.expression_model.speech_acts.values())
        }
    
    def save_state(self) -> Dict[str, Any]:
        """
        Save the current state for persistence
        
        Returns:
            Dict with serializable state
        """
        return {
            "module_id": self.module_id,
            "expression_model": self.expression_model.dict(),
            "developmental_level": self.development_level,
            "neural_state": {
                "development": self.neural_state.expression_generation_development,
                "accuracy": self.neural_state.expression_generation_accuracy
            }
        }
    
    def load_state(self, state: Dict[str, Any]) -> None:
        """
        Load a previously saved state
        
        Args:
            state: The state to load
        """
        # Load module ID
        self.module_id = state["module_id"]
        
        # Load development level
        self.development_level = state["developmental_level"]
        self.network.set_development_level(self.development_level)
        
        # Load expression model
        if "expression_model" in state:
            try:
                # Create new model from dict
                from pydantic import parse_obj_as
                self.expression_model = parse_obj_as(ExpressionModel, state["expression_model"])
            except Exception as e:
                print(f"Error loading expression model: {e}")
        
        # Load neural state
        if "neural_state" in state:
            ns = state["neural_state"]
            self.neural_state.expression_generation_development = ns.get("development", self.development_level)
            self.neural_state.expression_generation_accuracy = ns.get("accuracy", 0.5) 


#######################

#language\grammar_acquisition.py#
#######################

# TODO: Implement the GrammarAcquisition class to learn and apply grammatical rules
# This component should be able to:
# - Identify grammatical patterns from language input
# - Extract and formalize grammatical rules
# - Apply learned rules in language comprehension and production
# - Handle syntactic processing and sentence structure

# TODO: Implement developmental progression in grammar acquisition:
# - Simple two-word combinations in early stages
# - Basic sentence structures in early childhood
# - Complex grammar and exceptions in later childhood
# - Advanced syntax and pragmatics in adolescence/adulthood

# TODO: Create mechanisms for:
# - Pattern detection: Identify recurring grammatical structures
# - Rule extraction: Formalize explicit and implicit rules
# - Syntactic parsing: Analyze sentence structure
# - Grammatical error detection: Identify violations of learned rules

# TODO: Implement different grammatical concepts:
# - Word order rules (syntax)
# - Morphological rules (word formation)
# - Agreement rules (subject-verb, etc.)
# - Dependency relationships between sentence elements

# TODO: Connect to word learning and semantic processing
# Grammar acquisition should work with lexical knowledge
# and contribute to meaning extraction

from typing import Dict, List, Any, Optional, Set, Tuple
import torch
import uuid
import numpy as np
from datetime import datetime
from collections import deque

from lmm_project.base.module import BaseModule
from lmm_project.event_bus import EventBus
from lmm_project.modules.language.models import GrammarModel, LanguageNeuralState
from lmm_project.modules.language.neural_net import GrammarNetwork, get_device
from lmm_project.utils.llm_client import LLMClient

class GrammarAcquisition(BaseModule):
    """
    Learns and processes grammatical structures
    
    This module is responsible for acquiring grammatical rules,
    syntactic patterns, and morphological regularities.
    """
    
    # Development milestones
    development_milestones = {
        0.0: "Basic word ordering",
        0.2: "Two-word combinations",
        0.4: "Early grammatical markers",
        0.6: "Complex sentence structures",
        0.8: "Rule generalization and exceptions",
        1.0: "Complete grammatical system"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the grammar acquisition module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level (0.0 to 1.0)
        """
        super().__init__(module_id, event_bus)
        
        # Initialize grammar model
        self.grammar_model = GrammarModel()
        
        # Set initial development level
        self.development_level = max(0.0, min(1.0, development_level))
        
        # Initialize neural network
        self.device = get_device()
        self.network = GrammarNetwork().to(self.device)
        self.network.set_development_level(self.development_level)
        
        # Initialize neural state
        self.neural_state = LanguageNeuralState()
        self.neural_state.grammar_acquisition_development = self.development_level
        
        # Initialize with basic grammar structures based on development level
        self._initialize_grammar_structures()
        
        # Recent inputs queue (for tracking grammar exposure)
        self.recent_inputs = deque(maxlen=100)
        
        # For embedding generation when needed
        self.llm_client = LLMClient()
    
    def _initialize_grammar_structures(self):
        """Initialize basic grammatical structures based on development level"""
        # Basic structures at earliest stages
        if self.development_level >= 0.0:
            # Simple word ordering patterns (SVO - Subject-Verb-Object)
            self.grammar_model.syntactic_patterns.append({
                "pattern_id": str(uuid.uuid4()),
                "name": "Basic word order",
                "pattern": "SV",  # Subject-Verb
                "examples": ["Baby sleep", "Dog run"],
                "confidence": 0.6 * max(0.2, self.development_level)
            })
            
            # Initialize grammatical categories
            self.grammar_model.grammatical_categories["subject"] = ["baby", "dog", "mama", "dada"]
            self.grammar_model.grammatical_categories["verb"] = ["sleep", "run", "eat", "go"]
        
        if self.development_level >= 0.2:
            # Two-word combinations and early object usage
            self.grammar_model.syntactic_patterns.append({
                "pattern_id": str(uuid.uuid4()),
                "name": "Subject-Verb-Object",
                "pattern": "SVO",  # Subject-Verb-Object
                "examples": ["Baby want milk", "Mama see dog"],
                "confidence": 0.6 * ((self.development_level - 0.2) / 0.8)
            })
            
            # Update grammatical categories
            self.grammar_model.grammatical_categories["object"] = ["milk", "ball", "dog", "book"]
            
            # Simple morphological rules
            self.grammar_model.morphological_rules["plural"] = {
                "rule_id": str(uuid.uuid4()),
                "description": "Add 's' to make plural",
                "pattern": "{word} + s",
                "examples": {"dog": "dogs", "cat": "cats"},
                "confidence": 0.5 * ((self.development_level - 0.2) / 0.8)
            }
        
        if self.development_level >= 0.4:
            # More complex structures
            self.grammar_model.syntactic_patterns.append({
                "pattern_id": str(uuid.uuid4()),
                "name": "Subject-Verb-Object with modifier",
                "pattern": "S V O M",  # Subject-Verb-Object-Modifier
                "examples": ["Baby drink milk now", "Dog play ball outside"],
                "confidence": 0.6 * ((self.development_level - 0.4) / 0.6)
            })
            
            # Add modifiers category
            self.grammar_model.grammatical_categories["modifier"] = ["now", "here", "there", "outside"]
            
            # More morphological rules
            self.grammar_model.morphological_rules["past_tense"] = {
                "rule_id": str(uuid.uuid4()),
                "description": "Add 'ed' for past tense",
                "pattern": "{word} + ed",
                "examples": {"play": "played", "jump": "jumped"},
                "confidence": 0.5 * ((self.development_level - 0.4) / 0.6)
            }
            
            # Update rule confidence for existing rules
            if "plural" in self.grammar_model.morphological_rules:
                self.grammar_model.morphological_rules["plural"]["confidence"] = 0.7
        
        if self.development_level >= 0.6:
            # Complex sentence structures
            self.grammar_model.syntactic_patterns.append({
                "pattern_id": str(uuid.uuid4()),
                "name": "Compound sentence",
                "pattern": "S1 V1 and S2 V2",
                "examples": ["Baby eat and mama smile", "Dog bark and cat run"],
                "confidence": 0.6 * ((self.development_level - 0.6) / 0.4)
            })
            
            # Add conjunction category
            self.grammar_model.grammatical_categories["conjunction"] = ["and", "but", "or"]
            
            # More complex morphological rules
            self.grammar_model.morphological_rules["present_progressive"] = {
                "rule_id": str(uuid.uuid4()),
                "description": "Add 'ing' for present progressive",
                "pattern": "{be} + {word} + ing",
                "examples": {"run": "is running", "eat": "is eating"},
                "confidence": 0.5 * ((self.development_level - 0.6) / 0.4)
            }
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to the grammar acquisition module
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Dict with processing results
        """
        # Validate input
        if not isinstance(input_data, dict):
            return {
                "status": "error",
                "message": "Input must be a dictionary"
            }
        
        # Extract process ID if provided
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Extract operation
        operation = input_data.get("operation", "analyze")
        
        # Dispatch to appropriate handler
        if operation == "analyze":
            return self._analyze_grammar(input_data, process_id)
        elif operation == "learn_rule":
            return self._learn_grammar_rule(input_data, process_id)
        elif operation == "check_grammar":
            return self._check_grammaticality(input_data, process_id)
        elif operation == "query_grammar":
            return self._query_grammar(input_data, process_id)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "process_id": process_id
            }
    
    def _analyze_grammar(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Analyze a sentence or utterance for grammatical structure
        
        Args:
            input_data: Input data dictionary including sentence
            process_id: Process identifier
            
        Returns:
            Dict with analysis results
        """
        # Check for sentence
        if "sentence" not in input_data:
            return {
                "status": "error",
                "message": "Missing sentence for grammar analysis",
                "process_id": process_id
            }
        
        sentence = input_data["sentence"]
        
        # Tokenize sentence (simple split for this implementation)
        tokens = sentence.split()
        
        # Convert tokens to feature vectors
        token_features = []
        for token in tokens:
            # Simple feature creation (in a real implementation, these would be richer)
            token_vector = np.zeros(128)
            
            # Set hash-based features
            hash_val = hash(token) % 100
            token_vector[hash_val] = 1.0
            
            # Add positional information
            pos_index = tokens.index(token) % 10
            token_vector[100 + pos_index] = 1.0
            
            token_features.append(token_vector)
        
        # Convert to tensor
        if token_features:
            token_tensor = torch.tensor(np.array(token_features), dtype=torch.float32)
            token_tensor = token_tensor.to(self.device)
            
            # Process through network
            with torch.no_grad():
                # First pass just using the first token as a representative
                initial_output = self.network(token_tensor[0:1], operation="recognize")
                
                # Second pass with the sequence if available
                if len(token_tensor) > 1:
                    sequence_output = self.network(token_tensor[0:1], operation="predict", sequence=token_tensor)
                    prediction_quality = sequence_output.get("quality", torch.tensor([0.5])).item()
                else:
                    prediction_quality = 0.5
        else:
            # Empty sentence
            return {
                "status": "error",
                "message": "Empty sentence provided",
                "process_id": process_id
            }
        
        # Identify grammatical structure
        structure_found = False
        matched_pattern = None
        
        # Simple grammatical categorization of tokens
        token_categories = []
        for token in tokens:
            category = "unknown"
            for cat, words in self.grammar_model.grammatical_categories.items():
                if token.lower() in words:
                    category = cat
                    break
            token_categories.append(category)
        
        # Convert to simplified pattern
        pattern_str = " ".join([cat[0].upper() if cat != "unknown" else "X" for cat in token_categories])
        
        # Match against known patterns
        for pattern in self.grammar_model.syntactic_patterns:
            pattern_tokens = pattern["pattern"].split()
            # Simple pattern matching (in a real implementation, would be more sophisticated)
            if len(pattern_tokens) == len(token_categories):
                matches = True
                for i, pat in enumerate(pattern_tokens):
                    if pat != token_categories[i][0].upper() and token_categories[i] != "unknown":
                        matches = False
                        break
                
                if matches:
                    structure_found = True
                    matched_pattern = pattern
                    break
        
        # Record activation in neural state
        self.neural_state.add_activation("grammar_acquisition", {
            'operation': 'analyze',
            'pattern_found': structure_found,
            'pattern': pattern_str if structure_found else None
        })
        
        # Record in recent inputs
        self.recent_inputs.append({
            "type": "grammar_analysis",
            "sentence": sentence,
            "pattern": pattern_str,
            "timestamp": datetime.now()
        })
        
        # Return analysis results
        result = {
            "status": "success",
            "sentence": sentence,
            "token_count": len(tokens),
            "detected_pattern": pattern_str,
            "token_categories": list(zip(tokens, token_categories)),
            "grammatical": structure_found,
            "prediction_quality": prediction_quality,
            "development_level": self.development_level,
            "process_id": process_id
        }
        
        if matched_pattern:
            result["matched_structure"] = {
                "name": matched_pattern["name"],
                "pattern": matched_pattern["pattern"],
                "confidence": matched_pattern["confidence"]
            }
        
        return result
    
    def _learn_grammar_rule(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Learn a new grammatical rule or update an existing one
        
        Args:
            input_data: Input data dictionary including rule info
            process_id: Process identifier
            
        Returns:
            Dict with learning results
        """
        # Check for required fields
        if "rule_type" not in input_data:
            return {
                "status": "error",
                "message": "Missing rule_type for grammar rule learning",
                "process_id": process_id
            }
        
        rule_type = input_data["rule_type"]
        
        # Different handling for different rule types
        if rule_type == "syntactic":
            # Syntactic pattern rule
            if "pattern" not in input_data or "name" not in input_data:
                return {
                    "status": "error",
                    "message": "Missing pattern or name for syntactic rule",
                    "process_id": process_id
                }
            
            pattern = input_data["pattern"]
            name = input_data["name"]
            examples = input_data.get("examples", [])
            
            # Check if this pattern already exists
            for existing in self.grammar_model.syntactic_patterns:
                if existing["pattern"] == pattern:
                    # Update existing pattern
                    existing["name"] = name
                    if examples:
                        existing["examples"] = examples
                    
                    # Increase confidence
                    existing["confidence"] = min(1.0, existing["confidence"] + 0.1)
                    
                    # Record activation in neural state
                    self.neural_state.add_activation("grammar_acquisition", {
                        'operation': 'update_syntactic_rule',
                        'pattern': pattern,
                        'confidence': existing["confidence"]
                    })
                    
                    return {
                        "status": "success",
                        "message": "Updated existing syntactic pattern",
                        "pattern": pattern,
                        "confidence": existing["confidence"],
                        "process_id": process_id
                    }
            
            # Create new pattern
            pattern_obj = {
                "pattern_id": str(uuid.uuid4()),
                "name": name,
                "pattern": pattern,
                "examples": examples,
                "confidence": 0.5  # Initial confidence
            }
            
            # Development level affects initial confidence
            pattern_obj["confidence"] *= max(0.5, self.development_level)
            
            # Add to patterns
            self.grammar_model.syntactic_patterns.append(pattern_obj)
            
            # Record activation in neural state
            self.neural_state.add_activation("grammar_acquisition", {
                'operation': 'learn_syntactic_rule',
                'pattern': pattern,
                'confidence': pattern_obj["confidence"]
            })
            
            return {
                "status": "success",
                "message": "Learned new syntactic pattern",
                "pattern": pattern,
                "confidence": pattern_obj["confidence"],
                "process_id": process_id
            }
            
        elif rule_type == "morphological":
            # Morphological rule
            if "rule_name" not in input_data or "description" not in input_data or "pattern" not in input_data:
                return {
                    "status": "error",
                    "message": "Missing rule_name, description, or pattern for morphological rule",
                    "process_id": process_id
                }
            
            rule_name = input_data["rule_name"]
            description = input_data["description"]
            pattern = input_data["pattern"]
            examples = input_data.get("examples", {})
            
            # Check if this rule already exists
            if rule_name in self.grammar_model.morphological_rules:
                # Update existing rule
                existing = self.grammar_model.morphological_rules[rule_name]
                existing["description"] = description
                existing["pattern"] = pattern
                
                if examples:
                    existing["examples"] = examples
                
                # Increase confidence
                existing["confidence"] = min(1.0, existing.get("confidence", 0.5) + 0.1)
                
                # Record activation in neural state
                self.neural_state.add_activation("grammar_acquisition", {
                    'operation': 'update_morphological_rule',
                    'rule_name': rule_name,
                    'confidence': existing["confidence"]
                })
                
                return {
                    "status": "success",
                    "message": "Updated existing morphological rule",
                    "rule_name": rule_name,
                    "confidence": existing["confidence"],
                    "process_id": process_id
                }
            
            # Create new rule
            rule_obj = {
                "rule_id": str(uuid.uuid4()),
                "description": description,
                "pattern": pattern,
                "examples": examples,
                "confidence": 0.5  # Initial confidence
            }
            
            # Development level affects initial confidence
            rule_obj["confidence"] *= max(0.5, self.development_level)
            
            # Add to rules
            self.grammar_model.morphological_rules[rule_name] = rule_obj
            
            # Record activation in neural state
            self.neural_state.add_activation("grammar_acquisition", {
                'operation': 'learn_morphological_rule',
                'rule_name': rule_name,
                'confidence': rule_obj["confidence"]
            })
            
            return {
                "status": "success",
                "message": "Learned new morphological rule",
                "rule_name": rule_name,
                "confidence": rule_obj["confidence"],
                "process_id": process_id
            }
            
        elif rule_type == "category":
            # Grammatical category
            if "category" not in input_data or "words" not in input_data:
                return {
                    "status": "error",
                    "message": "Missing category or words for category rule",
                    "process_id": process_id
                }
            
            category = input_data["category"]
            words = input_data["words"]
            
            if not isinstance(words, list):
                words = [words]
            
            # Check if this category already exists
            if category in self.grammar_model.grammatical_categories:
                # Update existing category
                existing_words = self.grammar_model.grammatical_categories[category]
                
                # Add new words
                for word in words:
                    if word not in existing_words:
                        existing_words.append(word)
                
                # Record activation in neural state
                self.neural_state.add_activation("grammar_acquisition", {
                    'operation': 'update_category',
                    'category': category,
                    'word_count': len(existing_words)
                })
                
                return {
                    "status": "success",
                    "message": "Updated existing grammatical category",
                    "category": category,
                    "word_count": len(existing_words),
                    "process_id": process_id
                }
            
            # Create new category
            self.grammar_model.grammatical_categories[category] = words
            
            # Record activation in neural state
            self.neural_state.add_activation("grammar_acquisition", {
                'operation': 'learn_category',
                'category': category,
                'word_count': len(words)
            })
            
            return {
                "status": "success",
                "message": "Learned new grammatical category",
                "category": category,
                "word_count": len(words),
                "process_id": process_id
            }
            
        else:
            return {
                "status": "error",
                "message": f"Unknown rule_type: {rule_type}",
                "process_id": process_id
            }
    
    def _check_grammaticality(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Check if a sentence is grammatically correct
        
        Args:
            input_data: Input data dictionary including sentence
            process_id: Process identifier
            
        Returns:
            Dict with grammaticality assessment
        """
        # Check for sentence
        if "sentence" not in input_data:
            return {
                "status": "error",
                "message": "Missing sentence for grammaticality check",
                "process_id": process_id
            }
        
        sentence = input_data["sentence"]
        
        # Check development level
        if self.development_level < 0.3:
            return {
                "status": "undeveloped",
                "message": "Grammaticality judgment requires higher development level (0.3+)",
                "current_level": self.development_level,
                "process_id": process_id
            }
        
        # Tokenize sentence
        tokens = sentence.split()
        
        # Convert tokens to features
        if tokens:
            # Create a simple feature vector for the whole sentence
            sentence_vector = np.zeros(128)
            
            for i, token in enumerate(tokens):
                pos = (hash(token) + i) % 120
                sentence_vector[pos] = 1.0
            
            # Convert to tensor
            sentence_tensor = torch.tensor(sentence_vector, dtype=torch.float32).unsqueeze(0)
            sentence_tensor = sentence_tensor.to(self.device)
            
            # Process through network
            with torch.no_grad():
                output = self.network(sentence_tensor, operation="judge")
            
            grammaticality = float(output["grammaticality"].cpu().item())
            certainty = float(output["certainty"].cpu().item())
        else:
            # Empty sentence
            grammaticality = 0.0
            certainty = 1.0
        
        # Record activation in neural state
        self.neural_state.add_activation("grammar_acquisition", {
            'operation': 'check_grammaticality',
            'grammaticality': grammaticality,
            'certainty': certainty
        })
        
        # Record in recent inputs
        self.recent_inputs.append({
            "type": "grammaticality_check",
            "sentence": sentence,
            "grammaticality": grammaticality,
            "timestamp": datetime.now()
        })
        
        # Return grammaticality assessment
        return {
            "status": "success",
            "sentence": sentence,
            "grammatical": grammaticality > 0.5,
            "grammaticality_score": grammaticality,
            "certainty": certainty,
            "development_level": self.development_level,
            "process_id": process_id
        }
    
    def _query_grammar(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Query the grammar knowledge
        
        Args:
            input_data: Input data dictionary including query parameters
            process_id: Process identifier
            
        Returns:
            Dict with query results
        """
        # Get query type
        query_type = input_data.get("query_type", "all")
        
        if query_type == "all":
            # Return summary of grammar knowledge
            return {
                "status": "success",
                "syntactic_patterns": len(self.grammar_model.syntactic_patterns),
                "morphological_rules": len(self.grammar_model.morphological_rules),
                "grammatical_categories": list(self.grammar_model.grammatical_categories.keys()),
                "development_level": self.development_level,
                "process_id": process_id
            }
        
        elif query_type == "syntactic":
            # Return syntactic patterns
            patterns = []
            for pattern in self.grammar_model.syntactic_patterns:
                patterns.append({
                    "name": pattern["name"],
                    "pattern": pattern["pattern"],
                    "confidence": pattern["confidence"]
                })
            
            return {
                "status": "success",
                "syntactic_patterns": patterns,
                "count": len(patterns),
                "process_id": process_id
            }
        
        elif query_type == "morphological":
            # Return morphological rules
            rules = {}
            for rule_name, rule in self.grammar_model.morphological_rules.items():
                rules[rule_name] = {
                    "description": rule["description"],
                    "pattern": rule["pattern"],
                    "confidence": rule["confidence"]
                }
            
            return {
                "status": "success",
                "morphological_rules": rules,
                "count": len(rules),
                "process_id": process_id
            }
        
        elif query_type == "category":
            # Check for category
            category = input_data.get("category")
            
            if category and category in self.grammar_model.grammatical_categories:
                # Return specific category
                return {
                    "status": "success",
                    "category": category,
                    "words": self.grammar_model.grammatical_categories[category],
                    "word_count": len(self.grammar_model.grammatical_categories[category]),
                    "process_id": process_id
                }
            else:
                # Return all categories
                categories = {}
                for cat, words in self.grammar_model.grammatical_categories.items():
                    categories[cat] = len(words)
                
                return {
                    "status": "success",
                    "categories": categories,
                    "count": len(categories),
                    "process_id": process_id
                }
        
        else:
            return {
                "status": "error",
                "message": f"Unknown query_type: {query_type}",
                "process_id": process_id
            }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of the module
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New development level
        """
        old_level = self.development_level
        
        # Update development level
        self.development_level = max(0.0, min(1.0, self.development_level + amount))
        
        # Update neural network
        self.network.set_development_level(self.development_level)
        
        # Update neural state
        self.neural_state.grammar_acquisition_development = self.development_level
        self.neural_state.last_updated = datetime.now()
        
        # Check if crossed a milestone
        for level in sorted(self.development_milestones.keys()):
            if old_level < level <= self.development_level:
                milestone = self.development_milestones[level]
                
                # Publish milestone event if we have an event bus
                if self.event_bus:
                    self.event_bus.publish({
                        "sender": self.module_id,
                        "message_type": "development_milestone",
                        "content": {
                            "module": "grammar_acquisition",
                            "milestone": milestone,
                            "level": level
                        }
                    })
                
                # Update grammar structures for new development level
                self._initialize_grammar_structures()
        
        return self.development_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the grammar acquisition module
        
        Returns:
            Dict representing the current state
        """
        return {
            "module_id": self.module_id,
            "developmental_level": self.development_level,
            "syntactic_patterns": len(self.grammar_model.syntactic_patterns),
            "morphological_rules": len(self.grammar_model.morphological_rules),
            "grammatical_categories": len(self.grammar_model.grammatical_categories)
        }
    
    def save_state(self) -> Dict[str, Any]:
        """
        Save the current state for persistence
        
        Returns:
            Dict with serializable state
        """
        return {
            "module_id": self.module_id,
            "grammar_model": self.grammar_model.dict(),
            "developmental_level": self.development_level,
            "neural_state": {
                "development": self.neural_state.grammar_acquisition_development,
                "accuracy": self.neural_state.grammar_acquisition_accuracy
            }
        }
    
    def load_state(self, state: Dict[str, Any]) -> None:
        """
        Load a previously saved state
        
        Args:
            state: The state to load
        """
        # Load module ID
        self.module_id = state["module_id"]
        
        # Load development level
        self.development_level = state["developmental_level"]
        self.network.set_development_level(self.development_level)
        
        # Load grammar model
        if "grammar_model" in state:
            try:
                # Create new model from dict
                from pydantic import parse_obj_as
                self.grammar_model = parse_obj_as(GrammarModel, state["grammar_model"])
            except Exception as e:
                print(f"Error loading grammar model: {e}")
        
        # Load neural state
        if "neural_state" in state:
            ns = state["neural_state"]
            self.neural_state.grammar_acquisition_development = ns.get("development", self.development_level)
            self.neural_state.grammar_acquisition_accuracy = ns.get("accuracy", 0.5) 


#######################

#language\models.py#
#######################

from typing import Dict, List, Any, Optional, Set, Tuple
from datetime import datetime
import uuid
from pydantic import BaseModel, Field

class PhonemeModel(BaseModel):
    """
    Model for phoneme recognition and processing
    
    Represents the sound units of language and their recognition capabilities
    """
    phoneme_inventory: Dict[str, float] = Field(
        default_factory=dict, 
        description="Phonemes and their recognition confidence (0.0-1.0)"
    )
    phonotactic_rules: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Rules for phoneme combinations in the language"
    )
    phoneme_categories: Dict[str, List[str]] = Field(
        default_factory=dict,
        description="Categories of phonemes (vowels, consonants, etc.)"
    )
    phoneme_features: Dict[str, Dict[str, Any]] = Field(
        default_factory=dict,
        description="Phonological features for each phoneme"
    )
    language_specific_sounds: Set[str] = Field(
        default_factory=set,
        description="Language-specific phonemes being learned"
    )
    last_updated: datetime = Field(default_factory=datetime.now)

class WordModel(BaseModel):
    """
    Model for word learning and lexical knowledge
    
    Represents the vocabulary and word-related knowledge
    """
    vocabulary: Dict[str, float] = Field(
        default_factory=dict,
        description="Known words and their familiarity (0.0-1.0)"
    )
    word_categories: Dict[str, List[str]] = Field(
        default_factory=dict,
        description="Categories of words (nouns, verbs, etc.)"
    )
    word_embeddings: Dict[str, List[float]] = Field(
        default_factory=dict,
        description="Semantic embeddings for known words"
    )
    word_associations: Dict[str, List[str]] = Field(
        default_factory=dict,
        description="Word associations and semantic networks"
    )
    word_frequencies: Dict[str, int] = Field(
        default_factory=dict,
        description="Frequency of word encounters"
    )
    last_updated: datetime = Field(default_factory=datetime.now)

class GrammarModel(BaseModel):
    """
    Model for grammatical knowledge and structure
    
    Represents the rules and patterns of language structure
    """
    grammatical_structures: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Known grammatical structures and patterns"
    )
    morphological_rules: Dict[str, Dict[str, Any]] = Field(
        default_factory=dict,
        description="Rules for word formation and inflection"
    )
    syntactic_patterns: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Sentence construction patterns"
    )
    grammatical_categories: Dict[str, List[str]] = Field(
        default_factory=dict,
        description="Categories of grammatical elements"
    )
    rule_confidence: Dict[str, float] = Field(
        default_factory=dict,
        description="Confidence in grammatical rules (0.0-1.0)"
    )
    last_updated: datetime = Field(default_factory=datetime.now)

class SemanticModel(BaseModel):
    """
    Model for semantic understanding and meaning
    
    Represents knowledge of word and sentence meanings
    """
    concept_network: Dict[str, Dict[str, Any]] = Field(
        default_factory=dict,
        description="Network of concepts and their relationships"
    )
    semantic_features: Dict[str, Dict[str, Any]] = Field(
        default_factory=dict,
        description="Semantic features for concepts"
    )
    contextual_meanings: Dict[str, Dict[str, Any]] = Field(
        default_factory=dict,
        description="Context-dependent meanings"
    )
    semantic_categories: Dict[str, List[str]] = Field(
        default_factory=dict,
        description="Categories of semantic elements"
    )
    concept_embeddings: Dict[str, List[float]] = Field(
        default_factory=dict,
        description="Vector representations of concepts"
    )
    last_updated: datetime = Field(default_factory=datetime.now)

class ExpressionModel(BaseModel):
    """
    Model for language expression and generation
    
    Represents capabilities for producing language
    """
    expression_templates: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Templates for language production"
    )
    communication_intents: Dict[str, Dict[str, Any]] = Field(
        default_factory=dict,
        description="Mappings of intents to expressions"
    )
    pragmatic_rules: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Rules for appropriate language use in context"
    )
    speech_acts: Dict[str, List[Dict[str, Any]]] = Field(
        default_factory=dict,
        description="Types of speech acts and their implementations"
    )
    fluency_metrics: Dict[str, float] = Field(
        default_factory=dict,
        description="Metrics for expression fluency (0.0-1.0)"
    )
    last_updated: datetime = Field(default_factory=datetime.now)

class LanguageModel(BaseModel):
    """
    Comprehensive language acquisition and processing model
    
    Integrates all aspects of language knowledge and capability
    """
    phonemes: PhonemeModel = Field(default_factory=PhonemeModel)
    vocabulary: WordModel = Field(default_factory=WordModel)
    grammar: GrammarModel = Field(default_factory=GrammarModel)
    semantics: SemanticModel = Field(default_factory=SemanticModel)
    expression: ExpressionModel = Field(default_factory=ExpressionModel)
    
    developmental_level: float = Field(
        default=0.0, 
        ge=0.0, 
        le=1.0,
        description="Overall developmental level of language (0.0-1.0)"
    )
    
    component_levels: Dict[str, float] = Field(
        default_factory=lambda: {
            "phoneme_recognition": 0.0,
            "word_learning": 0.0,
            "grammar_acquisition": 0.0,
            "semantic_processing": 0.0,
            "expression_generation": 0.0
        },
        description="Development levels of individual components"
    )
    
    module_id: str = Field(default="language")
    last_updated: datetime = Field(default_factory=datetime.now)

class LanguageNeuralState(BaseModel):
    """
    Neural state information for language networks
    
    Tracks the state of neural networks for language components
    """
    phoneme_recognition_development: float = Field(
        0.0, ge=0.0, le=1.0, 
        description="Development level of phoneme recognition network"
    )
    word_learning_development: float = Field(
        0.0, ge=0.0, le=1.0, 
        description="Development level of word learning network"
    )
    grammar_acquisition_development: float = Field(
        0.0, ge=0.0, le=1.0, 
        description="Development level of grammar acquisition network"
    )
    semantic_processing_development: float = Field(
        0.0, ge=0.0, le=1.0, 
        description="Development level of semantic processing network"
    )
    expression_generation_development: float = Field(
        0.0, ge=0.0, le=1.0, 
        description="Development level of expression generation network"
    )
    
    # Track recent activations for each neural component
    recent_phoneme_activations: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Recent activations of the phoneme recognition network"
    )
    recent_word_activations: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Recent activations of the word learning network"
    )
    recent_grammar_activations: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Recent activations of the grammar acquisition network"
    )
    recent_semantic_activations: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Recent activations of the semantic processing network"
    )
    recent_expression_activations: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Recent activations of the expression generation network"
    )
    
    # Network performance metrics
    phoneme_recognition_accuracy: float = Field(
        0.5, ge=0.0, le=1.0, 
        description="Accuracy of phoneme recognition network"
    )
    word_learning_accuracy: float = Field(
        0.5, ge=0.0, le=1.0, 
        description="Accuracy of word learning network"
    )
    grammar_acquisition_accuracy: float = Field(
        0.5, ge=0.0, le=1.0, 
        description="Accuracy of grammar acquisition network"
    )
    semantic_processing_accuracy: float = Field(
        0.5, ge=0.0, le=1.0, 
        description="Accuracy of semantic processing network"
    )
    expression_generation_accuracy: float = Field(
        0.5, ge=0.0, le=1.0, 
        description="Accuracy of expression generation network"
    )
    
    # Last update timestamp
    last_updated: datetime = Field(
        default_factory=datetime.now, 
        description="When neural state was last updated"
    )
    
    def update_accuracy(self, component: str, accuracy: float) -> None:
        """
        Update the accuracy for a specific component
        
        Args:
            component: The component to update
            accuracy: The new accuracy value
        """
        if component == "phoneme_recognition":
            self.phoneme_recognition_accuracy = accuracy
        elif component == "word_learning":
            self.word_learning_accuracy = accuracy
        elif component == "grammar_acquisition":
            self.grammar_acquisition_accuracy = accuracy
        elif component == "semantic_processing":
            self.semantic_processing_accuracy = accuracy
        elif component == "expression_generation":
            self.expression_generation_accuracy = accuracy
            
        self.last_updated = datetime.now()
    
    def add_activation(self, component: str, activation: Dict[str, Any]) -> None:
        """
        Add a new activation for a specific component
        
        Args:
            component: The component with the activation
            activation: The activation details
        """
        if component == "phoneme_recognition":
            self.recent_phoneme_activations.append(activation)
            # Keep only the most recent activations
            if len(self.recent_phoneme_activations) > 20:
                self.recent_phoneme_activations.pop(0)
        elif component == "word_learning":
            self.recent_word_activations.append(activation)
            if len(self.recent_word_activations) > 20:
                self.recent_word_activations.pop(0)
        elif component == "grammar_acquisition":
            self.recent_grammar_activations.append(activation)
            if len(self.recent_grammar_activations) > 20:
                self.recent_grammar_activations.pop(0)
        elif component == "semantic_processing":
            self.recent_semantic_activations.append(activation)
            if len(self.recent_semantic_activations) > 20:
                self.recent_semantic_activations.pop(0)
        elif component == "expression_generation":
            self.recent_expression_activations.append(activation)
            if len(self.recent_expression_activations) > 20:
                self.recent_expression_activations.pop(0)
                
        self.last_updated = datetime.now()


#######################

#language\neural_net.py#
#######################

import torch 
import torch.nn as nn 
import torch.nn.functional as F
import numpy as np
from typing import Dict, Any, Optional, Tuple, List, Union

def get_device() -> torch.device:
    """
    Get the appropriate device for tensor operations.
    
    Returns:
        torch.device: CUDA device if available, otherwise CPU
    """
    if torch.cuda.is_available():
        return torch.device("cuda")
    return torch.device("cpu")

class PhonemeNetwork(nn.Module):
    """
    Neural network for phoneme recognition and processing
    
    This network processes audio input to recognize phonemes and
    learns phonological patterns in the language.
    """
    
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 64, num_phonemes: int = 50):
        """
        Initialize the phoneme recognition network
        
        Args:
            input_dim: Dimension of input features
            hidden_dim: Dimension of hidden layers
            output_dim: Dimension of output features
            num_phonemes: Number of phonemes to recognize
        """
        super().__init__()
        
        # Main layers
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Phoneme recognition head
        self.phoneme_classifier = nn.Linear(hidden_dim, num_phonemes)
        
        # Phoneme embedding
        self.phoneme_embedding = nn.Linear(hidden_dim, output_dim)
        
        # Feature extraction
        self.feature_extractor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU()
        )
        
        # Developmental factor (grows with learning)
        self.developmental_factor = nn.Parameter(torch.tensor(0.1), requires_grad=False)
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Initialize network weights"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, input_data: torch.Tensor, operation: str = "recognize") -> Dict[str, torch.Tensor]:
        """
        Forward pass through the network
        
        Args:
            input_data: Input tensor [batch_size, input_dim]
            operation: Type of operation to perform
                "recognize": Recognize phonemes in input
                "analyze": Extract phonological features
                "embed": Generate phoneme embeddings
            
        Returns:
            Dictionary of output tensors
        """
        # Get base encoding
        x = self.encoder(input_data)
        
        # Apply developmental scaling to more complex operations
        dev_factor = torch.sigmoid(self.developmental_factor * 10)
        
        # Perform operation
        if operation == "recognize":
            # Phoneme recognition
            logits = self.phoneme_classifier(x)
            probabilities = F.softmax(logits, dim=-1)
            
            # Confidence increases with development
            confidence = torch.sigmoid(torch.max(logits, dim=1)[0]) * dev_factor
            
            return {
                "phoneme_logits": logits,
                "phoneme_probs": probabilities,
                "confidence": confidence
            }
            
        elif operation == "analyze":
            # Extract phonological features
            features = self.feature_extractor(x)
            
            # Feature clarity improves with development
            feature_clarity = features * dev_factor
            
            return {
                "phoneme_features": feature_clarity,
                "encoding": x
            }
            
        elif operation == "embed":
            # Generate phoneme embeddings
            embeddings = self.phoneme_embedding(x)
            
            return {
                "phoneme_embedding": embeddings,
                "encoding": x
            }
            
        else:
            # Default to basic encoding
            return {
                "encoding": x
            }
    
    def set_development_level(self, level: float) -> None:
        """
        Set the developmental level of the network
        
        Args:
            level: Development level (0.0 to 1.0)
        """
        with torch.no_grad():
            self.developmental_factor.copy_(torch.tensor(max(0.0, min(1.0, level))))

class WordNetwork(nn.Module):
    """
    Neural network for word learning and lexical processing
    
    This network learns words and their associations.
    """
    
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 64, vocab_size: int = 1000):
        """
        Initialize the word learning network
        
        Args:
            input_dim: Dimension of input features
            hidden_dim: Dimension of hidden layers
            output_dim: Dimension of output features
            vocab_size: Maximum vocabulary size
        """
        super().__init__()
        
        # Main layers
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Word recognition head
        self.word_classifier = nn.Linear(hidden_dim, vocab_size)
        
        # Word embedding
        self.word_embedding = nn.Linear(hidden_dim, output_dim)
        
        # Word generation (from meaning to word)
        self.word_generator = nn.Sequential(
            nn.Linear(output_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, vocab_size)
        )
        
        # Association network
        self.association_network = nn.Bilinear(output_dim, output_dim, 1)
        
        # Developmental factor (grows with learning)
        self.developmental_factor = nn.Parameter(torch.tensor(0.1), requires_grad=False)
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Initialize network weights"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, input_data: torch.Tensor, operation: str = "recognize", 
               context: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        Forward pass through the network
        
        Args:
            input_data: Input tensor [batch_size, input_dim]
            operation: Type of operation to perform
                "recognize": Recognize words in input
                "embed": Generate word embeddings
                "associate": Find associations between words
                "generate": Generate word from meaning
            context: Optional context tensor for operations that need it
            
        Returns:
            Dictionary of output tensors
        """
        # Get base encoding
        x = self.encoder(input_data)
        
        # Apply developmental scaling
        dev_factor = torch.sigmoid(self.developmental_factor * 10)
        
        # Perform operation
        if operation == "recognize":
            # Word recognition
            logits = self.word_classifier(x)
            probabilities = F.softmax(logits, dim=-1)
            
            # Confidence increases with development
            confidence = torch.sigmoid(torch.max(logits, dim=1)[0]) * dev_factor
            
            return {
                "word_logits": logits,
                "word_probs": probabilities,
                "confidence": confidence
            }
            
        elif operation == "embed":
            # Generate word embeddings
            embeddings = self.word_embedding(x)
            
            # Semantic richness increases with development
            embeddings = embeddings * (0.5 + 0.5 * dev_factor)
            
            return {
                "word_embedding": embeddings,
                "encoding": x
            }
            
        elif operation == "associate" and context is not None:
            # Generate embeddings for input and context
            input_embedding = self.word_embedding(x)
            context_encoded = self.encoder(context)
            context_embedding = self.word_embedding(context_encoded)
            
            # Compute association strength
            association = self.association_network(input_embedding, context_embedding)
            
            # Association strength influenced by development
            association = association * dev_factor
            
            return {
                "association_strength": association,
                "input_embedding": input_embedding,
                "context_embedding": context_embedding
            }
            
        elif operation == "generate":
            # Generate word from meaning embedding
            embedding = self.word_embedding(x)
            word_logits = self.word_generator(embedding)
            word_probs = F.softmax(word_logits, dim=-1)
            
            # Generation quality improves with development
            quality = torch.sigmoid(torch.max(word_logits, dim=1)[0]) * dev_factor
            
            return {
                "word_logits": word_logits,
                "word_probs": word_probs,
                "generation_quality": quality
            }
            
        else:
            # Default to basic encoding
            return {
                "encoding": x
            }
    
    def set_development_level(self, level: float) -> None:
        """
        Set the developmental level of the network
        
        Args:
            level: Development level (0.0 to 1.0)
        """
        with torch.no_grad():
            self.developmental_factor.copy_(torch.tensor(max(0.0, min(1.0, level))))

class GrammarNetwork(nn.Module):
    """
    Neural network for grammar acquisition and processing
    
    This network learns grammatical structures and rules.
    """
    
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 64, num_structures: int = 50):
        """
        Initialize the grammar acquisition network
        
        Args:
            input_dim: Dimension of input features
            hidden_dim: Dimension of hidden layers
            output_dim: Dimension of output features
            num_structures: Number of grammatical structures to recognize
        """
        super().__init__()
        
        # Main layers
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Structure recognition head
        self.structure_classifier = nn.Linear(hidden_dim, num_structures)
        
        # Structure embedding
        self.structure_embedding = nn.Linear(hidden_dim, output_dim)
        
        # Grammaticality judgment
        self.grammaticality_judge = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
        
        # Sequence prediction (for next-word prediction based on grammar)
        self.sequence_predictor = nn.GRU(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True
        )
        self.prediction_head = nn.Linear(hidden_dim, input_dim)
        
        # Developmental factor (grows with learning)
        self.developmental_factor = nn.Parameter(torch.tensor(0.1), requires_grad=False)
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Initialize network weights"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, input_data: torch.Tensor, operation: str = "recognize", 
               sequence: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        Forward pass through the network
        
        Args:
            input_data: Input tensor [batch_size, input_dim]
            operation: Type of operation to perform
                "recognize": Recognize grammatical structures
                "judge": Judge grammaticality
                "embed": Generate structure embeddings
                "predict": Predict next element in sequence
            sequence: Optional sequence tensor for sequence operations
            
        Returns:
            Dictionary of output tensors
        """
        # Get base encoding
        x = self.encoder(input_data)
        
        # Apply developmental scaling
        dev_factor = torch.sigmoid(self.developmental_factor * 10)
        
        # Perform operation
        if operation == "recognize":
            # Structure recognition
            logits = self.structure_classifier(x)
            probabilities = F.softmax(logits, dim=-1)
            
            # Confidence increases with development
            confidence = torch.sigmoid(torch.max(logits, dim=1)[0]) * dev_factor
            
            return {
                "structure_logits": logits,
                "structure_probs": probabilities,
                "confidence": confidence
            }
            
        elif operation == "judge":
            # Judge grammaticality
            grammaticality = self.grammaticality_judge(x)
            
            # Judgment accuracy increases with development
            certainty = grammaticality * (0.5 + 0.5 * dev_factor)
            
            return {
                "grammaticality": grammaticality,
                "certainty": certainty
            }
            
        elif operation == "embed":
            # Generate structure embeddings
            embeddings = self.structure_embedding(x)
            
            return {
                "structure_embedding": embeddings,
                "encoding": x
            }
            
        elif operation == "predict" and sequence is not None:
            # Process sequence through GRU
            outputs, hidden = self.sequence_predictor(sequence)
            
            # Predict next element
            prediction = self.prediction_head(outputs[:, -1, :])
            
            # Prediction quality improves with development
            quality = torch.norm(prediction, dim=1, keepdim=True) * dev_factor
            
            return {
                "prediction": prediction,
                "quality": quality,
                "hidden": hidden
            }
            
        else:
            # Default to basic encoding
            return {
                "encoding": x
            }
    
    def set_development_level(self, level: float) -> None:
        """
        Set the developmental level of the network
        
        Args:
            level: Development level (0.0 to 1.0)
        """
        with torch.no_grad():
            self.developmental_factor.copy_(torch.tensor(max(0.0, min(1.0, level))))

class SemanticNetwork(nn.Module):
    """
    Neural network for semantic processing
    
    This network learns meanings, concepts, and semantic relations.
    """
    
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 64, num_concepts: int = 200):
        """
        Initialize the semantic processing network
        
        Args:
            input_dim: Dimension of input features
            hidden_dim: Dimension of hidden layers
            output_dim: Dimension of output features
            num_concepts: Number of basic concepts to recognize
        """
        super().__init__()
        
        # Main layers
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Concept recognition
        self.concept_classifier = nn.Linear(hidden_dim, num_concepts)
        
        # Semantic embedding
        self.semantic_embedding = nn.Linear(hidden_dim, output_dim)
        
        # Relation recognition
        self.relation_network = nn.Bilinear(output_dim, output_dim, hidden_dim // 4)
        self.relation_classifier = nn.Linear(hidden_dim // 4, 10)  # 10 basic relation types
        
        # Contextual understanding
        self.context_network = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        
        # Developmental factor (grows with learning)
        self.developmental_factor = nn.Parameter(torch.tensor(0.1), requires_grad=False)
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Initialize network weights"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, input_data: torch.Tensor, operation: str = "understand", 
               context: Optional[torch.Tensor] = None, 
               second_concept: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        Forward pass through the network
        
        Args:
            input_data: Input tensor [batch_size, input_dim]
            operation: Type of operation to perform
                "understand": Extract meaning from input
                "embed": Generate semantic embeddings
                "relate": Find relations between concepts
                "contextualize": Understand meaning in context
            context: Optional context tensor for context operations
            second_concept: Optional tensor for relation operations
            
        Returns:
            Dictionary of output tensors
        """
        # Get base encoding
        x = self.encoder(input_data)
        
        # Apply developmental scaling
        dev_factor = torch.sigmoid(self.developmental_factor * 10)
        
        # Perform operation
        if operation == "understand":
            # Concept recognition
            logits = self.concept_classifier(x)
            probabilities = F.softmax(logits, dim=-1)
            
            # Understanding depth increases with development
            depth = torch.max(probabilities, dim=1)[0] * dev_factor
            
            return {
                "concept_logits": logits,
                "concept_probs": probabilities,
                "understanding_depth": depth
            }
            
        elif operation == "embed":
            # Generate semantic embeddings
            embeddings = self.semantic_embedding(x)
            
            # Semantic richness increases with development
            richness = torch.norm(embeddings, dim=1, keepdim=True) * dev_factor
            
            return {
                "semantic_embedding": embeddings,
                "semantic_richness": richness,
                "encoding": x
            }
            
        elif operation == "relate" and second_concept is not None:
            # Generate embeddings for both concepts
            input_embedding = self.semantic_embedding(x)
            
            # Either use encoded second concept or encode it
            if second_concept.size(-1) == output_dim:
                concept2_embedding = second_concept
            else:
                concept2_encoded = self.encoder(second_concept)
                concept2_embedding = self.semantic_embedding(concept2_encoded)
            
            # Compute relation features
            relation_features = self.relation_network(input_embedding, concept2_embedding)
            relation_logits = self.relation_classifier(relation_features)
            relation_probs = F.softmax(relation_logits, dim=-1)
            
            # Relation understanding improves with development
            clarity = torch.max(relation_probs, dim=1)[0] * dev_factor
            
            return {
                "relation_logits": relation_logits,
                "relation_probs": relation_probs,
                "relation_clarity": clarity
            }
            
        elif operation == "contextualize" and context is not None:
            # Process context
            context_encoded = self.encoder(context)
            
            # Combine input and context
            combined = torch.cat([x, context_encoded], dim=1)
            contextualized = self.context_network(combined)
            
            # Contextual understanding improves with development
            context_effect = torch.norm(contextualized - self.semantic_embedding(x), dim=1, keepdim=True) * dev_factor
            
            return {
                "contextualized_embedding": contextualized,
                "context_effect": context_effect
            }
            
        else:
            # Default to basic encoding
            return {
                "encoding": x
            }
    
    def set_development_level(self, level: float) -> None:
        """
        Set the developmental level of the network
        
        Args:
            level: Development level (0.0 to 1.0)
        """
        with torch.no_grad():
            self.developmental_factor.copy_(torch.tensor(max(0.0, min(1.0, level))))

class ExpressionNetwork(nn.Module):
    """
    Neural network for language expression generation
    
    This network generates language expressions based on intent.
    """
    
    def __init__(self, input_dim: int = 128, hidden_dim: int = 256, output_dim: int = 64, vocab_size: int = 1000):
        """
        Initialize the expression generation network
        
        Args:
            input_dim: Dimension of input features
            hidden_dim: Dimension of hidden layers
            output_dim: Dimension of output features
            vocab_size: Vocabulary size for word generation
        """
        super().__init__()
        
        # Main intent encoder
        self.intent_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Expression planning
        self.expression_planner = nn.GRU(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True
        )
        
        # Word generation
        self.word_generator = nn.Linear(hidden_dim, vocab_size)
        
        # Fluency evaluation
        self.fluency_evaluator = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
        
        # Expression embedding
        self.expression_embedding = nn.Linear(hidden_dim, output_dim)
        
        # Developmental factor (grows with learning)
        self.developmental_factor = nn.Parameter(torch.tensor(0.1), requires_grad=False)
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Initialize network weights"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, input_data: torch.Tensor, operation: str = "generate", 
               sequence_length: int = 5, 
               context: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        Forward pass through the network
        
        Args:
            input_data: Input tensor [batch_size, input_dim] (intent representation)
            operation: Type of operation to perform
                "generate": Generate language expression
                "plan": Plan expression structure
                "evaluate": Evaluate expression fluency
            sequence_length: Length of sequence to generate
            context: Optional context for expression generation
            
        Returns:
            Dictionary of output tensors
        """
        # Get intent encoding
        x = self.intent_encoder(input_data)
        
        # Apply developmental scaling
        dev_factor = torch.sigmoid(self.developmental_factor * 10)
        
        # Batch size
        batch_size = x.size(0)
        
        # Perform operation
        if operation == "generate":
            # Initialize generation
            hidden = x.unsqueeze(0).repeat(2, 1, 1)  # Initial hidden state from intent
            current_input = x.unsqueeze(1)  # [batch_size, 1, hidden_dim]
            
            # Generate sequence
            word_logits_sequence = []
            
            for i in range(sequence_length):
                # Generate next step
                output, hidden = self.expression_planner(current_input, hidden)
                
                # Predict word
                word_logits = self.word_generator(output.squeeze(1))
                word_logits_sequence.append(word_logits)
                
                # Set up next input (teacher forcing would use actual words here)
                current_input = output
            
            # Stack logits sequences
            word_logits_sequence = torch.stack(word_logits_sequence, dim=1)
            
            # Generate word probabilities
            word_probs_sequence = F.softmax(word_logits_sequence, dim=-1)
            
            # Fluency increases with development
            fluency = self.fluency_evaluator(hidden[-1]) * dev_factor
            
            return {
                "word_logits_sequence": word_logits_sequence,
                "word_probs_sequence": word_probs_sequence,
                "fluency": fluency,
                "hidden_state": hidden
            }
            
        elif operation == "plan":
            # Plan expression structure without generating specific words
            hidden = x.unsqueeze(0).repeat(2, 1, 1)
            
            # Create sequence input (repeat intent)
            sequence_input = x.unsqueeze(1).repeat(1, sequence_length, 1)
            
            # Generate plan
            plan_sequence, hidden = self.expression_planner(sequence_input, hidden)
            
            # Plan quality improves with development
            plan_quality = self.fluency_evaluator(hidden[-1]) * dev_factor
            
            return {
                "expression_plan": plan_sequence,
                "plan_quality": plan_quality
            }
            
        elif operation == "evaluate" and context is not None:
            # Encode context (existing expression)
            _, hidden = self.expression_planner(context, None)
            
            # Evaluate fluency
            fluency = self.fluency_evaluator(hidden[-1])
            
            # Evaluation accuracy improves with development
            confidence = fluency * dev_factor
            
            return {
                "fluency": fluency,
                "evaluation_confidence": confidence
            }
            
        else:
            # Default to basic encoding
            embedding = self.expression_embedding(x)
            
            return {
                "expression_embedding": embedding,
                "intent_encoding": x
            }
    
    def set_development_level(self, level: float) -> None:
        """
        Set the developmental level of the network
        
        Args:
            level: Development level (0.0 to 1.0)
        """
        with torch.no_grad():
            self.developmental_factor.copy_(torch.tensor(max(0.0, min(1.0, level))))


#######################

#language\phoneme_recognition.py#
#######################

# TODO: Implement the PhonemeRecognition class to identify basic speech sounds
# This component should be able to:
# - Recognize phonemes in speech input
# - Differentiate between similar phonemes
# - Adapt to different speakers and accents
# - Develop phonological awareness

# TODO: Implement developmental progression in phoneme recognition:
# - Basic categorical perception in early stages
# - Growing phoneme differentiation in early childhood
# - Phonological rule understanding in later childhood
# - Automaticity in phoneme processing in adulthood

# TODO: Create mechanisms for:
# - Acoustic analysis: Extract relevant sound features
# - Phoneme categorization: Classify sounds as specific phonemes
# - Speaker normalization: Adjust for speaker differences
# - Phonological rule learning: Understand phoneme patterns

# TODO: Implement phonological awareness capabilities:
# - Phoneme identification: Recognize distinct sound units
# - Phoneme manipulation: Add/remove/change sounds
# - Syllable awareness: Recognize syllable boundaries
# - Pattern recognition: Identify rhymes and alliteration

# TODO: Connect to perception and word learning systems
# Phoneme recognition should draw on auditory perception
# and feed into word learning processes

from typing import Dict, List, Any, Optional, Set, Tuple
import torch
import uuid
import numpy as np
from datetime import datetime
from collections import deque

from lmm_project.base.module import BaseModule
from lmm_project.event_bus import EventBus
from lmm_project.modules.language.models import PhonemeModel, LanguageNeuralState
from lmm_project.modules.language.neural_net import PhonemeNetwork, get_device
from lmm_project.utils.llm_client import LLMClient

class PhonemeRecognition(BaseModule):
    """
    Recognizes and processes phonemes (speech sounds)
    
    This module is responsible for learning to recognize phonemes,
    differentiate between similar sounds, and learn phonotactic rules.
    """
    
    # Development milestones
    development_milestones = {
        0.0: "Basic sound discrimination",
        0.2: "Native language sound category formation",
        0.4: "Phoneme boundary detection",
        0.6: "Phonotactic rule learning",
        0.8: "Non-native phoneme attenuation",
        1.0: "Complete phonological system"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the phoneme recognition module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level (0.0 to 1.0)
        """
        super().__init__(module_id, event_bus)
        
        # Initialize phoneme model
        self.phoneme_model = PhonemeModel()
        
        # Set initial development level
        self.development_level = max(0.0, min(1.0, development_level))
        
        # Initialize neural network
        self.device = get_device()
        self.network = PhonemeNetwork().to(self.device)
        self.network.set_development_level(self.development_level)
        
        # Initialize neural state
        self.neural_state = LanguageNeuralState()
        self.neural_state.phoneme_recognition_development = self.development_level
        
        # Initialize basic phoneme inventory based on development level
        self._initialize_phoneme_inventory()
        
        # Recent inputs queue (for tracking recent phoneme exposures)
        self.recent_inputs = deque(maxlen=100)
        
        # For embedding generation when needed
        self.llm_client = LLMClient()
    
    def _initialize_phoneme_inventory(self):
        """Initialize basic phoneme inventory based on development level"""
        # Basic vowels (recognized at earliest stages)
        basic_vowels = {"a": 0.7, "i": 0.7, "u": 0.7}
        
        # Add basic vowels to inventory
        for phoneme, recognition in basic_vowels.items():
            # Scale recognition by development level
            scaled_recognition = recognition * max(0.3, self.development_level)
            self.phoneme_model.phoneme_inventory[phoneme] = scaled_recognition
        
        # Add to vowel category
        self.phoneme_model.phoneme_categories["vowels"] = list(basic_vowels.keys())
        
        if self.development_level >= 0.2:
            # Basic consonants (recognized at slightly later stages)
            basic_consonants = {"m": 0.6, "b": 0.6, "p": 0.6, "t": 0.6, "d": 0.6}
            
            # Add basic consonants to inventory
            for phoneme, recognition in basic_consonants.items():
                # Scale recognition by development level
                scaled_recognition = recognition * ((self.development_level - 0.2) / 0.8)
                self.phoneme_model.phoneme_inventory[phoneme] = scaled_recognition
            
            # Add to consonant category
            self.phoneme_model.phoneme_categories["consonants"] = list(basic_consonants.keys())
        
        if self.development_level >= 0.4:
            # More complex phonemes
            complex_phonemes = {"f": 0.5, "v": 0.5, "s": 0.5, "z": 0.5, "k": 0.5, "g": 0.5}
            
            # Add complex phonemes to inventory
            for phoneme, recognition in complex_phonemes.items():
                # Scale recognition by development level
                scaled_recognition = recognition * ((self.development_level - 0.4) / 0.6)
                self.phoneme_model.phoneme_inventory[phoneme] = scaled_recognition
            
            # Add to consonant category
            self.phoneme_model.phoneme_categories["consonants"].extend(list(complex_phonemes.keys()))
            
            # Basic phonotactic rules
            self.phoneme_model.phonotactic_rules.append({
                "description": "Consonant-vowel sequence",
                "pattern": "CV",
                "confidence": 0.6 * ((self.development_level - 0.4) / 0.6)
            })
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to the phoneme recognition module
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Dict with processing results
        """
        # Validate input
        if not isinstance(input_data, dict):
            return {
                "status": "error",
                "message": "Input must be a dictionary"
            }
        
        # Extract process ID if provided
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Extract operation
        operation = input_data.get("operation", "recognize")
        
        # Dispatch to appropriate handler
        if operation == "recognize":
            return self._recognize_phonemes(input_data, process_id)
        elif operation == "analyze_patterns":
            return self._analyze_phoneme_patterns(input_data, process_id)
        elif operation == "learn_phoneme":
            return self._learn_phoneme(input_data, process_id)
        elif operation == "query_inventory":
            return self._query_phoneme_inventory(input_data, process_id)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "process_id": process_id
            }
    
    def _recognize_phonemes(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Recognize phonemes in audio input
        
        Args:
            input_data: Input data dictionary including audio features
            process_id: Process identifier
            
        Returns:
            Dict with recognition results
        """
        # Check for audio features
        if "audio_features" not in input_data:
            return {
                "status": "error",
                "message": "Missing audio_features for phoneme recognition",
                "process_id": process_id
            }
        
        # Get audio features
        audio_features = input_data["audio_features"]
        
        # Convert to tensor if needed
        if not isinstance(audio_features, torch.Tensor):
            audio_features = torch.tensor(audio_features, dtype=torch.float32)
        
        # Ensure batch dimension
        if len(audio_features.shape) == 1:
            audio_features = audio_features.unsqueeze(0)
        
        # Process through network
        audio_features = audio_features.to(self.device)
        with torch.no_grad():
            output = self.network(audio_features, operation="recognize")
        
        # Get top phonemes
        phoneme_probs = output["phoneme_probs"].cpu().numpy()[0]
        confidence = output["confidence"].cpu().item()
        
        # Create list of phonemes with probabilities
        all_phonemes = list(self.phoneme_model.phoneme_inventory.keys())
        phoneme_probs_dict = {}
        
        # Map probabilities to phonemes (up to the size of our inventory)
        for i, prob in enumerate(phoneme_probs[:min(len(all_phonemes), len(phoneme_probs))]):
            phoneme_probs_dict[all_phonemes[i]] = float(prob)
        
        # Get top phonemes
        top_phonemes = sorted(phoneme_probs_dict.items(), key=lambda x: x[1], reverse=True)[:5]
        
        # Record in recent inputs
        self.recent_inputs.append({
            "type": "phoneme_recognition",
            "phonemes": phoneme_probs_dict,
            "confidence": confidence,
            "timestamp": datetime.now()
        })
        
        # Record activation in neural state
        self.neural_state.add_activation("phoneme_recognition", {
            'operation': 'recognize',
            'confidence': confidence,
            'top_phoneme': top_phonemes[0][0] if top_phonemes else None
        })
        
        # Return recognized phonemes
        return {
            "status": "success",
            "recognized_phonemes": top_phonemes,
            "confidence": confidence,
            "developmental_level": self.development_level,
            "process_id": process_id
        }
    
    def _analyze_phoneme_patterns(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Analyze phoneme patterns to identify phonotactic rules
        
        Args:
            input_data: Input data dictionary including phoneme sequence
            process_id: Process identifier
            
        Returns:
            Dict with analysis results
        """
        # Check for phoneme sequence
        if "phoneme_sequence" not in input_data:
            return {
                "status": "error",
                "message": "Missing phoneme_sequence for pattern analysis",
                "process_id": process_id
            }
        
        # Get phoneme sequence
        phoneme_sequence = input_data["phoneme_sequence"]
        
        # Check development level
        if self.development_level < 0.4:
            return {
                "status": "undeveloped",
                "message": "Pattern analysis requires higher development level (0.4+)",
                "current_level": self.development_level,
                "process_id": process_id
            }
        
        # Convert sequence to features
        sequence_features = []
        for phoneme in phoneme_sequence:
            # Create simple one-hot-like features
            # In a real implementation, would use actual phonological features
            feature_vec = np.zeros(128)
            
            # Set simple hash-based feature
            hash_val = hash(phoneme) % 100
            feature_vec[hash_val] = 1.0
            
            sequence_features.append(feature_vec)
        
        # Convert to tensor
        sequence_tensor = torch.tensor(np.array(sequence_features), dtype=torch.float32)
        sequence_tensor = sequence_tensor.to(self.device)
        
        # Process through network
        with torch.no_grad():
            output = self.network(sequence_tensor.mean(dim=0, keepdim=True), operation="analyze")
        
        # Extract pattern features
        pattern_features = output["phoneme_features"].cpu().numpy()[0]
        
        # Build a simplified pattern description
        # Identify consonant-vowel patterns
        cv_pattern = ""
        for phoneme in phoneme_sequence:
            if phoneme in self.phoneme_model.phoneme_categories.get("vowels", []):
                cv_pattern += "V"
            elif phoneme in self.phoneme_model.phoneme_categories.get("consonants", []):
                cv_pattern += "C"
            else:
                cv_pattern += "?"
        
        # Check for existing patterns or create new ones
        pattern_found = False
        for rule in self.phoneme_model.phonotactic_rules:
            if rule["pattern"] == cv_pattern:
                # Increase confidence in existing rule
                rule["confidence"] = min(1.0, rule["confidence"] + 0.05)
                pattern_found = True
                break
        
        # Add new pattern if not found and meets minimum criteria
        if not pattern_found and len(cv_pattern) >= 2:
            # Only mature enough systems can create new rules
            if self.development_level >= 0.6:
                self.phoneme_model.phonotactic_rules.append({
                    "description": f"Observed phoneme pattern",
                    "pattern": cv_pattern,
                    "confidence": 0.3,
                    "examples": [phoneme_sequence]
                })
        
        # Record activation in neural state
        self.neural_state.add_activation("phoneme_recognition", {
            'operation': 'analyze_patterns',
            'pattern': cv_pattern
        })
        
        # Return analysis results
        return {
            "status": "success",
            "cv_pattern": cv_pattern,
            "pattern_features": pattern_features.tolist(),
            "developmental_level": self.development_level,
            "process_id": process_id
        }
    
    def _learn_phoneme(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Learn a new phoneme or update recognition for existing phoneme
        
        Args:
            input_data: Input data dictionary including phoneme and examples
            process_id: Process identifier
            
        Returns:
            Dict with learning results
        """
        # Check for phoneme
        if "phoneme" not in input_data:
            return {
                "status": "error",
                "message": "Missing phoneme for learning",
                "process_id": process_id
            }
        
        # Get phoneme and examples
        phoneme = input_data["phoneme"]
        examples = input_data.get("examples", [])
        
        # Get category if provided
        category = input_data.get("category", "unknown")
        
        # Check if phoneme already exists
        if phoneme in self.phoneme_model.phoneme_inventory:
            # Update recognition confidence
            current_confidence = self.phoneme_model.phoneme_inventory[phoneme]
            # Increase recognition, but limited by development level
            max_confidence = min(0.95, 0.3 + (self.development_level * 0.7))
            new_confidence = min(max_confidence, current_confidence + 0.05)
            self.phoneme_model.phoneme_inventory[phoneme] = new_confidence
            
            status = "updated"
        else:
            # Add new phoneme with initial confidence based on development
            initial_confidence = min(0.5, 0.2 + (self.development_level * 0.3))
            self.phoneme_model.phoneme_inventory[phoneme] = initial_confidence
            
            # Add to appropriate category
            if category in self.phoneme_model.phoneme_categories:
                if phoneme not in self.phoneme_model.phoneme_categories[category]:
                    self.phoneme_model.phoneme_categories[category].append(phoneme)
            else:
                self.phoneme_model.phoneme_categories[category] = [phoneme]
            
            status = "added"
        
        # If examples provided, create phoneme features
        if examples and len(examples) > 0:
            # Simple feature creation using mean of example features
            example_features = []
            
            for example in examples[:5]:  # Limit to 5 examples
                # Convert to feature vector (simple hash-based features)
                feature_vec = np.zeros(128)
                
                # Set features based on example characters
                for i, c in enumerate(example[:10]):  # Limit to 10 chars
                    pos = (hash(c) + i) % 120
                    feature_vec[pos] = 1.0
                
                example_features.append(feature_vec)
            
            # Calculate mean features if we have examples
            if example_features:
                mean_features = np.mean(example_features, axis=0)
                
                # Store in phoneme features
                self.phoneme_model.phoneme_features[phoneme] = {
                    "vector": mean_features.tolist(),
                    "examples": examples[:5]
                }
        
        # Record activation in neural state
        self.neural_state.add_activation("phoneme_recognition", {
            'operation': 'learn_phoneme',
            'phoneme': phoneme,
            'status': status
        })
        
        # Return learning results
        return {
            "status": "success",
            "phoneme": phoneme,
            "recognition_confidence": self.phoneme_model.phoneme_inventory[phoneme],
            "learning_status": status,
            "process_id": process_id
        }
    
    def _query_phoneme_inventory(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Query the phoneme inventory
        
        Args:
            input_data: Input data dictionary including query parameters
            process_id: Process identifier
            
        Returns:
            Dict with query results
        """
        # Get query type
        query_type = input_data.get("query_type", "all")
        
        if query_type == "all":
            # Return all phonemes
            return {
                "status": "success",
                "phoneme_inventory": dict(self.phoneme_model.phoneme_inventory),
                "phoneme_categories": dict(self.phoneme_model.phoneme_categories),
                "phonotactic_rules": self.phoneme_model.phonotactic_rules,
                "process_id": process_id
            }
        
        elif query_type == "category":
            # Check for category
            if "category" not in input_data:
                return {
                    "status": "error",
                    "message": "Missing category for category query",
                    "process_id": process_id
                }
            
            category = input_data["category"]
            
            # Check if category exists
            if category not in self.phoneme_model.phoneme_categories:
                return {
                    "status": "error",
                    "message": f"Category not found: {category}",
                    "available_categories": list(self.phoneme_model.phoneme_categories.keys()),
                    "process_id": process_id
                }
            
            # Get phonemes in category with their recognition confidence
            category_phonemes = {}
            for phoneme in self.phoneme_model.phoneme_categories[category]:
                if phoneme in self.phoneme_model.phoneme_inventory:
                    category_phonemes[phoneme] = self.phoneme_model.phoneme_inventory[phoneme]
            
            return {
                "status": "success",
                "category": category,
                "phonemes": category_phonemes,
                "process_id": process_id
            }
        
        elif query_type == "rules":
            # Return phonotactic rules
            return {
                "status": "success",
                "phonotactic_rules": self.phoneme_model.phonotactic_rules,
                "rule_count": len(self.phoneme_model.phonotactic_rules),
                "process_id": process_id
            }
        
        else:
            return {
                "status": "error",
                "message": f"Unknown query_type: {query_type}",
                "process_id": process_id
        }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of the module
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New development level
        """
        old_level = self.development_level
        
        # Update development level
        self.development_level = max(0.0, min(1.0, self.development_level + amount))
        
        # Update neural network
        self.network.set_development_level(self.development_level)
        
        # Update neural state
        self.neural_state.phoneme_recognition_development = self.development_level
        self.neural_state.last_updated = datetime.now()
        
        # Check if crossed a milestone
        for level in sorted(self.development_milestones.keys()):
            if old_level < level <= self.development_level:
                milestone = self.development_milestones[level]
                
                # Publish milestone event if we have an event bus
                if self.event_bus:
                    self.event_bus.publish({
                        "sender": self.module_id,
                        "message_type": "development_milestone",
                        "content": {
                            "module": "phoneme_recognition",
                            "milestone": milestone,
                            "level": level
                        }
                    })
                
                # Update phoneme inventory for new development level
                self._initialize_phoneme_inventory()
        
        return self.development_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the phoneme recognition module
        
        Returns:
            Dict representing the current state
        """
        return {
            "module_id": self.module_id,
            "phoneme_model": self.phoneme_model.dict(),
            "developmental_level": self.development_level,
            "inventory_size": len(self.phoneme_model.phoneme_inventory),
            "category_count": len(self.phoneme_model.phoneme_categories),
            "rule_count": len(self.phoneme_model.phonotactic_rules)
        }
    
    def save_state(self) -> Dict[str, Any]:
        """
        Save the current state for persistence
        
        Returns:
            Dict with serializable state
        """
        return {
            "module_id": self.module_id,
            "phoneme_model": self.phoneme_model.dict(),
            "developmental_level": self.development_level,
            "neural_state": {
                "development": self.neural_state.phoneme_recognition_development,
                "accuracy": self.neural_state.phoneme_recognition_accuracy
            }
        }
    
    def load_state(self, state: Dict[str, Any]) -> None:
        """
        Load a previously saved state
        
        Args:
            state: The state to load
        """
        # Load module ID
        self.module_id = state["module_id"]
        
        # Load development level
        self.development_level = state["developmental_level"]
        self.network.set_development_level(self.development_level)
        
        # Load phoneme model
        if "phoneme_model" in state:
            try:
                # Create new model from dict
                from pydantic import parse_obj_as
                self.phoneme_model = parse_obj_as(PhonemeModel, state["phoneme_model"])
            except Exception as e:
                print(f"Error loading phoneme model: {e}")
        
        # Load neural state
        if "neural_state" in state:
            ns = state["neural_state"]
            self.neural_state.phoneme_recognition_development = ns.get("development", self.development_level)
            self.neural_state.phoneme_recognition_accuracy = ns.get("accuracy", 0.5) 


#######################

#language\semantic_processing.py#
#######################

# TODO: Implement the SemanticProcessing class to extract meaning from language
# This component should be able to:
# - Understand the meaning of words in context
# - Extract relationships between concepts in language
# - Interpret literal and non-literal language
# - Build semantic representations of sentences and discourse

# TODO: Implement developmental progression in semantic processing:
# - Simple direct meanings in early stages
# - Growing comprehension of relationships in childhood
# - Basic figurative language in later childhood
# - Complex abstractions and nuance in adolescence/adulthood

# TODO: Create mechanisms for:
# - Semantic composition: Combine word meanings into phrase meanings
# - Contextual interpretation: Adjust meanings based on context
# - Reference resolution: Determine what pronouns and references point to
# - Implication extraction: Infer unstated meanings and entailments

# TODO: Implement different semantic phenomena:
# - Polysemy: Multiple related meanings of words
# - Metaphor and simile: Figurative comparisons
# - Pragmatics: Social and contextual aspects of meaning
# - Entailment: Logical relationships between statements

# TODO: Connect to conceptual knowledge and memory
# Semantic processing should leverage conceptual knowledge
# and store extracted meanings in memory

from typing import Dict, List, Any, Optional, Set, Tuple
import torch
import uuid
import numpy as np
from datetime import datetime
from collections import deque

from lmm_project.base.module import BaseModule
from lmm_project.event_bus import EventBus
from lmm_project.modules.language.models import SemanticModel, LanguageNeuralState
from lmm_project.modules.language.neural_net import SemanticNetwork, get_device
from lmm_project.utils.llm_client import LLMClient

class SemanticProcessing(BaseModule):
    """
    Processes and extracts meaning from language
    
    This module is responsible for understanding the semantic content
    of language, including concepts, relationships, and contextual meaning.
    """
    
    # Development milestones
    development_milestones = {
        0.0: "Basic concept recognition",
        0.2: "Simple word meanings",
        0.4: "Semantic categorization",
        0.6: "Relational understanding",
        0.8: "Contextual meaning",
        1.0: "Abstract semantics"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the semantic processing module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level (0.0 to 1.0)
        """
        super().__init__(module_id, event_bus)
        
        # Initialize semantic model
        self.semantic_model = SemanticModel()
        
        # Set initial development level
        self.development_level = max(0.0, min(1.0, development_level))
        
        # Initialize neural network
        self.device = get_device()
        self.network = SemanticNetwork().to(self.device)
        self.network.set_development_level(self.development_level)
        
        # Initialize neural state
        self.neural_state = LanguageNeuralState()
        self.neural_state.semantic_processing_development = self.development_level
        
        # Initialize with basic semantics based on development level
        self._initialize_semantic_concepts()
        
        # Recent inputs queue (for tracking recent semantic processing)
        self.recent_inputs = deque(maxlen=100)
        
        # For embedding generation when needed
        self.llm_client = LLMClient()
    
    def _initialize_semantic_concepts(self):
        """Initialize basic semantic concepts based on development level"""
        # Basic concepts at earliest stages
        basic_concepts = {
            "mama": {
                "category": "people",
                "features": {"person": True, "female": True, "caregiver": True}
            },
            "dada": {
                "category": "people",
                "features": {"person": True, "male": True, "caregiver": True}
            },
            "milk": {
                "category": "food",
                "features": {"liquid": True, "white": True, "drink": True}
            },
            "dog": {
                "category": "animals",
                "features": {"animal": True, "furry": True, "bark": True}
            }
        }
        
        # Add concepts with confidence based on development level
        for concept, info in basic_concepts.items():
            # Only add if not already present
            if concept not in self.semantic_model.concept_network:
                self.semantic_model.concept_network[concept] = {
                    "concept_id": str(uuid.uuid4()),
                    "category": info["category"],
                    "features": info["features"],
                    "related_concepts": [],
                    "confidence": 0.7 * max(0.3, self.development_level)
                }
                
                # Add to semantic categories
                if info["category"] in self.semantic_model.semantic_categories:
                    if concept not in self.semantic_model.semantic_categories[info["category"]]:
                        self.semantic_model.semantic_categories[info["category"]].append(concept)
                else:
                    self.semantic_model.semantic_categories[info["category"]] = [concept]
                
                # Initialize concept embedding if development level is sufficient
                if self.development_level >= 0.3:
                    self._generate_concept_embedding(concept)
        
        # Add more complex concepts with increased development
        if self.development_level >= 0.4:
            # More advanced concepts
            advanced_concepts = {
                "happy": {
                    "category": "emotions",
                    "features": {"feeling": True, "positive": True}
                },
                "sad": {
                    "category": "emotions",
                    "features": {"feeling": True, "negative": True}
                },
                "big": {
                    "category": "properties",
                    "features": {"size": True, "large": True}
                },
                "small": {
                    "category": "properties",
                    "features": {"size": True, "tiny": True}
                }
            }
            
            # Add advanced concepts
            for concept, info in advanced_concepts.items():
                if concept not in self.semantic_model.concept_network:
                    self.semantic_model.concept_network[concept] = {
                        "concept_id": str(uuid.uuid4()),
                        "category": info["category"],
                        "features": info["features"],
                        "related_concepts": [],
                        "confidence": 0.6 * ((self.development_level - 0.4) / 0.6)
                    }
                    
                    # Add to semantic categories
                    if info["category"] in self.semantic_model.semantic_categories:
                        if concept not in self.semantic_model.semantic_categories[info["category"]]:
                            self.semantic_model.semantic_categories[info["category"]].append(concept)
                    else:
                        self.semantic_model.semantic_categories[info["category"]] = [concept]
                    
                    # Generate concept embedding
                    self._generate_concept_embedding(concept)
        
        # Add contextual meanings with higher development
        if self.development_level >= 0.6:
            # Simple contextual meanings
            contexts = {
                "hot": {
                    "food": {"meaning": "high temperature", "features": {"temperature": "high"}},
                    "weather": {"meaning": "high outdoor temperature", "features": {"outdoor": True, "temperature": "high"}}
                },
                "good": {
                    "food": {"meaning": "tasty", "features": {"taste": "pleasant"}},
                    "behavior": {"meaning": "well-behaved", "features": {"obedient": True, "pleasant": True}}
                }
            }
            
            # Add contextual meanings
            for word, context_info in contexts.items():
                if word not in self.semantic_model.contextual_meanings:
                    self.semantic_model.contextual_meanings[word] = {}
                    
                for context, meaning in context_info.items():
                    self.semantic_model.contextual_meanings[word][context] = {
                        "meaning": meaning["meaning"],
                        "features": meaning["features"],
                        "confidence": 0.6 * ((self.development_level - 0.6) / 0.4)
                    }
    
    def _generate_concept_embedding(self, concept: str):
        """
        Generate and store embedding for a concept
        
        Args:
            concept: The concept to generate embedding for
        """
        try:
            # Use LLM client to get embedding
            embedding = self.llm_client.get_embedding(concept)
            
            # Handle different return formats
            if isinstance(embedding, list):
                if isinstance(embedding[0], list):
                    # Handle nested list output
                    self.semantic_model.concept_embeddings[concept] = embedding[0]
                else:
                    # Handle flat list output
                    self.semantic_model.concept_embeddings[concept] = embedding
        except Exception as e:
            # Fall back to simplified embedding if LLM client fails
            print(f"Warning: Failed to get embedding for '{concept}': {e}")
            
            # Create a simple hash-based embedding
            self.semantic_model.concept_embeddings[concept] = [(hash(concept + str(i)) % 10000) / 10000 for i in range(64)]
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to the semantic processing module
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Dict with processing results
        """
        # Validate input
        if not isinstance(input_data, dict):
            return {
                "status": "error",
                "message": "Input must be a dictionary"
            }
        
        # Extract process ID if provided
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Extract operation
        operation = input_data.get("operation", "understand")
        
        # Dispatch to appropriate handler
        if operation == "understand":
            return self._understand_meaning(input_data, process_id)
        elif operation == "learn_concept":
            return self._learn_concept(input_data, process_id)
        elif operation == "relate_concepts":
            return self._relate_concepts(input_data, process_id)
        elif operation == "query_semantics":
            return self._query_semantics(input_data, process_id)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "process_id": process_id
            }
    
    def _understand_meaning(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Understand the meaning of a word or phrase
        
        Args:
            input_data: Input data dictionary including text to understand
            process_id: Process identifier
            
        Returns:
            Dict with understanding results
        """
        # Check for text to understand
        if "text" not in input_data:
            return {
                "status": "error",
                "message": "Missing text for semantic understanding",
                "process_id": process_id
            }
        
        text = input_data["text"]
        context = input_data.get("context")
        
        # Extract features from text
        text_features = self._extract_features(text)
        text_features = text_features.to(self.device)
        
        # Process through network
        with torch.no_grad():
            if context:
                # Process with context
                context_features = self._extract_features(context)
                context_features = context_features.to(self.device)
                
                output = self.network(
                    text_features, 
                    operation="contextualize",
                    context=context_features
                )
                
                understanding_depth = output["context_effect"].cpu().item()
                text_embedding = output["contextualized_embedding"].cpu().squeeze(0)
            else:
                # Process without context
                output = self.network(text_features, operation="understand")
                understanding_depth = output["understanding_depth"].cpu().item()
                text_embedding = output["semantic_embedding"].cpu().squeeze(0) if "semantic_embedding" in output else None
        
        # Identify relevant concepts
        relevant_concepts = []
        
        # Simple word matching for concepts
        words = text.lower().split()
        for word in words:
            if word in self.semantic_model.concept_network:
                relevant_concepts.append({
                    "concept": word,
                    "category": self.semantic_model.concept_network[word]["category"],
                    "confidence": self.semantic_model.concept_network[word]["confidence"]
                })
        
        # If we have embeddings, find similar concepts by embedding similarity
        if text_embedding is not None and self.development_level >= 0.5:
            similarities = {}
            
            for concept, embedding in self.semantic_model.concept_embeddings.items():
                if concept in [c["concept"] for c in relevant_concepts]:
                    continue  # Skip already matched concepts
                
                # Convert to tensor
                concept_tensor = torch.tensor(embedding, dtype=torch.float32)
                
                # Ensure tensors have same dimensions
                if text_embedding.shape != concept_tensor.shape:
                    continue
                
                # Calculate similarity
                similarity = torch.cosine_similarity(
                    text_embedding.unsqueeze(0),
                    concept_tensor.unsqueeze(0)
                ).item()
                
                if similarity > 0.6:  # Threshold for relevance
                    similarities[concept] = similarity
            
            # Get top similar concepts
            for concept, similarity in sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:3]:
                if concept in self.semantic_model.concept_network:
                    relevant_concepts.append({
                        "concept": concept,
                        "category": self.semantic_model.concept_network[concept]["category"],
                        "confidence": similarity * 0.8,  # Reduce confidence for similarity-based matches
                        "similarity_match": True
                    })
        
        # Check for contextual meanings
        contextual_meanings = []
        
        if context and self.development_level >= 0.6:
            for word in words:
                if word in self.semantic_model.contextual_meanings:
                    # Look for matching context
                    context_words = context.lower().split()
                    for context_type, meaning in self.semantic_model.contextual_meanings[word].items():
                        if context_type in context_words:
                            contextual_meanings.append({
                                "word": word,
                                "context": context_type,
                                "meaning": meaning["meaning"],
                                "confidence": meaning["confidence"]
                            })
        
        # Record activation in neural state
        self.neural_state.add_activation("semantic_processing", {
            'operation': 'understand',
            'text': text,
            'relevant_concepts_count': len(relevant_concepts),
            'understanding_depth': understanding_depth
        })
        
        # Record in recent inputs
        self.recent_inputs.append({
            "type": "semantic_understanding",
            "text": text,
            "context": context,
            "timestamp": datetime.now()
        })
        
        # Return understanding results
        return {
            "status": "success",
            "text": text,
            "understanding_depth": understanding_depth,
            "relevant_concepts": relevant_concepts,
            "contextual_meanings": contextual_meanings,
            "development_level": self.development_level,
            "process_id": process_id
        }
    
    def _learn_concept(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Learn a new concept or update an existing one
        
        Args:
            input_data: Input data dictionary including concept information
            process_id: Process identifier
            
        Returns:
            Dict with learning results
        """
        # Check for required fields
        if "concept" not in input_data:
            return {
                "status": "error",
                "message": "Missing concept for learning",
                "process_id": process_id
            }
        
        concept = input_data["concept"]
        category = input_data.get("category", "uncategorized")
        features = input_data.get("features", {})
        
        # Check if concept already exists
        if concept in self.semantic_model.concept_network:
            # Update existing concept
            existing = self.semantic_model.concept_network[concept]
            
            # Update category if specified
            if category != "uncategorized":
                old_category = existing["category"]
                existing["category"] = category
                
                # Update semantic categories
                if old_category in self.semantic_model.semantic_categories and concept in self.semantic_model.semantic_categories[old_category]:
                    self.semantic_model.semantic_categories[old_category].remove(concept)
                
                if category in self.semantic_model.semantic_categories:
                    if concept not in self.semantic_model.semantic_categories[category]:
                        self.semantic_model.semantic_categories[category].append(concept)
                else:
                    self.semantic_model.semantic_categories[category] = [concept]
            
            # Update features if provided
            if features:
                for feature, value in features.items():
                    existing["features"][feature] = value
            
            # Increase confidence
            existing["confidence"] = min(1.0, existing["confidence"] + 0.05)
            
            # Generate or update embedding if development level is sufficient
            if self.development_level >= 0.3 and concept not in self.semantic_model.concept_embeddings:
                self._generate_concept_embedding(concept)
            
            # Record activation in neural state
            self.neural_state.add_activation("semantic_processing", {
                'operation': 'update_concept',
                'concept': concept,
                'confidence': existing["confidence"]
            })
            
            return {
                "status": "success",
                "message": "Updated existing concept",
                "concept": concept,
                "confidence": existing["confidence"],
                "process_id": process_id
            }
        else:
            # Create new concept
            concept_obj = {
                "concept_id": str(uuid.uuid4()),
                "category": category,
                "features": features,
                "related_concepts": [],
                "confidence": 0.5  # Initial confidence
            }
            
            # Development level affects initial confidence
            concept_obj["confidence"] *= max(0.5, self.development_level)
            
            # Add to concept network
            self.semantic_model.concept_network[concept] = concept_obj
            
            # Add to semantic categories
            if category in self.semantic_model.semantic_categories:
                if concept not in self.semantic_model.semantic_categories[category]:
                    self.semantic_model.semantic_categories[category].append(concept)
            else:
                self.semantic_model.semantic_categories[category] = [concept]
            
            # Generate embedding if development level is sufficient
            if self.development_level >= 0.3:
                self._generate_concept_embedding(concept)
            
            # Record activation in neural state
            self.neural_state.add_activation("semantic_processing", {
                'operation': 'learn_concept',
                'concept': concept,
                'confidence': concept_obj["confidence"]
            })
            
            return {
                "status": "success",
                "message": "Learned new concept",
                "concept": concept,
                "confidence": concept_obj["confidence"],
                "process_id": process_id
            }
    
    def _relate_concepts(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Create or identify relationships between concepts
        
        Args:
            input_data: Input data dictionary including concepts to relate
            process_id: Process identifier
            
        Returns:
            Dict with relation results
        """
        # Check for required fields
        if "concept1" not in input_data or "concept2" not in input_data:
            return {
                "status": "error",
                "message": "Missing concept1 or concept2 for relation",
                "process_id": process_id
            }
        
        concept1 = input_data["concept1"]
        concept2 = input_data["concept2"]
        relation_type = input_data.get("relation_type")
        
        # Check if both concepts exist
        if concept1 not in self.semantic_model.concept_network or concept2 not in self.semantic_model.concept_network:
            missing = []
            if concept1 not in self.semantic_model.concept_network:
                missing.append(concept1)
            if concept2 not in self.semantic_model.concept_network:
                missing.append(concept2)
                
            return {
                "status": "error",
                "message": f"Concepts not found: {', '.join(missing)}",
                "process_id": process_id
            }
        
        # Check development level for relations
        if self.development_level < 0.4:
            return {
                "status": "undeveloped",
                "message": "Concept relations require higher development level (0.4+)",
                "current_level": self.development_level,
                "process_id": process_id
            }
        
        # Get concept embeddings if available
        if concept1 in self.semantic_model.concept_embeddings and concept2 in self.semantic_model.concept_embeddings:
            embedding1 = torch.tensor(self.semantic_model.concept_embeddings[concept1], dtype=torch.float32).unsqueeze(0)
            embedding2 = torch.tensor(self.semantic_model.concept_embeddings[concept2], dtype=torch.float32).unsqueeze(0)
            
            # Process through network to identify relationship
            with torch.no_grad():
                output = self.network(
                    embedding1.to(self.device),
                    operation="relate",
                    second_concept=embedding2.to(self.device)
                )
                
                relation_clarity = float(output["relation_clarity"].cpu().item())
                
                # If relation type not specified, try to determine from network
                if not relation_type and "relation_probs" in output:
                    relation_probs = output["relation_probs"].cpu().numpy()[0]
                    relation_types = ["is_a", "has_a", "part_of", "similar_to", "opposite_of", "used_for", "located_at", "made_of", "caused_by", "member_of"]
                    
                    # Get most likely relation type
                    max_idx = np.argmax(relation_probs)
                    if max_idx < len(relation_types):
                        relation_type = relation_types[max_idx]
        else:
            # No embeddings available, use simpler approach
            relation_clarity = 0.5
        
        # Add to related concepts
        if concept2 not in self.semantic_model.concept_network[concept1]["related_concepts"]:
            self.semantic_model.concept_network[concept1]["related_concepts"].append(concept2)
        
        if concept1 not in self.semantic_model.concept_network[concept2]["related_concepts"]:
            self.semantic_model.concept_network[concept2]["related_concepts"].append(concept1)
        
        # Record activation in neural state
        self.neural_state.add_activation("semantic_processing", {
            'operation': 'relate_concepts',
            'concept1': concept1,
            'concept2': concept2,
            'relation_type': relation_type,
            'relation_clarity': relation_clarity
        })
        
        # Return relation results
        result = {
            "status": "success",
            "concept1": concept1,
            "concept2": concept2,
            "relation_clarity": relation_clarity,
            "process_id": process_id
        }
        
        if relation_type:
            result["relation_type"] = relation_type
            
        return result
    
    def _query_semantics(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Query semantic information
        
        Args:
            input_data: Input data dictionary including query parameters
            process_id: Process identifier
            
        Returns:
            Dict with query results
        """
        # Get query type
        query_type = input_data.get("query_type", "all")
        
        if query_type == "all":
            # Return summary of semantic knowledge
            return {
                "status": "success",
                "concept_count": len(self.semantic_model.concept_network),
                "categories": list(self.semantic_model.semantic_categories.keys()),
                "contextual_meanings_count": len(self.semantic_model.contextual_meanings),
                "development_level": self.development_level,
                "process_id": process_id
            }
        
        elif query_type == "concept":
            # Check for concept
            if "concept" not in input_data:
                return {
                    "status": "error",
                    "message": "Missing concept for concept query",
                    "process_id": process_id
                }
                
            concept = input_data["concept"]
            
            # Check if concept exists
            if concept not in self.semantic_model.concept_network:
                return {
                    "status": "error",
                    "message": f"Concept not found: {concept}",
                    "process_id": process_id
                }
                
            # Get concept information
            concept_info = self.semantic_model.concept_network[concept]
            
            result = {
                "status": "success",
                "concept": concept,
                "category": concept_info["category"],
                "features": concept_info["features"],
                "related_concepts": concept_info["related_concepts"],
                "confidence": concept_info["confidence"],
                "process_id": process_id
            }
            
            # Add embedding info if available but don't return the full embedding
            if concept in self.semantic_model.concept_embeddings:
                result["has_embedding"] = True
                
            # Add contextual meanings if available
            if concept in self.semantic_model.contextual_meanings:
                result["contextual_meanings"] = {
                    context: meaning["meaning"] 
                    for context, meaning in self.semantic_model.contextual_meanings[concept].items()
                }
                
            return result
        
        elif query_type == "category":
            # Check for category
            if "category" not in input_data:
                return {
                    "status": "error",
                    "message": "Missing category for category query",
                    "process_id": process_id
                }
                
            category = input_data["category"]
            
            # Check if category exists
            if category not in self.semantic_model.semantic_categories:
                return {
                    "status": "error",
                    "message": f"Category not found: {category}",
                    "available_categories": list(self.semantic_model.semantic_categories.keys()),
                    "process_id": process_id
                }
                
            # Get concepts in category
            category_concepts = []
            for concept in self.semantic_model.semantic_categories[category]:
                if concept in self.semantic_model.concept_network:
                    category_concepts.append({
                        "concept": concept,
                        "confidence": self.semantic_model.concept_network[concept]["confidence"]
                    })
                    
            return {
                "status": "success",
                "category": category,
                "concepts": category_concepts,
                "concept_count": len(category_concepts),
                "process_id": process_id
            }
            
        elif query_type == "similar":
            # Check for query concept
            if "concept" not in input_data:
                return {
                    "status": "error",
                    "message": "Missing concept for similarity query",
                    "process_id": process_id
                }
                
            concept = input_data["concept"]
            
            # Check development level
            if self.development_level < 0.5:
                return {
                    "status": "undeveloped",
                    "message": "Concept similarity query requires higher development level (0.5+)",
                    "current_level": self.development_level,
                    "process_id": process_id
                }
                
            # Check if concept exists and has embedding
            if concept not in self.semantic_model.concept_network:
                return {
                    "status": "error",
                    "message": f"Concept not found: {concept}",
                    "process_id": process_id
                }
                
            if concept not in self.semantic_model.concept_embeddings:
                return {
                    "status": "error",
                    "message": f"No embedding available for concept: {concept}",
                    "process_id": process_id
                }
                
            # Find similar concepts by embedding similarity
            similarities = {}
            target_embedding = torch.tensor(self.semantic_model.concept_embeddings[concept], dtype=torch.float32)
            
            for other_concept, embedding in self.semantic_model.concept_embeddings.items():
                if other_concept != concept:
                    # Convert to tensor
                    other_tensor = torch.tensor(embedding, dtype=torch.float32)
                    
                    # Ensure tensors have same dimensions
                    if target_embedding.shape != other_tensor.shape:
                        continue
                    
                    # Calculate similarity
                    similarity = torch.cosine_similarity(
                        target_embedding.unsqueeze(0),
                        other_tensor.unsqueeze(0)
                    ).item()
                    
                    similarities[other_concept] = similarity
                    
            # Get top similar concepts
            top_similar = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:5]
            
            return {
                "status": "success",
                "concept": concept,
                "similar_concepts": top_similar,
                "process_id": process_id
            }
            
        else:
            return {
                "status": "error",
                "message": f"Unknown query_type: {query_type}",
                "process_id": process_id
            }
    
    def _extract_features(self, text: str) -> torch.Tensor:
        """
        Extract features from text
        
        Args:
            text: The text to extract features from
            
        Returns:
            Tensor of features
        """
        # Simple feature extraction
        words = text.lower().split()
        
        # Create a simple embedding for the text
        embedding = torch.zeros(128, dtype=torch.float32)
        
        for i, word in enumerate(words[:min(len(words), 20)]):  # Limit to 20 words
            # Get position in embedding
            pos = (hash(word) + i) % 120  # Keep a few positions for special features
            embedding[pos] = 1.0
            
            # Add emphasis on first and last words
            if i == 0:
                embedding[120] = 1.0  # First word marker
            if i == len(words) - 1:
                embedding[121] = 1.0  # Last word marker
                
            # Add known concept marker if word is a known concept
            if word in self.semantic_model.concept_network:
                embedding[122] = 1.0
                
                # Add category information
                category = self.semantic_model.concept_network[word]["category"]
                category_hash = hash(category) % 5
                embedding[123 + category_hash] = 1.0
        
        return embedding.unsqueeze(0)  # Add batch dimension
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of the module
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New development level
        """
        old_level = self.development_level
        
        # Update development level
        self.development_level = max(0.0, min(1.0, self.development_level + amount))
        
        # Update neural network
        self.network.set_development_level(self.development_level)
        
        # Update neural state
        self.neural_state.semantic_processing_development = self.development_level
        self.neural_state.last_updated = datetime.now()
        
        # Check if crossed a milestone
        for level in sorted(self.development_milestones.keys()):
            if old_level < level <= self.development_level:
                milestone = self.development_milestones[level]
                
                # Publish milestone event if we have an event bus
                if self.event_bus:
                    self.event_bus.publish({
                        "sender": self.module_id,
                        "message_type": "development_milestone",
                        "content": {
                            "module": "semantic_processing",
                            "milestone": milestone,
                            "level": level
                        }
                    })
                
                # Update semantic concepts for new development level
                self._initialize_semantic_concepts()
        
        return self.development_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the semantic processing module
        
        Returns:
            Dict representing the current state
        """
        return {
            "module_id": self.module_id,
            "developmental_level": self.development_level,
            "concept_count": len(self.semantic_model.concept_network),
            "category_count": len(self.semantic_model.semantic_categories),
            "embedding_count": len(self.semantic_model.concept_embeddings),
            "contextual_meanings_count": len(self.semantic_model.contextual_meanings)
        }
    
    def save_state(self) -> Dict[str, Any]:
        """
        Save the current state for persistence
        
        Returns:
            Dict with serializable state
        """
        # Convert embeddings to lists for serialization (they should already be lists,
        # but ensure they're in a serializable format)
        concept_embeddings_serialized = {}
        for concept, embedding in self.semantic_model.concept_embeddings.items():
            if isinstance(embedding, torch.Tensor):
                concept_embeddings_serialized[concept] = embedding.numpy().tolist()
            else:
                concept_embeddings_serialized[concept] = embedding
                
        # Create a copy of the semantic model to modify
        semantic_model_dict = self.semantic_model.dict()
        semantic_model_dict["concept_embeddings"] = concept_embeddings_serialized
        
        return {
            "module_id": self.module_id,
            "semantic_model": semantic_model_dict,
            "developmental_level": self.development_level,
            "neural_state": {
                "development": self.neural_state.semantic_processing_development,
                "accuracy": self.neural_state.semantic_processing_accuracy
            }
        }
    
    def load_state(self, state: Dict[str, Any]) -> None:
        """
        Load a previously saved state
        
        Args:
            state: The state to load
        """
        # Load module ID
        self.module_id = state["module_id"]
        
        # Load development level
        self.development_level = state["developmental_level"]
        self.network.set_development_level(self.development_level)
        
        # Load semantic model
        if "semantic_model" in state:
            try:
                # Create new model from dict
                from pydantic import parse_obj_as
                self.semantic_model = parse_obj_as(SemanticModel, state["semantic_model"])
            except Exception as e:
                print(f"Error loading semantic model: {e}")
        
        # Load neural state
        if "neural_state" in state:
            ns = state["neural_state"]
            self.neural_state.semantic_processing_development = ns.get("development", self.development_level)
            self.neural_state.semantic_processing_accuracy = ns.get("accuracy", 0.5)


#######################

#language\word_learning.py#
#######################

# TODO: Implement the WordLearning class to acquire and manage vocabulary
# This component should be able to:
# - Learn new words from context and direct instruction
# - Connect words to meanings and concepts
# - Build and maintain a lexicon of known words
# - Track word frequency and familiarity

# TODO: Implement developmental progression in word learning:
# - Simple sound-object associations in early stages
# - Vocabulary explosion in early childhood
# - Growing semantic networks in later childhood
# - Abstract and specialized vocabulary in adolescence/adulthood

# TODO: Create mechanisms for:
# - Fast mapping: Form initial word-concept connections
# - Semantic enrichment: Develop deeper word meanings over time
# - Word retrieval: Access words efficiently from memory
# - Lexical organization: Structure vocabulary by semantic relationships

# TODO: Implement different word types and learning patterns:
# - Concrete nouns: Objects, people, places
# - Action verbs: Physical and mental actions
# - Descriptive words: Adjectives and adverbs
# - Relational words: Prepositions, conjunctions, etc.

# TODO: Connect to memory and perception systems
# Word learning should be tied to perceptual experiences
# and should store word knowledge in semantic memory

from typing import Dict, List, Any, Optional, Set, Tuple
import torch
import uuid
import numpy as np
from datetime import datetime
from collections import deque

from lmm_project.base.module import BaseModule
from lmm_project.event_bus import EventBus
from lmm_project.modules.language.models import WordModel, LanguageNeuralState
from lmm_project.modules.language.neural_net import WordNetwork, get_device
from lmm_project.utils.llm_client import LLMClient

class WordLearning(BaseModule):
    """
    Learns and processes words and their meanings
    
    This module is responsible for vocabulary acquisition,
    word-meaning associations, and lexical development.
    """
    
    # Development milestones
    development_milestones = {
        0.0: "Basic word recognition",
        0.2: "First words acquisition",
        0.4: "Vocabulary spurt",
        0.6: "Semantic network formation",
        0.8: "Word-definition comprehension",
        1.0: "Complete lexical system"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the word learning module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level (0.0 to 1.0)
        """
        super().__init__(module_id, event_bus)
        
        # Initialize word model
        self.word_model = WordModel()
        
        # Set initial development level
        self.development_level = max(0.0, min(1.0, development_level))
        
        # Initialize neural network
        self.device = get_device()
        self.network = WordNetwork().to(self.device)
        self.network.set_development_level(self.development_level)
        
        # Initialize neural state
        self.neural_state = LanguageNeuralState()
        self.neural_state.word_learning_development = self.development_level
        
        # Initialize with basic vocabulary based on development level
        self._initialize_vocabulary()
        
        # Recent inputs queue (for tracking word exposure)
        self.recent_inputs = deque(maxlen=100)
        
        # For embedding generation
        self.llm_client = LLMClient()
    
    def _initialize_vocabulary(self):
        """Initialize basic vocabulary based on development level"""
        # Basic vocabulary at earliest stages (first words)
        basic_words = {
            "mama": 0.8, "dada": 0.8, "baby": 0.7, "milk": 0.7, "hi": 0.7, "bye": 0.7
        }
        
        # Add basic words to vocabulary
        for word, familiarity in basic_words.items():
            # Scale familiarity by development level
            scaled_familiarity = familiarity * max(0.3, self.development_level)
            self.word_model.vocabulary[word] = scaled_familiarity
        
        # Set basic categories
        self.word_model.word_categories["people"] = ["mama", "dada", "baby"]
        self.word_model.word_categories["food"] = ["milk"]
        self.word_model.word_categories["greetings"] = ["hi", "bye"]
        
        # Initialize word frequencies
        for word in basic_words:
            self.word_model.word_frequencies[word] = max(1, int(10 * self.development_level))
        
        # Add more vocabulary with increased development
        if self.development_level >= 0.3:
            # Early childhood vocabulary expansion
            early_words = {
                "water": 0.7, "dog": 0.7, "cat": 0.7, "ball": 0.7, 
                "yes": 0.7, "no": 0.7, "more": 0.7, "want": 0.7
            }
            
            # Add early words
            for word, familiarity in early_words.items():
                # Scale familiarity by development beyond threshold
                scaled_familiarity = familiarity * ((self.development_level - 0.3) / 0.7)
                self.word_model.vocabulary[word] = scaled_familiarity
                self.word_model.word_frequencies[word] = max(1, int(8 * (self.development_level - 0.3) / 0.7))
            
            # Update categories
            self.word_model.word_categories["animals"] = ["dog", "cat"]
            self.word_model.word_categories["objects"] = ["ball"]
            self.word_model.word_categories["food"].append("water")
            self.word_model.word_categories["communication"] = ["yes", "no", "more", "want"]
        
        if self.development_level >= 0.5:
            # Generate word embeddings for known words
            self._initialize_word_embeddings()
    
    def _initialize_word_embeddings(self):
        """Initialize embeddings for words in vocabulary"""
        # Only get embeddings for words we don't already have
        words_to_embed = [w for w in self.word_model.vocabulary 
                          if w not in self.word_model.word_embeddings]
        
        if not words_to_embed:
            return
        
        try:
            # Get embeddings from LLM client
            embeddings = self.llm_client.get_embedding(words_to_embed)
            
            # Add embeddings to model
            for i, word in enumerate(words_to_embed):
                if isinstance(embeddings, list) and i < len(embeddings):
                    # Handle nested list structure or flat list
                    embedding = embeddings[i] if isinstance(embeddings[0], list) else embeddings
                    self.word_model.word_embeddings[word] = embedding
                    
                    # Also initialize word associations for more developed vocabulary
                    if self.development_level >= 0.6:
                        self.word_model.word_associations[word] = []
        except Exception as e:
            # Fall back to simplified embeddings if LLM client fails
            print(f"Warning: Failed to get embeddings from LLM client: {e}")
            
            # Create simple hash-based embeddings
            for word in words_to_embed:
                # Create a simple embedding based on hash
                simple_embedding = [(hash(word + str(i)) % 10000) / 10000 for i in range(64)]
                self.word_model.word_embeddings[word] = simple_embedding
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to the word learning module
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Dict with processing results
        """
        # Validate input
        if not isinstance(input_data, dict):
            return {
                "status": "error",
                "message": "Input must be a dictionary"
            }
        
        # Extract process ID if provided
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Extract operation
        operation = input_data.get("operation", "recognize")
        
        # Dispatch to appropriate handler
        if operation == "recognize":
            return self._recognize_word(input_data, process_id)
        elif operation == "learn_word":
            return self._learn_word(input_data, process_id)
        elif operation == "associate_words":
            return self._associate_words(input_data, process_id)
        elif operation == "query_vocabulary":
            return self._query_vocabulary(input_data, process_id)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "process_id": process_id
            }
    
    def _recognize_word(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Recognize a word from input features
        
        Args:
            input_data: Input data dictionary including word features
            process_id: Process identifier
            
        Returns:
            Dict with recognition results
        """
        # Check for word features or phoneme sequence
        if "word_features" not in input_data and "phoneme_sequence" not in input_data and "word" not in input_data:
            return {
                "status": "error",
                "message": "Missing word_features, phoneme_sequence, or word for recognition",
                "process_id": process_id
            }
        
        # Process based on input type
        if "word" in input_data:
            # Direct word recognition (already segmented)
            word = input_data["word"]
            
            # Check if word is in vocabulary
            if word in self.word_model.vocabulary:
                familiarity = self.word_model.vocabulary[word]
                
                # Increase familiarity and frequency with exposure
                new_familiarity = min(1.0, familiarity + 0.01)
                self.word_model.vocabulary[word] = new_familiarity
                self.word_model.word_frequencies[word] = self.word_model.word_frequencies.get(word, 0) + 1
                
                # Record in recent inputs
                self.recent_inputs.append({
                    "type": "word_recognition",
                    "word": word,
                    "familiarity": new_familiarity,
                    "timestamp": datetime.now()
                })
                
                # Record activation in neural state
                self.neural_state.add_activation("word_learning", {
                    'operation': 'recognize',
                    'word': word,
                    'familiarity': new_familiarity
                })
                
                # Return recognition results
                return {
                    "status": "success",
                    "word": word,
                    "recognized": True,
                    "familiarity": new_familiarity,
                    "frequency": self.word_model.word_frequencies[word],
                    "categories": [cat for cat, words in self.word_model.word_categories.items() if word in words],
                    "process_id": process_id
                }
            else:
                # Word not in vocabulary
                return {
                    "status": "success",
                    "word": word,
                    "recognized": False,
                    "message": "Word not in vocabulary",
                    "process_id": process_id
                }
        
        elif "word_features" in input_data:
            # Features-based recognition
            word_features = input_data["word_features"]
            
            # Convert to tensor if needed
            if not isinstance(word_features, torch.Tensor):
                word_features = torch.tensor(word_features, dtype=torch.float32)
            
            # Ensure batch dimension
            if len(word_features.shape) == 1:
                word_features = word_features.unsqueeze(0)
            
            # Process through network
            word_features = word_features.to(self.device)
            with torch.no_grad():
                output = self.network(word_features, operation="recognize")
            
            # Get word probabilities and confidence
            word_probs = output["word_probs"].cpu().numpy()[0]
            confidence = output["confidence"].cpu().item()
            
            # Get top words based on vocabulary we know
            vocab_words = list(self.word_model.vocabulary.keys())
            word_probs_dict = {}
            
            # Map probabilities to words (up to vocabulary size)
            num_words = min(len(vocab_words), len(word_probs))
            for i in range(num_words):
                word_probs_dict[vocab_words[i]] = float(word_probs[i])
            
            # Get top words
            top_words = sorted(word_probs_dict.items(), key=lambda x: x[1], reverse=True)[:5]
            
            # Record activation in neural state
            self.neural_state.add_activation("word_learning", {
                'operation': 'recognize_features',
                'confidence': confidence,
                'top_word': top_words[0][0] if top_words else None
            })
            
            # Return recognition results
            return {
                "status": "success",
                "recognized_words": top_words,
                "confidence": confidence,
                "developmental_level": self.development_level,
                "process_id": process_id
            }
            
        else:
            # Phoneme sequence recognition
            phoneme_sequence = input_data["phoneme_sequence"]
            phoneme_str = "".join(phoneme_sequence)
            
            # Very simple word recognition from phoneme sequence
            # In a real implementation, would use more sophisticated matching
            recognized_word = None
            max_similarity = 0.0
            
            for word in self.word_model.vocabulary:
                # Calculate simple string similarity
                similarity = self._string_similarity(phoneme_str, word)
                if similarity > 0.7 and similarity > max_similarity:
                    max_similarity = similarity
                    recognized_word = word
            
            if recognized_word:
                # Update familiarity and frequency
                familiarity = self.word_model.vocabulary[recognized_word]
                new_familiarity = min(1.0, familiarity + 0.01)
                self.word_model.vocabulary[recognized_word] = new_familiarity
                self.word_model.word_frequencies[recognized_word] = self.word_model.word_frequencies.get(recognized_word, 0) + 1
                
                # Record activation in neural state
                self.neural_state.add_activation("word_learning", {
                    'operation': 'recognize_phonemes',
                    'word': recognized_word,
                    'similarity': max_similarity
                })
                
                # Return recognition results
                return {
                    "status": "success",
                    "phoneme_sequence": phoneme_sequence,
                    "recognized_word": recognized_word,
                    "similarity": max_similarity,
                    "familiarity": new_familiarity,
                    "process_id": process_id
                }
            else:
                # No matching word found
                return {
                    "status": "success",
                    "phoneme_sequence": phoneme_sequence,
                    "recognized": False,
                    "message": "No matching word found",
                    "process_id": process_id
                }
    
    def _learn_word(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Learn a new word or update an existing word
        
        Args:
            input_data: Input data dictionary including word and optional meaning
            process_id: Process identifier
            
        Returns:
            Dict with learning results
        """
        # Check for word
        if "word" not in input_data:
            return {
                "status": "error",
                "message": "Missing word for learning",
                "process_id": process_id
            }
        
        # Get word and category
        word = input_data["word"]
        category = input_data.get("category")
        meaning = input_data.get("meaning")
        
        # Check if word already in vocabulary
        if word in self.word_model.vocabulary:
            # Update existing word
            current_familiarity = self.word_model.vocabulary[word]
            
            # Increase familiarity with reinforcement, limited by development level
            max_familiarity = min(0.95, 0.4 + (self.development_level * 0.6))
            new_familiarity = min(max_familiarity, current_familiarity + 0.05)
            self.word_model.vocabulary[word] = new_familiarity
            
            # Update frequency
            self.word_model.word_frequencies[word] = self.word_model.word_frequencies.get(word, 0) + 1
            
            # Update category if provided and different
            if category and category in self.word_model.word_categories:
                if word not in self.word_model.word_categories[category]:
                    self.word_model.word_categories[category].append(word)
                    
            status = "updated"
        else:
            # Add new word with initial familiarity based on development
            initial_familiarity = min(0.6, 0.2 + (self.development_level * 0.4))
            self.word_model.vocabulary[word] = initial_familiarity
            self.word_model.word_frequencies[word] = 1
            
            # Add to category if provided
            if category:
                if category in self.word_model.word_categories:
                    self.word_model.word_categories[category].append(word)
                else:
                    self.word_model.word_categories[category] = [word]
                    
            status = "added"
        
        # Get word embedding if development level is sufficient and meaning is provided
        if self.development_level >= 0.5 and (meaning or word not in self.word_model.word_embeddings):
            # Try to get embedding from LLM client
            try:
                embedding = self.llm_client.get_embedding(word)
                if isinstance(embedding, list):
                    if isinstance(embedding[0], list):
                        # Handle nested list output
                        self.word_model.word_embeddings[word] = embedding[0]
                    else:
                        # Handle flat list output
                        self.word_model.word_embeddings[word] = embedding
            except Exception as e:
                # Fall back to simplified embedding
                print(f"Warning: Failed to get embedding for '{word}': {e}")
                # Create a simple hash-based embedding
                self.word_model.word_embeddings[word] = [(hash(word + str(i)) % 10000) / 10000 for i in range(64)]
        
        # Record activation in neural state
        self.neural_state.add_activation("word_learning", {
            'operation': 'learn_word',
            'word': word,
            'status': status
        })
        
        # Return learning results
        return {
            "status": "success",
            "word": word,
            "familiarity": self.word_model.vocabulary[word],
            "learning_status": status,
            "has_embedding": word in self.word_model.word_embeddings,
            "process_id": process_id
        }
    
    def _associate_words(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Create associations between words
        
        Args:
            input_data: Input data dictionary including words to associate
            process_id: Process identifier
            
        Returns:
            Dict with association results
        """
        # Check for words to associate
        if "word1" not in input_data or "word2" not in input_data:
            return {
                "status": "error",
                "message": "Missing word1 or word2 for association",
                "process_id": process_id
            }
        
        word1 = input_data["word1"]
        word2 = input_data["word2"]
        
        # Check if both words are in vocabulary
        if word1 not in self.word_model.vocabulary or word2 not in self.word_model.vocabulary:
            return {
                "status": "error",
                "message": "One or both words not in vocabulary",
                "process_id": process_id
            }
        
        # Check development level for associations
        if self.development_level < 0.4:
            return {
                "status": "undeveloped",
                "message": "Word association requires higher development level (0.4+)",
                "current_level": self.development_level,
                "process_id": process_id
            }
        
        # Create embedding tensors
        if word1 in self.word_model.word_embeddings and word2 in self.word_model.word_embeddings:
            # Use existing embeddings
            word1_embedding = self.word_model.word_embeddings[word1]
            word2_embedding = self.word_model.word_embeddings[word2]
            
            # Convert to tensors
            tensor1 = torch.tensor(word1_embedding, dtype=torch.float32).unsqueeze(0)
            tensor2 = torch.tensor(word2_embedding, dtype=torch.float32).unsqueeze(0)
            
            # Simple cosine similarity
            if tensor1.shape == tensor2.shape:
                similarity = torch.cosine_similarity(tensor1, tensor2).item()
            else:
                # Adjust dimensions if necessary
                max_dim = max(tensor1.size(1), tensor2.size(1))
                if tensor1.size(1) < max_dim:
                    tensor1 = torch.cat([tensor1, torch.zeros(1, max_dim - tensor1.size(1))], dim=1)
                if tensor2.size(1) < max_dim:
                    tensor2 = torch.cat([tensor2, torch.zeros(1, max_dim - tensor2.size(1))], dim=1)
                
                similarity = torch.cosine_similarity(tensor1, tensor2).item()
        else:
            # Fall back to simple string similarity if embeddings unavailable
            similarity = self._string_similarity(word1, word2)
        
        # Update word associations in model
        if word1 not in self.word_model.word_associations:
            self.word_model.word_associations[word1] = []
        if word2 not in self.word_model.word_associations:
            self.word_model.word_associations[word2] = []
        
        # Add bidirectional associations if they don't exist
        if word2 not in self.word_model.word_associations[word1]:
            self.word_model.word_associations[word1].append(word2)
        if word1 not in self.word_model.word_associations[word2]:
            self.word_model.word_associations[word2].append(word1)
        
        # Record activation in neural state
        self.neural_state.add_activation("word_learning", {
            'operation': 'associate_words',
            'word1': word1,
            'word2': word2,
            'similarity': similarity
        })
        
        # Return association results
        return {
            "status": "success",
            "word1": word1,
            "word2": word2,
            "similarity": similarity,
            "word1_associations": self.word_model.word_associations.get(word1, []),
            "word2_associations": self.word_model.word_associations.get(word2, []),
            "process_id": process_id
        }
    
    def _query_vocabulary(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Query the vocabulary information
        
        Args:
            input_data: Input data dictionary including query parameters
            process_id: Process identifier
            
        Returns:
            Dict with query results
        """
        # Get query type
        query_type = input_data.get("query_type", "all")
        
        if query_type == "all":
            # Return summary of vocabulary
            return {
                "status": "success",
                "vocabulary_size": len(self.word_model.vocabulary),
                "categories": list(self.word_model.word_categories.keys()),
                "most_frequent": sorted(self.word_model.word_frequencies.items(), 
                                      key=lambda x: x[1], reverse=True)[:10],
                "developmental_level": self.development_level,
                "process_id": process_id
            }
        
        elif query_type == "word":
            # Check for word
            if "word" not in input_data:
                return {
                    "status": "error",
                    "message": "Missing word for word query",
                    "process_id": process_id
                }
                
            word = input_data["word"]
            
            # Check if word exists in vocabulary
            if word not in self.word_model.vocabulary:
                return {
                    "status": "error",
                    "message": f"Word not found: {word}",
                    "process_id": process_id
                }
                
            # Get word information
            response = {
                "status": "success",
                "word": word,
                "familiarity": self.word_model.vocabulary[word],
                "frequency": self.word_model.word_frequencies.get(word, 0),
                "categories": [cat for cat, words in self.word_model.word_categories.items() if word in words],
                "process_id": process_id
            }
            
            # Add associations if available
            if word in self.word_model.word_associations:
                response["associations"] = self.word_model.word_associations[word]
                
            # Add embedding information if available
            if word in self.word_model.word_embeddings:
                # Don't include full embedding as it can be large
                response["has_embedding"] = True
                
            return response
        
        elif query_type == "category":
            # Check for category
            if "category" not in input_data:
                return {
                    "status": "error",
                    "message": "Missing category for category query",
                    "process_id": process_id
                }
                
            category = input_data["category"]
            
            # Check if category exists
            if category not in self.word_model.word_categories:
                return {
                    "status": "error",
                    "message": f"Category not found: {category}",
                    "available_categories": list(self.word_model.word_categories.keys()),
                    "process_id": process_id
                }
                
            # Get words in category with their familiarity
            category_words = {}
            for word in self.word_model.word_categories[category]:
                if word in self.word_model.vocabulary:
                    category_words[word] = self.word_model.vocabulary[word]
                    
            return {
                "status": "success",
                "category": category,
                "words": category_words,
                "word_count": len(category_words),
                "process_id": process_id
            }
            
        elif query_type == "similar":
            # Check for query word
            if "word" not in input_data:
                return {
                    "status": "error",
                    "message": "Missing word for similarity query",
                    "process_id": process_id
                }
                
            word = input_data["word"]
            
            # Check development level
            if self.development_level < 0.5:
                return {
                    "status": "undeveloped",
                    "message": "Similarity search requires higher development level (0.5+)",
                    "current_level": self.development_level,
                    "process_id": process_id
                }
                
            # Check if word exists in vocabulary and has embedding
            if word not in self.word_model.vocabulary:
                return {
                    "status": "error",
                    "message": f"Word not found in vocabulary: {word}",
                    "process_id": process_id
                }
                
            if word not in self.word_model.word_embeddings:
                return {
                    "status": "error",
                    "message": f"No embedding available for word: {word}",
                    "process_id": process_id
                }
                
            # Find similar words by embedding similarity
            similarities = {}
            target_embedding = torch.tensor(self.word_model.word_embeddings[word], dtype=torch.float32)
            
            for other_word, other_embedding in self.word_model.word_embeddings.items():
                if other_word != word:
                    other_tensor = torch.tensor(other_embedding, dtype=torch.float32)
                    
                    # Ensure tensors have same dimensions
                    if target_embedding.shape != other_tensor.shape:
                        continue
                        
                    # Calculate similarity
                    similarity = torch.cosine_similarity(
                        target_embedding.unsqueeze(0), 
                        other_tensor.unsqueeze(0)
                    ).item()
                    
                    similarities[other_word] = similarity
                    
            # Get top similar words
            top_similar = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:5]
            
            return {
                "status": "success",
                "word": word,
                "similar_words": top_similar,
                "process_id": process_id
            }
            
        else:
            return {
                "status": "error",
                "message": f"Unknown query_type: {query_type}",
                "process_id": process_id
            }
    
    def _string_similarity(self, str1: str, str2: str) -> float:
        """
        Calculate simple string similarity
        
        Args:
            str1: First string
            str2: Second string
            
        Returns:
            Similarity score between 0 and 1
        """
        # Simplified Levenshtein distance ratio
        if not str1 or not str2:
            return 0.0
            
        len1, len2 = len(str1), len(str2)
        if len1 == 0 or len2 == 0:
            return 0.0
            
        # Simple length ratio as a baseline
        length_ratio = min(len1, len2) / max(len1, len2)
        
        # Count matching characters
        matches = sum(c1 == c2 for c1, c2 in zip(str1, str2))
        match_ratio = matches / max(len1, len2)
        
        # Combine metrics
        return (length_ratio * 0.4) + (match_ratio * 0.6)
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of the module
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New development level
        """
        old_level = self.development_level
        
        # Update development level
        self.development_level = max(0.0, min(1.0, self.development_level + amount))
        
        # Update neural network
        self.network.set_development_level(self.development_level)
        
        # Update neural state
        self.neural_state.word_learning_development = self.development_level
        self.neural_state.last_updated = datetime.now()
        
        # Check if crossed a milestone
        for level in sorted(self.development_milestones.keys()):
            if old_level < level <= self.development_level:
                milestone = self.development_milestones[level]
                
                # Publish milestone event if we have an event bus
                if self.event_bus:
                    self.event_bus.publish({
                        "sender": self.module_id,
                        "message_type": "development_milestone",
                        "content": {
                            "module": "word_learning",
                            "milestone": milestone,
                            "level": level
                        }
                    })
                
                # Update vocabulary for new development level
                self._initialize_vocabulary()
        
        return self.development_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the word learning module
        
        Returns:
            Dict representing the current state
        """
        return {
            "module_id": self.module_id,
            "developmental_level": self.development_level,
            "vocabulary_size": len(self.word_model.vocabulary),
            "category_count": len(self.word_model.word_categories),
            "embedding_count": len(self.word_model.word_embeddings),
            "association_count": sum(len(assocs) for assocs in self.word_model.word_associations.values())
        }
    
    def save_state(self) -> Dict[str, Any]:
        """
        Save the current state for persistence
        
        Returns:
            Dict with serializable state
        """
        return {
            "module_id": self.module_id,
            "word_model": self.word_model.dict(),
            "developmental_level": self.development_level,
            "neural_state": {
                "development": self.neural_state.word_learning_development,
                "accuracy": self.neural_state.word_learning_accuracy
            }
        }
    
    def load_state(self, state: Dict[str, Any]) -> None:
        """
        Load a previously saved state
        
        Args:
            state: The state to load
        """
        # Load module ID
        self.module_id = state["module_id"]
        
        # Load development level
        self.development_level = state["developmental_level"]
        self.network.set_development_level(self.development_level)
        
        # Load word model
        if "word_model" in state:
            try:
                # Create new model from dict
                from pydantic import parse_obj_as
                self.word_model = parse_obj_as(WordModel, state["word_model"])
            except Exception as e:
                print(f"Error loading word model: {e}")
        
        # Load neural state
        if "neural_state" in state:
            ns = state["neural_state"]
            self.neural_state.word_learning_development = ns.get("development", self.development_level)
            self.neural_state.word_learning_accuracy = ns.get("accuracy", 0.5)


#######################

#language\__init__.py#
#######################

"""
Language module

This module is responsible for all aspects of language acquisition,
comprehension, and production within the LMM system.
"""

from typing import Dict, List, Any, Optional, Union
import uuid
from datetime import datetime

from lmm_project.base.module import BaseModule
from lmm_project.event_bus import EventBus
from lmm_project.modules.language.models import LanguageModel, LanguageNeuralState

# Import all language submodules
from lmm_project.modules.language.phoneme_recognition import PhonemeRecognition
from lmm_project.modules.language.word_learning import WordLearning
from lmm_project.modules.language.grammar_acquisition import GrammarAcquisition
from lmm_project.modules.language.semantic_processing import SemanticProcessing
from lmm_project.modules.language.expression_generator import ExpressionGenerator

class LanguageSystem(BaseModule):
    """
    Integrated language system that brings together all language components
    
    The LanguageSystem coordinates phoneme recognition, word learning, 
    grammar acquisition, semantic processing, and expression generation 
    to create a complete language capability.
    """
    
    # Development milestones
    development_milestones = {
        0.0: "Basic sound recognition",
        0.2: "First words and simple understanding",
        0.4: "Growing vocabulary and basic grammar",
        0.6: "Sentence comprehension and production",
        0.8: "Complex grammar and contextual understanding",
        1.0: "Full language mastery"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the language system
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level (0.0 to 1.0)
        """
        super().__init__(module_id, event_bus)
        
        # Set initial development level
        self.development_level = max(0.0, min(1.0, development_level))
        
        # Initialize component modules
        self.phoneme_recognition = PhonemeRecognition(
            module_id=f"{module_id}.phoneme_recognition",
            event_bus=event_bus,
            development_level=development_level
        )
        
        self.word_learning = WordLearning(
            module_id=f"{module_id}.word_learning",
            event_bus=event_bus,
            development_level=development_level
        )
        
        self.grammar_acquisition = GrammarAcquisition(
            module_id=f"{module_id}.grammar_acquisition",
            event_bus=event_bus,
            development_level=development_level
        )
        
        self.semantic_processing = SemanticProcessing(
            module_id=f"{module_id}.semantic_processing",
            event_bus=event_bus,
            development_level=development_level
        )
        
        self.expression_generator = ExpressionGenerator(
            module_id=f"{module_id}.expression_generator",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Initialize language model
        self.language_model = LanguageModel(
            phonemes=self.phoneme_recognition.phoneme_model,
            vocabulary=self.word_learning.word_model,
            grammar=self.grammar_acquisition.grammar_model,
            semantics=self.semantic_processing.semantic_model,
            expression=self.expression_generator.expression_model,
            developmental_level=development_level,
            module_id=module_id
        )
        
        # Initialize neural state
        self.neural_state = LanguageNeuralState()
        self.neural_state.phoneme_recognition_development = development_level
        self.neural_state.word_learning_development = development_level
        self.neural_state.grammar_acquisition_development = development_level
        self.neural_state.semantic_processing_development = development_level
        self.neural_state.expression_generation_development = development_level
        
        # Register for event subscriptions
        if event_bus:
            event_bus.subscribe(
                sender=f"{module_id}.phoneme_recognition", 
                callback=self._handle_phoneme_event
            )
            event_bus.subscribe(
                sender=f"{module_id}.word_learning", 
                callback=self._handle_word_event
            )
            event_bus.subscribe(
                sender=f"{module_id}.grammar_acquisition", 
                callback=self._handle_grammar_event
            )
            event_bus.subscribe(
                sender=f"{module_id}.semantic_processing", 
                callback=self._handle_semantic_event
            )
            event_bus.subscribe(
                sender=f"{module_id}.expression_generator", 
                callback=self._handle_expression_event
            )
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to the language system
        
        Args:
            input_data: Input data dictionary
            
        Returns:
            Dict with processing results
        """
        # Validate input
        if not isinstance(input_data, dict):
            return {
                "status": "error",
                "message": "Input must be a dictionary"
            }
        
        # Extract process ID if provided
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Extract target component and operation
        component = input_data.get("component", "language")
        operation = input_data.get("operation", "")
        
        # Route to appropriate component or handle at system level
        if component == "phoneme_recognition":
            return self.phoneme_recognition.process_input(input_data)
            
        elif component == "word_learning":
            return self.word_learning.process_input(input_data)
            
        elif component == "grammar_acquisition":
            return self.grammar_acquisition.process_input(input_data)
            
        elif component == "semantic_processing":
            return self.semantic_processing.process_input(input_data)
            
        elif component == "expression_generator":
            return self.expression_generator.process_input(input_data)
            
        elif component == "language":
            # Handle language-level operations
            if operation == "comprehend":
                return self._comprehend_input(input_data, process_id)
                
            elif operation == "produce":
                return self._produce_output(input_data, process_id)
                
            elif operation == "get_state":
                return self._get_language_state(input_data, process_id)
                
            else:
                return {
                    "status": "error",
                    "message": f"Unknown operation for language component: {operation}",
                    "process_id": process_id
                }
                
        else:
            return {
                "status": "error",
                "message": f"Unknown component: {component}",
                "process_id": process_id
            }
    
    def _comprehend_input(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Comprehend language input by coordinating across components
        
        Args:
            input_data: Input data dictionary
            process_id: Process identifier
            
        Returns:
            Dict with comprehension results
        """
        # Process depends on input type
        input_type = input_data.get("input_type", "text")
        
        if input_type == "audio":
            # Handle audio input (speech)
            if "audio_features" not in input_data:
                return {
                    "status": "error",
                    "message": "Missing audio_features for audio input",
                    "process_id": process_id
                }
                
            # First process with phoneme recognition
            phoneme_result = self.phoneme_recognition.process_input({
                "operation": "recognize",
                "audio_features": input_data["audio_features"],
                "process_id": process_id
            })
            
            if phoneme_result["status"] != "success":
                return phoneme_result
                
            # Convert phonemes to words
            word_result = self.word_learning.process_input({
                "operation": "recognize",
                "phoneme_sequence": [p[0] for p in phoneme_result["recognized_phonemes"]],
                "process_id": process_id
            })
            
            if word_result["status"] != "success" or not word_result.get("recognized", False):
                return word_result
                
            # Now we have a word or sequence of words to process
            if "recognized_word" in word_result:
                text = word_result["recognized_word"]
            else:
                # Construct from phoneme sequence as fallback
                text = "".join([p[0] for p in phoneme_result["recognized_phonemes"]])
                
        elif input_type == "text":
            # Handle text input directly
            if "text" not in input_data:
                return {
                    "status": "error",
                    "message": "Missing text for text input",
                    "process_id": process_id
                }
                
            text = input_data["text"]
            
        else:
            return {
                "status": "error",
                "message": f"Unknown input_type: {input_type}",
                "process_id": process_id
            }
        
        # Now process the text through grammar and semantics
        
        # Analyze grammar
        grammar_result = self.grammar_acquisition.process_input({
            "operation": "analyze",
            "sentence": text,
            "process_id": process_id
        })
        
        # Extract meaning
        semantic_result = self.semantic_processing.process_input({
            "operation": "understand",
            "text": text,
            "process_id": process_id
        })
        
        # Integrate results
        comprehension = {
            "status": "success",
            "text": text,
            "grammar": {
                "pattern": grammar_result.get("detected_pattern", ""),
                "grammatical": grammar_result.get("grammatical", False)
            },
            "semantics": {
                "concepts": [c["concept"] for c in semantic_result.get("relevant_concepts", [])],
                "depth": semantic_result.get("understanding_depth", 0.0)
            },
            "development_level": self.development_level,
            "process_id": process_id
        }
        
        return comprehension
    
    def _produce_output(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Produce language output by coordinating across components
        
        Args:
            input_data: Input data dictionary
            process_id: Process identifier
            
        Returns:
            Dict with production results
        """
        # Check for intent
        if "intent" not in input_data:
            return {
                "status": "error",
                "message": "Missing intent for language production",
                "process_id": process_id
            }
            
        intent = input_data["intent"]
        context = input_data.get("context", {})
        concepts = input_data.get("concepts", [])
        
        # If concepts are provided, translate to semantic representation
        if concepts:
            # Enrich context with concept information
            for concept in concepts:
                if isinstance(concept, str) and concept not in context:
                    # Query semantic information
                    concept_info = self.semantic_processing.process_input({
                        "operation": "query_semantics",
                        "query_type": "concept",
                        "concept": concept,
                        "process_id": process_id
                    })
                    
                    # Add to context if found
                    if concept_info["status"] == "success":
                        context[concept_info.get("category", "object")] = concept
        
        # Generate expression
        expression_result = self.expression_generator.process_input({
            "operation": "generate",
            "intent": intent,
            "context": context,
            "process_id": process_id
        })
        
        # Check grammaticality if development level permits
        if self.development_level >= 0.4 and "expression" in expression_result:
            grammar_result = self.grammar_acquisition.process_input({
                "operation": "check_grammar",
                "sentence": expression_result["expression"],
                "process_id": process_id
            })
            
            # Add grammar check results
            if grammar_result["status"] == "success":
                expression_result["grammatical"] = grammar_result.get("grammatical", False)
                expression_result["grammaticality_score"] = grammar_result.get("grammaticality_score", 0.0)
        
        return expression_result
    
    def _get_language_state(self, input_data: Dict[str, Any], process_id: str) -> Dict[str, Any]:
        """
        Get the current language state
        
        Args:
            input_data: Input data dictionary
            process_id: Process identifier
            
        Returns:
            Dict with language state information
        """
        # Get component states
        phoneme_state = self.phoneme_recognition.get_state()
        word_state = self.word_learning.get_state()
        grammar_state = self.grammar_acquisition.get_state()
        semantic_state = self.semantic_processing.get_state()
        expression_state = self.expression_generator.get_state()
        
        # Update language model component levels
        self.language_model.component_levels = {
            "phoneme_recognition": self.phoneme_recognition.development_level,
            "word_learning": self.word_learning.development_level,
            "grammar_acquisition": self.grammar_acquisition.development_level,
            "semantic_processing": self.semantic_processing.development_level,
            "expression_generation": self.expression_generator.development_level
        }
        
        # Overall language capability assessment
        # This weighs different components based on development stage
        if self.development_level < 0.3:
            # Early stage - phonemes and words most important
            phoneme_weight = 0.5
            word_weight = 0.3
            grammar_weight = 0.1
            semantic_weight = 0.05
            expression_weight = 0.05
        elif self.development_level < 0.6:
            # Middle stage - words and grammar most important
            phoneme_weight = 0.2
            word_weight = 0.3
            grammar_weight = 0.3
            semantic_weight = 0.1
            expression_weight = 0.1
        else:
            # Advanced stage - semantics and expression most important
            phoneme_weight = 0.1
            word_weight = 0.2
            grammar_weight = 0.2
            semantic_weight = 0.25
            expression_weight = 0.25
            
        # Calculate weighted language capability
        capability = (
            phoneme_weight * self.phoneme_recognition.development_level +
            word_weight * self.word_learning.development_level +
            grammar_weight * self.grammar_acquisition.development_level +
            semantic_weight * self.semantic_processing.development_level +
            expression_weight * self.expression_generator.development_level
        )
        
        # Return language state
        return {
            "status": "success",
            "module_id": self.module_id,
            "developmental_level": self.development_level,
            "language_capability": capability,
            "components": {
                "phoneme_recognition": phoneme_state,
                "word_learning": word_state,
                "grammar_acquisition": grammar_state,
                "semantic_processing": semantic_state,
                "expression_generator": expression_state
            },
            "process_id": process_id
        }
    
    def _handle_phoneme_event(self, event: Dict[str, Any]) -> None:
        """Handle events from the phoneme recognition component"""
        message_type = event.get("message_type", "")
        
        if message_type == "development_milestone":
            # A milestone was reached in phoneme development
            content = event.get("content", {})
            level = content.get("level", 0.0)
            
            # Update language development tracking
            self._check_language_milestones()
            
    def _handle_word_event(self, event: Dict[str, Any]) -> None:
        """Handle events from the word learning component"""
        message_type = event.get("message_type", "")
        
        if message_type == "development_milestone":
            # A milestone was reached in word learning development
            content = event.get("content", {})
            level = content.get("level", 0.0)
            
            # Update language development tracking
            self._check_language_milestones()
            
    def _handle_grammar_event(self, event: Dict[str, Any]) -> None:
        """Handle events from the grammar acquisition component"""
        message_type = event.get("message_type", "")
        
        if message_type == "development_milestone":
            # A milestone was reached in grammar development
            content = event.get("content", {})
            level = content.get("level", 0.0)
            
            # Update language development tracking
            self._check_language_milestones()
            
    def _handle_semantic_event(self, event: Dict[str, Any]) -> None:
        """Handle events from the semantic processing component"""
        message_type = event.get("message_type", "")
        
        if message_type == "development_milestone":
            # A milestone was reached in semantic development
            content = event.get("content", {})
            level = content.get("level", 0.0)
            
            # Update language development tracking
            self._check_language_milestones()
            
    def _handle_expression_event(self, event: Dict[str, Any]) -> None:
        """Handle events from the expression generator component"""
        message_type = event.get("message_type", "")
        
        if message_type == "development_milestone":
            # A milestone was reached in expression development
            content = event.get("content", {})
            level = content.get("level", 0.0)
            
            # Update language development tracking
            self._check_language_milestones()
    
    def _check_language_milestones(self):
        """Check if any language-level milestones have been reached"""
        # Language development is influenced by component development
        component_levels = [
            self.phoneme_recognition.development_level,
            self.word_learning.development_level,
            self.grammar_acquisition.development_level,
            self.semantic_processing.development_level,
            self.expression_generator.development_level
        ]
        
        # Average component development
        avg_component_level = sum(component_levels) / len(component_levels)
        
        # Get previous development level
        old_level = self.development_level
        
        # Update overall development level (weighted average of components and existing level)
        self.development_level = (self.development_level * 0.3) + (avg_component_level * 0.7)
        self.development_level = max(0.0, min(1.0, self.development_level))
        
        # Check if crossed a milestone
        for level in sorted(self.development_milestones.keys()):
            if old_level < level <= self.development_level:
                milestone = self.development_milestones[level]
                
                # Publish milestone event if we have an event bus
                if self.event_bus:
                    self.event_bus.publish({
                        "sender": self.module_id,
                        "message_type": "development_milestone",
                        "content": {
                            "module": "language",
                            "milestone": milestone,
                            "level": level
                        }
                    })
                
                print(f"Language Development Milestone: {milestone} (level {level})")
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of the module and its components
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New development level
        """
        old_level = self.development_level
        
        # Update component development levels
        self.phoneme_recognition.update_development(amount)
        self.word_learning.update_development(amount)
        self.grammar_acquisition.update_development(amount)
        self.semantic_processing.update_development(amount)
        self.expression_generator.update_development(amount)
        
        # Check language milestones
        self._check_language_milestones()
        
        return self.development_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the language system
        
        Returns:
            Dict representing the current state
        """
        return {
            "module_id": self.module_id,
            "developmental_level": self.development_level,
            "components": {
                "phoneme_recognition": self.phoneme_recognition.get_state(),
                "word_learning": self.word_learning.get_state(),
                "grammar_acquisition": self.grammar_acquisition.get_state(),
                "semantic_processing": self.semantic_processing.get_state(),
                "expression_generator": self.expression_generator.get_state()
            }
        }
    
    def save_state(self) -> Dict[str, Any]:
        """
        Save the current state for persistence
        
        Returns:
            Dict with serializable state
        """
        return {
            "module_id": self.module_id,
            "developmental_level": self.development_level,
            "language_model": self.language_model.dict(),
            "components": {
                "phoneme_recognition": self.phoneme_recognition.save_state(),
                "word_learning": self.word_learning.save_state(),
                "grammar_acquisition": self.grammar_acquisition.save_state(),
                "semantic_processing": self.semantic_processing.save_state(),
                "expression_generator": self.expression_generator.save_state()
            },
            "neural_state": {
                "phoneme_recognition": {
                    "development": self.neural_state.phoneme_recognition_development,
                    "accuracy": self.neural_state.phoneme_recognition_accuracy
                },
                "word_learning": {
                    "development": self.neural_state.word_learning_development,
                    "accuracy": self.neural_state.word_learning_accuracy
                },
                "grammar_acquisition": {
                    "development": self.neural_state.grammar_acquisition_development,
                    "accuracy": self.neural_state.grammar_acquisition_accuracy
                },
                "semantic_processing": {
                    "development": self.neural_state.semantic_processing_development,
                    "accuracy": self.neural_state.semantic_processing_accuracy
                },
                "expression_generation": {
                    "development": self.neural_state.expression_generation_development,
                    "accuracy": self.neural_state.expression_generation_accuracy
                }
            }
        }
    
    def load_state(self, state: Dict[str, Any]) -> None:
        """
        Load a previously saved state
        
        Args:
            state: The state to load
        """
        # Load module ID
        self.module_id = state["module_id"]
        
        # Load development level
        self.development_level = state["developmental_level"]
        
        # Load language model
        if "language_model" in state:
            try:
                # Create new model from dict
                from pydantic import parse_obj_as
                self.language_model = parse_obj_as(LanguageModel, state["language_model"])
            except Exception as e:
                print(f"Error loading language model: {e}")
        
        # Load component states
        if "components" in state:
            components = state["components"]
            
            if "phoneme_recognition" in components:
                self.phoneme_recognition.load_state(components["phoneme_recognition"])
                
            if "word_learning" in components:
                self.word_learning.load_state(components["word_learning"])
                
            if "grammar_acquisition" in components:
                self.grammar_acquisition.load_state(components["grammar_acquisition"])
                
            if "semantic_processing" in components:
                self.semantic_processing.load_state(components["semantic_processing"])
                
            if "expression_generator" in components:
                self.expression_generator.load_state(components["expression_generator"])
        
        # Load neural state
        if "neural_state" in state:
            ns = state["neural_state"]
            
            if "phoneme_recognition" in ns:
                self.neural_state.phoneme_recognition_development = ns["phoneme_recognition"].get("development", self.development_level)
                self.neural_state.phoneme_recognition_accuracy = ns["phoneme_recognition"].get("accuracy", 0.5)
                
            if "word_learning" in ns:
                self.neural_state.word_learning_development = ns["word_learning"].get("development", self.development_level)
                self.neural_state.word_learning_accuracy = ns["word_learning"].get("accuracy", 0.5)
                
            if "grammar_acquisition" in ns:
                self.neural_state.grammar_acquisition_development = ns["grammar_acquisition"].get("development", self.development_level)
                self.neural_state.grammar_acquisition_accuracy = ns["grammar_acquisition"].get("accuracy", 0.5)
                
            if "semantic_processing" in ns:
                self.neural_state.semantic_processing_development = ns["semantic_processing"].get("development", self.development_level)
                self.neural_state.semantic_processing_accuracy = ns["semantic_processing"].get("accuracy", 0.5)
                
            if "expression_generation" in ns:
                self.neural_state.expression_generation_development = ns["expression_generation"].get("development", self.development_level)
                self.neural_state.expression_generation_accuracy = ns["expression_generation"].get("accuracy", 0.5)


def get_module(module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0) -> Union[LanguageSystem, BaseModule]:
    """
    Factory function to create a language module
    
    This function is responsible for creating a language system that can:
    - Comprehend language input (written or spoken)
    - Produce appropriate language output
    - Acquire new language skills through experience
    - Process semantic meaning from language
    - Connect language to concepts and experiences
    
    Args:
        module_id: Unique identifier for the module
        event_bus: Event bus for communication with other modules
        development_level: Initial developmental level (0.0 to 1.0)
        
    Returns:
        An instance of the LanguageSystem class or a specific component
    """
    # Check if requesting a specific component or the full system
    module_parts = module_id.split('.')
    
    if len(module_parts) > 1 and module_parts[0] == "language":
        component = module_parts[1]
        
        # Return specific component
        if component == "phoneme_recognition":
            return PhonemeRecognition(module_id, event_bus, development_level)
            
        elif component == "word_learning":
            return WordLearning(module_id, event_bus, development_level)
            
        elif component == "grammar_acquisition":
            return GrammarAcquisition(module_id, event_bus, development_level)
            
        elif component == "semantic_processing":
            return SemanticProcessing(module_id, event_bus, development_level)
            
        elif component == "expression_generator":
            return ExpressionGenerator(module_id, event_bus, development_level)
            
        else:
            # Unknown component, return full system
            print(f"Unknown language component '{component}', returning full language system")
            return LanguageSystem(module_id, event_bus, development_level)
    
    # Return full language system
    return LanguageSystem(module_id, event_bus, development_level)


#######################

#learning\associative_learning.py#
#######################

import numpy as np
import torch
from typing import Dict, List, Any, Optional, Set, Tuple
from datetime import datetime
import uuid
import logging
import os

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.modules.learning.models import AssociativeLearningEvent

logger = logging.getLogger(__name__)

class AssociativeLearning(BaseModule):
    """
    Learns relationships between stimuli and events
    
    This module detects correlations, forms associative links,
    strengthens connections through experience, and applies
    associations to predict outcomes.
    """
    
    # Development milestones for associative learning
    development_milestones = {
        0.0: "Simple stimulus-response associations",
        0.2: "Multiple associations per stimulus",
        0.4: "Temporal sequence learning",
        0.6: "Context-dependent associations",
        0.8: "Advanced statistical correlation detection",
        1.0: "Abstract relational learning"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the associative learning module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level
        """
        super().__init__(
            module_id=module_id, 
            module_type="associative_learning", 
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Stimulus-response associations (stimulus -> list of responses)
        self.associations = {}
        
        # Association strengths (stimulus-response pair -> strength)
        self.association_strengths = {}
        
        # Temporal sequence tracking
        self.recent_stimuli = []
        self.max_sequence_length = 3  # Will increase with development
        
        # Co-occurrence matrix for statistical learning
        self.co_occurrence = {}
        
        # Adjust capabilities based on developmental level
        self._adjust_for_development()
        
        # Subscribe to perception events to create associations
        if self.event_bus:
            self.subscribe_to_message("perception_input", self._handle_perception_input)
            self.subscribe_to_message("learning_reinforce", self._handle_reinforcement)
    
    def _adjust_for_development(self):
        """Adjust capabilities based on current developmental level"""
        # Sequence length increases with development
        self.max_sequence_length = max(2, int(3 + (self.development_level * 7)))
        
        # Activation threshold decreases with development (becomes more sensitive)
        self.activation_threshold = max(0.3, 0.7 - (self.development_level * 0.4))
        
        # Statistical learning complexity increases with development
        self.statistical_complexity = self.development_level
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to learn associations
        
        Args:
            input_data: Dictionary containing stimuli and events for association
            
        Returns:
            Dictionary with the learned associations and predictions
        """
        operation = input_data.get("operation", "learn")
        
        if operation == "learn":
            return self._learn_association(input_data)
        elif operation == "predict":
            return self._predict_from_stimulus(input_data)
        elif operation == "reinforce":
            return self._reinforce_association(input_data)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "module_id": self.module_id
            }
    
    def _learn_association(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Learn a new association between stimulus and response"""
        stimulus = input_data.get("stimulus")
        response = input_data.get("response")
        
        if not stimulus or not response:
            return {"status": "error", "message": "Missing stimulus or response"}
        
        # Calculate initial association strength based on development
        base_strength = 0.3 + (self.development_level * 0.2)
        strength = input_data.get("strength", base_strength)
        
        # Create the association
        if stimulus not in self.associations:
            self.associations[stimulus] = []
        
        # Add if not already present
        if response not in self.associations[stimulus]:
            self.associations[stimulus].append(response)
        
        # Set or update association strength
        pair_key = f"{stimulus}|{response}"
        self.association_strengths[pair_key] = strength
        
        # Update co-occurrence matrix for statistical learning
        if self.development_level >= 0.3:  # Only with sufficient development
            self._update_co_occurrence(stimulus, response)
        
        # Create learning event
        event = AssociativeLearningEvent(
            source=input_data.get("source", "experience"),
            content=f"Association between '{stimulus}' and '{response}'",
            stimulus=stimulus,
            response=response,
            association_strength=strength,
            conditioning_type=input_data.get("conditioning_type", "classical"),
            temporal_delay=input_data.get("delay", 0.0),
            developmental_level=self.development_level
        )
        
        # Add to recent stimuli for sequence learning
        self._update_recent_stimuli(stimulus)
        
        return {
            "status": "success",
            "association_id": pair_key,
            "stimulus": stimulus,
            "response": response,
            "strength": strength,
            "learning_event_id": event.id
        }
    
    def _predict_from_stimulus(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Predict response based on stimulus"""
        stimulus = input_data.get("stimulus")
        
        if not stimulus:
            return {"status": "error", "message": "Missing stimulus"}
        
        if stimulus not in self.associations:
            return {
                "status": "not_found",
                "message": f"No associations found for stimulus: {stimulus}"
            }
        
        # Get all responses and their strengths
        responses = self.associations[stimulus]
        prediction_threshold = input_data.get("threshold", self.activation_threshold)
        
        # Calculate response probabilities based on association strengths
        predictions = []
        for response in responses:
            pair_key = f"{stimulus}|{response}"
            strength = self.association_strengths.get(pair_key, 0.0)
            
            if strength >= prediction_threshold:
                predictions.append({
                    "response": response,
                    "confidence": strength,
                    "association_id": pair_key
                })
        
        # Sort by confidence
        predictions.sort(key=lambda x: x["confidence"], reverse=True)
        
        # Update recent stimuli for sequence learning
        self._update_recent_stimuli(stimulus)
        
        # Add sequence prediction if we have enough development
        if self.development_level >= 0.4 and len(self.recent_stimuli) >= 2:
            sequence_predictions = self._predict_from_sequence()
            if sequence_predictions:
                return {
                    "status": "success",
                    "predictions": predictions,
                    "sequence_predictions": sequence_predictions,
                    "stimulus": stimulus
                }
        
        return {
            "status": "success" if predictions else "no_predictions",
            "predictions": predictions,
            "stimulus": stimulus
        }
    
    def _reinforce_association(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Reinforce or weaken an existing association"""
        stimulus = input_data.get("stimulus")
        response = input_data.get("response")
        
        if not stimulus or not response:
            return {"status": "error", "message": "Missing stimulus or response"}
        
        pair_key = f"{stimulus}|{response}"
        if pair_key not in self.association_strengths:
            return {
                "status": "not_found",
                "message": f"Association not found: {stimulus}->{response}"
            }
        
        # Get reinforcement amount (positive = strengthen, negative = weaken)
        amount = input_data.get("amount", 0.1)
        
        # Update strength
        current_strength = self.association_strengths[pair_key]
        new_strength = max(0.0, min(1.0, current_strength + amount))
        self.association_strengths[pair_key] = new_strength
        
        # If strength drops to zero, remove the association
        if new_strength <= 0.0:
            if response in self.associations[stimulus]:
                self.associations[stimulus].remove(response)
            if not self.associations[stimulus]:
                del self.associations[stimulus]
            del self.association_strengths[pair_key]
        
        return {
            "status": "success",
            "association_id": pair_key,
            "previous_strength": current_strength,
            "new_strength": new_strength,
            "change": amount
        }
    
    def _update_recent_stimuli(self, stimulus: str):
        """Update the list of recent stimuli for sequence learning"""
        self.recent_stimuli.append(stimulus)
        if len(self.recent_stimuli) > self.max_sequence_length:
            self.recent_stimuli.pop(0)
    
    def _predict_from_sequence(self) -> List[Dict[str, Any]]:
        """Predict next stimulus based on recent sequence"""
        if len(self.recent_stimuli) < 2:
            return []
        
        # Create sequence key
        sequence = "|".join(self.recent_stimuli)
        
        # Check if this sequence exists in associations
        if sequence not in self.associations:
            return []
        
        # Return predictions
        predictions = []
        for response in self.associations[sequence]:
            pair_key = f"{sequence}|{response}"
            strength = self.association_strengths.get(pair_key, 0.0)
            
            if strength >= self.activation_threshold:
                predictions.append({
                    "response": response,
                    "confidence": strength,
                    "sequence": self.recent_stimuli.copy(),
                    "association_id": pair_key
                })
        
        # Sort by confidence
        predictions.sort(key=lambda x: x["confidence"], reverse=True)
        return predictions
    
    def _update_co_occurrence(self, stimulus: str, response: str):
        """Update co-occurrence matrix for statistical learning"""
        if stimulus not in self.co_occurrence:
            self.co_occurrence[stimulus] = {}
        
        if response not in self.co_occurrence[stimulus]:
            self.co_occurrence[stimulus][response] = 0
            
        self.co_occurrence[stimulus][response] += 1
    
    def _handle_perception_input(self, message):
        """Handle perception input events for automatic association learning"""
        if not message.content:
            return
            
        # Extract perception data
        perception_data = message.content
        
        # Only process if we have both current and previous perceptions
        if "current" in perception_data and "previous" in perception_data:
            stimulus = perception_data["previous"].get("pattern", "")
            response = perception_data["current"].get("pattern", "")
            
            if stimulus and response:
                # Automatically learn the association
                self._learn_association({
                    "stimulus": stimulus,
                    "response": response,
                    "source": "perception",
                    "delay": perception_data.get("time_delta", 0.0)
                })
    
    def _handle_reinforcement(self, message):
        """Handle reinforcement events"""
        if not message.content:
            return
            
        reinforcement_data = message.content
        
        if "stimulus" in reinforcement_data and "response" in reinforcement_data:
            self._reinforce_association(reinforcement_data)
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        previous_level = self.development_level
        new_level = super().update_development(amount)
        
        # If development level changed significantly, adjust capabilities
        if abs(new_level - previous_level) >= 0.05:
            self._adjust_for_development()
            
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """Get the current state of the module"""
        base_state = super().get_state()
        
        # Add associative learning specific state
        module_state = {
            "association_count": sum(len(responses) for responses in self.associations.values()),
            "unique_stimuli": len(self.associations),
            "sequence_capacity": self.max_sequence_length,
            "activation_threshold": self.activation_threshold,
            "statistical_complexity": self.statistical_complexity
        }
        
        base_state.update(module_state)
        return base_state


#######################

#learning\meta_learning.py#
#######################

import numpy as np
import torch
from typing import Dict, List, Any, Optional, Tuple, Union, Set
from datetime import datetime
import uuid
import logging
import os
from collections import defaultdict

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.modules.learning.models import MetaLearningEvent, LearningStrategy

logger = logging.getLogger(__name__)

class MetaLearning(BaseModule):
    """
    Learning how to learn more effectively
    
    This module develops strategies for learning, monitors learning effectiveness,
    and optimizes the application of learning techniques across domains.
    """
    
    # Development milestones for meta-learning
    development_milestones = {
        0.0: "Basic learning reflection",
        0.2: "Simple strategy selection",
        0.4: "Learning strategy adaptation",
        0.6: "Strategic knowledge transfer",
        0.8: "Learning efficiency optimization",
        1.0: "Advanced meta-cognitive control"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the meta-learning module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level
        """
        super().__init__(
            module_id=module_id,
            module_type="meta_learning",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Learning strategies repository
        self.strategies = {}
        
        # Learning effectiveness by domain
        self.domain_effectiveness = {}
        
        # Strategy usage history
        self.strategy_history = []
        
        # Learning rate adjustment factor (meta-learning rate)
        self.meta_learning_rate = 0.05
        
        # Initialize with basic strategies
        self._initialize_basic_strategies()
        
        # Adjust parameters based on development level
        self._adjust_for_development()
        
        # Subscribe to relevant events
        if self.event_bus:
            self.subscribe_to_message("learning_outcome", self._handle_learning_outcome)
            self.subscribe_to_message("strategy_effectiveness", self._handle_strategy_effectiveness)
    
    def _initialize_basic_strategies(self):
        """Initialize a set of basic learning strategies"""
        basic_strategies = [
            {
                "name": "repetition",
                "description": "Learn through repeated exposure and practice",
                "effectiveness": 0.5,
                "cognitive_load": 0.3,
                "min_developmental_level": 0.0,
                "applicable_domains": ["procedural", "factual", "language"]
            },
            {
                "name": "association",
                "description": "Learn by associating new information with known concepts",
                "effectiveness": 0.6,
                "cognitive_load": 0.4,
                "min_developmental_level": 0.1,
                "applicable_domains": ["semantic", "factual", "conceptual"]
            },
            {
                "name": "trial_and_error",
                "description": "Learn through experimentation and feedback",
                "effectiveness": 0.5,
                "cognitive_load": 0.5,
                "min_developmental_level": 0.0,
                "applicable_domains": ["procedural", "problem-solving"]
            },
            {
                "name": "chunking",
                "description": "Group information into meaningful chunks",
                "effectiveness": 0.7,
                "cognitive_load": 0.6,
                "min_developmental_level": 0.3,
                "applicable_domains": ["memory", "factual", "conceptual"]
            },
        ]
        
        # Create strategy objects
        for strategy_data in basic_strategies:
            strategy = LearningStrategy(
                name=strategy_data["name"],
                description=strategy_data["description"],
                effectiveness=strategy_data["effectiveness"],
                cognitive_load=strategy_data["cognitive_load"],
                min_developmental_level=strategy_data["min_developmental_level"],
                applicable_domains=strategy_data["applicable_domains"],
                created_at=datetime.now(),
                usage_count=0,
                success_rate=0.5
            )
            self.strategies[strategy.id] = strategy
    
    def _adjust_for_development(self):
        """Adjust capabilities based on developmental level"""
        # Meta-learning rate increases with development
        self.meta_learning_rate = 0.05 + (self.development_level * 0.1)
        
        # At higher development levels, unlock more advanced strategies
        if self.development_level >= 0.4 and not any(s.name == "comparison" for s in self.strategies.values()):
            self._add_advanced_strategies()
    
    def _add_advanced_strategies(self):
        """Add more advanced learning strategies that unlock at higher development levels"""
        advanced_strategies = [
            {
                "name": "comparison",
                "description": "Learn by comparing similarities and differences",
                "effectiveness": 0.7,
                "cognitive_load": 0.6,
                "min_developmental_level": 0.4,
                "applicable_domains": ["conceptual", "analytical", "relational"]
            },
            {
                "name": "elaboration",
                "description": "Expand on information by adding details or connections",
                "effectiveness": 0.8,
                "cognitive_load": 0.7,
                "min_developmental_level": 0.5,
                "applicable_domains": ["conceptual", "factual", "semantic"]
            },
            {
                "name": "self_explanation",
                "description": "Explain concepts to oneself to deepen understanding",
                "effectiveness": 0.8,
                "cognitive_load": 0.7,
                "min_developmental_level": 0.6,
                "applicable_domains": ["conceptual", "procedural", "analytical"]
            },
            {
                "name": "interleaving",
                "description": "Alternate between different topics or skills during learning",
                "effectiveness": 0.8,
                "cognitive_load": 0.8,
                "min_developmental_level": 0.7,
                "applicable_domains": ["procedural", "problem-solving", "motor"]
            },
        ]
        
        # Create strategy objects
        for strategy_data in advanced_strategies:
            # Only add if development level is sufficient
            if self.development_level >= strategy_data["min_developmental_level"]:
                strategy = LearningStrategy(
                    name=strategy_data["name"],
                    description=strategy_data["description"],
                    effectiveness=strategy_data["effectiveness"],
                    cognitive_load=strategy_data["cognitive_load"],
                    min_developmental_level=strategy_data["min_developmental_level"],
                    applicable_domains=strategy_data["applicable_domains"],
                    created_at=datetime.now(),
                    usage_count=0,
                    success_rate=0.5
                )
                self.strategies[strategy.id] = strategy
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input for meta-learning operations
        
        Args:
            input_data: Dictionary containing meta-learning parameters
            
        Returns:
            Dictionary with meta-learning results
        """
        operation = input_data.get("operation", "select_strategy")
        
        if operation == "select_strategy":
            return self._select_strategy(input_data)
        elif operation == "evaluate_outcome":
            return self._evaluate_learning_outcome(input_data)
        elif operation == "create_strategy":
            return self._create_learning_strategy(input_data)
        elif operation == "get_strategy":
            return self._get_strategy_details(input_data)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "module_id": self.module_id
            }
    
    def _select_strategy(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Select an appropriate learning strategy for a given context"""
        domain = input_data.get("domain", "general")
        content_type = input_data.get("content_type", "factual")
        available_cognitive_resources = input_data.get("cognitive_resources", 0.8)
        
        # Filter strategies by developmental level and cognitive resources
        available_strategies = [
            s for s in self.strategies.values()
            if s.min_developmental_level <= self.development_level
            and s.cognitive_load <= available_cognitive_resources
        ]
        
        if not available_strategies:
            return {
                "status": "error",
                "message": "No suitable strategies available",
                "developmental_level": self.development_level
            }
        
        # Calculate strategy scores based on multiple factors
        strategy_scores = {}
        for strategy in available_strategies:
            # Base score is the strategy's effectiveness
            score = strategy.effectiveness
            
            # Bonus if the strategy applies to this domain
            if domain in strategy.applicable_domains:
                score += 0.2
            
            # Bonus for content type match (using domain as proxy)
            if content_type in strategy.applicable_domains:
                score += 0.1
            
            # Success rate influences score (if used before)
            if strategy.usage_count > 0:
                score = (score + strategy.success_rate) / 2
            
            # Efficiency factor (effectiveness/cognitive_load ratio)
            efficiency = strategy.effectiveness / max(0.1, strategy.cognitive_load)
            score = (score + efficiency * 0.3) / 1.3
            
            strategy_scores[strategy.id] = score
        
        # Select best strategy
        best_strategy_id = max(strategy_scores, key=strategy_scores.get)
        best_strategy = self.strategies[best_strategy_id]
        
        # Increase usage count
        best_strategy.usage_count += 1
        
        # Record in history
        self.strategy_history.append({
            "strategy_id": best_strategy_id,
            "domain": domain,
            "content_type": content_type,
            "cognitive_resources": available_cognitive_resources,
            "timestamp": datetime.now(),
            "score": strategy_scores[best_strategy_id]
        })
        
        # Create meta-learning event
        event = MetaLearningEvent(
            source=input_data.get("source", "meta_learning"),
            content=f"Strategy selection for {domain}/{content_type} learning",
            strategy=best_strategy.name,
            effectiveness=best_strategy.effectiveness,
            applicable_contexts=[domain, content_type],
            target_learning_types=input_data.get("learning_types", [content_type]),
            resource_cost=best_strategy.cognitive_load,
            developmental_level=self.development_level
        )
        
        return {
            "status": "success",
            "selected_strategy": {
                "id": best_strategy_id,
                "name": best_strategy.name,
                "description": best_strategy.description,
                "effectiveness": best_strategy.effectiveness,
                "cognitive_load": best_strategy.cognitive_load
            },
            "domain": domain,
            "content_type": content_type,
            "strategy_score": strategy_scores[best_strategy_id],
            "learning_event_id": event.id
        }
    
    def _evaluate_learning_outcome(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Evaluate the outcome of a learning strategy application"""
        strategy_id = input_data.get("strategy_id")
        domain = input_data.get("domain", "general")
        success_level = input_data.get("success_level", 0.5)  # 0.0 to 1.0
        
        if not strategy_id or strategy_id not in self.strategies:
            return {"status": "error", "message": "Invalid strategy ID"}
        
        strategy = self.strategies[strategy_id]
        
        # Update success rate using exponential moving average
        if strategy.usage_count <= 1:
            strategy.success_rate = success_level
        else:
            # More weight on recent outcomes at higher development levels
            alpha = 0.2 + (self.development_level * 0.3)
            strategy.success_rate = (alpha * success_level) + ((1 - alpha) * strategy.success_rate)
        
        # Update domain effectiveness
        if domain not in self.domain_effectiveness:
            self.domain_effectiveness[domain] = {}
        
        if strategy_id not in self.domain_effectiveness[domain]:
            self.domain_effectiveness[domain][strategy_id] = {
                "success_sum": 0.0,
                "usage_count": 0
            }
        
        self.domain_effectiveness[domain][strategy_id]["success_sum"] += success_level
        self.domain_effectiveness[domain][strategy_id]["usage_count"] += 1
        
        # Calculate domain-specific effectiveness
        domain_success_rate = (
            self.domain_effectiveness[domain][strategy_id]["success_sum"] / 
            self.domain_effectiveness[domain][strategy_id]["usage_count"]
        )
        
        # At higher development levels, adjust strategy effectiveness based on outcomes
        if self.development_level >= 0.6:
            effectiveness_delta = (success_level - strategy.effectiveness) * self.meta_learning_rate
            strategy.effectiveness = max(0.1, min(1.0, strategy.effectiveness + effectiveness_delta))
        
        return {
            "status": "success",
            "strategy_id": strategy_id,
            "strategy_name": strategy.name,
            "domain": domain,
            "previous_success_rate": strategy.success_rate - ((success_level - strategy.success_rate) * (0.2 + (self.development_level * 0.3))),
            "updated_success_rate": strategy.success_rate,
            "domain_success_rate": domain_success_rate,
            "updated_effectiveness": strategy.effectiveness,
            "meta_learning_rate": self.meta_learning_rate
        }
    
    def _create_learning_strategy(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Create a new learning strategy"""
        # Only possible at higher developmental levels
        if self.development_level < 0.5:
            return {
                "status": "error",
                "message": "Creating new strategies requires higher developmental level",
                "current_level": self.development_level,
                "required_level": 0.5
            }
        
        name = input_data.get("name")
        description = input_data.get("description")
        applicable_domains = input_data.get("applicable_domains", [])
        
        if not name or not description:
            return {"status": "error", "message": "Missing strategy name or description"}
        
        # Check if similar strategy already exists
        for strategy in self.strategies.values():
            if strategy.name.lower() == name.lower():
                return {"status": "error", "message": f"Strategy '{name}' already exists"}
        
        # Create new strategy with conservative initial values
        strategy = LearningStrategy(
            name=name,
            description=description,
            effectiveness=input_data.get("effectiveness", 0.5),
            cognitive_load=input_data.get("cognitive_load", 0.6),
            min_developmental_level=input_data.get("min_developmental_level", self.development_level),
            applicable_domains=applicable_domains,
            created_at=datetime.now(),
            usage_count=0,
            success_rate=0.5
        )
        
        # Add to strategies repository
        self.strategies[strategy.id] = strategy
        
        # Create meta-learning event
        event = MetaLearningEvent(
            source=input_data.get("source", "strategy_creation"),
            content=f"Creation of new learning strategy: {name}",
            strategy=name,
            effectiveness=strategy.effectiveness,
            applicable_contexts=applicable_domains,
            target_learning_types=input_data.get("target_learning_types", applicable_domains),
            resource_cost=strategy.cognitive_load,
            developmental_level=self.development_level
        )
        
        return {
            "status": "success",
            "strategy_id": strategy.id,
            "strategy_name": strategy.name,
            "description": strategy.description,
            "effectiveness": strategy.effectiveness,
            "cognitive_load": strategy.cognitive_load,
            "applicable_domains": strategy.applicable_domains,
            "learning_event_id": event.id
        }
    
    def _get_strategy_details(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Get details about a specific strategy or list all strategies"""
        strategy_id = input_data.get("strategy_id")
        
        if not strategy_id:
            # Return list of all strategies available at current development level
            available_strategies = [
                {
                    "id": s.id,
                    "name": s.name,
                    "effectiveness": s.effectiveness,
                    "cognitive_load": s.cognitive_load,
                    "applicable_domains": s.applicable_domains,
                    "usage_count": s.usage_count
                }
                for s in self.strategies.values()
                if s.min_developmental_level <= self.development_level
            ]
            
            return {
                "status": "success",
                "strategies": available_strategies,
                "strategy_count": len(available_strategies),
                "developmental_level": self.development_level
            }
        
        # Get specific strategy
        if strategy_id not in self.strategies:
            return {"status": "error", "message": f"Strategy with ID {strategy_id} not found"}
        
        strategy = self.strategies[strategy_id]
        
        # Gather domain-specific effectiveness
        domain_effectiveness = {}
        for domain, strategies in self.domain_effectiveness.items():
            if strategy_id in strategies:
                domain_effectiveness[domain] = (
                    strategies[strategy_id]["success_sum"] / 
                    strategies[strategy_id]["usage_count"]
                )
        
        return {
            "status": "success",
            "strategy": {
                "id": strategy.id,
                "name": strategy.name,
                "description": strategy.description,
                "effectiveness": strategy.effectiveness,
                "cognitive_load": strategy.cognitive_load,
                "min_developmental_level": strategy.min_developmental_level,
                "applicable_domains": strategy.applicable_domains,
                "usage_count": strategy.usage_count,
                "success_rate": strategy.success_rate,
                "created_at": strategy.created_at.isoformat(),
                "domain_effectiveness": domain_effectiveness
            }
        }
    
    def _handle_learning_outcome(self, message):
        """Handle learning outcome events"""
        if not message.content:
            return
            
        outcome_data = message.content
        
        # Process the learning outcome
        if "strategy_id" in outcome_data and "success_level" in outcome_data:
            self._evaluate_learning_outcome(outcome_data)
    
    def _handle_strategy_effectiveness(self, message):
        """Handle strategy effectiveness feedback"""
        if not message.content:
            return
            
        effectiveness_data = message.content
        
        # Update strategy effectiveness
        if "strategy_id" in effectiveness_data and "effectiveness" in effectiveness_data:
            strategy_id = effectiveness_data["strategy_id"]
            
            if strategy_id in self.strategies:
                strategy = self.strategies[strategy_id]
                
                # Update effectiveness with a weighted average
                current = strategy.effectiveness
                new_value = effectiveness_data["effectiveness"]
                weight = effectiveness_data.get("weight", 0.3)
                
                strategy.effectiveness = (current * (1 - weight)) + (new_value * weight)
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        previous_level = self.development_level
        new_level = super().update_development(amount)
        
        # If development changed significantly, adjust parameters
        if abs(new_level - previous_level) >= 0.05:
            self._adjust_for_development()
            
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """Get the current state of the module"""
        base_state = super().get_state()
        
        # Calculate strategy statistics
        strategy_count = len(self.strategies)
        available_strategy_count = sum(
            1 for s in self.strategies.values() 
            if s.min_developmental_level <= self.development_level
        )
        avg_effectiveness = 0.0
        if strategy_count > 0:
            avg_effectiveness = sum(s.effectiveness for s in self.strategies.values()) / strategy_count
        
        # Add meta-learning specific state
        module_state = {
            "strategy_count": strategy_count,
            "available_strategy_count": available_strategy_count,
            "average_effectiveness": avg_effectiveness,
            "meta_learning_rate": self.meta_learning_rate,
            "domain_count": len(self.domain_effectiveness),
            "strategy_usage_history": len(self.strategy_history)
        }
        
        base_state.update(module_state)
        return base_state


#######################

#learning\models.py#
#######################

from pydantic import BaseModel, Field, field_validator
from typing import List, Dict, Any, Optional, Set, Union, Literal
from datetime import datetime
import uuid
import numpy as np

class LearningEvent(BaseModel):
    """Base model for all learning events in the system"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: datetime = Field(default_factory=datetime.now)
    # Source of the learning event (experience, instruction, observation, etc.)
    source: str
    # Content being learned
    content: str
    # Type of learning event
    learning_type: str
    # Developmental level when this learning occurred (0.0 to 1.0)
    developmental_level: float = Field(default=0.0, ge=0.0, le=1.0)
    # Confidence in the learned information (0.0 to 1.0)
    confidence: float = Field(default=0.5, ge=0.0, le=1.0)
    # How many times this has been reinforced
    reinforcement_count: int = Field(default=0, ge=0)
    # Last time this was reinforced
    last_reinforced: Optional[datetime] = None
    # Learning rate for this event (how quickly it's learned)
    learning_rate: float = Field(default=0.1, ge=0.0, le=1.0)
    # Tags for categorizing the learning event
    tags: Set[str] = Field(default_factory=set)
    
    model_config = {
        "arbitrary_types_allowed": True
    }

class AssociativeLearningEvent(LearningEvent):
    """Model for associative learning events"""
    learning_type: str = "associative"
    # Stimulus that triggered the learning
    stimulus: str
    # Response or associated concept
    response: str
    # Strength of the association (0.0 to 1.0)
    association_strength: float = Field(default=0.3, ge=0.0, le=1.0)
    # Whether this is classical (stimulus-response) or operant (action-consequence) conditioning
    conditioning_type: Literal["classical", "operant"] = "classical"
    # Delay between stimulus and response (in seconds)
    temporal_delay: float = Field(default=0.0, ge=0.0)

class ReinforcementLearningEvent(LearningEvent):
    """Model for reinforcement learning events"""
    learning_type: str = "reinforcement"
    # Action that was taken
    action: str
    # Reward or punishment received
    consequence: str
    # Value of the reward/punishment (-1.0 to 1.0, negative for punishment)
    reward_value: float = Field(default=0.0, ge=-1.0, le=1.0)
    # Delay between action and consequence (in seconds)
    delay: float = Field(default=0.0, ge=0.0)
    # Context in which the action was taken
    context: str
    # Whether this is positive reinforcement, negative reinforcement, or punishment
    reinforcement_type: Literal["positive", "negative", "punishment"] = "positive"

class ProceduralLearningEvent(LearningEvent):
    """Model for procedural learning events"""
    learning_type: str = "procedural"
    # Skill or procedure being learned
    skill: str
    # Current proficiency level (0.0 to 1.0)
    proficiency: float = Field(default=0.1, ge=0.0, le=1.0)
    # Number of practice repetitions
    practice_count: int = Field(default=1, ge=0)
    # Time spent practicing (in seconds)
    practice_time: float = Field(default=0.0, ge=0.0)
    # Whether this is explicit (conscious) or implicit (unconscious) learning
    learning_mode: Literal["explicit", "implicit"] = "explicit"
    # Steps or components of the procedure
    procedure_steps: List[str] = Field(default_factory=list)

class MetaLearningEvent(LearningEvent):
    """Model for meta-learning events"""
    learning_type: str = "meta"
    # Learning strategy being developed
    strategy: str
    # Effectiveness of the strategy (0.0 to 1.0)
    effectiveness: float = Field(default=0.5, ge=0.0, le=1.0)
    # Contexts where this strategy works well
    applicable_contexts: List[str] = Field(default_factory=list)
    # Types of learning this strategy helps with
    target_learning_types: List[str] = Field(default_factory=list)
    # Cognitive resource cost (0.0 to 1.0, higher = more resource intensive)
    resource_cost: float = Field(default=0.5, ge=0.0, le=1.0)

class LearningStrategy(BaseModel):
    """Model for learning strategies"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    name: str
    description: str
    # Overall effectiveness (0.0 to 1.0)
    effectiveness: float = Field(default=0.5, ge=0.0, le=1.0)
    # Cognitive load required (0.0 to 1.0)
    cognitive_load: float = Field(default=0.5, ge=0.0, le=1.0)
    # Minimum developmental level needed to use this strategy
    min_developmental_level: float = Field(default=0.0, ge=0.0, le=1.0)
    # Domains this strategy works well in
    applicable_domains: List[str] = Field(default_factory=list)
    # When this strategy was learned/created
    created_at: datetime = Field(default_factory=datetime.now)
    # How many times this strategy has been used
    usage_count: int = Field(default=0, ge=0)
    # Success rate when using this strategy (0.0 to 1.0)
    success_rate: float = Field(default=0.5, ge=0.0, le=1.0)

class SkillModel(BaseModel):
    """Model for procedural skills"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    name: str
    description: str
    # Current proficiency level (0.0 to 1.0)
    proficiency: float = Field(default=0.1, ge=0.0, le=1.0)
    # Whether the skill has been automated
    automated: bool = Field(default=False)
    # Sequence of steps or actions in this skill
    steps: List[str] = Field(default_factory=list)
    # Prerequisites for this skill
    prerequisites: List[str] = Field(default_factory=list)
    # Domains where this skill is applicable
    domains: List[str] = Field(default_factory=list)
    # Cognitive resources required (0.0 to 1.0)
    cognitive_demand: float = Field(default=0.5, ge=0.0, le=1.0)
    # Practice history
    practice_history: List[Dict[str, Any]] = Field(default_factory=list)

class AssociationModel(BaseModel):
    """Model for associative connections"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    stimulus: str
    response: str
    # Strength of association (0.0 to 1.0)
    strength: float = Field(default=0.3, ge=0.0, le=1.0)
    # Bidirectional association
    bidirectional: bool = Field(default=False)
    # Contexts where this association is valid
    contexts: List[str] = Field(default_factory=list)
    # How many times this association has been reinforced
    reinforcement_count: int = Field(default=1, ge=1)
    # When this association was first created
    created_at: datetime = Field(default_factory=datetime.now)
    # When this association was last activated
    last_activated: Optional[datetime] = None

class ReinforcementModel(BaseModel):
    """Model for reinforcement learning patterns"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    state: str
    action: str
    # Expected reward value (-1.0 to 1.0)
    q_value: float = Field(default=0.0, ge=-1.0, le=1.0)
    # Number of times this state-action pair has been experienced
    experience_count: int = Field(default=1, ge=1)
    # Variance in observed rewards (uncertainty)
    reward_variance: float = Field(default=0.1, ge=0.0)
    # Contexts where this reinforcement pattern applies
    applicable_contexts: List[str] = Field(default_factory=list)
    # History of rewards for this state-action pair
    reward_history: List[float] = Field(default_factory=list)

class LearningNeuralState(BaseModel):
    """Model for tracking neural states related to learning"""
    # Current activation patterns for different learning components
    activations: Dict[str, List[Dict[str, Any]]] = Field(default_factory=dict)
    
    # Development levels for different learning components
    associative_learning_development: float = Field(default=0.0, ge=0.0, le=1.0)
    reinforcement_learning_development: float = Field(default=0.0, ge=0.0, le=1.0) 
    procedural_learning_development: float = Field(default=0.0, ge=0.0, le=1.0)
    meta_learning_development: float = Field(default=0.0, ge=0.0, le=1.0)
    
    # Learning efficacy metrics
    associative_accuracy: float = Field(default=0.5, ge=0.0, le=1.0)
    reinforcement_efficiency: float = Field(default=0.5, ge=0.0, le=1.0)
    procedural_automaticity: float = Field(default=0.1, ge=0.0, le=1.0)
    meta_strategy_effectiveness: float = Field(default=0.3, ge=0.0, le=1.0)
    
    # Maximum activation storage (more recent activations only)
    max_activations_per_type: int = Field(default=20, ge=5)
    
    def add_activation(self, activation_type: str, data: Dict[str, Any]) -> None:
        """Add a new activation pattern for a learning component"""
        if activation_type not in self.activations:
            self.activations[activation_type] = []
            
        # Add timestamp if not present
        if 'timestamp' not in data:
            data['timestamp'] = datetime.now()
            
        self.activations[activation_type].append(data)
        
        # Trim to max size
        if len(self.activations[activation_type]) > self.max_activations_per_type:
            self.activations[activation_type] = self.activations[activation_type][-self.max_activations_per_type:]
    
    def get_recent_activations(self, activation_type: str, count: int = 5) -> List[Dict[str, Any]]:
        """Get the most recent activations for a component"""
        if activation_type not in self.activations:
            return []
            
        return self.activations[activation_type][-min(count, len(self.activations[activation_type])):]
    
    def clear_activations(self, activation_type: Optional[str] = None) -> None:
        """Clear activations for a type or all activations"""
        if activation_type:
            if activation_type in self.activations:
                self.activations[activation_type] = []
        else:
            self.activations = {}
            
class LearningExperience(BaseModel):
    """Model for tracking complete learning experiences"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: datetime = Field(default_factory=datetime.now)
    
    # Learning types involved in this experience
    learning_types: List[str] = Field(default_factory=list)
    
    # Input that triggered the learning
    input_data: Dict[str, Any] = Field(default_factory=dict)
    
    # Results from each learning component
    results: Dict[str, Any] = Field(default_factory=dict)
    
    # Overall success of the learning experience
    success: bool = Field(default=True)
    
    # Learning strategies applied
    strategies_used: List[str] = Field(default_factory=list)
    
    # Developmental level when this experience occurred
    developmental_level: float = Field(default=0.0, ge=0.0, le=1.0)
    
    # Tags for categorizing the experience
    tags: Set[str] = Field(default_factory=set)


#######################

#learning\neural_net.py#
#######################

import torch 
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Any, Optional, Union, Tuple
import uuid
import logging
import os
from datetime import datetime

logger = logging.getLogger(__name__)

def get_device():
    """Get the appropriate device for tensor operations."""
    if torch.cuda.is_available():
        return torch.device("cuda")
    return torch.device("cpu")

class LearningNetwork(nn.Module):
    """
    Neural network for the learning module that processes learning experiences
    and adapts based on developmental level.
    
    This network handles:
    1. Experience encoding - Converting learning experiences to vector representations
    2. Pattern extraction - Identifying patterns in learning experiences
    3. Reinforcement - Strengthening relevant connections
    4. Strategy learning - Developing higher-level learning strategies
    """
    
    def __init__(
        self,
        input_dim: int = 64,
        hidden_dim: int = 128,
        output_dim: int = 32,
        developmental_level: float = 0.0
    ):
        super().__init__()
        
        # Store configuration
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.developmental_level = developmental_level
        
        # Get device (CUDA if available)
        self.device = get_device()
        
        # Experience encoding network
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Pattern extraction network
        self.pattern_extractor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, output_dim)
        )
        
        # Reinforcement prediction network
        self.reinforcement_predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Tanh()  # Output between -1 and 1 for reward prediction
        )
        
        # Strategy selection network (develops with higher developmental levels)
        self.strategy_selector = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, 4)  # 4 learning strategies
        )
        
        # Apply developmental scaling
        self._apply_developmental_scaling()
        
        # Initialize learning history
        self.learning_history = []
        
        # Move model to appropriate device
        self.to(self.device)
        logger.info(f"Learning network initialized on device: {self.device}")
        
        # Enable cuDNN benchmark for performance optimization if available
        if self.device.type == 'cuda':
            torch.backends.cudnn.benchmark = True
            logger.info(f"CUDA version: {torch.version.cuda}")
            logger.info(f"Using GPU: {torch.cuda.get_device_name(0)}")
        
    def _apply_developmental_scaling(self):
        """
        Adjust network parameters based on developmental level
        
        At lower levels:
        - Higher dropout (less reliable learning)
        - Simpler pattern recognition
        - Basic reinforcement learning only
        
        At higher levels:
        - More reliable processing
        - Complex pattern recognition
        - Strategy-based learning
        """
        # Scale dropout based on development
        dropout_rate = max(0.1, 0.5 - (self.developmental_level * 0.4))
        
        # Update dropout layers
        for module in self.modules():
            if isinstance(module, nn.Dropout):
                module.p = dropout_rate
                
        # Scaling factor for weights (lower dev level = simplified processing)
        scaling = 0.5 + (self.developmental_level * 0.5)
        
        # Scale the strategy network based on development level
        # (less influence at lower developmental levels)
        for param in self.strategy_selector.parameters():
            param.data *= self.developmental_level
    
    def update_developmental_level(self, new_level: float):
        """
        Update the developmental level and adjust network accordingly
        
        Args:
            new_level: New developmental level (0.0 to 1.0)
        """
        self.developmental_level = max(0.0, min(1.0, new_level))
        self._apply_developmental_scaling()
        
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Forward pass through the learning network
        
        Args:
            x: Input tensor representing a learning experience
            
        Returns:
            Dictionary with encoded experience, extracted patterns,
            reinforcement prediction, and strategy selection
        """
        # Ensure input is on the correct device
        x = x.to(self.device)
        
        # Encode the experience
        encoded = self.encoder(x)
        
        # Extract patterns
        patterns = self.pattern_extractor(encoded)
        
        # Predict reinforcement value
        reinforcement = self.reinforcement_predictor(encoded)
        
        # Select learning strategy (influenced by developmental level)
        strategy_logits = self.strategy_selector(encoded)
        strategy_probs = F.softmax(strategy_logits * self.developmental_level, dim=-1)
        
        return {
            "encoded_experience": encoded,
            "extracted_patterns": patterns,
            "reinforcement_prediction": reinforcement,
            "strategy_selection": strategy_probs
        }
    
    def adapt_to_reinforcement(self, 
                              experience: torch.Tensor, 
                              reward: float, 
                              learning_rate: float = 0.01) -> Dict[str, Any]:
        """
        Adapt network based on reinforcement signal
        
        Args:
            experience: Input tensor representing the experience
            reward: Actual reward/reinforcement value (-1.0 to 1.0)
            learning_rate: How quickly to adapt to the reinforcement
            
        Returns:
            Dictionary with prediction error and updated prediction
        """
        # Ensure experience is on the correct device
        experience = experience.to(self.device)
        
        # Get current prediction
        with torch.no_grad():
            output = self.forward(experience)
            current_prediction = output["reinforcement_prediction"].item()
        
        # Calculate prediction error
        prediction_error = reward - current_prediction
        
        # Record learning event
        self.learning_history.append({
            "timestamp": datetime.now(),
            "prediction": current_prediction,
            "actual": reward,
            "error": prediction_error,
            "developmental_level": self.developmental_level
        })
        
        # Return results
        return {
            "prediction_error": prediction_error,
            "updated_prediction": current_prediction + (prediction_error * learning_rate)
        }
    
    def batch_process_experiences(self, experiences: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Process a batch of experiences efficiently
        
        Args:
            experiences: Batch of experience tensors [batch_size, input_dim]
            
        Returns:
            Dictionary with batch results
        """
        # Ensure experiences are on the correct device
        experiences = experiences.to(self.device)
        
        # Forward pass
        results = self.forward(experiences)
        
        return results
        
    def save(self, path: str):
        """Save model to disk"""
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        # Move model to CPU for saving to avoid GPU memory issues
        device_backup = next(self.parameters()).device
        cpu_state_dict = {k: v.cpu() for k, v in self.state_dict().items()}
        
        torch.save({
            "model_state": cpu_state_dict,
            "config": {
                "input_dim": self.input_dim,
                "hidden_dim": self.hidden_dim,
                "output_dim": self.output_dim,
                "developmental_level": self.developmental_level
            },
            "learning_history": self.learning_history
        }, path)
        
        # Move model back to original device
        self.to(device_backup)
        logger.info(f"Learning network saved to {path}")
    
    def load(self, path: str):
        """Load model from disk"""
        if not os.path.exists(path):
            logger.error(f"Model file not found: {path}")
            return False
        
        try:
            checkpoint = torch.load(path, map_location=self.device)
            self.load_state_dict(checkpoint["model_state"])
            self.developmental_level = checkpoint["config"]["developmental_level"]
            self.learning_history = checkpoint.get("learning_history", [])
            self._apply_developmental_scaling()
            logger.info(f"Learning network loaded from {path}")
            return True
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            return False
    
    def get_state(self) -> Dict[str, Any]:
        """Get the current state of the network"""
        return {
            "developmental_level": self.developmental_level,
            "learning_history_length": len(self.learning_history),
            "recent_errors": [item["error"] for item in self.learning_history[-10:]] if self.learning_history else [],
            "config": {
                "input_dim": self.input_dim,
                "hidden_dim": self.hidden_dim,
                "output_dim": self.output_dim,
                "device": str(self.device)
            }
        }
        
    def free_memory(self):
        """Free GPU memory if using CUDA"""
        if self.device.type == 'cuda':
            # Move model to CPU
            self.to(torch.device('cpu'))
            # Clear CUDA cache
            torch.cuda.empty_cache()
            logger.info("Freed GPU memory")
            
    def to_gpu(self):
        """Move model to GPU if available"""
        if torch.cuda.is_available():
            self.to(torch.device('cuda'))
            self.device = torch.device('cuda')
            logger.info("Moved model to GPU")
        else:
            logger.warning("GPU not available, model remains on CPU")
            
    def to_cpu(self):
        """Move model to CPU"""
        self.to(torch.device('cpu'))
        self.device = torch.device('cpu')
        logger.info("Moved model to CPU")


class AssociativeLearningNetwork(LearningNetwork):
    """Specialized network for associative learning with pattern detection focus"""
    
    def __init__(
        self,
        input_dim: int = 64,
        hidden_dim: int = 128,
        output_dim: int = 32,
        developmental_level: float = 0.0
    ):
        super().__init__(input_dim, hidden_dim, output_dim, developmental_level)
        
        # Add pattern similarity network
        self.similarity_network = nn.Sequential(
            nn.Linear(output_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        ).to(self.device)
    
    def compute_similarity(self, pattern1: torch.Tensor, pattern2: torch.Tensor) -> float:
        """Compute similarity between two patterns"""
        # Ensure patterns are on the correct device
        pattern1 = pattern1.to(self.device)
        pattern2 = pattern2.to(self.device)
        
        # Concatenate patterns
        combined = torch.cat((pattern1, pattern2), dim=-1)
        
        # Compute similarity
        with torch.no_grad():
            similarity = self.similarity_network(combined)
            
        return similarity.item()


class ReinforcementLearningNetwork(LearningNetwork):
    """Specialized network for reinforcement learning with policy focus"""
    
    def __init__(
        self,
        input_dim: int = 64,
        hidden_dim: int = 128,
        output_dim: int = 32,
        action_dim: int = 10,
        developmental_level: float = 0.0
    ):
        super().__init__(input_dim, hidden_dim, output_dim, developmental_level)
        
        # Add Q-value network for reinforcement learning
        self.action_dim = action_dim
        self.q_network = nn.Sequential(
            nn.Linear(output_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        ).to(self.device)
    
    def compute_q_values(self, state: torch.Tensor) -> torch.Tensor:
        """Compute Q-values for a state"""
        # Ensure state is on the correct device
        state = state.to(self.device)
        
        # Extract patterns from state
        with torch.no_grad():
            output = self.forward(state)
            patterns = output["extracted_patterns"]
            
            # Compute Q-values
            q_values = self.q_network(patterns)
            
        return q_values


class ProceduralLearningNetwork(LearningNetwork):
    """Specialized network for procedural learning with sequence focus"""
    
    def __init__(
        self,
        input_dim: int = 64,
        hidden_dim: int = 128,
        output_dim: int = 32,
        seq_length: int = 5,
        developmental_level: float = 0.0
    ):
        super().__init__(input_dim, hidden_dim, output_dim, developmental_level)
        
        # Add sequence prediction network
        self.seq_length = seq_length
        self.lstm = nn.LSTM(
            input_size=output_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.3
        ).to(self.device)
        
        self.sequence_predictor = nn.Sequential(
            nn.Linear(hidden_dim, output_dim)
        ).to(self.device)
    
    def predict_next_in_sequence(self, sequence: torch.Tensor) -> torch.Tensor:
        """Predict the next item in a sequence"""
        # Ensure sequence is on the correct device
        sequence = sequence.to(self.device)
        
        # Process each item in sequence
        seq_features = []
        for i in range(sequence.shape[0]):
            with torch.no_grad():
                item = sequence[i:i+1]
                output = self.forward(item)
                seq_features.append(output["extracted_patterns"])
        
        # Stack features
        seq_features = torch.cat(seq_features, dim=0).unsqueeze(0)  # Add batch dimension
        
        # Process with LSTM
        with torch.no_grad():
            lstm_out, _ = self.lstm(seq_features)
            prediction = self.sequence_predictor(lstm_out[:, -1])
            
        return prediction


class MetaLearningNetwork(LearningNetwork):
    """Specialized network for meta-learning with strategy focus"""
    
    def __init__(
        self,
        input_dim: int = 64,
        hidden_dim: int = 128,
        output_dim: int = 32,
        strategy_dim: int = 10,
        developmental_level: float = 0.0
    ):
        super().__init__(input_dim, hidden_dim, output_dim, developmental_level)
        
        # Add strategy optimization network
        self.strategy_dim = strategy_dim
        self.strategy_optimizer = nn.Sequential(
            nn.Linear(output_dim + strategy_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, strategy_dim)
        ).to(self.device)
    
    def optimize_strategy(self, state: torch.Tensor, current_strategy: torch.Tensor) -> torch.Tensor:
        """Optimize a learning strategy for the current state"""
        # Ensure tensors are on the correct device
        state = state.to(self.device)
        current_strategy = current_strategy.to(self.device)
        
        # Extract features
        with torch.no_grad():
            output = self.forward(state)
            features = output["extracted_patterns"]
            
            # Combine features with current strategy
            combined = torch.cat((features, current_strategy), dim=-1)
            
            # Optimize strategy
            optimized_strategy = self.strategy_optimizer(combined)
            
        return optimized_strategy


#######################

#learning\procedural_learning.py#
#######################

import numpy as np
import torch
from typing import Dict, List, Any, Optional, Tuple, Set
from datetime import datetime
import uuid
import logging
import os
from collections import defaultdict

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.modules.learning.models import ProceduralLearningEvent

logger = logging.getLogger(__name__)

class ProceduralLearning(BaseModule):
    """
    Learning skills and procedures through practice
    
    This module develops procedural knowledge through repetition and practice,
    gradually improving performance on tasks and automating sequences of actions.
    """
    
    # Development milestones for procedural learning
    development_milestones = {
        0.0: "Simple action sequences",
        0.2: "Basic skill coordination",
        0.4: "Skill refinement through practice",
        0.6: "Efficient procedure optimization",
        0.8: "Automated procedural execution",
        1.0: "Complex skill integration"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the procedural learning module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level
        """
        super().__init__(
            module_id=module_id,
            module_type="procedural_learning",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Skills and procedures being learned
        # {skill_name: {proficiency, practice_count, steps, etc.}}
        self.skills = {}
        
        # Performance metrics for each skill
        self.performance_history = defaultdict(list)
        
        # Developmental parameters
        self.learning_rate = 0.1  # Base rate for skill improvement
        self.forgetting_rate = 0.01  # How quickly skills decay without practice
        self.automation_threshold = 0.8  # Proficiency level for automation
        
        # Adjust parameters based on development level
        self._adjust_for_development()
        
        # Subscribe to relevant events
        if self.event_bus:
            self.subscribe_to_message("skill_practice", self._handle_practice)
            self.subscribe_to_message("skill_performance", self._handle_performance)
    
    def _adjust_for_development(self):
        """Adjust learning mechanisms based on developmental level"""
        # Learning rate increases with development (faster skill acquisition)
        self.learning_rate = 0.1 + (self.development_level * 0.15)
        
        # Forgetting rate decreases with development (better retention)
        self.forgetting_rate = max(0.001, 0.02 - (self.development_level * 0.019))
        
        # Automation threshold decreases with development (easier automation)
        self.automation_threshold = max(0.5, 0.9 - (self.development_level * 0.4))
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input for procedural learning
        
        Args:
            input_data: Dictionary containing skill practice information
            
        Returns:
            Dictionary with updated skill proficiency and performance
        """
        operation = input_data.get("operation", "practice")
        
        if operation == "practice":
            return self._practice_skill(input_data)
        elif operation == "learn_sequence":
            return self._learn_sequence(input_data)
        elif operation == "recall_skill":
            return self._recall_skill(input_data)
        elif operation == "check_automation":
            return self._check_automation(input_data)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "module_id": self.module_id
            }
    
    def _practice_skill(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Practice an existing skill or create a new one"""
        skill_name = input_data.get("skill")
        practice_quality = input_data.get("quality", 0.5)  # How well the practice was performed
        practice_duration = input_data.get("duration", 1.0)  # Duration in minutes
        
        if not skill_name:
            return {"status": "error", "message": "Missing skill name"}
        
        # Get or create skill
        if skill_name not in self.skills:
            # Create new skill
            self.skills[skill_name] = {
                "proficiency": 0.1,  # Starting proficiency
                "practice_count": 0,
                "total_practice_time": 0.0,
                "last_practiced": datetime.now(),
                "automated": False,
                "steps": input_data.get("steps", []),
                "dependencies": input_data.get("dependencies", []),
                "created_at": datetime.now()
            }
        
        skill = self.skills[skill_name]
        
        # Calculate proficiency improvement based on practice quality and duration
        # Apply developmental learning rate and diminishing returns
        current_proficiency = skill["proficiency"]
        
        # Calculate practice effectiveness
        # Higher quality practice with longer duration is more effective
        # Diminishing returns as proficiency increases
        effectiveness = practice_quality * practice_duration * self.learning_rate
        
        # Apply diminishing returns (harder to improve as proficiency increases)
        room_for_improvement = 1.0 - current_proficiency
        improvement = effectiveness * room_for_improvement
        
        # Update skill data
        new_proficiency = min(1.0, current_proficiency + improvement)
        skill["proficiency"] = new_proficiency
        skill["practice_count"] += 1
        skill["total_practice_time"] += practice_duration
        skill["last_practiced"] = datetime.now()
        
        # Check if skill should now be automated
        if new_proficiency >= self.automation_threshold and not skill["automated"]:
            skill["automated"] = True
        
        # Update performance history
        self.performance_history[skill_name].append({
            "timestamp": datetime.now(),
            "proficiency": new_proficiency,
            "practice_quality": practice_quality,
            "improvement": improvement
        })
        
        # Trim history if needed
        if len(self.performance_history[skill_name]) > 100:
            self.performance_history[skill_name] = self.performance_history[skill_name][-100:]
        
        # Create learning event
        event = ProceduralLearningEvent(
            source=input_data.get("source", "practice"),
            content=f"Practice of skill '{skill_name}'",
            skill=skill_name,
            proficiency=new_proficiency,
            practice_count=skill["practice_count"],
            practice_time=practice_duration,
            learning_mode=input_data.get("learning_mode", "explicit"),
            procedure_steps=skill["steps"],
            developmental_level=self.development_level
        )
        
        return {
            "status": "success",
            "skill": skill_name,
            "previous_proficiency": current_proficiency,
            "new_proficiency": new_proficiency,
            "improvement": improvement,
            "practice_count": skill["practice_count"],
            "automated": skill["automated"],
            "learning_event_id": event.id
        }
    
    def _learn_sequence(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Learn a new sequence or procedure"""
        skill_name = input_data.get("skill")
        steps = input_data.get("steps", [])
        
        if not skill_name or not steps:
            return {"status": "error", "message": "Missing skill name or steps"}
        
        # Check if skill exists
        if skill_name in self.skills:
            # Update existing skill's steps
            self.skills[skill_name]["steps"] = steps
            action = "updated"
        else:
            # Create new skill with these steps
            self.skills[skill_name] = {
                "proficiency": 0.1,  # Starting proficiency
                "practice_count": 0,
                "total_practice_time": 0.0,
                "last_practiced": datetime.now(),
                "automated": False,
                "steps": steps,
                "dependencies": input_data.get("dependencies", []),
                "created_at": datetime.now()
            }
            action = "created"
        
        # Create learning event
        event = ProceduralLearningEvent(
            source=input_data.get("source", "instruction"),
            content=f"Learning sequence for skill '{skill_name}'",
            skill=skill_name,
            proficiency=self.skills[skill_name]["proficiency"],
            practice_count=self.skills[skill_name]["practice_count"],
            practice_time=0.0,
            learning_mode=input_data.get("learning_mode", "explicit"),
            procedure_steps=steps,
            developmental_level=self.development_level
        )
        
        return {
            "status": "success",
            "skill": skill_name,
            "action": action,
            "step_count": len(steps),
            "proficiency": self.skills[skill_name]["proficiency"],
            "learning_event_id": event.id
        }
    
    def _recall_skill(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Recall a learned skill and its steps"""
        skill_name = input_data.get("skill")
        
        if not skill_name:
            return {"status": "error", "message": "Missing skill name"}
        
        if skill_name not in self.skills:
            return {"status": "not_found", "message": f"Skill not found: {skill_name}"}
        
        skill = self.skills[skill_name]
        
        # Apply forgetting based on time since last practice
        if "last_practiced" in skill:
            time_since_practice = (datetime.now() - skill["last_practiced"]).total_seconds() / 86400.0  # days
            forgetting = self.forgetting_rate * time_since_practice
            
            # More automated skills are forgotten more slowly
            if skill["automated"]:
                forgetting *= 0.2
            
            # Apply forgetting
            skill["proficiency"] = max(0.1, skill["proficiency"] - forgetting)
        
        # Calculate recall quality based on proficiency
        # Add some randomness to simulate variability in recall
        recall_noise = np.random.normal(0, 0.1)  # Mean 0, std 0.1
        recall_quality = min(1.0, max(0.0, skill["proficiency"] + recall_noise))
        
        # Determine which steps are recalled correctly
        recalled_steps = []
        missed_steps = []
        
        for i, step in enumerate(skill["steps"]):
            # Higher proficiency means better recall
            # Steps are easier to forget as sequence length increases
            step_recall_prob = recall_quality * (1.0 - 0.01 * i)
            
            if np.random.random() < step_recall_prob:
                recalled_steps.append(step)
            else:
                missed_steps.append(step)
        
        # Update last practiced timestamp (recall is a form of practice)
        skill["last_practiced"] = datetime.now()
        
        return {
            "status": "success",
            "skill": skill_name,
            "proficiency": skill["proficiency"],
            "recall_quality": recall_quality,
            "recalled_steps": recalled_steps,
            "missed_steps": missed_steps,
            "total_steps": len(skill["steps"]),
            "recall_success_rate": len(recalled_steps) / max(1, len(skill["steps"])),
            "automated": skill["automated"]
        }
    
    def _check_automation(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Check if a skill is automated and can be performed without conscious effort"""
        skill_name = input_data.get("skill")
        
        if not skill_name:
            return {"status": "error", "message": "Missing skill name"}
        
        if skill_name not in self.skills:
            return {"status": "not_found", "message": f"Skill not found: {skill_name}"}
        
        skill = self.skills[skill_name]
        
        # A skill is considered automated if:
        # 1. Proficiency is above automation threshold
        # 2. It has been practiced sufficiently
        # 3. Time since last practice isn't too long
        
        proficiency_check = skill["proficiency"] >= self.automation_threshold
        practice_check = skill["practice_count"] >= 5 + (len(skill["steps"]) * 2)
        
        recency_check = True
        if "last_practiced" in skill:
            days_since_practice = (datetime.now() - skill["last_practiced"]).total_seconds() / 86400.0
            recency_check = days_since_practice < (7.0 + (skill["proficiency"] * 30.0))
        
        # Update automation status
        is_automated = proficiency_check and practice_check and recency_check
        skill["automated"] = is_automated
        
        # Calculate cognitive load reduction from automation
        if is_automated:
            # More proficient = less cognitive load
            cognitive_load = max(0.1, 1.0 - skill["proficiency"])
        else:
            # Non-automated skills have high cognitive load
            cognitive_load = 0.5 + (0.5 * (1.0 - skill["proficiency"]))
        
        return {
            "status": "success",
            "skill": skill_name,
            "automated": is_automated,
            "proficiency": skill["proficiency"],
            "practice_count": skill["practice_count"],
            "cognitive_load": cognitive_load,
            "automation_checks": {
                "proficiency_sufficient": proficiency_check,
                "practice_sufficient": practice_check,
                "recency_sufficient": recency_check
            }
        }
    
    def _handle_practice(self, message):
        """Handle skill practice events"""
        if not message.content:
            return
            
        practice_data = message.content
        
        # Process the practice event
        if "skill" in practice_data:
            self._practice_skill(practice_data)
    
    def _handle_performance(self, message):
        """Handle skill performance feedback"""
        if not message.content:
            return
            
        performance_data = message.content
        
        # Process the performance data
        if "skill" in performance_data and "quality" in performance_data:
            # Use performance quality to adjust proficiency
            self._practice_skill({
                "skill": performance_data["skill"],
                "quality": performance_data["quality"],
                "duration": performance_data.get("duration", 0.5),
                "source": "performance"
            })
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        previous_level = self.development_level
        new_level = super().update_development(amount)
        
        # If development changed significantly, adjust parameters
        if abs(new_level - previous_level) >= 0.05:
            self._adjust_for_development()
            
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """Get the current state of the module"""
        base_state = super().get_state()
        
        # Calculate skill statistics
        skill_count = len(self.skills)
        automated_count = sum(1 for skill in self.skills.values() if skill.get("automated", False))
        avg_proficiency = 0.0
        if skill_count > 0:
            avg_proficiency = sum(skill["proficiency"] for skill in self.skills.values()) / skill_count
        
        # Add procedural learning specific state
        module_state = {
            "skill_count": skill_count,
            "automated_skills": automated_count,
            "average_proficiency": avg_proficiency,
            "learning_rate": self.learning_rate,
            "forgetting_rate": self.forgetting_rate,
            "automation_threshold": self.automation_threshold
        }
        
        base_state.update(module_state)
        return base_state


#######################

#learning\reinforcement_learning.py#
#######################

import numpy as np
import torch
import torch.nn.functional as F
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import uuid
import logging
import os
import random
from collections import deque

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.modules.learning.models import ReinforcementLearningEvent

logger = logging.getLogger(__name__)

class ReinforcementLearning(BaseModule):
    """
    Learning from rewards and punishments
    
    This module implements reinforcement learning mechanisms that allow the system
    to learn which actions lead to positive outcomes in different contexts.
    """
    
    # Development milestones for reinforcement learning
    development_milestones = {
        0.0: "Basic reward-based learning",
        0.2: "Delayed reward processing",
        0.4: "Context-sensitive reinforcement",
        0.6: "Value-based decision making",
        0.8: "Complex reward integration",
        1.0: "Abstract goal-directed behavior"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the reinforcement learning module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level
        """
        super().__init__(
            module_id=module_id,
            module_type="reinforcement_learning",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Q-values for action-state pairs
        self.q_values = {}
        
        # Policy mapping states to action probabilities
        self.policy = {}
        
        # Experience replay buffer (only active at higher developmental levels)
        self.experience_buffer = deque(maxlen=100)
        
        # Developmental parameters
        self.learning_rate = 0.1
        self.discount_factor = 0.7  # How much future rewards matter
        self.exploration_rate = 0.3  # Probability of random exploration
        
        # Recent action history
        self.action_history = []
        self.max_history = 20
        
        # Adjust parameters based on development level
        self._adjust_for_development()
        
        # Subscribe to relevant events
        if self.event_bus:
            self.subscribe_to_message("reward_signal", self._handle_reward)
            self.subscribe_to_message("action_performed", self._handle_action)
    
    def _adjust_for_development(self):
        """Adjust learning mechanisms based on developmental level"""
        # Learning rate decreases with development (more stable learning)
        self.learning_rate = max(0.05, 0.3 - (self.development_level * 0.25))
        
        # Discount factor increases with development (more future-oriented)
        self.discount_factor = min(0.95, 0.6 + (self.development_level * 0.35))
        
        # Exploration decreases with development (more exploitation)
        self.exploration_rate = max(0.05, 0.5 - (self.development_level * 0.45))
        
        # Experience buffer size increases with development
        self.experience_buffer = deque(maxlen=max(50, int(100 + (self.development_level * 400))))
        
        # History tracking increases with development
        self.max_history = max(10, int(20 + (self.development_level * 80)))
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process reinforcement learning operations
        
        Args:
            input_data: Dictionary containing operation and parameters
            
        Returns:
            Dictionary with operation results
        """
        operation = input_data.get("operation", "learn")
        
        if operation == "learn":
            return self._learn_from_experience(input_data)
        elif operation == "select_action":
            return self._select_action(input_data)
        elif operation == "update_policy":
            return self._update_policy(input_data)
        else:
            return {
                "status": "error",
                "message": f"Unknown operation: {operation}",
                "module_id": self.module_id
            }
    
    def _learn_from_experience(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Learn from a reinforcement experience"""
        # Extract required parameters
        state = input_data.get("state")
        action = input_data.get("action")
        reward = input_data.get("reward", 0.0)
        next_state = input_data.get("next_state")
        
        if not state or not action:
            return {"status": "error", "message": "Missing state or action"}
        
        # Initialize Q-values if needed
        if state not in self.q_values:
            self.q_values[state] = {}
        if action not in self.q_values[state]:
            self.q_values[state][action] = 0.0
        
        # Calculate Q-value update
        current_q = self.q_values[state][action]
        
        if next_state:
            # Use Q-learning formula if we have next state
            # Q(s,a) = Q(s,a) + α * (r + γ * max(Q(s',a')) - Q(s,a))
            max_next_q = self._get_max_q_value(next_state)
            new_q = current_q + self.learning_rate * (
                reward + self.discount_factor * max_next_q - current_q
            )
        else:
            # Simple update if no next state (terminal state)
            # Q(s,a) = Q(s,a) + α * (r - Q(s,a))
            new_q = current_q + self.learning_rate * (reward - current_q)
        
        # Update Q-value
        self.q_values[state][action] = new_q
        
        # Add to experience buffer (for experience replay)
        if self.development_level >= 0.3 and next_state:
            experience = {
                "state": state,
                "action": action,
                "reward": reward,
                "next_state": next_state,
                "timestamp": datetime.now()
            }
            self.experience_buffer.append(experience)
        
        # Create learning event
        event = ReinforcementLearningEvent(
            source=input_data.get("source", "experience"),
            content=f"Reinforcement learning for action '{action}' in state '{state}'",
            action=action,
            consequence=input_data.get("consequence", "reward" if reward > 0 else "punishment"),
            reward_value=reward,
            delay=input_data.get("delay", 0.0),
            context=state,
            reinforcement_type="positive" if reward > 0 else "negative" if reward == 0 else "punishment",
            developmental_level=self.development_level
        )
        
        # Update policy based on new Q-values
        self._update_state_policy(state)
        
        # Perform experience replay if development level is high enough
        replay_results = None
        if self.development_level >= 0.4 and len(self.experience_buffer) >= 10:
            replay_results = self._perform_experience_replay(input_data.get("replay_batch_size", 5))
        
        return {
            "status": "success",
            "state": state,
            "action": action,
            "previous_q": current_q,
            "updated_q": new_q,
            "reward": reward,
            "learning_event_id": event.id,
            "replay_performed": replay_results is not None,
            "replay_results": replay_results
        }
    
    def _select_action(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Select an action based on current state and policy"""
        state = input_data.get("state")
        available_actions = input_data.get("available_actions", [])
        
        if not state:
            return {"status": "error", "message": "Missing state parameter"}
        
        # If we don't have this state in our policy, initialize it
        if state not in self.policy:
            self._initialize_state_policy(state, available_actions)
        
        # If we need to update available actions
        elif available_actions and not all(action in self.policy[state] for action in available_actions):
            self._update_state_policy(state, available_actions)
        
        # Apply exploration-exploitation tradeoff
        if random.random() < self.exploration_rate:
            # Exploration: select random action
            actions = list(self.policy[state].keys())
            selected_action = random.choice(actions)
            selection_type = "exploration"
        else:
            # Exploitation: select best action
            selected_action = self._get_best_action(state)
            selection_type = "exploitation"
        
        # Record this action in history
        self._record_action(state, selected_action)
        
        return {
            "status": "success",
            "state": state,
            "selected_action": selected_action,
            "selection_type": selection_type,
            "action_probability": self.policy[state].get(selected_action, 0.0),
            "exploration_rate": self.exploration_rate
        }
    
    def _update_policy(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Explicitly update the policy for a state"""
        state = input_data.get("state")
        action_probs = input_data.get("action_probabilities", {})
        
        if not state or not action_probs:
            return {"status": "error", "message": "Missing state or action probabilities"}
        
        # Update policy for this state
        if state not in self.policy:
            self.policy[state] = {}
        
        # Integrate new probabilities
        for action, prob in action_probs.items():
            self.policy[state][action] = prob
        
        # Normalize probabilities
        self._normalize_policy(state)
        
        return {
            "status": "success",
            "state": state,
            "updated_policy": self.policy[state]
        }
    
    def _get_max_q_value(self, state: str) -> float:
        """Get the maximum Q-value for a state"""
        if state not in self.q_values or not self.q_values[state]:
            return 0.0
        
        return max(self.q_values[state].values())
    
    def _get_best_action(self, state: str) -> str:
        """Get the action with highest probability in the policy"""
        if state not in self.policy or not self.policy[state]:
            return ""
        
        return max(self.policy[state].items(), key=lambda x: x[1])[0]
    
    def _initialize_state_policy(self, state: str, available_actions: List[str] = None):
        """Initialize policy for a new state"""
        self.policy[state] = {}
        
        if not available_actions:
            # If no actions provided, check if we have Q-values for this state
            if state in self.q_values:
                available_actions = list(self.q_values[state].keys())
            else:
                return
        
        # Initialize with uniform distribution
        prob = 1.0 / len(available_actions)
        for action in available_actions:
            self.policy[state][action] = prob
    
    def _update_state_policy(self, state: str, available_actions: List[str] = None):
        """Update policy for a state based on Q-values"""
        if state not in self.q_values:
            if available_actions:
                self._initialize_state_policy(state, available_actions)
            return
        
        # Get actions from Q-values if not provided
        if not available_actions:
            available_actions = list(self.q_values[state].keys())
        
        # Make sure all available actions have Q-values
        for action in available_actions:
            if action not in self.q_values[state]:
                self.q_values[state][action] = 0.0
        
        # Using Softmax policy: P(a|s) = exp(Q(s,a)/τ) / Σ exp(Q(s,a')/τ)
        # Where τ is temperature (higher = more exploration)
        temperature = max(0.1, 1.0 - self.development_level * 0.8)
        
        # Initialize or clear existing policy
        if state not in self.policy:
            self.policy[state] = {}
        
        # Calculate denominator (sum of exp(Q/τ) for all actions)
        exp_values = [np.exp(self.q_values[state][a] / temperature) for a in available_actions]
        sum_exp = sum(exp_values)
        
        # Calculate probabilities
        if sum_exp > 0:
            for i, action in enumerate(available_actions):
                self.policy[state][action] = exp_values[i] / sum_exp
        else:
            # Fallback to uniform if numerical issues
            prob = 1.0 / len(available_actions)
            for action in available_actions:
                self.policy[state][action] = prob
    
    def _normalize_policy(self, state: str):
        """Ensure policy probabilities sum to 1.0"""
        if state not in self.policy or not self.policy[state]:
            return
        
        total = sum(self.policy[state].values())
        if total <= 0:
            # Reset to uniform if invalid probabilities
            prob = 1.0 / len(self.policy[state])
            for action in self.policy[state]:
                self.policy[state][action] = prob
        elif total != 1.0:
            # Normalize
            for action in self.policy[state]:
                self.policy[state][action] /= total
    
    def _record_action(self, state: str, action: str):
        """Record an action in the history"""
        self.action_history.append({
            "state": state,
            "action": action,
            "timestamp": datetime.now()
        })
        
        # Trim history if needed
        if len(self.action_history) > self.max_history:
            self.action_history.pop(0)
    
    def _perform_experience_replay(self, batch_size: int = 5) -> Dict[str, Any]:
        """Perform experience replay to improve learning"""
        if len(self.experience_buffer) < batch_size:
            return None
        
        # Sample random experiences
        samples = random.sample(list(self.experience_buffer), batch_size)
        
        results = []
        for exp in samples:
            # Apply Q-learning update to each sample
            state = exp["state"]
            action = exp["action"]
            reward = exp["reward"]
            next_state = exp["next_state"]
            
            if state not in self.q_values:
                self.q_values[state] = {}
            if action not in self.q_values[state]:
                self.q_values[state][action] = 0.0
            
            current_q = self.q_values[state][action]
            max_next_q = self._get_max_q_value(next_state)
            
            # Apply discount factor based on time elapsed since experience
            time_factor = 1.0
            if self.development_level >= 0.7:
                # More developed minds can adjust based on recency
                time_delta = (datetime.now() - exp["timestamp"]).total_seconds()
                time_factor = np.exp(-0.001 * time_delta)  # Exponential decay
            
            # Q-learning update
            new_q = current_q + self.learning_rate * time_factor * (
                reward + self.discount_factor * max_next_q - current_q
            )
            
            self.q_values[state][action] = new_q
            
            # Track result
            results.append({
                "state": state,
                "action": action,
                "previous_q": current_q,
                "updated_q": new_q,
                "q_change": new_q - current_q
            })
            
            # Update policy for this state
            self._update_state_policy(state)
        
        return {
            "samples_processed": len(results),
            "updates": results
        }
    
    def _handle_reward(self, message):
        """Handle incoming reward signals"""
        if not message.content:
            return
            
        reward_data = message.content
        
        # Check if we have required fields
        if "state" in reward_data and "action" in reward_data and "reward" in reward_data:
            # Process the reward signal
            self._learn_from_experience(reward_data)
    
    def _handle_action(self, message):
        """Handle action performance messages"""
        if not message.content:
            return
            
        action_data = message.content
        
        # Record this action
        if "state" in action_data and "action" in action_data:
            self._record_action(action_data["state"], action_data["action"])
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        previous_level = self.development_level
        new_level = super().update_development(amount)
        
        # If development changed significantly, adjust parameters
        if abs(new_level - previous_level) >= 0.05:
            self._adjust_for_development()
            
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """Get the current state of the module"""
        base_state = super().get_state()
        
        # Add reinforcement learning specific state
        module_state = {
            "learning_rate": self.learning_rate,
            "discount_factor": self.discount_factor,
            "exploration_rate": self.exploration_rate,
            "states_learned": len(self.q_values),
            "experience_buffer_size": len(self.experience_buffer),
            "action_history_size": len(self.action_history)
        }
        
        base_state.update(module_state)
        return base_state


#######################

#learning\__init__.py#
#######################

"""
Learning Module

This module is responsible for different learning mechanisms,
knowledge acquisition, and skill development. It integrates
multiple learning approaches to enable the mind to learn from
experiences and adapt its behavior.
"""

import logging
from typing import Dict, List, Any, Optional, Union
from datetime import datetime

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message

from lmm_project.modules.learning.associative_learning import AssociativeLearning
from lmm_project.modules.learning.reinforcement_learning import ReinforcementLearning
from lmm_project.modules.learning.procedural_learning import ProceduralLearning
from lmm_project.modules.learning.meta_learning import MetaLearning

logger = logging.getLogger(__name__)

def get_module(
    module_id: str = "learning",
    event_bus: Optional[EventBus] = None,
    development_level: float = 0.0
) -> "LearningSystem":
    """
    Factory function to create a learning module
    
    This function is responsible for creating a learning system that can:
    - Acquire new knowledge through various learning mechanisms
    - Develop skills through practice and experience
    - Adapt learning strategies based on context and results
    - Integrate different types of learning for optimal knowledge acquisition
    - Monitor and regulate the learning process itself
    
    Args:
        module_id: Unique identifier for the module
        event_bus: Event bus for communication with other modules
        development_level: Initial developmental level for the system
        
    Returns:
        An instance of the LearningSystem class
    """
    return LearningSystem(
        module_id=module_id,
        event_bus=event_bus,
        development_level=development_level
    )

class LearningSystem(BaseModule):
    """
    Integrated learning system with multiple learning mechanisms
    
    The learning system develops from simple associative learning in early stages
    to complex integrated learning approaches in later stages.
    """
    
    # Development milestones for learning
    development_milestones = {
        0.0: "Basic associative learning",
        0.2: "Simple reinforcement learning",
        0.4: "Procedural skill acquisition",
        0.6: "Multi-modal learning integration",
        0.8: "Strategic learning optimization",
        1.0: "Advanced meta-learning capabilities"
    }
    
    def __init__(
        self,
        module_id: str,
        event_bus: Optional[EventBus] = None,
        development_level: float = 0.0
    ):
        """
        Initialize the learning system
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level
        """
        super().__init__(
            module_id=module_id,
            module_type="learning",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Create learning sub-modules
        self.associative_learning = AssociativeLearning(
            module_id=f"{module_id}_associative",
            event_bus=event_bus,
            development_level=development_level
        )
        
        self.reinforcement_learning = ReinforcementLearning(
            module_id=f"{module_id}_reinforcement",
            event_bus=event_bus,
            development_level=development_level
        )
        
        self.procedural_learning = ProceduralLearning(
            module_id=f"{module_id}_procedural",
            event_bus=event_bus,
            development_level=development_level
        )
        
        self.meta_learning = MetaLearning(
            module_id=f"{module_id}_meta",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Track learning events and outcomes
        self.learning_events = []
        self.max_events = 100
        
        # Map learning operations to their handlers
        self.operation_handlers = {
            "associative": self._handle_associative_learning,
            "reinforcement": self._handle_reinforcement_learning,
            "procedural": self._handle_procedural_learning,
            "meta": self._handle_meta_learning,
            "integrate": self._handle_integrated_learning
        }
        
        # Subscribe to relevant events
        if self.event_bus:
            self.subscribe_to_message("perception_input", self._handle_perception)
            self.subscribe_to_message("memory_retrieval", self._handle_memory_retrieval)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input for learning
        
        Args:
            input_data: Dictionary containing learning parameters and data
            
        Returns:
            Dictionary with learning results
        """
        # Extract learning type
        learning_type = input_data.get("learning_type", "associative")
        
        # Get appropriate handler for this learning type
        handler = self.operation_handlers.get(learning_type)
        
        if not handler:
            return {
                "status": "error",
                "message": f"Unknown learning type: {learning_type}",
                "module_id": self.module_id
            }
        
        # Process with the appropriate learning mechanism
        result = handler(input_data)
        
        # Record learning event if successful
        if result.get("status") == "success":
            self._record_learning_event(learning_type, input_data, result)
        
        return result
    
    def _handle_associative_learning(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle associative learning operations"""
        result = self.associative_learning.process_input(input_data)
        return result
    
    def _handle_reinforcement_learning(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle reinforcement learning operations"""
        result = self.reinforcement_learning.process_input(input_data)
        return result
    
    def _handle_procedural_learning(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle procedural learning operations"""
        result = self.procedural_learning.process_input(input_data)
        return result
    
    def _handle_meta_learning(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Handle meta-learning operations"""
        result = self.meta_learning.process_input(input_data)
        return result
    
    def _handle_integrated_learning(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Handle integrated learning that combines multiple approaches
        
        This function becomes more effective at higher developmental levels
        """
        # At lower developmental levels, just use the primary learning type
        if self.development_level < 0.5:
            primary_type = input_data.get("primary_type", "associative")
            handler = self.operation_handlers.get(primary_type)
            
            if not handler:
                return {
                    "status": "error",
                    "message": f"Unknown primary learning type: {primary_type}",
                    "module_id": self.module_id
                }
                
            return handler(input_data)
        
        # At higher developmental levels, integrate multiple learning types
        results = {}
        learning_types = input_data.get("learning_types", ["associative", "reinforcement"])
        
        for learning_type in learning_types:
            handler = self.operation_handlers.get(learning_type)
            if handler and learning_type != "integrate":  # Avoid recursion
                # Create type-specific input by copying and updating
                type_input = input_data.copy()
                type_input["learning_type"] = learning_type
                
                # Process with this learning type
                result = handler(type_input)
                results[learning_type] = result
        
        # Get learning strategy if development is high enough
        learning_strategy = None
        if self.development_level >= 0.7:
            strategy_input = {
                "domain": input_data.get("domain", "general"),
                "content_type": input_data.get("content_type", "general"),
                "operation": "select_strategy"
            }
            strategy_result = self.meta_learning.process_input(strategy_input)
            if strategy_result.get("status") == "success":
                learning_strategy = strategy_result.get("selected_strategy")
        
        return {
            "status": "success",
            "integrated_results": results,
            "learning_strategy": learning_strategy,
            "integration_level": min(1.0, self.development_level * 1.2)  # Higher dev = better integration
        }
    
    def _handle_perception(self, message: Message):
        """Handle perception inputs for learning opportunities"""
        if not message.content:
            return
            
        perception_data = message.content
        
        # Only process if perception has pattern information
        if "pattern" in perception_data:
            pattern = perception_data.get("pattern")
            salience = perception_data.get("salience", 0.5)
            
            # Only learn from salient perceptions
            if salience >= 0.3:
                # Simple associative learning from perception
                if "previous" in perception_data and "pattern" in perception_data["previous"]:
                    previous_pattern = perception_data["previous"]["pattern"]
                    
                    # Learn association between consecutive patterns
                    self.associative_learning.process_input({
                        "operation": "learn",
                        "stimulus": previous_pattern,
                        "response": pattern,
                        "strength": salience,
                        "source": "perception"
                    })
    
    def _handle_memory_retrieval(self, message: Message):
        """Handle memory retrievals for learning enhancement"""
        if not message.content:
            return
            
        memory_data = message.content
        
        # Use retrieved memories to enhance learning
        if "memory" in memory_data and "retrieval_context" in memory_data:
            memory = memory_data["memory"]
            context = memory_data["retrieval_context"]
            
            # If memory retrieval was for learning purposes, update meta-learning
            if "learning" in context or "strategy" in context:
                # Update strategy effectiveness if applicable
                if "strategy_id" in context and "success_level" in memory:
                    self.meta_learning.process_input({
                        "operation": "evaluate_outcome",
                        "strategy_id": context["strategy_id"],
                        "success_level": memory.get("success_level", 0.5),
                        "domain": context.get("domain", "general")
                    })
    
    def _record_learning_event(self, learning_type: str, input_data: Dict[str, Any], result: Dict[str, Any]):
        """Record a learning event for future reference"""
        event = {
            "timestamp": datetime.now(),
            "learning_type": learning_type,
            "content": input_data.get("content", ""),
            "domain": input_data.get("domain", "general"),
            "success": result.get("status") == "success",
            "developmental_level": self.development_level
        }
        
        self.learning_events.append(event)
        
        # Trim history if needed
        if len(self.learning_events) > self.max_events:
            self.learning_events = self.learning_events[-self.max_events:]
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module and all sub-modules
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        previous_level = self.development_level
        new_level = super().update_development(amount)
        
        # Update sub-modules with appropriate amounts
        # Different learning mechanisms develop at slightly different rates
        self.associative_learning.update_development(amount * 1.1)  # Develops slightly faster
        self.reinforcement_learning.update_development(amount * 1.0)
        self.procedural_learning.update_development(amount * 0.9)
        
        # Meta-learning develops more slowly until later stages
        meta_modifier = 0.7 if self.development_level < 0.5 else 1.2
        self.meta_learning.update_development(amount * meta_modifier)
        
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """Get the current state of the learning system"""
        base_state = super().get_state()
        
        # Get state from all sub-modules
        module_state = {
            "associative_learning": self.associative_learning.get_state(),
            "reinforcement_learning": self.reinforcement_learning.get_state(),
            "procedural_learning": self.procedural_learning.get_state(),
            "meta_learning": self.meta_learning.get_state(),
            "learning_events_count": len(self.learning_events),
            "recent_learning_types": [e["learning_type"] for e in self.learning_events[-5:]] if self.learning_events else []
        }
        
        base_state.update(module_state)
        return base_state
        
    def save_state(self) -> Dict[str, Any]:
        """
        Save the current state of the learning system
        
        Returns:
            Dictionary containing the serialized state
        """
        # Get base state
        state = self.get_state()
        
        # Add full learning events history (limited to max events)
        state["learning_events"] = self.learning_events
        
        # Save states of all submodules
        state["associative_learning_full"] = self.associative_learning.save_state()
        state["reinforcement_learning_full"] = self.reinforcement_learning.save_state()
        state["procedural_learning_full"] = self.procedural_learning.save_state()
        state["meta_learning_full"] = self.meta_learning.save_state()
        
        # Add operation handlers mapping (just names, not functions)
        state["operations"] = list(self.operation_handlers.keys())
        
        # Add timestamp
        state["saved_at"] = datetime.now().isoformat()
        
        return state
        
    def load_state(self, state: Dict[str, Any]) -> None:
        """
        Load a previously saved state
        
        Args:
            state: Dictionary containing the state to load
        """
        # Load development level first
        if "development_level" in state:
            self.development_level = state["development_level"]
            
        # Load learning events history
        if "learning_events" in state:
            self.learning_events = state["learning_events"]
            # Ensure we don't exceed max events
            if len(self.learning_events) > self.max_events:
                self.learning_events = self.learning_events[-self.max_events:]
        
        # Load states for all submodules
        if "associative_learning_full" in state:
            self.associative_learning.load_state(state["associative_learning_full"])
            
        if "reinforcement_learning_full" in state:
            self.reinforcement_learning.load_state(state["reinforcement_learning_full"])
            
        if "procedural_learning_full" in state:
            self.procedural_learning.load_state(state["procedural_learning_full"])
            
        if "meta_learning_full" in state:
            self.meta_learning.load_state(state["meta_learning_full"])
            
        # Re-register event handlers if event bus exists
        if self.event_bus:
            self.subscribe_to_message("perception_input", self._handle_perception)
            self.subscribe_to_message("memory_retrieval", self._handle_memory_retrieval)
            
        logger.info(f"Loaded learning system state with {len(self.learning_events)} learning events")
        
    def subscribe_to_message(self, message_type: str, callback: callable) -> None:
        """
        Subscribe to a message type on the event bus
        
        Args:
            message_type: Type of message to subscribe to
            callback: Function to call when a message is received
        """
        if self.event_bus:
            self.event_bus.subscribe(message_type, callback)
            
    def publish_message(self, message_type: str, content: Any) -> None:
        """
        Publish a message to the event bus
        
        Args:
            message_type: Type of message to publish
            content: Content of the message
        """
        if self.event_bus:
            message = Message(
                sender=self.module_id,
                message_type=message_type,
                content=content
            )
            self.event_bus.publish(message)
            
    def get_learning_stats(self) -> Dict[str, Any]:
        """
        Get statistics about learning progress
        
        Returns:
            Dictionary with learning statistics
        """
        # Count learning events by type
        event_counts = {}
        for event in self.learning_events:
            learning_type = event.get("learning_type", "unknown")
            if learning_type not in event_counts:
                event_counts[learning_type] = 0
            event_counts[learning_type] += 1
            
        # Calculate success rates
        success_rates = {}
        for learning_type, count in event_counts.items():
            successes = sum(1 for e in self.learning_events if e.get("learning_type") == learning_type and e.get("success", False))
            success_rates[learning_type] = successes / count if count > 0 else 0
            
        # Get development levels across components
        development_levels = {
            "associative": self.associative_learning.development_level,
            "reinforcement": self.reinforcement_learning.development_level,
            "procedural": self.procedural_learning.development_level,
            "meta": self.meta_learning.development_level,
            "overall": self.development_level
        }
        
        return {
            "event_counts": event_counts,
            "success_rates": success_rates,
            "development_levels": development_levels,
            "total_events": len(self.learning_events),
            "milestone": self.get_current_milestone()
        }
        
    def get_current_milestone(self) -> str:
        """Get the current developmental milestone for learning"""
        for level in sorted(self.development_milestones.keys(), reverse=True):
            if self.development_level >= level:
                return self.development_milestones[level]
        return self.development_milestones[0.0]


#######################

#memory\associative_memory.py#
#######################

from typing import Dict, List, Any, Optional, Tuple, Union, Set
from pydantic import BaseModel, Field
from datetime import datetime
import numpy as np
import uuid
import os
import json
from pathlib import Path

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.memory.models import Memory, AssociativeLink
from lmm_project.utils.vector_store import VectorStore

class AssociativeMemoryModule(BaseModule):
    """
    Associative memory system for linking memories
    
    Associative memory creates and manages connections between different 
    memories, allowing for the spread of activation and emergent 
    associations between concepts, episodes, and other mental content.
    """
    # Association storage
    associations: Dict[str, AssociativeLink] = Field(default_factory=dict)
    # Memory source index (memory_id -> list of association_ids where memory is source)
    source_index: Dict[str, Set[str]] = Field(default_factory=dict)
    # Memory target index (memory_id -> list of association_ids where memory is target)
    target_index: Dict[str, Set[str]] = Field(default_factory=dict)
    # Association types index (type -> list of association_ids)
    type_index: Dict[str, Set[str]] = Field(default_factory=dict)
    # Hebbian learning rate (how quickly associations strengthen)
    hebbian_rate: float = Field(default=0.01)
    # Association decay rate (how quickly associations weaken when unused)
    decay_rate: float = Field(default=0.001)
    # Storage directory
    storage_dir: str = Field(default="storage/memories/associations")
    
    model_config = {
        "arbitrary_types_allowed": True
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, **data):
        """Initialize associative memory module"""
        super().__init__(
            module_id=module_id,
            module_type="associative_memory",
            event_bus=event_bus,
            **data
        )
        
        # Create storage directory
        os.makedirs(self.storage_dir, exist_ok=True)
        
        # Try to load previous associations
        self._load_associations()
        
        # Subscribe to relevant events
        if self.event_bus:
            self.subscribe_to_message("memory_stored", self._handle_memory_stored)
            self.subscribe_to_message("memory_retrieved", self._handle_memory_retrieved)
            self.subscribe_to_message("concept_added", self._handle_concept_added)
            self.subscribe_to_message("episode_added", self._handle_episode_added)
            self.subscribe_to_message("association_query", self._handle_association_query)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process associative memory operations
        
        Parameters:
        input_data: Dictionary containing operation data
            - operation: The operation to perform (associate, get_associations,
                         spread_activation, find_path, etc.)
            - Additional parameters depend on the operation
            
        Returns:
        Dictionary containing operation results
        """
        operation = input_data.get("operation", "")
        
        if operation == "associate":
            source_id = input_data.get("source_id", "")
            target_id = input_data.get("target_id", "")
            link_type = input_data.get("link_type", "general")
            strength = input_data.get("strength", 0.5)
            return self.associate(source_id, target_id, link_type, strength)
        
        elif operation == "get_associations":
            memory_id = input_data.get("memory_id", "")
            return self.get_associations(memory_id)
        
        elif operation == "spread_activation":
            source_id = input_data.get("source_id", "")
            activation = input_data.get("activation", 0.5)
            depth = input_data.get("depth", 2)
            return self.spread_activation(source_id, activation, depth)
        
        elif operation == "find_path":
            source_id = input_data.get("source_id", "")
            target_id = input_data.get("target_id", "")
            return self.find_path(source_id, target_id)
        
        elif operation == "get_association_by_type":
            link_type = input_data.get("link_type", "")
            limit = input_data.get("limit", 10)
            return self.get_associations_by_type(link_type, limit)
            
        return {"status": "error", "message": f"Unknown operation: {operation}"}
    
    def update_development(self, amount: float) -> float:
        """
        Update associative memory's developmental level
        
        As associative memory develops:
        - Association formation becomes more sophisticated
        - Connections become more stable
        - Pattern recognition improves
        
        Parameters:
        amount: Amount to increase development level
        
        Returns:
        New development level
        """
        prev_level = self.development_level
        self.development_level = min(1.0, self.development_level + amount)
        
        # Update parameters based on development
        delta = self.development_level - prev_level
        
        # Improve hebbian learning rate
        hebbian_increase = delta * 0.01
        self.hebbian_rate = min(0.1, self.hebbian_rate + hebbian_increase)
        
        # Decrease decay rate (associations become more stable)
        decay_decrease = delta * 0.0005
        self.decay_rate = max(0.0001, self.decay_rate - decay_decrease)
        
        return self.development_level
    
    def associate(
        self, 
        source_id: str, 
        target_id: str, 
        link_type: str = "general", 
        strength: float = 0.5
    ) -> Dict[str, Any]:
        """
        Create or strengthen an association between two memories
        
        Parameters:
        source_id: Source memory ID
        target_id: Target memory ID
        link_type: Type of association
        strength: Initial association strength (0.0-1.0)
        
        Returns:
        Operation result
        """
        # Check if source and target are different
        if source_id == target_id:
            return {"status": "error", "message": "Cannot associate a memory with itself"}
        
        # Check if association already exists
        existing_association = self._find_association(source_id, target_id)
        
        if existing_association:
            # Strengthen existing association
            association = self.associations[existing_association]
            old_strength = association.strength
            association.update_strength(self.hebbian_rate)
            association.activation_count += 1
            
            # Save association
            self._save_association(association)
            
            # Publish event
            self.publish_message("association_strengthened", {
                "association_id": existing_association,
                "source_id": source_id,
                "target_id": target_id,
                "old_strength": old_strength,
                "new_strength": association.strength
            })
            
            return {
                "status": "success",
                "association_id": existing_association,
                "operation": "strengthened",
                "old_strength": old_strength,
                "new_strength": association.strength
            }
        else:
            # Create new association
            association_id = f"assoc_{uuid.uuid4().hex[:8]}"
            
            association = AssociativeLink(
                source_id=source_id,
                target_id=target_id,
                strength=strength,
                link_type=link_type,
                formed_at=datetime.now(),
                activation_count=1
            )
            
            # Store association
            self.associations[association_id] = association
            
            # Update indices
            if source_id not in self.source_index:
                self.source_index[source_id] = set()
            self.source_index[source_id].add(association_id)
            
            if target_id not in self.target_index:
                self.target_index[target_id] = set()
            self.target_index[target_id].add(association_id)
            
            if link_type not in self.type_index:
                self.type_index[link_type] = set()
            self.type_index[link_type].add(association_id)
            
            # Save association
            self._save_association(association, association_id)
            
            # Publish event
            self.publish_message("association_created", {
                "association_id": association_id,
                "source_id": source_id,
                "target_id": target_id,
                "link_type": link_type,
                "strength": strength
            })
            
            return {
                "status": "success",
                "association_id": association_id,
                "operation": "created",
                "strength": strength
            }
    
    def get_associations(self, memory_id: str) -> Dict[str, Any]:
        """
        Get all associations for a memory
        
        Parameters:
        memory_id: Memory ID to get associations for
        
        Returns:
        Operation result containing associated memories
        """
        # Check if memory exists in indices
        if memory_id not in self.source_index and memory_id not in self.target_index:
            return {
                "status": "error", 
                "message": f"No associations found for memory: {memory_id}",
                "outgoing": [],
                "incoming": []
            }
        
        # Get outgoing associations (memory is source)
        outgoing_assocs = self.source_index.get(memory_id, set())
        outgoing = []
        
        for assoc_id in outgoing_assocs:
            if assoc_id in self.associations:
                assoc = self.associations[assoc_id]
                outgoing.append({
                    "association_id": assoc_id,
                    "target_id": assoc.target_id,
                    "link_type": assoc.link_type,
                    "strength": assoc.strength
                })
        
        # Get incoming associations (memory is target)
        incoming_assocs = self.target_index.get(memory_id, set())
        incoming = []
        
        for assoc_id in incoming_assocs:
            if assoc_id in self.associations:
                assoc = self.associations[assoc_id]
                incoming.append({
                    "association_id": assoc_id,
                    "source_id": assoc.source_id,
                    "link_type": assoc.link_type,
                    "strength": assoc.strength
                })
        
        return {
            "status": "success",
            "memory_id": memory_id,
            "outgoing": sorted(outgoing, key=lambda x: x["strength"], reverse=True),
            "incoming": sorted(incoming, key=lambda x: x["strength"], reverse=True),
            "total_associations": len(outgoing) + len(incoming)
        }
    
    def spread_activation(
        self, 
        source_id: str, 
        activation: float = 0.5, 
        depth: int = 2
    ) -> Dict[str, Any]:
        """
        Spread activation from a source memory
        
        Parameters:
        source_id: Starting memory ID
        activation: Initial activation level
        depth: How many steps to spread activation
        
        Returns:
        Operation result containing activated memories
        """
        if activation <= 0.0 or depth <= 0:
            return {"status": "error", "message": "Invalid activation or depth"}
        
        # Track visited nodes and their activation
        activations = {source_id: activation}
        visited = set()
        
        # Queue of nodes to process (memory_id, current_depth, current_activation)
        queue = [(source_id, 0, activation)]
        
        while queue:
            current_id, current_depth, current_activation = queue.pop(0)
            
            # Skip if already visited or max depth reached
            if current_id in visited or current_depth >= depth:
                continue
                
            visited.add(current_id)
            
            # Get outgoing associations
            outgoing_assocs = self.source_index.get(current_id, set())
            
            for assoc_id in outgoing_assocs:
                if assoc_id in self.associations:
                    assoc = self.associations[assoc_id]
                    
                    # Calculate propagated activation
                    propagated = current_activation * assoc.strength
                    
                    # Skip weak activations
                    if propagated < 0.1:
                        continue
                        
                    target_id = assoc.target_id
                    
                    # Update target activation (take maximum if already activated)
                    if target_id in activations:
                        activations[target_id] = max(activations[target_id], propagated)
                    else:
                        activations[target_id] = propagated
                    
                    # Add to queue for further propagation
                    if current_depth + 1 < depth:
                        queue.append((target_id, current_depth + 1, propagated))
            
            # Get incoming associations
            incoming_assocs = self.target_index.get(current_id, set())
            
            for assoc_id in incoming_assocs:
                if assoc_id in self.associations:
                    assoc = self.associations[assoc_id]
                    
                    # Calculate propagated activation (slightly weaker for incoming)
                    propagated = current_activation * assoc.strength * 0.8
                    
                    # Skip weak activations
                    if propagated < 0.1:
                        continue
                        
                    source_id = assoc.source_id
                    
                    # Update source activation (take maximum if already activated)
                    if source_id in activations:
                        activations[source_id] = max(activations[source_id], propagated)
                    else:
                        activations[source_id] = propagated
                    
                    # Add to queue for further propagation
                    if current_depth + 1 < depth:
                        queue.append((source_id, current_depth + 1, propagated))
        
        # Remove the original source memory
        del activations[source_id]
        
        # Sort by activation level
        sorted_activations = [
            {"memory_id": mid, "activation": act}
            for mid, act in activations.items()
        ]
        sorted_activations.sort(key=lambda x: x["activation"], reverse=True)
        
        return {
            "status": "success",
            "source_id": source_id,
            "activated_memories": sorted_activations,
            "activation_count": len(sorted_activations)
        }
    
    def find_path(self, source_id: str, target_id: str, max_depth: int = 4) -> Dict[str, Any]:
        """
        Find a path between two memories
        
        Parameters:
        source_id: Starting memory ID
        target_id: Target memory ID
        max_depth: Maximum path length to consider
        
        Returns:
        Operation result containing the path if found
        """
        if source_id == target_id:
            return {
                "status": "success",
                "path_found": True,
                "path": [{"memory_id": source_id}],
                "path_length": 0,
                "path_strength": 1.0
            }
        
        # Use BFS to find shortest path
        visited = set()
        queue = [
            (source_id, [], 1.0)  # (current_id, path, path_strength)
        ]
        
        while queue:
            current_id, path, path_strength = queue.pop(0)
            
            # Skip if already visited or path too long
            if current_id in visited or len(path) >= max_depth:
                continue
                
            visited.add(current_id)
            
            # Get outgoing associations
            outgoing_assocs = self.source_index.get(current_id, set())
            
            for assoc_id in outgoing_assocs:
                if assoc_id in self.associations:
                    assoc = self.associations[assoc_id]
                    next_id = assoc.target_id
                    
                    # Calculate new path strength
                    new_strength = path_strength * assoc.strength
                    
                    # Create new path
                    new_path = path + [{
                        "memory_id": current_id, 
                        "association": {
                            "id": assoc_id,
                            "type": assoc.link_type,
                            "strength": assoc.strength
                        }
                    }]
                    
                    # Check if target found
                    if next_id == target_id:
                        # Complete path
                        final_path = new_path + [{"memory_id": target_id}]
                        
                        return {
                            "status": "success",
                            "path_found": True,
                            "path": final_path,
                            "path_length": len(final_path) - 1,
                            "path_strength": new_strength
                        }
                    
                    # Add to queue
                    queue.append((next_id, new_path, new_strength))
        
        # Check incoming direction (reverse path finding) if no path found
        visited = set()
        queue = [
            (target_id, [], 1.0)  # (current_id, path, path_strength)
        ]
        
        while queue:
            current_id, path, path_strength = queue.pop(0)
            
            # Skip if already visited or path too long
            if current_id in visited or len(path) >= max_depth:
                continue
                
            visited.add(current_id)
            
            # Get incoming associations
            incoming_assocs = self.target_index.get(current_id, set())
            
            for assoc_id in incoming_assocs:
                if assoc_id in self.associations:
                    assoc = self.associations[assoc_id]
                    next_id = assoc.source_id
                    
                    # Calculate new path strength
                    new_strength = path_strength * assoc.strength
                    
                    # Create new path (reverse order)
                    new_path = [{
                        "memory_id": current_id, 
                        "association": {
                            "id": assoc_id,
                            "type": assoc.link_type,
                            "strength": assoc.strength
                        }
                    }] + path
                    
                    # Check if source found
                    if next_id == source_id:
                        # Complete path (reverse order)
                        final_path = [{"memory_id": source_id}] + new_path
                        
                        return {
                            "status": "success",
                            "path_found": True,
                            "path": final_path,
                            "path_length": len(final_path) - 1,
                            "path_strength": new_strength
                        }
                    
                    # Add to queue
                    queue.append((next_id, new_path, new_strength))
        
        return {
            "status": "success",
            "path_found": False,
            "max_depth_searched": max_depth,
            "message": f"No path found between {source_id} and {target_id} within depth {max_depth}"
        }
    
    def get_associations_by_type(self, link_type: str, limit: int = 10) -> Dict[str, Any]:
        """
        Get associations by type
        
        Parameters:
        link_type: Type of association to find
        limit: Maximum number of associations to return
        
        Returns:
        Operation result containing associations of the specified type
        """
        if link_type not in self.type_index:
            return {
                "status": "error",
                "message": f"No associations found of type: {link_type}",
                "associations": []
            }
        
        assoc_ids = self.type_index.get(link_type, set())
        associations = []
        
        for assoc_id in assoc_ids:
            if assoc_id in self.associations:
                assoc = self.associations[assoc_id]
                associations.append({
                    "association_id": assoc_id,
                    "source_id": assoc.source_id,
                    "target_id": assoc.target_id,
                    "strength": assoc.strength,
                    "activation_count": assoc.activation_count
                })
        
        # Sort by strength and limit results
        associations.sort(key=lambda x: x["strength"], reverse=True)
        associations = associations[:limit]
        
        return {
            "status": "success",
            "link_type": link_type,
            "associations": associations,
            "total_found": len(assoc_ids),
            "returned": len(associations)
        }
    
    def decay_associations(self) -> Dict[str, Any]:
        """
        Apply decay to associations over time
        
        Less frequently used associations weaken over time.
        
        Returns:
        Operation result
        """
        before_count = len(self.associations)
        decayed_count = 0
        removed_count = 0
        
        to_remove = []
        
        for assoc_id, assoc in self.associations.items():
            # Calculate time-based decay
            days_since_formed = (datetime.now() - assoc.formed_at).days
            
            # Base decay amount
            decay_amount = self.decay_rate
            
            # Adjust decay based on activation count (frequently used associations decay slower)
            activation_factor = 1.0 / (1.0 + 0.1 * assoc.activation_count)
            decay_amount *= activation_factor
            
            # Apply decay
            if decay_amount > 0:
                old_strength = assoc.strength
                assoc.strength = max(0.0, assoc.strength - decay_amount)
                
                # If decayed significantly, count it
                if old_strength - assoc.strength > 0.01:
                    decayed_count += 1
                
                # If strength falls below threshold, mark for removal
                if assoc.strength < 0.05:
                    to_remove.append(assoc_id)
                else:
                    # Save updated association
                    self._save_association(assoc, assoc_id)
        
        # Remove weak associations
        for assoc_id in to_remove:
            self._remove_association(assoc_id)
            removed_count += 1
        
        return {
            "status": "success",
            "before_count": before_count,
            "decayed_count": decayed_count,
            "removed_count": removed_count,
            "after_count": len(self.associations)
        }
    
    def count_associations(self) -> int:
        """Count the number of stored associations"""
        return len(self.associations)
    
    def save_state(self) -> str:
        """
        Save the current state of associative memory
        
        Returns:
        Path to saved state directory
        """
        # Save associations
        for assoc_id, assoc in self.associations.items():
            self._save_association(assoc, assoc_id)
        
        # Save indices
        self._save_indices()
        
        return self.storage_dir
    
    def _find_association(self, source_id: str, target_id: str) -> Optional[str]:
        """Find an existing association between source and target"""
        # Check source index
        source_assocs = self.source_index.get(source_id, set())
        
        for assoc_id in source_assocs:
            if assoc_id in self.associations:
                assoc = self.associations[assoc_id]
                if assoc.target_id == target_id:
                    return assoc_id
        
        return None
    
    def _save_association(self, association: AssociativeLink, association_id: Optional[str] = None) -> None:
        """Save a single association to disk"""
        if association_id is None:
            # Try to find the association ID
            for aid, assoc in self.associations.items():
                if assoc == association:
                    association_id = aid
                    break
            
            if association_id is None:
                # If still not found, can't save
                return
        
        try:
            assocs_dir = Path(self.storage_dir) / "associations"
            assocs_dir.mkdir(parents=True, exist_ok=True)
            
            assoc_path = assocs_dir / f"{association_id}.json"
            with open(assoc_path, "w") as f:
                # Convert to dict and handle datetime
                assoc_dict = association.model_dump()
                # Convert datetime to string
                for key, value in assoc_dict.items():
                    if isinstance(value, datetime):
                        assoc_dict[key] = value.isoformat()
                json.dump(assoc_dict, f, indent=2)
        except Exception as e:
            print(f"Error saving association {association_id}: {e}")
    
    def _save_indices(self) -> None:
        """Save indices to disk"""
        try:
            # Convert sets to lists for JSON serialization
            source_dict = {src: list(assocs) for src, assocs in self.source_index.items()}
            target_dict = {tgt: list(assocs) for tgt, assocs in self.target_index.items()}
            type_dict = {typ: list(assocs) for typ, assocs in self.type_index.items()}
            
            indices_dir = Path(self.storage_dir) / "indices"
            indices_dir.mkdir(parents=True, exist_ok=True)
            
            with open(indices_dir / "source_index.json", "w") as f:
                json.dump(source_dict, f, indent=2)
                
            with open(indices_dir / "target_index.json", "w") as f:
                json.dump(target_dict, f, indent=2)
                
            with open(indices_dir / "type_index.json", "w") as f:
                json.dump(type_dict, f, indent=2)
        except Exception as e:
            print(f"Error saving indices: {e}")
    
    def _load_associations(self) -> None:
        """Load associations from disk"""
        try:
            # Load associations
            assocs_dir = Path(self.storage_dir) / "associations"
            assocs_dir.mkdir(parents=True, exist_ok=True)
            
            for file_path in assocs_dir.glob("*.json"):
                try:
                    assoc_id = file_path.stem
                    with open(file_path, "r") as f:
                        assoc_data = json.load(f)
                        # Convert string back to datetime
                        if "formed_at" in assoc_data and isinstance(assoc_data["formed_at"], str):
                            assoc_data["formed_at"] = datetime.fromisoformat(assoc_data["formed_at"])
                        
                        # Create association object
                        association = AssociativeLink(**assoc_data)
                        self.associations[assoc_id] = association
                except Exception as e:
                    print(f"Error loading association from {file_path}: {e}")
            
            # Load indices
            indices_dir = Path(self.storage_dir) / "indices"
            indices_dir.mkdir(parents=True, exist_ok=True)
            
            # Source index
            source_path = indices_dir / "source_index.json"
            if source_path.exists():
                with open(source_path, "r") as f:
                    source_dict = json.load(f)
                    # Convert lists back to sets
                    self.source_index = {src: set(assocs) for src, assocs in source_dict.items()}
            
            # Target index
            target_path = indices_dir / "target_index.json"
            if target_path.exists():
                with open(target_path, "r") as f:
                    target_dict = json.load(f)
                    # Convert lists back to sets
                    self.target_index = {tgt: set(assocs) for tgt, assocs in target_dict.items()}
            
            # Type index
            type_path = indices_dir / "type_index.json"
            if type_path.exists():
                with open(type_path, "r") as f:
                    type_dict = json.load(f)
                    # Convert lists back to sets
                    self.type_index = {typ: set(assocs) for typ, assocs in type_dict.items()}
            
            # Rebuild indices if loaded associations but indices are empty
            if self.associations and (not self.source_index or not self.target_index or not self.type_index):
                self._rebuild_indices()
                
            print(f"Loaded {len(self.associations)} associations from disk")
        except Exception as e:
            print(f"Error loading associations: {e}")
    
    def _rebuild_indices(self) -> None:
        """Rebuild indices from associations"""
        self.source_index = {}
        self.target_index = {}
        self.type_index = {}
        
        for assoc_id, assoc in self.associations.items():
            # Source index
            if assoc.source_id not in self.source_index:
                self.source_index[assoc.source_id] = set()
            self.source_index[assoc.source_id].add(assoc_id)
            
            # Target index
            if assoc.target_id not in self.target_index:
                self.target_index[assoc.target_id] = set()
            self.target_index[assoc.target_id].add(assoc_id)
            
            # Type index
            if assoc.link_type not in self.type_index:
                self.type_index[assoc.link_type] = set()
            self.type_index[assoc.link_type].add(assoc_id)
    
    def _remove_association(self, association_id: str) -> None:
        """Remove an association from memory and disk"""
        if association_id not in self.associations:
            return
            
        assoc = self.associations[association_id]
        
        # Remove from source index
        if assoc.source_id in self.source_index:
            if association_id in self.source_index[assoc.source_id]:
                self.source_index[assoc.source_id].remove(association_id)
                if not self.source_index[assoc.source_id]:
                    del self.source_index[assoc.source_id]
        
        # Remove from target index
        if assoc.target_id in self.target_index:
            if association_id in self.target_index[assoc.target_id]:
                self.target_index[assoc.target_id].remove(association_id)
                if not self.target_index[assoc.target_id]:
                    del self.target_index[assoc.target_id]
        
        # Remove from type index
        if assoc.link_type in self.type_index:
            if association_id in self.type_index[assoc.link_type]:
                self.type_index[assoc.link_type].remove(association_id)
                if not self.type_index[assoc.link_type]:
                    del self.type_index[assoc.link_type]
        
        # Remove from associations dict
        del self.associations[association_id]
        
        # Remove file
        try:
            assoc_path = Path(self.storage_dir) / "associations" / f"{association_id}.json"
            if assoc_path.exists():
                assoc_path.unlink()
        except Exception as e:
            print(f"Error deleting association file {association_id}: {e}")
    
    # Event handlers
    
    def _handle_memory_stored(self, message: Message) -> None:
        """
        Handle memory stored events
        
        When a memory is stored, we may create associations between it and 
        recent or related memories.
        """
        content = message.content
        memory_id = content.get("memory_id")
        memory_content = content.get("content")
        
        if not memory_id or not memory_content:
            return
            
        # This is a simplified approach - in a real system, you would:
        # 1. Analyze memory content to find related concepts/episodes
        # 2. Create meaningful associations based on content similarity
        # 3. Adjust strength based on relevance
        
        # For demonstration, let's create weak associations with a few recent memories
        self._associate_with_recent(memory_id)
    
    def _handle_memory_retrieved(self, message: Message) -> None:
        """
        Handle memory retrieved events
        
        When a memory is retrieved, we strengthen associations to it.
        """
        content = message.content
        memory_id = content.get("memory_id")
        
        if not memory_id:
            return
            
        # Get associations involving this memory
        source_assocs = self.source_index.get(memory_id, set())
        target_assocs = self.target_index.get(memory_id, set())
        
        # Strengthen each association slightly
        for assoc_id in source_assocs.union(target_assocs):
            if assoc_id in self.associations:
                assoc = self.associations[assoc_id]
                assoc.update_strength(self.hebbian_rate * 0.3)  # Smaller strengthening
                self._save_association(assoc, assoc_id)
    
    def _handle_concept_added(self, message: Message) -> None:
        """
        Handle concept added events
        
        When a new concept is added, we may create associations with related concepts.
        """
        content = message.content
        concept_id = content.get("concept_id")
        concept_content = content.get("content", "")
        
        if not concept_id:
            return
            
        # This is a simplified approach - in a real system, you'd use NLP
        # to identify related concepts and create appropriate associations
        
        # For demonstration, associate with recent concepts
        self._associate_with_recent(concept_id, link_type="conceptual")
    
    def _handle_episode_added(self, message: Message) -> None:
        """
        Handle episode added events
        
        When a new episode is added, we may create temporal associations with previous episodes.
        """
        content = message.content
        episode_id = content.get("episode_id")
        context = content.get("context", "")
        
        if not episode_id:
            return
            
        # Associate with recent episodes in the same context
        self._associate_with_context(episode_id, context, link_type="sequential")
    
    def _handle_association_query(self, message: Message) -> None:
        """Handle association query events"""
        content = message.content
        query_type = content.get("query_type", "")
        
        if query_type == "get_associations":
            memory_id = content.get("memory_id")
            if memory_id:
                result = self.get_associations(memory_id)
                
                if self.event_bus and result["status"] == "success":
                    self.publish_message("association_query_response", {
                        "requester": message.sender,
                        "result": result,
                        "memory_id": memory_id
                    })
        
        elif query_type == "spread_activation":
            source_id = content.get("source_id")
            activation = content.get("activation", 0.5)
            depth = content.get("depth", 2)
            
            if source_id:
                result = self.spread_activation(source_id, activation, depth)
                
                if self.event_bus and result["status"] == "success":
                    self.publish_message("association_query_response", {
                        "requester": message.sender,
                        "result": result,
                        "query_type": "spread_activation"
                    })
    
    def _associate_with_recent(self, memory_id: str, limit: int = 3, link_type: str = "temporal") -> None:
        """Create associations with recent memories"""
        # Get a few recent source memories (excluding the current one)
        recent_sources = [
            src for src in self.source_index.keys() 
            if src != memory_id
        ]
        
        # Sort by recency (if we had timestamps) and limit
        # For simplicity, we'll just take a few random ones
        if recent_sources:
            import random
            sample_size = min(limit, len(recent_sources))
            selected = random.sample(recent_sources, sample_size)
            
            # Create weak associations
            for src_id in selected:
                self.associate(
                    source_id=memory_id, 
                    target_id=src_id, 
                    link_type=link_type, 
                    strength=0.3  # Weak initial association
                )
    
    def _associate_with_context(self, memory_id: str, context: str, link_type: str = "contextual") -> None:
        """Create associations with memories in the same context"""
        # This would typically be implemented using context information from episodic memory
        # For simplicity, we'll just use a weak random association if other memories exist
        
        if len(self.associations) > 10:
            # Associate with a random existing memory
            import random
            other_id = random.choice(list(self.source_index.keys()))
            
            if other_id != memory_id:
                self.associate(
                    source_id=memory_id, 
                    target_id=other_id, 
                    link_type=link_type, 
                    strength=0.25  # Weak contextual association
                ) 

#######################

#memory\episodic_memory.py#
#######################

"""
Episodic Memory Module

This module implements episodic memory capabilities, storing temporally organized
experiences and events with their contextual details. Episodic memories include
information about what happened, where it happened, and when it happened.
"""

from typing import Dict, List, Any, Optional, Set, Tuple, Union
import time
import uuid
import logging
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import os
import pickle
import json
from collections import deque
import torch

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus 
from lmm_project.core.message import Message
from lmm_project.modules.memory.models import EpisodicMemory, MemoryConsolidationEvent
from lmm_project.modules.memory.neural_net import MemoryNeuralNetwork

logger = logging.getLogger(__name__)

class EpisodicMemorySystem:
    """
    Episodic memory system for storing and retrieving event memories
    
    Episodic memory stores experiences with their temporal, spatial, and emotional
    context. It develops over time, starting with basic event recording and
    eventually supporting detailed autobiographical memory.
    
    Development stages:
    - Stage 0 (0.0-0.2): Basic event storage with minimal context
    - Stage 1 (0.2-0.4): Time-based memory organization and simple retrieval
    - Stage 2 (0.4-0.6): Context-based episodic recall and emotional tagging
    - Stage 3 (0.6-0.8): Narrative connection between related episodes
    - Stage 4 (0.8-1.0): Autobiographical memory with self-reference
    """
    
    def __init__(
        self, 
        max_episodes: int = 1000,
        development_level: float = 0.0,
        embedding_dim: int = 128,
        base_directory: Optional[str] = None
    ):
        """
        Initialize episodic memory system
        
        Args:
            max_episodes: Maximum number of episodes to store
            development_level: Initial development level (0.0-1.0)
            embedding_dim: Dimension for memory embeddings
            base_directory: Directory for persistent storage
        """
        self.episodes: Dict[str, EpisodicMemory] = {}
        self.temporal_index: List[str] = []  # Episode IDs in temporal order
        self.context_index: Dict[str, List[str]] = {}  # Context -> episode IDs
        self.entity_index: Dict[str, List[str]] = {}  # Entity -> episode IDs
        self.narrative_index: Dict[str, List[str]] = {}  # Narrative ID -> episode IDs
        self.emotion_index: Dict[str, List[str]] = {}  # Emotion category -> episode IDs
        
        self.max_episodes = max_episodes
        self.episode_count = 0
        self.development_level = development_level
        self.embedding_dim = embedding_dim
        self.last_consolidation = datetime.now()
        self.base_directory = base_directory
        
        # Recent episodes cache (for faster access to recent memories)
        self.recent_episodes = deque(maxlen=50)
        
        # Memory for important life events (never pruned)
        self.important_events: Dict[str, EpisodicMemory] = {}
        
        # Neural network for encoding and retrieval
        self.neural_network = MemoryNeuralNetwork(
            input_dim=embedding_dim,
            hidden_dim=256,
            output_dim=embedding_dim,
            memory_type="episodic",
            learning_rate=0.01,
            device="auto"  # Use GPU if available
        )
        
        # Update neural network development level
        self.neural_network.update_development(development_level)
        
        # Load stored episodes if base directory is provided
        if base_directory:
            self._load_episodes()
    
    def store_episode(self, episode_data: Dict[str, Any]) -> str:
        """
        Store a new episode in memory
        
        Args:
            episode_data: Data representing the episode
                Must include 'content' and 'context' keys
                
        Returns:
            ID of the stored episode
        """
        # Basic validation
        if 'content' not in episode_data or 'context' not in episode_data:
            logger.error("Episode data must include 'content' and 'context'")
            return ""
            
        # Generate episode ID
        episode_id = f"ep_{uuid.uuid4().hex[:8]}"
        
        # Add timestamp if not provided
        if "timestamp" not in episode_data:
            episode_data["timestamp"] = datetime.now()
            
        # Add event_time if not provided
        if "event_time" not in episode_data:
            episode_data["event_time"] = episode_data["timestamp"]
            
        # Generate embedding based on development level
        embedding = self._generate_embedding(episode_data)
        if embedding is not None:
            episode_data["embedding"] = embedding.tolist()
            
        # Create EpisodicMemory object
        try:
            episode = EpisodicMemory(**episode_data)
        except Exception as e:
            logger.error(f"Error creating episodic memory: {e}")
            # Create with minimal data
            episode = EpisodicMemory(
                content=episode_data.get('content', ''),
                context=episode_data.get('context', ''),
                importance=episode_data.get('importance', 0.5)
            )
            
        # Store the episode
        self.episodes[episode_id] = episode
        
        # Add to indices based on development level
        self._add_to_indices(episode_id, episode)
        
        # Add to recent episodes
        self.recent_episodes.appendleft(episode_id)
        
        # If it's an important event, add to important events
        if episode.importance >= 0.8:
            self.important_events[episode_id] = episode
            
        # Increment episode count
        self.episode_count += 1
        
        # Check if we need to consolidate/prune
        current_time = datetime.now()
        if (current_time - self.last_consolidation).total_seconds() > 3600:  # Every hour
            self._consolidate_memories()
            self.last_consolidation = current_time
            
        # If we're over capacity, prune
        if len(self.episodes) > self.max_episodes:
            self._prune_episodes()
            
        # Save episode if base directory is set
        if self.base_directory and self.development_level >= 0.4:
            self._save_episode(episode_id, episode)
            
        return episode_id
        
    def retrieve_episode(self, episode_id: str) -> Optional[EpisodicMemory]:
        """
        Retrieve a specific episode by ID
        
        Args:
            episode_id: ID of the episode to retrieve
            
        Returns:
            EpisodicMemory object or None if not found
        """
        # Check if episode exists
        if episode_id in self.episodes:
            episode = self.episodes[episode_id]
            
            # Update access count and timestamp
            episode.access_count += 1
            episode.last_accessed = datetime.now()
            
            # Increase activation level
            episode.update_activation(0.2)
            
            return episode
            
        return None
        
    def search_by_content(self, query: str, limit: int = 10) -> List[EpisodicMemory]:
        """
        Search for episodes by content
        
        Args:
            query: Text to search for in episode content
            limit: Maximum number of results to return
            
        Returns:
            List of matching EpisodicMemory objects
        """
        # Simple text search by default
        results = []
        
        # For more advanced developmental stages, use embeddings
        if self.development_level >= 0.4 and hasattr(self, 'neural_network'):
            # Generate query embedding
            query_embedding = self._text_to_embedding(query)
            
            if query_embedding is not None:
                return self._search_by_embedding(query_embedding, limit)
        
        # Fallback to simple text search
        for episode_id, episode in self.episodes.items():
            if query.lower() in episode.content.lower():
                results.append(episode)
                if len(results) >= limit:
                    break
                    
        return results
        
    def search_by_time_range(
        self, 
        start_time: datetime, 
        end_time: datetime, 
        limit: int = 10
    ) -> List[EpisodicMemory]:
        """
        Search for episodes within a time range
        
        Args:
            start_time: Start of time range
            end_time: End of time range
            limit: Maximum number of results to return
            
        Returns:
            List of matching EpisodicMemory objects
        """
        results = []
        
        # Only enabled at development level 0.2+
        if self.development_level < 0.2:
            logger.warning("Time-based search requires development level 0.2+")
            return results
            
        # Search through temporal index
        for episode_id in self.temporal_index:
            if episode_id in self.episodes:
                episode = self.episodes[episode_id]
                
                # Check if event_time is within range
                if start_time <= episode.event_time <= end_time:
                    results.append(episode)
                    if len(results) >= limit:
                        break
                        
        return results
        
    def search_by_context(self, context: str, limit: int = 10) -> List[EpisodicMemory]:
        """
        Search for episodes by context
        
        Args:
            context: Context to search for
            limit: Maximum number of results to return
            
        Returns:
            List of matching EpisodicMemory objects
        """
        results = []
        
        # Only enabled at development level 0.4+
        if self.development_level < 0.4:
            logger.warning("Context-based search requires development level 0.4+")
            return results
            
        # Direct lookup in context index
        if context in self.context_index:
            episode_ids = self.context_index[context][:limit]
            for episode_id in episode_ids:
                if episode_id in self.episodes:
                    results.append(self.episodes[episode_id])
        
        # If we didn't find enough direct matches, try partial matching
        if len(results) < limit:
            for ctx, episode_ids in self.context_index.items():
                if context.lower() in ctx.lower():
                    for episode_id in episode_ids:
                        if episode_id in self.episodes and self.episodes[episode_id] not in results:
                            results.append(self.episodes[episode_id])
                            if len(results) >= limit:
                                break
                                
        return results
        
    def get_recent_episodes(self, limit: int = 10) -> List[EpisodicMemory]:
        """
        Get most recent episodes
        
        Args:
            limit: Maximum number of episodes to return
            
        Returns:
            List of recent EpisodicMemory objects
        """
        results = []
        
        # Use recent episodes cache for faster access
        for episode_id in list(self.recent_episodes)[:limit]:
            if episode_id in self.episodes:
                results.append(self.episodes[episode_id])
                
        return results
        
    def get_emotional_episodes(
        self, 
        valence: Optional[float] = None, 
        arousal: Optional[float] = None,
        limit: int = 10
    ) -> List[EpisodicMemory]:
        """
        Get episodes with specific emotional characteristics
        
        Args:
            valence: Target emotional valence (-1.0 to 1.0), or None for any
            arousal: Target emotional arousal (0.0 to 1.0), or None for any
            limit: Maximum number of episodes to return
            
        Returns:
            List of matching EpisodicMemory objects
        """
        results = []
        
        # Only enabled at development level 0.6+
        if self.development_level < 0.6:
            logger.warning("Emotional search requires development level 0.6+")
            return results
            
        # Search all episodes and filter by emotional characteristics
        for episode_id, episode in self.episodes.items():
            match = True
            
            if valence is not None:
                # Match within a range of the target valence
                if abs(episode.emotional_valence - valence) > 0.3:
                    match = False
                    
            if arousal is not None and match:
                # Match within a range of the target arousal
                if abs(episode.emotional_arousal - arousal) > 0.3:
                    match = False
                    
            if match:
                results.append(episode)
                if len(results) >= limit:
                    break
                    
        return results
        
    def get_narrative_episodes(self, narrative_id: str) -> List[EpisodicMemory]:
        """
        Get all episodes in a narrative sequence
        
        Args:
            narrative_id: ID of the narrative to retrieve
            
        Returns:
            List of episodes in the narrative, ordered by sequence position
        """
        results = []
        
        # Only enabled at development level 0.8+
        if self.development_level < 0.8:
            logger.warning("Narrative retrieval requires development level 0.8+")
            return results
            
        # Check if narrative exists
        if narrative_id in self.narrative_index:
            episode_ids = self.narrative_index[narrative_id]
            
            # Get all episodes in the narrative
            episodes = []
            for episode_id in episode_ids:
                if episode_id in self.episodes:
                    episodes.append(self.episodes[episode_id])
                    
            # Sort by sequence position
            episodes.sort(key=lambda x: x.sequence_position if x.sequence_position is not None else 9999)
            results = episodes
            
        return results
        
    def create_narrative(self, episode_ids: List[str], narrative_name: str = "") -> str:
        """
        Create a narrative by linking episodes in sequence
        
        Args:
            episode_ids: List of episode IDs to include in the narrative
            narrative_name: Optional name for the narrative
            
        Returns:
            ID of the created narrative
        """
        # Only enabled at development level 0.8+
        if self.development_level < 0.8:
            logger.warning("Narrative creation requires development level 0.8+")
            return ""
            
        # Generate narrative ID
        narrative_id = f"narr_{uuid.uuid4().hex[:8]}"
        if not narrative_name:
            narrative_name = f"Narrative {narrative_id}"
            
        # Create narrative
        self.narrative_index[narrative_id] = []
        
        # Add episodes to narrative
        for i, episode_id in enumerate(episode_ids):
            if episode_id in self.episodes:
                # Update episode with narrative information
                episode = self.episodes[episode_id]
                episode.narrative_id = narrative_id
                episode.sequence_position = i
                
                # Add to narrative index
                self.narrative_index[narrative_id].append(episode_id)
                
        return narrative_id
        
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of episodic memory
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New development level
        """
        old_level = self.development_level
        self.development_level = min(1.0, max(0.0, self.development_level + amount))
        
        # If significant change, adjust neural network
        if abs(self.development_level - old_level) >= 0.1:
            self.neural_network.update_development(amount)
            
            # Log development change
            logger.info(f"Episodic memory development updated to {self.development_level:.2f}")
            
            # If development crosses certain thresholds, enable new features
            if old_level < 0.4 <= self.development_level:
                logger.info("Episodic memory now supports context-based recall")
                
            if old_level < 0.6 <= self.development_level:
                logger.info("Episodic memory now supports emotional tagging")
                
            if old_level < 0.8 <= self.development_level:
                logger.info("Episodic memory now supports narrative sequencing")
                
        return self.development_level
        
    def _generate_embedding(self, episode_data: Dict[str, Any]) -> Optional[np.ndarray]:
        """Generate embedding for an episode based on content and context"""
        # Simple embedding generation at basic development levels
        if self.development_level < 0.3:
            # Random embedding as placeholder
            return np.random.randn(self.embedding_dim).astype(np.float32)
            
        # More sophisticated embedding at higher development levels
        if hasattr(self, 'neural_network'):
            # Combine content and context for input
            input_text = f"{episode_data['content']} {episode_data.get('context', '')}"
            return self._text_to_embedding(input_text)
            
        return None
        
    def _text_to_embedding(self, text: str) -> Optional[np.ndarray]:
        """Convert text to an embedding vector"""
        try:
            # Simple character-based embedding for demonstration
            # In a real system, this would use a more sophisticated embedding model
            chars = list(text.lower())
            char_codes = [ord(c) % 256 for c in chars]
            
            # Pad or truncate to embedding_dim
            if len(char_codes) >= self.embedding_dim:
                char_codes = char_codes[:self.embedding_dim]
            else:
                char_codes = char_codes + [0] * (self.embedding_dim - len(char_codes))
                
            # Convert to numpy array and normalize
            embedding = np.array(char_codes, dtype=np.float32)
            norm = np.linalg.norm(embedding)
            if norm > 0:
                embedding = embedding / norm
                
            return embedding
            
        except Exception as e:
            logger.error(f"Error generating text embedding: {e}")
            return None
            
    def _search_by_embedding(self, query_embedding: np.ndarray, limit: int = 10) -> List[EpisodicMemory]:
        """Search for episodes by embedding similarity"""
        results = []
        similarities = []
        
        # Convert query embedding to tensor
        query_tensor = torch.tensor(query_embedding, dtype=torch.float32)
        query_tensor = query_tensor.to(self.neural_network.device)
        
        # Compare with all episode embeddings
        for episode_id, episode in self.episodes.items():
            if episode.embedding is not None:
                # Convert episode embedding to tensor
                episode_tensor = torch.tensor(episode.embedding, dtype=torch.float32)
                episode_tensor = episode_tensor.to(self.neural_network.device)
                
                # Calculate cosine similarity
                similarity = torch.nn.functional.cosine_similarity(
                    query_tensor.unsqueeze(0),
                    episode_tensor.unsqueeze(0)
                ).item()
                
                similarities.append((episode, similarity))
                
        # Sort by similarity (descending)
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # Return top matches
        results = [item[0] for item in similarities[:limit]]
        return results
        
    def _add_to_indices(self, episode_id: str, episode: EpisodicMemory) -> None:
        """Add episode to appropriate indices based on development level"""
        # Temporal index (available at all levels)
        # Insert at appropriate position based on event_time
        inserted = False
        for i, existing_id in enumerate(self.temporal_index):
            if existing_id in self.episodes:
                existing_episode = self.episodes[existing_id]
                if episode.event_time < existing_episode.event_time:
                    self.temporal_index.insert(i, episode_id)
                    inserted = True
                    break
        if not inserted:
            self.temporal_index.append(episode_id)
            
        # Context index (level 0.4+)
        if self.development_level >= 0.4:
            if episode.context not in self.context_index:
                self.context_index[episode.context] = []
            if episode_id not in self.context_index[episode.context]:
                self.context_index[episode.context].append(episode_id)
                
        # Entity index (level 0.5+)
        if self.development_level >= 0.5:
            for entity in episode.involved_entities:
                if entity not in self.entity_index:
                    self.entity_index[entity] = []
                if episode_id not in self.entity_index[entity]:
                    self.entity_index[entity].append(episode_id)
                    
        # Narrative index (level 0.8+)
        if self.development_level >= 0.8 and episode.narrative_id:
            if episode.narrative_id not in self.narrative_index:
                self.narrative_index[episode.narrative_id] = []
            if episode_id not in self.narrative_index[episode.narrative_id]:
                self.narrative_index[episode.narrative_id].append(episode_id)
                
        # Emotion index (level 0.6+)
        if self.development_level >= 0.6:
            # Categorize emotions into buckets
            valence = episode.emotional_valence
            arousal = episode.emotional_arousal
            
            # Simple emotion categorization
            if valence > 0.3 and arousal > 0.5:
                emotion = "excitement"
            elif valence > 0.3 and arousal <= 0.5:
                emotion = "contentment"
            elif valence <= 0.3 and valence >= -0.3:
                emotion = "neutral"
            elif valence < -0.3 and arousal > 0.5:
                emotion = "fear/anger"
            else:
                emotion = "sadness"
                
            if emotion not in self.emotion_index:
                self.emotion_index[emotion] = []
            if episode_id not in self.emotion_index[emotion]:
                self.emotion_index[emotion].append(episode_id)
                
    def _consolidate_memories(self) -> None:
        """
        Consolidate memories to strengthen important ones and link related episodes
        
        This process simulates memory consolidation during rest/sleep phases.
        """
        # Only run consolidation at development level 0.3+
        if self.development_level < 0.3:
            return
            
        # Track changes for consolidation event
        strength_changes = {}
        
        # Strengthen frequently accessed memories
        for episode_id, episode in self.episodes.items():
            if episode.access_count > 0:
                # Calculate memory strength increase based on access count and importance
                strength_increase = min(0.1, 0.01 * episode.access_count * (0.5 + episode.importance))
                episode.decay_rate = max(0.001, episode.decay_rate - 0.0005 * strength_increase)
                
                strength_changes[episode_id] = strength_increase
                
                # Reset access count after consolidation
                episode.access_count = 0
                
        # Link related episodes at higher development levels
        if self.development_level >= 0.7:
            self._link_related_episodes()
            
        # Create consolidation event record
        if strength_changes:
            consolidation_event = MemoryConsolidationEvent(
                memory_ids=list(strength_changes.keys()),
                strength_changes=strength_changes,
                reason="sleep"
            )
            
            logger.info(f"Memory consolidation complete: {len(strength_changes)} episodes affected")
            
    def _link_related_episodes(self) -> None:
        """Link episodes that are related by context, time, or content"""
        # Only available at development level 0.7+
        if self.development_level < 0.7:
            return
            
        # Get recent episodes to analyze
        recent_ids = list(self.recent_episodes)[:100]  # Limit to 100 most recent
        
        # Group by context
        context_groups = {}
        for episode_id in recent_ids:
            if episode_id in self.episodes:
                episode = self.episodes[episode_id]
                if episode.context not in context_groups:
                    context_groups[episode.context] = []
                context_groups[episode.context].append(episode_id)
                
        # Create narratives for sequential episodes in same context
        for context, episode_ids in context_groups.items():
            if len(episode_ids) >= 3:  # Need at least 3 episodes to form a meaningful narrative
                # Sort by time
                episodes_with_time = []
                for ep_id in episode_ids:
                    if ep_id in self.episodes:
                        episodes_with_time.append((ep_id, self.episodes[ep_id].event_time))
                
                # Sort chronologically
                episodes_with_time.sort(key=lambda x: x[1])
                sorted_ids = [x[0] for x in episodes_with_time]
                
                # Check if these episodes span a reasonable timeframe (e.g., hours not years)
                if len(episodes_with_time) >= 2:
                    earliest = episodes_with_time[0][1]
                    latest = episodes_with_time[-1][1]
                    timespan = (latest - earliest).total_seconds()
                    
                    # If timespan is reasonable (< 24 hours), create a narrative
                    if timespan < 86400:  # 24 hours in seconds
                        narrative_name = f"Events at {context} on {earliest.strftime('%Y-%m-%d')}"
                        self.create_narrative(sorted_ids, narrative_name)
                        
    def _prune_episodes(self) -> None:
        """Remove least important episodes when over capacity"""
        # Don't prune if under capacity
        if len(self.episodes) <= self.max_episodes:
            return
            
        # Number to remove
        to_remove = len(self.episodes) - self.max_episodes
        
        # Sort episodes by importance (least important first)
        candidates = []
        for episode_id, episode in self.episodes.items():
            # Skip important life events
            if episode_id in self.important_events:
                continue
                
            # Calculate pruning score (lower = more likely to be pruned)
            # Consider: importance, recency, access count, emotional impact
            recency_factor = 1.0
            
            if episode.last_accessed:
                # Calculate days since last access
                days_since_access = (datetime.now() - episode.last_accessed).days
                recency_factor = max(0.1, 1.0 - (days_since_access / 100.0))
                
            pruning_score = (
                episode.importance * 0.4 + 
                recency_factor * 0.3 + 
                min(1.0, episode.access_count / 10.0) * 0.2 +
                abs(episode.emotional_valence) * 0.1
            )
            
            candidates.append((episode_id, pruning_score))
            
        # Sort by pruning score (ascending)
        candidates.sort(key=lambda x: x[1])
        
        # Remove the lowest-scoring episodes
        for episode_id, _ in candidates[:to_remove]:
            # Remove from all indices
            self._remove_from_indices(episode_id)
            
            # Remove the episode
            if episode_id in self.episodes:
                del self.episodes[episode_id]
                
            # Also remove from recent episodes if present
            if episode_id in self.recent_episodes:
                self.recent_episodes.remove(episode_id)
                
        logger.info(f"Pruned {to_remove} episodes from episodic memory")
        
    def _remove_from_indices(self, episode_id: str) -> None:
        """Remove episode from all indices"""
        # Temporal index
        if episode_id in self.temporal_index:
            self.temporal_index.remove(episode_id)
            
        # Context index
        for context, ids in self.context_index.items():
            if episode_id in ids:
                ids.remove(episode_id)
                
        # Entity index
        for entity, ids in self.entity_index.items():
            if episode_id in ids:
                ids.remove(episode_id)
                
        # Narrative index
        for narrative, ids in self.narrative_index.items():
            if episode_id in ids:
                ids.remove(episode_id)
                
        # Emotion index
        for emotion, ids in self.emotion_index.items():
            if episode_id in ids:
                ids.remove(episode_id)
                
    def _save_episode(self, episode_id: str, episode: EpisodicMemory) -> None:
        """Save episode to disk for persistence"""
        if not self.base_directory:
            return
            
        try:
            # Create directory if it doesn't exist
            episodes_dir = os.path.join(self.base_directory, "episodes")
            os.makedirs(episodes_dir, exist_ok=True)
            
            # Save episode to JSON file
            episode_path = os.path.join(episodes_dir, f"{episode_id}.json")
            with open(episode_path, 'w') as f:
                # Convert episode to dictionary
                episode_dict = episode.model_dump()
                # Convert datetime objects to strings
                for key, value in episode_dict.items():
                    if isinstance(value, datetime):
                        episode_dict[key] = value.isoformat()
                
                json.dump(episode_dict, f, indent=2)
                
        except Exception as e:
            logger.error(f"Error saving episode {episode_id}: {e}")
            
    def _load_episodes(self) -> None:
        """Load episodes from disk"""
        if not self.base_directory:
            return
            
        try:
            # Check if episodes directory exists
            episodes_dir = os.path.join(self.base_directory, "episodes")
            if not os.path.exists(episodes_dir):
                return
                
            # Load all episode files
            episode_files = [f for f in os.listdir(episodes_dir) if f.endswith('.json')]
            
            for filename in episode_files:
                try:
                    episode_path = os.path.join(episodes_dir, filename)
                    with open(episode_path, 'r') as f:
                        episode_data = json.load(f)
                        
                    # Convert string timestamps back to datetime
                    for key in ['timestamp', 'event_time', 'last_accessed']:
                        if key in episode_data and episode_data[key]:
                            try:
                                episode_data[key] = datetime.fromisoformat(episode_data[key])
                            except:
                                episode_data[key] = datetime.now()
                                
                    # Create episode object
                    episode = EpisodicMemory(**episode_data)
                    
                    # Extract episode ID from filename
                    episode_id = filename.split('.')[0]
                    
                    # Store in memory
                    self.episodes[episode_id] = episode
                    
                    # Add to indices
                    self._add_to_indices(episode_id, episode)
                    
                    # Update episode count
                    self.episode_count += 1
                    
                except Exception as e:
                    logger.error(f"Error loading episode file {filename}: {e}")
                    
            logger.info(f"Loaded {self.episode_count} episodes from disk")
            
        except Exception as e:
            logger.error(f"Error loading episodes: {e}")
    
    def get_state(self) -> Dict[str, Any]:
        """Get the current state of episodic memory"""
        return {
            "episode_count": len(self.episodes),
            "important_events_count": len(self.important_events),
            "development_level": self.development_level,
            "max_capacity": self.max_episodes,
            "contexts": list(self.context_index.keys()),
            "narratives": list(self.narrative_index.keys()),
            "emotion_categories": list(self.emotion_index.keys())
        } 

#######################

#memory\long_term_memory.py#
#######################

"""
Long-Term Memory Module

This module implements long-term memory for storing persistent information
after consolidation from working memory. It includes mechanisms for storage,
retrieval, forgetting, and consolidation of memories.
"""

from typing import Dict, List, Any, Optional, Tuple, Union, Set
from pydantic import BaseModel, Field
from datetime import datetime, timedelta
import numpy as np
import uuid
import os
import json
from pathlib import Path

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.memory.models import Memory, MemoryConsolidationEvent
from lmm_project.utils.vector_store import VectorStore

class LongTermMemory(BaseModule):
    """
    Long-term memory storage system
    
    Long-term memory provides persistent storage for memories that have
    been consolidated from working memory. It includes mechanisms for
    retrieval, forgetting, and consolidation.
    """
    # Memory storage
    memories: Dict[str, Memory] = Field(default_factory=dict)
    # Vector store for semantic search
    vector_store: Optional[VectorStore] = None
    # Embedding dimension
    embedding_dimension: int = Field(default=768)
    # Consolidation threshold (memories above this activation get consolidated)
    consolidation_threshold: float = Field(default=0.6)
    # Minimum time between consolidation events (seconds)
    consolidation_interval: float = Field(default=300)
    # Forgetting rate (memories below this importance may be forgotten)
    forgetting_rate: float = Field(default=0.01)
    # Last consolidation timestamp
    last_consolidation: datetime = Field(default_factory=datetime.now)
    # Storage directory
    storage_dir: str = Field(default="storage/memories")
    
    model_config = {
        "arbitrary_types_allowed": True
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, **data):
        """Initialize long-term memory module"""
        super().__init__(
            module_id=module_id,
            module_type="long_term_memory",
            event_bus=event_bus,
            **data
        )
        
        # Create storage directory
        os.makedirs(self.storage_dir, exist_ok=True)
        
        # Initialize vector store
        self.vector_store = VectorStore(
            dimension=self.embedding_dimension,
            storage_dir="storage/embeddings/memories"
        )
        
        # Try to load previous memories
        self._load_memories()
        
        # Subscribe to relevant events
        if self.event_bus:
            self.subscribe_to_message("working_memory_update", self._handle_working_memory_update)
            self.subscribe_to_message("memory_search", self._handle_memory_search)
            self.subscribe_to_message("consolidation_trigger", self._handle_consolidation_trigger)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process memory operations
        
        Parameters:
        input_data: Dictionary containing operation data
            - operation: The operation to perform (store, retrieve, search, forget)
            - memory: Memory data for store operation
            - memory_id: Memory ID for retrieve/forget operations
            - query: Search query for search operation
            - embedding: Optional query embedding for search
            - limit: Optional result limit for search
            
        Returns:
        Dictionary containing operation results
        """
        operation = input_data.get("operation", "")
        
        if operation == "store":
            memory_data = input_data.get("memory", {})
            return self.store_memory(memory_data)
        
        elif operation == "retrieve":
            memory_id = input_data.get("memory_id", "")
            return self.retrieve_memory(memory_id)
        
        elif operation == "search":
            query = input_data.get("query", "")
            embedding = input_data.get("embedding")
            limit = input_data.get("limit", 5)
            return self.search_memories(query, embedding, limit)
        
        elif operation == "forget":
            memory_id = input_data.get("memory_id", "")
            return self.forget_memory(memory_id)
        
        elif operation == "consolidate":
            return self.consolidate_memories()
            
        return {"status": "error", "message": f"Unknown operation: {operation}"}
    
    def update_development(self, amount: float) -> float:
        """
        Update long-term memory's developmental level
        
        As long-term memory develops:
        - Capacity increases
        - Retrieval becomes more efficient
        - Consolidation threshold decreases
        - Forgetting becomes more selective
        
        Parameters:
        amount: Amount to increase development level
        
        Returns:
        New development level
        """
        prev_level = self.development_level
        self.development_level = min(1.0, self.development_level + amount)
        
        # Update parameters based on development
        delta = self.development_level - prev_level
        
        # Improve consolidation threshold
        threshold_decrease = delta * 0.05
        self.consolidation_threshold = max(0.3, self.consolidation_threshold - threshold_decrease)
        
        # Decrease forgetting rate
        forgetting_decrease = delta * 0.002
        self.forgetting_rate = max(0.001, self.forgetting_rate - forgetting_decrease)
        
        return self.development_level
    
    def store_memory(self, memory_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Store a new memory in long-term memory
        
        Parameters:
        memory_data: Dictionary containing memory data
        
        Returns:
        Operation result
        """
        # Create a new Memory instance
        memory_type = memory_data.get("type", "generic")
        
        if "id" not in memory_data:
            memory_data["id"] = str(uuid.uuid4())
            
        memory_id = memory_data["id"]
        
        # Create Memory object with appropriate type
        if memory_type == "memory":
            from lmm_project.modules.memory.models import Memory
            memory = Memory(**memory_data)
        else:
            # Default to base Memory type
            memory = Memory(**memory_data)
        
        # Generate embedding if not provided
        if not memory.embedding and "content" in memory_data:
            memory.embedding = self._generate_embedding(memory.content)
        
        # Store memory
        self.memories[memory_id] = memory
        
        # Add to vector store if embedding exists
        if memory.embedding:
            self.vector_store.add(
                embeddings=[memory.embedding],
                metadata_list=[{"id": memory_id, "content": memory.content}]
            )
        
        # Save to disk
        self._save_memory(memory)
        
        # Publish event
        self.publish_message("memory_stored", {
            "memory_id": memory_id,
            "content": memory.content,
            "timestamp": memory.timestamp.isoformat()
        })
        
        return {
            "status": "success",
            "memory_id": memory_id
        }
    
    def retrieve_memory(self, memory_id: str) -> Dict[str, Any]:
        """
        Retrieve a memory by ID
        
        Parameters:
        memory_id: ID of the memory to retrieve
        
        Returns:
        Operation result containing memory data
        """
        # Check if memory exists
        if memory_id not in self.memories:
            return {"status": "error", "message": f"Memory not found: {memory_id}"}
        
        memory = self.memories[memory_id]
        
        # Update activation
        memory.update_activation(0.3)
        
        # Publish event
        self.publish_message("memory_retrieved", {
            "memory_id": memory_id,
            "content": memory.content,
            "importance": memory.importance
        })
        
        # Return memory data
        return {
            "status": "success",
            "memory": memory.model_dump(),
            "memory_id": memory_id
        }
    
    def search_memories(
        self, 
        query: str, 
        query_embedding: Optional[List[float]] = None, 
        limit: int = 5
    ) -> Dict[str, Any]:
        """
        Search for memories by semantic similarity
        
        Parameters:
        query: Text query
        query_embedding: Optional pre-generated embedding
        limit: Maximum number of results
        
        Returns:
        Operation result containing matching memories
        """
        # Generate embedding if not provided
        if not query_embedding:
            query_embedding = self._generate_embedding(query)
            
        if not query_embedding:
            return {"status": "error", "message": "Failed to generate embedding for query"}
        
        # Search vector store
        try:
            indices, distances, metadata = self.vector_store.search(
                query_embedding=query_embedding,
                k=limit
            )
            
            # Collect results
            results = []
            for idx, dist, meta in zip(indices, distances, metadata):
                memory_id = meta.get("id")
                if memory_id in self.memories:
                    memory = self.memories[memory_id]
                    # Update activation
                    memory.update_activation(0.2)
                    results.append({
                        "memory_id": memory_id,
                        "content": memory.content,
                        "importance": memory.importance,
                        "similarity_score": 1.0 - min(1.0, float(dist))
                    })
            
            # Publish event
            self.publish_message("memory_search_results", {
                "query": query,
                "result_count": len(results)
            })
            
            return {
                "status": "success",
                "results": results,
                "query": query
            }
            
        except Exception as e:
            return {"status": "error", "message": f"Search failed: {str(e)}"}
    
    def forget_memory(self, memory_id: str) -> Dict[str, Any]:
        """
        Forget (remove) a memory
        
        Parameters:
        memory_id: ID of the memory to forget
        
        Returns:
        Operation result
        """
        # Check if memory exists
        if memory_id not in self.memories:
            return {"status": "error", "message": f"Memory not found: {memory_id}"}
        
        # Get memory before removing
        memory = self.memories[memory_id]
        
        # Remove from memory store
        del self.memories[memory_id]
        
        # Remove from vector store
        # (Note: This is a simplified approach - in reality you'd need to track the index)
        # self.vector_store.delete([memory_id])
        
        # Remove from disk
        self._delete_memory_file(memory_id)
        
        # Publish event
        self.publish_message("memory_forgotten", {
            "memory_id": memory_id,
            "content": memory.content
        })
        
        return {
            "status": "success",
            "memory_id": memory_id
        }
    
    def consolidate_memories(self) -> Dict[str, Any]:
        """
        Consolidate memories from working memory to long-term memory
        
        Returns:
        Consolidation result
        """
        now = datetime.now()
        time_since_last = (now - self.last_consolidation).total_seconds()
        
        # Check if it's time to consolidate
        if time_since_last < self.consolidation_interval:
            return {
                "status": "skipped", 
                "message": "Consolidation interval not reached",
                "next_consolidation": self.consolidation_interval - time_since_last
            }
        
        self.last_consolidation = now
        
        # Get items from working memory module
        if self.event_bus:
            # Request working memory items
            self.publish_message("working_memory_request", {
                "action": "get_items",
                "requester": self.module_id
            })
            
            # Note: In a real system, you'd handle the response asynchronously
            # For simplicity, we'll simulate it with a direct consolidation
            
            consolidation_event = MemoryConsolidationEvent(
                memory_ids=[],
                strength_changes={},
                reason="sleep"
            )
            
            # Publish consolidation event
            self.publish_message("memory_consolidation", {
                "event": consolidation_event.model_dump()
            })
            
            return {
                "status": "success",
                "consolidated_count": len(consolidation_event.memory_ids)
            }
        
        return {"status": "error", "message": "No event bus available for consolidation"}
    
    def get_all_memories(self) -> List[Memory]:
        """Get all memories in long-term storage"""
        return list(self.memories.values())
    
    def count_memories(self) -> int:
        """Count the number of stored memories"""
        return len(self.memories)
    
    def save_state(self) -> str:
        """
        Save the current state of long-term memory
        
        Returns:
        Path to saved state file
        """
        # Save vector store
        vector_path = self.vector_store.save()
        
        # Save memories to disk
        for memory_id, memory in self.memories.items():
            self._save_memory(memory)
        
        return self.storage_dir
    
    def _save_memory(self, memory: Memory) -> None:
        """Save a single memory to disk"""
        try:
            memory_path = Path(self.storage_dir) / f"{memory.id}.json"
            with open(memory_path, "w") as f:
                # We need to convert the memory to a dict and handle datetime objects
                memory_dict = memory.model_dump()
                # Convert datetime to string
                for key, value in memory_dict.items():
                    if isinstance(value, datetime):
                        memory_dict[key] = value.isoformat()
                json.dump(memory_dict, f, indent=2)
        except Exception as e:
            print(f"Error saving memory {memory.id}: {e}")
    
    def _load_memories(self) -> None:
        """Load memories from disk"""
        try:
            memory_path = Path(self.storage_dir)
            for file_path in memory_path.glob("*.json"):
                try:
                    with open(file_path, "r") as f:
                        memory_data = json.load(f)
                        # Convert string back to datetime
                        if "timestamp" in memory_data and isinstance(memory_data["timestamp"], str):
                            memory_data["timestamp"] = datetime.fromisoformat(memory_data["timestamp"])
                        if "last_accessed" in memory_data and memory_data["last_accessed"] and isinstance(memory_data["last_accessed"], str):
                            memory_data["last_accessed"] = datetime.fromisoformat(memory_data["last_accessed"])
                        
                        # Create memory object
                        memory = Memory(**memory_data)
                        self.memories[memory.id] = memory
                        
                        # Add to vector store if embedding exists
                        if memory.embedding:
                            self.vector_store.add(
                                embeddings=[memory.embedding],
                                metadata_list=[{"id": memory.id, "content": memory.content}]
                            )
                except Exception as e:
                    print(f"Error loading memory from {file_path}: {e}")
            
            print(f"Loaded {len(self.memories)} memories from disk")
        except Exception as e:
            print(f"Error loading memories: {e}")
    
    def _delete_memory_file(self, memory_id: str) -> None:
        """Delete a memory file from disk"""
        try:
            memory_path = Path(self.storage_dir) / f"{memory_id}.json"
            if memory_path.exists():
                memory_path.unlink()
        except Exception as e:
            print(f"Error deleting memory file {memory_id}: {e}")
    
    def _generate_embedding(self, text: str) -> Optional[List[float]]:
        """Generate an embedding for text"""
        try:
            from lmm_project.utils.llm_client import LLMClient
            client = LLMClient()
            embedding = client.get_embedding(text)
            return embedding
        except Exception as e:
            print(f"Error generating embedding: {e}")
            return None
    
    # Event handlers
    
    def _handle_working_memory_update(self, message: Message) -> None:
        """
        Handle updates from working memory
        
        If an item is removed from working memory and has high activation,
        it may be consolidated to long-term memory.
        """
        content = message.content
        action = content.get("action")
        
        if action == "remove":
            # Item was removed from working memory
            item_id = content.get("item_id")
            item_content = content.get("content")
            
            if item_content and item_id:
                # Check if this should be consolidated
                activation = content.get("activation", 0.0)
                importance = content.get("importance", 0.5)
                
                if activation >= self.consolidation_threshold:
                    # Store in long-term memory
                    self.store_memory({
                        "content": item_content,
                        "importance": importance,
                        "source_id": item_id
                    })
    
    def _handle_memory_search(self, message: Message) -> None:
        """Handle memory search requests"""
        content = message.content
        query = content.get("query", "")
        limit = content.get("limit", 5)
        
        if query:
            results = self.search_memories(query, None, limit)
            
            if self.event_bus and results.get("status") == "success":
                # Publish results
                self.publish_message("memory_search_response", {
                    "requester": message.sender,
                    "results": results.get("results", []),
                    "query": query
                })
    
    def _handle_consolidation_trigger(self, message: Message) -> None:
        """Handle explicit consolidation triggers"""
        self.consolidate_memories() 


#######################

#memory\models.py#
#######################

from pydantic import BaseModel, Field, field_validator
from typing import List, Dict, Any, Optional, Set, Union, Literal
from datetime import datetime
import numpy as np
import uuid

class Memory(BaseModel):
    """Base class for all memory types in the system"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    content: str
    timestamp: datetime = Field(default_factory=datetime.now)
    importance: float = Field(default=0.5, ge=0.0, le=1.0)
    embedding: Optional[List[float]] = None
    activation_level: float = Field(default=0.0, ge=0.0, le=1.0)
    # How quickly this memory decays over time (lower = more stable)
    decay_rate: float = Field(default=0.01, ge=0.0, le=1.0)
    # How many times this memory has been accessed
    access_count: int = Field(default=0, ge=0)
    # Last time this memory was accessed
    last_accessed: Optional[datetime] = None
    # Tags for this memory
    tags: Set[str] = Field(default_factory=set)
    # Emotional valence (-1.0 to 1.0)
    emotional_valence: float = Field(default=0.0, ge=-1.0, le=1.0)
    # Emotional arousal (0.0 to 1.0)
    emotional_arousal: float = Field(default=0.0, ge=0.0, le=1.0)
    
    model_config = {
        "arbitrary_types_allowed": True
    }
    
    def update_activation(self, amount: float) -> None:
        """Update the activation level of this memory"""
        self.activation_level = max(0.0, min(1.0, self.activation_level + amount))
        self.access_count += 1
        self.last_accessed = datetime.now()
    
    def decay_activation(self, time_delta: float) -> None:
        """Decay the activation level over time"""
        decay_amount = self.decay_rate * time_delta
        self.activation_level = max(0.0, self.activation_level - decay_amount)


class WorkingMemoryItem(Memory):
    """An item in working memory - these are temporary and have limited capacity"""
    # Position in working memory buffer (lower = more attended to)
    buffer_position: int = Field(default=0, ge=0)
    # Time remaining until this item expires from working memory
    time_remaining: float = Field(default=30.0, ge=0.0)  # in seconds
    # Whether this item is being actively maintained through rehearsal
    is_rehearsed: bool = Field(default=False)
    # Reference to source in long-term memory, if any
    source_memory_id: Optional[str] = None


class SemanticMemory(Memory):
    """Semantic memory represents factual knowledge and concepts"""
    # Concept/knowledge represented
    concept: str
    # Confidence in this knowledge (0.0 to 1.0)
    confidence: float = Field(default=0.7, ge=0.0, le=1.0)
    # Related concepts
    related_concepts: Dict[str, float] = Field(default_factory=dict)
    # Whether this is derived from experience or taught directly
    source_type: Literal["experience", "instruction"] = "experience"
    # Knowledge domain this belongs to
    domain: Optional[str] = None
    # Hierarchical position (if any)
    is_subconcept_of: Optional[str] = None
    has_subconcepts: List[str] = Field(default_factory=list)


class EpisodicMemory(Memory):
    """Episodic memory represents specific experiences and events"""
    # Where this memory took place
    context: str
    # When this memory took place (may be different than storage timestamp)
    event_time: datetime = Field(default_factory=datetime.now)
    # Other entities involved in this memory
    involved_entities: List[str] = Field(default_factory=list)
    # First-person perspective flag
    is_first_person: bool = Field(default=True)
    # How vivid/detailed this memory is (0.0 to 1.0)
    vividness: float = Field(default=0.8, ge=0.0, le=1.0)
    # Narrative sequence (if part of larger narrative)
    sequence_position: Optional[int] = None
    narrative_id: Optional[str] = None
    # Emotional impact at time of event (-1.0 to 1.0 for valence, 0.0 to 1.0 for intensity)
    emotional_impact: Dict[str, float] = Field(default_factory=dict)


class AssociativeLink(BaseModel):
    """A link between two memories"""
    source_id: str
    target_id: str
    # Strength of association (0.0 to 1.0)
    strength: float = Field(default=0.5, ge=0.0, le=1.0)
    # Type of association
    link_type: str = "general"
    # When this association was formed
    formed_at: datetime = Field(default_factory=datetime.now)
    # How many times this association has been activated
    activation_count: int = Field(default=0, ge=0)
    
    def update_strength(self, amount: float) -> None:
        """Update the strength of this association"""
        self.strength = max(0.0, min(1.0, self.strength + amount))
        self.activation_count += 1


class MemoryConsolidationEvent(BaseModel):
    """Represents a consolidation event where memories are strengthened or weakened"""
    timestamp: datetime = Field(default_factory=datetime.now)
    memory_ids: List[str]
    # How much each memory was strengthened (positive) or weakened (negative)
    strength_changes: Dict[str, float]
    # Why this consolidation happened
    reason: Literal["sleep", "rehearsal", "emotional_salience", "relevance"] = "rehearsal"
    # Any pattern discovered during consolidation
    discovered_pattern: Optional[str] = None


class MemoryRetrievalRequest(BaseModel):
    """Request model for memory retrieval operations"""
    request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    memory_type: Literal["working", "episodic", "semantic", "associative", "all"] = "all"
    # Query parameters
    query: Optional[str] = None
    # Specific memory ID to retrieve
    memory_id: Optional[str] = None
    # Time frame for episodic memories (if applicable)
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    # Context for context-based retrieval
    context: Optional[str] = None
    # Domain for semantic memory queries
    domain: Optional[str] = None
    # Tags to search for
    tags: Set[str] = Field(default_factory=set)
    # Whether to include memory content in results
    include_content: bool = Field(default=True)
    # Maximum number of results to return
    limit: int = Field(default=10, ge=1)
    # Additional parameters based on memory type
    parameters: Dict[str, Any] = Field(default_factory=dict)


class MemoryRetrievalResult(BaseModel):
    """Result model for memory retrieval operations"""
    request_id: str
    status: Literal["success", "partial", "error"] = "success"
    memory_type: Literal["working", "episodic", "semantic", "associative", "all"]
    # Retrieved memory items
    memories: List[Dict[str, Any]] = Field(default_factory=list)
    # Number of memories found
    count: int = Field(default=0)
    # Error message if status is error
    error: Optional[str] = None
    # Time taken for retrieval (ms)
    retrieval_time: float = Field(default=0.0)
    # Memory IDs that matched the query
    memory_ids: List[str] = Field(default_factory=list)
    # Additional result metadata
    metadata: Dict[str, Any] = Field(default_factory=dict)


class MemoryStoreRequest(BaseModel):
    """Request model for memory storage operations"""
    request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    memory_type: Literal["working", "episodic", "semantic", "associative"]
    # Memory content to store
    content: str
    # Memory metadata
    metadata: Dict[str, Any] = Field(default_factory=dict)
    # Importance of the memory (0.0 to 1.0)
    importance: float = Field(default=0.5, ge=0.0, le=1.0)
    # Tags for categorizing the memory
    tags: Set[str] = Field(default_factory=set)
    # Emotional factors
    emotional_valence: float = Field(default=0.0, ge=-1.0, le=1.0)
    emotional_arousal: float = Field(default=0.0, ge=0.0, le=1.0)


class MemoryStoreResult(BaseModel):
    """Result model for memory storage operations"""
    request_id: str
    status: Literal["success", "error"] = "success"
    memory_type: Literal["working", "episodic", "semantic", "associative"]
    # ID of the stored memory
    memory_id: Optional[str] = None
    # Error message if status is error
    error: Optional[str] = None
    # Storage time (ms)
    storage_time: float = Field(default=0.0)
    # Additional result metadata
    metadata: Dict[str, Any] = Field(default_factory=dict)


class MemoryIndex(BaseModel):
    """Index for efficient memory retrieval"""
    # Type of index
    index_type: Literal["temporal", "semantic", "tag", "context", "association"] = "temporal"
    # Mapping of keys to memory IDs
    keys_to_ids: Dict[str, List[str]] = Field(default_factory=dict)
    # Whether this index is enabled (for development-based enablement)
    enabled: bool = Field(default=True)
    # Last updated timestamp
    last_updated: datetime = Field(default_factory=datetime.now)
    
    def add_entry(self, key: str, memory_id: str) -> None:
        """Add an entry to the index"""
        if key not in self.keys_to_ids:
            self.keys_to_ids[key] = []
        if memory_id not in self.keys_to_ids[key]:
            self.keys_to_ids[key].append(memory_id)
        self.last_updated = datetime.now()
    
    def remove_entry(self, key: str, memory_id: str) -> None:
        """Remove an entry from the index"""
        if key in self.keys_to_ids and memory_id in self.keys_to_ids[key]:
            self.keys_to_ids[key].remove(memory_id)
            if not self.keys_to_ids[key]:
                del self.keys_to_ids[key]
        self.last_updated = datetime.now()
    
    def get_memory_ids(self, key: str) -> List[str]:
        """Get memory IDs associated with a key"""
        return self.keys_to_ids.get(key, [])
    
    def get_all_memory_ids(self) -> List[str]:
        """Get all memory IDs in the index"""
        all_ids = []
        for ids in self.keys_to_ids.values():
            all_ids.extend(ids)
        return list(set(all_ids))  # Remove duplicates


class MemoryNeuralState(BaseModel):
    """Model for tracking neural states related to memory processing"""
    # Current activation patterns for different memory components
    activations: Dict[str, List[Dict[str, Any]]] = Field(default_factory=dict)
    
    # Development levels for different memory components
    working_memory_development: float = Field(default=0.0, ge=0.0, le=1.0)
    episodic_memory_development: float = Field(default=0.0, ge=0.0, le=1.0) 
    semantic_memory_development: float = Field(default=0.0, ge=0.0, le=1.0)
    associative_memory_development: float = Field(default=0.0, ge=0.0, le=1.0)
    
    # Memory efficacy metrics
    working_memory_capacity: int = Field(default=3, ge=1)
    episodic_recall_accuracy: float = Field(default=0.5, ge=0.0, le=1.0)
    semantic_organization: float = Field(default=0.3, ge=0.0, le=1.0)
    associative_strength: float = Field(default=0.4, ge=0.0, le=1.0)
    
    # Maximum activation storage (more recent activations only)
    max_activations_per_type: int = Field(default=20, ge=5)
    
    def add_activation(self, activation_type: str, data: Dict[str, Any]) -> None:
        """Add a new activation pattern for a memory component"""
        if activation_type not in self.activations:
            self.activations[activation_type] = []
            
        # Add timestamp if not present
        if 'timestamp' not in data:
            data['timestamp'] = datetime.now()
            
        self.activations[activation_type].append(data)
        
        # Trim to max size
        if len(self.activations[activation_type]) > self.max_activations_per_type:
            self.activations[activation_type] = self.activations[activation_type][-self.max_activations_per_type:]
    
    def get_recent_activations(self, activation_type: str, count: int = 5) -> List[Dict[str, Any]]:
        """Get the most recent activations for a component"""
        if activation_type not in self.activations:
            return []
            
        return self.activations[activation_type][-min(count, len(self.activations[activation_type])):]
    
    def clear_activations(self, activation_type: Optional[str] = None) -> None:
        """Clear activations for a type or all activations"""
        if activation_type:
            if activation_type in self.activations:
                self.activations[activation_type] = []
        else:
            self.activations = {}


#######################

#memory\neural_net.py#
#######################

from typing import Dict, List, Any, Optional, Tuple, Union, Callable
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from pathlib import Path
import os
import pickle
import logging

from lmm_project.neural_substrate.neural_network import NeuralNetwork
from lmm_project.neural_substrate.neuron import Neuron
from lmm_project.neural_substrate.synapse import Synapse
from lmm_project.neural_substrate.activation_functions import sigmoid, relu, tanh

logger = logging.getLogger(__name__)

def get_device():
    """Get the appropriate device for tensor operations."""
    if torch.cuda.is_available():
        return torch.device("cuda")
    return torch.device("cpu")

class MemoryNeuralNetwork:
    """
    Neural network architecture for memory systems
    
    This class provides specialized neural network models for different 
    memory types, implementing the computational aspects of memory processes.
    """
    def __init__(
        self, 
        input_dim: int, 
        hidden_dim: int, 
        output_dim: int, 
        memory_type: str = "generic",
        learning_rate: float = 0.01,
        device: str = "auto"  # "auto", "cpu", or "cuda"
    ):
        """
        Initialize memory neural network
        
        Parameters:
        input_dim: Input dimension
        hidden_dim: Hidden layer dimension
        output_dim: Output dimension
        memory_type: Type of memory network (working, semantic, episodic, associative)
        learning_rate: Learning rate for training
        device: Device to run computations on ("auto", "cpu" or "cuda")
        """
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.memory_type = memory_type
        self.learning_rate = learning_rate
        
        # Set device based on availability
        if device == "auto":
            self.device = get_device()
        else:
            self.device = torch.device(device)
            
        logger.info(f"Memory neural network created on device: {self.device}")
        
        # Create neural network based on memory type
        if memory_type == "working":
            self.network = self._create_working_memory_network()
        elif memory_type == "semantic":
            self.network = self._create_semantic_memory_network()
        elif memory_type == "episodic":
            self.network = self._create_episodic_memory_network()
        elif memory_type == "associative":
            self.network = self._create_associative_memory_network()
        else:
            self.network = self._create_generic_memory_network()
            
        # Move network to the appropriate device
        self.network.to(self.device)
        
        # Set up optimizer
        self.optimizer = optim.Adam(self.network.parameters(), lr=self.learning_rate)
        
        # Hebbian learning matrix for associative memory
        self.hebbian_matrix = None
        if memory_type == "associative":
            self.hebbian_matrix = np.zeros((output_dim, output_dim))
            
        # Track development level
        self.development_level = 0.0
        
        # Optimize for performance if using CUDA
        if self.device.type == "cuda":
            # Enable cuDNN benchmark for potentially faster performance
            torch.backends.cudnn.benchmark = True
            logger.info(f"CUDA version: {torch.version.cuda}")
            logger.info(f"Using GPU: {torch.cuda.get_device_name(0)}")
            
            # Use mixed precision if available (PyTorch 1.6+)
            if hasattr(torch.cuda, 'amp') and torch.cuda.is_available():
                logger.info("Enabling mixed precision training for memory networks")
                self.scaler = torch.cuda.amp.GradScaler()
                self.use_mixed_precision = True
            else:
                self.use_mixed_precision = False
                
    def _create_working_memory_network(self) -> nn.Module:
        """
        Create neural network for working memory
        
        Working memory maintains active representations for current processing.
        """
        # Define working memory network
        class WorkingMemoryNetwork(nn.Module):
            def __init__(self, input_dim, hidden_dim, output_dim):
                super().__init__()
                
                # Encoder
                self.encoder = nn.Sequential(
                    nn.Linear(input_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, hidden_dim // 2),
                    nn.ReLU()
                )
                
                # LSTM for maintaining items over time
                self.lstm = nn.LSTM(hidden_dim // 2, hidden_dim // 2, batch_first=True)
                
                # Output layer
                self.output = nn.Linear(hidden_dim // 2, output_dim)
                
            def forward(self, x, hidden=None):
                # Encode input
                encoded = self.encoder(x)
                
                # Reshape for LSTM if needed (batch, seq, features)
                if len(encoded.shape) == 2:
                    encoded = encoded.unsqueeze(1)
                
                # Process through LSTM
                if hidden is None:
                    lstm_out, hidden = self.lstm(encoded)
                else:
                    lstm_out, hidden = self.lstm(encoded, hidden)
                
                # Get output
                output = self.output(lstm_out[:, -1])
                
                return output, hidden
        
        return WorkingMemoryNetwork(self.input_dim, self.hidden_dim, self.output_dim)
    
    def _create_semantic_memory_network(self) -> nn.Module:
        """
        Create neural network for semantic memory
        
        Semantic memory focuses on knowledge representation and concept relationships.
        """
        # Define semantic memory network
        class SemanticMemoryNetwork(nn.Module):
            def __init__(self, input_dim, hidden_dim, output_dim):
                super().__init__()
                
                # Concept encoder
                self.concept_encoder = nn.Sequential(
                    nn.Linear(input_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, hidden_dim),
                    nn.ReLU()
                )
                
                # Concept embedding
                self.embedding = nn.Linear(hidden_dim, output_dim)
                
                # Concept completion network
                self.completion = nn.Sequential(
                    nn.Linear(output_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, input_dim)
                )
                
                # Relation network
                self.relation = nn.Bilinear(output_dim, output_dim, 1)
                
            def forward(self, x, completion_target=None):
                # Encode features
                encoded = self.concept_encoder(x)
                
                # Get concept embedding
                embedding = self.embedding(encoded)
                
                # If completion target is provided, compute reconstruction loss
                if completion_target is not None:
                    completion = self.completion(embedding)
                    completion_loss = F.mse_loss(completion, completion_target)
                else:
                    completion_loss = torch.tensor(0.0)
                    
                return {
                    "embedding": embedding,
                    "completion_loss": completion_loss
                }
                
        return SemanticMemoryNetwork(self.input_dim, self.hidden_dim, self.output_dim)
    
    def _create_episodic_memory_network(self) -> nn.Module:
        """
        Create neural network for episodic memory
        
        Episodic memory stores specific events and experiences with temporal context.
        """
        # Define episodic memory network
        class EpisodicMemoryNetwork(nn.Module):
            def __init__(self, input_dim, hidden_dim, output_dim):
                super().__init__()
                
                # Content encoder
                self.content_encoder = nn.Sequential(
                    nn.Linear(input_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, hidden_dim // 2),
                    nn.ReLU()
                )
                
                # Context encoder
                self.context_encoder = nn.Sequential(
                    nn.Linear(input_dim, hidden_dim // 2),
                    nn.ReLU()
                )
                
                # LSTM for temporal sequence
                self.temporal = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)
                
                # Output layer
                self.output = nn.Linear(hidden_dim, output_dim)
                
                # Emotional salience layer
                self.emotional = nn.Sequential(
                    nn.Linear(hidden_dim, 2),  # valence and arousal
                    nn.Tanh()  # -1 to 1 for valence, 0 to 1 for arousal after adjustment
                )
                
            def forward(self, content, context=None, prev_hidden=None):
                # Encode content
                content_encoded = self.content_encoder(content)
                
                # Encode context if provided
                if context is not None:
                    context_encoded = self.context_encoder(context)
                    # Combine content and context
                    combined = torch.cat([content_encoded, context_encoded], dim=-1)
                else:
                    # Just use content with zero padding for context portion
                    padding = torch.zeros_like(content_encoded)
                    combined = torch.cat([content_encoded, padding], dim=-1)
                
                # Reshape for LSTM if needed (batch, seq, features)
                if len(combined.shape) == 2:
                    combined = combined.unsqueeze(1)
                
                # Process through LSTM
                if prev_hidden is None:
                    lstm_out, hidden = self.temporal(combined)
                else:
                    lstm_out, hidden = self.temporal(combined, prev_hidden)
                
                # Get episode embedding
                embedding = self.output(lstm_out[:, -1])
                
                # Get emotional salience
                emotion = self.emotional(lstm_out[:, -1])
                valence = emotion[:, 0]  # -1 to 1
                arousal = (emotion[:, 1] + 1) / 2  # Scale to 0 to 1
                
                return embedding, valence, arousal, hidden
                
        return EpisodicMemoryNetwork(self.input_dim, self.hidden_dim, self.output_dim)
    
    def _create_associative_memory_network(self) -> nn.Module:
        """
        Create neural network for associative memory
        
        Associative memory focuses on connections between different memories.
        """
        # Define associative memory network
        class AssociativeMemoryNetwork(nn.Module):
            def __init__(self, input_dim, hidden_dim, output_dim):
                super().__init__()
                
                # Feature encoder
                self.encoder = nn.Sequential(
                    nn.Linear(input_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, output_dim)
                )
                
                # Association strength predictor (for two patterns)
                self.association_strength = nn.Sequential(
                    nn.Linear(output_dim * 2, hidden_dim // 2),
                    nn.ReLU(),
                    nn.Linear(hidden_dim // 2, 1),
                    nn.Sigmoid()  # 0 to 1 strength
                )
                
                # Stored patterns (for Hebbian learning)
                self.patterns = {}
                self.pattern_count = 0
                self.max_patterns = 1000
                
            def store_pattern(self, pattern):
                """Store a pattern for Hebbian learning"""
                pattern_id = str(self.pattern_count)
                self.patterns[pattern_id] = pattern
                self.pattern_count += 1
                
                # Remove oldest if at capacity
                if len(self.patterns) > self.max_patterns:
                    oldest_id = str(self.pattern_count - self.max_patterns - 1)
                    if oldest_id in self.patterns:
                        del self.patterns[oldest_id]
                
                return pattern_id
                
            def forward(self, x, hebbian_matrix=None):
                # Extract features
                features = self.encoder(x)
                
                # If Hebbian matrix provided, compute associative activations
                if hebbian_matrix is not None:
                    # Convert features to numpy for Hebbian computation
                    features_np = features.detach().cpu().numpy()
                    # Apply Hebbian associative recall
                    associations = np.dot(features_np, hebbian_matrix)
                    # Convert back to tensor
                    associations_tensor = torch.from_numpy(associations).to(features.device)
                    # Combine with direct features (residual connection)
                    output = features + 0.5 * torch.tensor(associations_tensor, dtype=features.dtype)
                else:
                    output = features
                
                return output
                
        return AssociativeMemoryNetwork(self.input_dim, self.hidden_dim, self.output_dim)
    
    def _create_generic_memory_network(self) -> nn.Module:
        """Create a generic memory network used as fallback"""
        # Define a simple generic network
        class GenericMemoryNetwork(nn.Module):
            def __init__(self, input_dim, hidden_dim, output_dim):
                super().__init__()
                self.network = nn.Sequential(
                    nn.Linear(input_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, output_dim)
                )
                
            def forward(self, x):
                return self.network(x)
                
        return GenericMemoryNetwork(self.input_dim, self.hidden_dim, self.output_dim)
    
    def forward(self, x: Union[np.ndarray, torch.Tensor], **kwargs) -> Tuple[torch.Tensor, Any]:
        """
        Forward pass through the memory network
        
        Parameters:
        x: Input tensor/array
        **kwargs: Additional arguments for specific memory types
        
        Returns:
        Tuple of output tensor and additional data
        """
        # Convert numpy arrays to tensors if needed
        if isinstance(x, np.ndarray):
            x = torch.tensor(x, dtype=torch.float32)
            
        # Ensure input is on the correct device
        x = x.to(self.device)
        
        # Forward through appropriate network
        if self.memory_type == "working":
            hidden = kwargs.get("hidden", None)
            output, new_hidden = self.network(x, hidden)
            return output, {"hidden": new_hidden}
            
        elif self.memory_type == "semantic":
            completion_target = kwargs.get("completion_target", None)
            if completion_target is not None and isinstance(completion_target, np.ndarray):
                completion_target = torch.tensor(completion_target, dtype=torch.float32).to(self.device)
                
            results = self.network(x, completion_target)
            return results["embedding"], {"completion_loss": results["completion_loss"]}
            
        elif self.memory_type == "episodic":
            context = kwargs.get("context", None)
            prev_hidden = kwargs.get("prev_hidden", None)
            
            if context is not None and isinstance(context, np.ndarray):
                context = torch.tensor(context, dtype=torch.float32).to(self.device)
                
            embedding, valence, arousal, hidden = self.network(x, context, prev_hidden)
            return embedding, {
                "valence": valence, 
                "arousal": arousal, 
                "hidden": hidden
            }
            
        elif self.memory_type == "associative":
            # Use Hebbian matrix if available
            output = self.network(x, self.hebbian_matrix)
            return output, {}
            
        else:
            # Generic memory network
            output = self.network(x)
            return output, {}
    
    def train(
        self, 
        inputs: Union[np.ndarray, torch.Tensor], 
        targets: Union[np.ndarray, torch.Tensor],
        **kwargs
    ) -> Dict[str, float]:
        """
        Train the memory network
        
        Parameters:
        inputs: Training inputs
        targets: Training targets
        **kwargs: Additional training parameters
        
        Returns:
        Dictionary with training metrics
        """
        # Convert numpy arrays to tensors if needed
        if isinstance(inputs, np.ndarray):
            inputs = torch.tensor(inputs, dtype=torch.float32)
        if isinstance(targets, np.ndarray):
            targets = torch.tensor(targets, dtype=torch.float32)
            
        # Ensure inputs and targets are on the correct device
        inputs = inputs.to(self.device)
        targets = targets.to(self.device)
        
        # Set network to training mode
        self.network.train()
        
        # Zero gradients
        self.optimizer.zero_grad()
        
        # Use mixed precision if available and enabled
        if hasattr(self, 'use_mixed_precision') and self.use_mixed_precision:
            with torch.cuda.amp.autocast():
                # Forward pass
                outputs, additional = self.forward(inputs, **kwargs)
                
                # Compute loss
                if self.memory_type == "semantic":
                    # For semantic memory, combine reconstruction and target losses
                    target_loss = F.mse_loss(outputs, targets)
                    completion_loss = additional.get("completion_loss", 0.0)
                    loss = target_loss + completion_loss
                else:
                    # Standard loss for other memory types
                    loss = F.mse_loss(outputs, targets)
                
            # Scale gradients and optimize
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            # Standard training process
            # Forward pass
            outputs, additional = self.forward(inputs, **kwargs)
            
            # Compute loss
            if self.memory_type == "semantic":
                # For semantic memory, combine reconstruction and target losses
                target_loss = F.mse_loss(outputs, targets)
                completion_loss = additional.get("completion_loss", 0.0)
                loss = target_loss + completion_loss
            else:
                # Standard loss for other memory types
                loss = F.mse_loss(outputs, targets)
            
            # Backward pass and optimize
            loss.backward()
            self.optimizer.step()
        
        # Update Hebbian matrix for associative memory
        if self.memory_type == "associative" and outputs.shape[0] > 0:
            output_np = outputs.detach().cpu().numpy()
            # Extract first example if batch
            if len(output_np.shape) > 1:
                output_np = output_np[0]
            self._update_hebbian_matrix(output_np)
        
        # Return training metrics
        return {
            "loss": loss.item(),
            "type": self.memory_type
        }
    
    def _update_hebbian_matrix(self, features: np.ndarray) -> None:
        """Update Hebbian matrix for associative memory"""
        if self.hebbian_matrix is None:
            return
            
        # Update Hebbian connections
        # Outer product represents connection strengths between all pairs of neurons
        outer_product = np.outer(features, features)
        
        # Scale learning based on development level
        # More primitive reinforcement at early stages, more nuanced at later stages
        hebbian_lr = 0.01 + (0.04 * self.development_level)
        
        # Apply development-dependent learning rate
        self.hebbian_matrix = (1.0 - hebbian_lr) * self.hebbian_matrix + hebbian_lr * outer_product
        
        # Apply normalization to prevent runaway values
        max_val = np.max(np.abs(self.hebbian_matrix))
        if max_val > 0:
            self.hebbian_matrix /= max_val
    
    def update_development(self, amount: float) -> float:
        """
        Update developmental level and adjust network accordingly
        
        Parameters:
        amount: Amount to increase development level by
        
        Returns:
        New development level
        """
        # Update development level
        old_level = self.development_level
        self.development_level = min(1.0, max(0.0, self.development_level + amount))
        
        # Only make adjustments if significant change
        if abs(self.development_level - old_level) < 0.01:
            return self.development_level
            
        # Adjust network based on development level
        if self.memory_type == "working":
            # Working memory LSTM layers might get more complex with development
            if hasattr(self.network, 'lstm'):
                # Adjust dropout based on development (lower dropout at higher levels)
                for module in self.network.modules():
                    if isinstance(module, nn.Dropout):
                        module.p = max(0.1, 0.5 - (self.development_level * 0.4))
        
        elif self.memory_type == "associative":
            # Associative memory might have more patterns with development
            if hasattr(self.network, 'max_patterns'):
                self.network.max_patterns = int(1000 + (9000 * self.development_level))
        
        # Adjust learning rate based on development
        # Higher development = more refined, slower learning
        self.learning_rate = max(0.001, 0.01 - (0.008 * self.development_level))
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = self.learning_rate
            
        logger.info(f"Updated memory network development to {self.development_level:.2f}")
        return self.development_level
    
    def save(self, path: str) -> None:
        """
        Save memory network to disk
        
        Parameters:
        path: Path to save the model
        """
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        # Save to CPU to avoid issues with GPU-specific tensors
        device_backup = next(self.network.parameters()).device
        cpu_state_dict = {k: v.cpu() for k, v in self.network.state_dict().items()}
        
        # Prepare data to save
        save_data = {
            "network_state": cpu_state_dict,
            "memory_type": self.memory_type,
            "dimensions": {
                "input_dim": self.input_dim,
                "hidden_dim": self.hidden_dim,
                "output_dim": self.output_dim
            },
            "development_level": self.development_level,
            "learning_rate": self.learning_rate
        }
        
        # Add Hebbian matrix if applicable
        if self.hebbian_matrix is not None:
            save_data["hebbian_matrix"] = self.hebbian_matrix
        
        # Save data
        torch.save(save_data, path)
        
        # Restore model to original device
        self.network.to(device_backup)
        logger.info(f"Memory network saved to {path}")
    
    def load(self, path: str) -> None:
        """
        Load memory network from disk
        
        Parameters:
        path: Path to the saved model
        """
        if not os.path.exists(path):
            logger.error(f"Model file not found: {path}")
            return
            
        try:
            # Load data using the current device
            save_data = torch.load(path, map_location=self.device)
            
            # Check if memory type matches
            if save_data["memory_type"] != self.memory_type:
                logger.warning(f"Memory type mismatch: saved={save_data['memory_type']}, current={self.memory_type}")
                # Create a new network of the correct type
                self.memory_type = save_data["memory_type"]
                if self.memory_type == "working":
                    self.network = self._create_working_memory_network()
                elif self.memory_type == "semantic":
                    self.network = self._create_semantic_memory_network()
                elif self.memory_type == "episodic":
                    self.network = self._create_episodic_memory_network()
                elif self.memory_type == "associative":
                    self.network = self._create_associative_memory_network()
                else:
                    self.network = self._create_generic_memory_network()
                
                # Move to correct device
                self.network.to(self.device)
            
            # Load network state
            self.network.load_state_dict(save_data["network_state"])
            
            # Update parameters
            self.input_dim = save_data["dimensions"]["input_dim"]
            self.hidden_dim = save_data["dimensions"]["hidden_dim"]
            self.output_dim = save_data["dimensions"]["output_dim"]
            self.development_level = save_data["development_level"]
            self.learning_rate = save_data["learning_rate"]
            
            # Update optimizer learning rate
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = self.learning_rate
            
            # Load Hebbian matrix if available
            if "hebbian_matrix" in save_data and self.memory_type == "associative":
                self.hebbian_matrix = save_data["hebbian_matrix"]
            
            logger.info(f"Memory network loaded from {path}")
            
        except Exception as e:
            logger.error(f"Error loading memory network: {e}")
            
    def to_gpu(self):
        """Move network to GPU if available"""
        if torch.cuda.is_available():
            device = torch.device("cuda")
            self.network.to(device)
            self.device = device
            logger.info("Memory network moved to GPU")
            
            # Enable mixed precision if available
            if hasattr(torch.cuda, 'amp'):
                logger.info("Enabling mixed precision for memory network")
                self.scaler = torch.cuda.amp.GradScaler()
                self.use_mixed_precision = True
        else:
            logger.warning("GPU not available, network remains on CPU")
            
    def to_cpu(self):
        """Move network to CPU"""
        device = torch.device("cpu")
        self.network.to(device)
        self.device = device
        self.use_mixed_precision = False
        logger.info("Memory network moved to CPU")
        
    def free_memory(self):
        """Free memory used by the network (useful for GPU memory management)"""
        if self.device.type == "cuda":
            # Move to CPU temporarily
            self.to_cpu()
            # Clear CUDA cache
            torch.cuda.empty_cache()
            logger.info("Memory network GPU memory freed")
                
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the memory network
        
        Returns:
        Dictionary with state information
        """
        return {
            "memory_type": self.memory_type,
            "dimensions": {
                "input_dim": self.input_dim,
                "hidden_dim": self.hidden_dim,
                "output_dim": self.output_dim
            },
            "development_level": self.development_level,
            "learning_rate": self.learning_rate,
            "device": str(self.device),
            "parameters": sum(p.numel() for p in self.network.parameters())
        }


#######################

#memory\semantic_memory.py#
#######################

from typing import Dict, List, Any, Optional, Tuple, Union, Set
from pydantic import BaseModel, Field
from datetime import datetime
import numpy as np
import uuid
import os
import json
from pathlib import Path

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.memory.models import SemanticMemory, Memory
from lmm_project.utils.vector_store import VectorStore

class SemanticMemoryModule(BaseModule):
    """
    Semantic memory system for knowledge, concepts, and facts
    
    Semantic memory represents factual knowledge and concepts that the 
    mind has learned, independent of specific episodes where they were 
    acquired. This module handles storage, organization, and retrieval
    of knowledge using a hierarchical concept network.
    """
    # Concept storage
    concepts: Dict[str, SemanticMemory] = Field(default_factory=dict)
    # Vector store for semantic search
    vector_store: Optional[VectorStore] = None
    # Concept relationships (mapping concept_id -> list of related concept_ids with strength)
    relationships: Dict[str, Dict[str, float]] = Field(default_factory=dict)
    # Domains for organizing concepts
    domains: Dict[str, Set[str]] = Field(default_factory=dict)
    # Knowledge confidence threshold (concepts below this are uncertain)
    confidence_threshold: float = Field(default=0.6)
    # Storage directory
    storage_dir: str = Field(default="storage/memories/semantic")
    
    model_config = {
        "arbitrary_types_allowed": True
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, **data):
        """Initialize semantic memory module"""
        super().__init__(
            module_id=module_id,
            module_type="semantic_memory",
            event_bus=event_bus,
            **data
        )
        
        # Create storage directory
        os.makedirs(self.storage_dir, exist_ok=True)
        
        # Initialize vector store
        self.vector_store = VectorStore(
            dimension=768,
            storage_dir="storage/embeddings/semantic"
        )
        
        # Try to load previous concepts
        self._load_concepts()
        
        # Subscribe to relevant events
        if self.event_bus:
            self.subscribe_to_message("memory_stored", self._handle_memory_stored)
            self.subscribe_to_message("instruction_received", self._handle_instruction)
            self.subscribe_to_message("semantic_query", self._handle_semantic_query)
            self.subscribe_to_message("concept_update", self._handle_concept_update)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process semantic memory operations
        
        Parameters:
        input_data: Dictionary containing operation data
            - operation: The operation to perform (add_concept, get_concept, 
                         search_concepts, relate_concepts, etc.)
            - Additional parameters depend on the operation
            
        Returns:
        Dictionary containing operation results
        """
        operation = input_data.get("operation", "")
        
        if operation == "add_concept":
            concept_data = input_data.get("concept", {})
            return self.add_concept(concept_data)
        
        elif operation == "get_concept":
            concept_id = input_data.get("concept_id", "")
            concept_name = input_data.get("concept_name", "")
            if concept_id:
                return self.get_concept_by_id(concept_id)
            elif concept_name:
                return self.get_concept_by_name(concept_name)
            else:
                return {"status": "error", "message": "No concept ID or name provided"}
        
        elif operation == "search_concepts":
            query = input_data.get("query", "")
            return self.search_concepts(query)
        
        elif operation == "relate_concepts":
            source_id = input_data.get("source_id", "")
            target_id = input_data.get("target_id", "")
            strength = input_data.get("strength", 0.5)
            return self.relate_concepts(source_id, target_id, strength)
        
        elif operation == "get_domain_concepts":
            domain = input_data.get("domain", "")
            return self.get_domain_concepts(domain)
            
        return {"status": "error", "message": f"Unknown operation: {operation}"}
    
    def update_development(self, amount: float) -> float:
        """
        Update semantic memory's developmental level
        
        As semantic memory develops:
        - Concept formation becomes more nuanced and abstract
        - Relationship detection improves
        - Knowledge integration becomes more sophisticated
        
        Parameters:
        amount: Amount to increase development level
        
        Returns:
        New development level
        """
        prev_level = self.development_level
        self.development_level = min(1.0, self.development_level + amount)
        
        # Update parameters based on development
        delta = self.development_level - prev_level
        
        # Improve confidence threshold
        confidence_decrease = delta * 0.05
        self.confidence_threshold = max(0.3, self.confidence_threshold - confidence_decrease)
        
        return self.development_level
    
    def add_concept(self, concept_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Add a new concept to semantic memory
        
        Parameters:
        concept_data: Dictionary containing concept data
            - concept: The concept name or label
            - content: Concept description
            - confidence: Optional confidence level (0.0-1.0)
            - domain: Optional knowledge domain
            - related_concepts: Optional dict of related concepts {concept_id: strength}
            
        Returns:
        Operation result
        """
        # Create concept ID if not provided
        if "id" not in concept_data:
            concept_data["id"] = str(uuid.uuid4())
            
        concept_id = concept_data["id"]
        
        # Ensure concept field is set
        if "concept" not in concept_data:
            if "content" in concept_data:
                concept_data["concept"] = concept_data["content"]
            else:
                return {"status": "error", "message": "No concept name provided"}
        
        # Create SemanticMemory object
        concept = SemanticMemory(**concept_data)
        
        # Store concept
        self.concepts[concept_id] = concept
        
        # Add to appropriate domain
        domain = concept_data.get("domain")
        if domain:
            if domain not in self.domains:
                self.domains[domain] = set()
            self.domains[domain].add(concept_id)
        
        # Add relationships if provided
        related_concepts = concept_data.get("related_concepts", {})
        for related_id, strength in related_concepts.items():
            self.relate_concepts(concept_id, related_id, strength)
        
        # Generate embedding if not provided
        if not concept.embedding:
            # Combine concept name and content for better embedding
            embedding_text = f"{concept.concept}: {concept.content}"
            concept.embedding = self._generate_embedding(embedding_text)
            
            # Add to vector store if embedding exists
            if concept.embedding:
                self.vector_store.add(
                    embeddings=[concept.embedding],
                    metadata_list=[{
                        "id": concept_id,
                        "concept": concept.concept,
                        "content": concept.content
                    }]
                )
        
        # Save to disk
        self._save_concept(concept)
        
        # Publish event
        self.publish_message("concept_added", {
            "concept_id": concept_id,
            "concept": concept.concept,
            "content": concept.content,
            "confidence": concept.confidence
        })
        
        return {
            "status": "success",
            "concept_id": concept_id
        }
    
    def get_concept_by_id(self, concept_id: str) -> Dict[str, Any]:
        """
        Get a concept by ID
        
        Parameters:
        concept_id: ID of the concept to retrieve
        
        Returns:
        Operation result containing concept data
        """
        # Check if concept exists
        if concept_id not in self.concepts:
            return {"status": "error", "message": f"Concept not found: {concept_id}"}
        
        concept = self.concepts[concept_id]
        
        # Update activation
        concept.update_activation(0.3)
        
        # Get related concepts
        related_concepts = {}
        if concept_id in self.relationships:
            related_concepts = self.relationships[concept_id]
        
        # Publish event
        self.publish_message("concept_retrieved", {
            "concept_id": concept_id,
            "concept": concept.concept
        })
        
        # Return concept data with related concepts
        return {
            "status": "success",
            "concept_id": concept_id,
            "concept": concept.concept,
            "content": concept.content,
            "confidence": concept.confidence,
            "domain": concept.domain,
            "related_concepts": related_concepts
        }
    
    def get_concept_by_name(self, concept_name: str) -> Dict[str, Any]:
        """
        Get a concept by name
        
        Parameters:
        concept_name: Name of the concept to retrieve
        
        Returns:
        Operation result containing concept data
        """
        # Search for concept by name
        for concept_id, concept in self.concepts.items():
            if concept.concept.lower() == concept_name.lower():
                return self.get_concept_by_id(concept_id)
        
        # If not found, try a partial match
        for concept_id, concept in self.concepts.items():
            if concept_name.lower() in concept.concept.lower():
                return self.get_concept_by_id(concept_id)
        
        # If still not found, try a semantic search
        search_results = self.search_concepts(concept_name)
        if search_results.get("status") == "success" and search_results.get("results"):
            # Return the top result
            top_result = search_results["results"][0]
            return self.get_concept_by_id(top_result["concept_id"])
        
        return {"status": "error", "message": f"Concept not found: {concept_name}"}
    
    def search_concepts(self, query: str, limit: int = 5) -> Dict[str, Any]:
        """
        Search for concepts by semantic similarity
        
        Parameters:
        query: Text query
        limit: Maximum number of results
        
        Returns:
        Operation result containing matching concepts
        """
        # Generate embedding for query
        query_embedding = self._generate_embedding(query)
        
        if not query_embedding:
            return {"status": "error", "message": "Failed to generate embedding for query"}
        
        # Search vector store
        try:
            indices, distances, metadata = self.vector_store.search(
                query_embedding=query_embedding,
                k=limit
            )
            
            # Collect results
            results = []
            for idx, dist, meta in zip(indices, distances, metadata):
                concept_id = meta.get("id")
                if concept_id in self.concepts:
                    concept = self.concepts[concept_id]
                    # Update activation
                    concept.update_activation(0.2)
                    results.append({
                        "concept_id": concept_id,
                        "concept": concept.concept,
                        "content": concept.content,
                        "confidence": concept.confidence,
                        "similarity_score": 1.0 - min(1.0, float(dist))
                    })
            
            # Publish event
            self.publish_message("concept_search_results", {
                "query": query,
                "result_count": len(results)
            })
            
            return {
                "status": "success",
                "results": results,
                "query": query
            }
            
        except Exception as e:
            return {"status": "error", "message": f"Search failed: {str(e)}"}
    
    def relate_concepts(self, source_id: str, target_id: str, strength: float = 0.5) -> Dict[str, Any]:
        """
        Create or update a relationship between concepts
        
        Parameters:
        source_id: Source concept ID
        target_id: Target concept ID
        strength: Relationship strength (0.0-1.0)
        
        Returns:
        Operation result
        """
        # Check if concepts exist
        if source_id not in self.concepts:
            return {"status": "error", "message": f"Source concept not found: {source_id}"}
        
        if target_id not in self.concepts:
            return {"status": "error", "message": f"Target concept not found: {target_id}"}
        
        # Initialize relationship dictionaries if needed
        if source_id not in self.relationships:
            self.relationships[source_id] = {}
        
        if target_id not in self.relationships:
            self.relationships[target_id] = {}
        
        # Create bidirectional relationship
        self.relationships[source_id][target_id] = strength
        self.relationships[target_id][source_id] = strength
        
        # Update related_concepts field in concepts
        source_concept = self.concepts[source_id]
        target_concept = self.concepts[target_id]
        
        source_concept.related_concepts[target_id] = strength
        target_concept.related_concepts[source_id] = strength
        
        # Save concepts
        self._save_concept(source_concept)
        self._save_concept(target_concept)
        
        # Save relationships
        self._save_relationships()
        
        # Publish event
        self.publish_message("concepts_related", {
            "source_id": source_id,
            "target_id": target_id,
            "source_concept": source_concept.concept,
            "target_concept": target_concept.concept,
            "strength": strength
        })
        
        return {
            "status": "success",
            "source_id": source_id,
            "target_id": target_id,
            "strength": strength
        }
    
    def get_domain_concepts(self, domain: str) -> Dict[str, Any]:
        """
        Get all concepts in a domain
        
        Parameters:
        domain: Domain name
        
        Returns:
        Operation result containing concepts in the domain
        """
        if domain not in self.domains:
            return {"status": "error", "message": f"Domain not found: {domain}", "concepts": []}
        
        domain_concept_ids = self.domains[domain]
        domain_concepts = []
        
        for concept_id in domain_concept_ids:
            if concept_id in self.concepts:
                concept = self.concepts[concept_id]
                domain_concepts.append({
                    "concept_id": concept_id,
                    "concept": concept.concept,
                    "content": concept.content,
                    "confidence": concept.confidence
                })
        
        return {
            "status": "success",
            "domain": domain,
            "concepts": domain_concepts,
            "count": len(domain_concepts)
        }
    
    def update_concept_confidence(
        self, 
        concept_id: str, 
        confidence_delta: float
    ) -> Dict[str, Any]:
        """
        Update confidence in a concept
        
        Parameters:
        concept_id: ID of the concept to update
        confidence_delta: Change in confidence (-1.0 to 1.0)
        
        Returns:
        Operation result
        """
        if concept_id not in self.concepts:
            return {"status": "error", "message": f"Concept not found: {concept_id}"}
        
        concept = self.concepts[concept_id]
        old_confidence = concept.confidence
        
        # Update confidence
        concept.confidence = max(0.0, min(1.0, concept.confidence + confidence_delta))
        
        # Save concept
        self._save_concept(concept)
        
        # Publish event
        self.publish_message("concept_confidence_updated", {
            "concept_id": concept_id,
            "concept": concept.concept,
            "old_confidence": old_confidence,
            "new_confidence": concept.confidence,
            "delta": confidence_delta
        })
        
        return {
            "status": "success",
            "concept_id": concept_id,
            "old_confidence": old_confidence,
            "new_confidence": concept.confidence
        }
    
    def count_concepts(self) -> int:
        """Count the number of stored concepts"""
        return len(self.concepts)
    
    def save_state(self) -> str:
        """
        Save the current state of semantic memory
        
        Returns:
        Path to saved state directory
        """
        # Save concepts
        for concept_id, concept in self.concepts.items():
            self._save_concept(concept)
        
        # Save relationships
        self._save_relationships()
        
        # Save domains
        self._save_domains()
        
        # Save vector store
        self.vector_store.save()
        
        return self.storage_dir
    
    def _save_concept(self, concept: SemanticMemory) -> None:
        """Save a single concept to disk"""
        try:
            concepts_dir = Path(self.storage_dir) / "concepts"
            concepts_dir.mkdir(parents=True, exist_ok=True)
            
            concept_path = concepts_dir / f"{concept.id}.json"
            with open(concept_path, "w") as f:
                # We need to convert the concept to a dict and handle datetime objects
                concept_dict = concept.model_dump()
                # Convert datetime to string
                for key, value in concept_dict.items():
                    if isinstance(value, datetime):
                        concept_dict[key] = value.isoformat()
                json.dump(concept_dict, f, indent=2)
        except Exception as e:
            print(f"Error saving concept {concept.id}: {e}")
    
    def _save_relationships(self) -> None:
        """Save concept relationships to disk"""
        try:
            relationship_path = Path(self.storage_dir) / "relationships.json"
            with open(relationship_path, "w") as f:
                json.dump(self.relationships, f, indent=2)
        except Exception as e:
            print(f"Error saving relationships: {e}")
    
    def _save_domains(self) -> None:
        """Save concept domains to disk"""
        try:
            # Convert sets to lists for JSON serialization
            domains_dict = {domain: list(concepts) for domain, concepts in self.domains.items()}
            
            domains_path = Path(self.storage_dir) / "domains.json"
            with open(domains_path, "w") as f:
                json.dump(domains_dict, f, indent=2)
        except Exception as e:
            print(f"Error saving domains: {e}")
    
    def _load_concepts(self) -> None:
        """Load concepts from disk"""
        try:
            # Load concepts
            concepts_dir = Path(self.storage_dir) / "concepts"
            concepts_dir.mkdir(parents=True, exist_ok=True)
            
            for file_path in concepts_dir.glob("*.json"):
                try:
                    with open(file_path, "r") as f:
                        concept_data = json.load(f)
                        # Convert string back to datetime
                        if "timestamp" in concept_data and isinstance(concept_data["timestamp"], str):
                            concept_data["timestamp"] = datetime.fromisoformat(concept_data["timestamp"])
                        if "last_accessed" in concept_data and concept_data["last_accessed"] and isinstance(concept_data["last_accessed"], str):
                            concept_data["last_accessed"] = datetime.fromisoformat(concept_data["last_accessed"])
                        
                        # Create concept object
                        concept = SemanticMemory(**concept_data)
                        self.concepts[concept.id] = concept
                        
                        # Add to vector store if embedding exists
                        if concept.embedding:
                            self.vector_store.add(
                                embeddings=[concept.embedding],
                                metadata_list=[{
                                    "id": concept.id,
                                    "concept": concept.concept,
                                    "content": concept.content
                                }]
                            )
                except Exception as e:
                    print(f"Error loading concept from {file_path}: {e}")
            
            # Load relationships
            relationship_path = Path(self.storage_dir) / "relationships.json"
            if relationship_path.exists():
                with open(relationship_path, "r") as f:
                    self.relationships = json.load(f)
            
            # Load domains
            domains_path = Path(self.storage_dir) / "domains.json"
            if domains_path.exists():
                with open(domains_path, "r") as f:
                    domains_dict = json.load(f)
                    # Convert lists back to sets
                    self.domains = {domain: set(concepts) for domain, concepts in domains_dict.items()}
            
            print(f"Loaded {len(self.concepts)} concepts from disk")
        except Exception as e:
            print(f"Error loading concepts: {e}")
    
    def _generate_embedding(self, text: str) -> Optional[List[float]]:
        """Generate an embedding for text"""
        try:
            from lmm_project.utils.llm_client import LLMClient
            client = LLMClient()
            embedding = client.get_embedding(text)
            return embedding
        except Exception as e:
            print(f"Error generating embedding: {e}")
            return None
    
    # Event handlers
    
    def _handle_memory_stored(self, message: Message) -> None:
        """
        Handle memory stored events
        
        When memories are stored in long-term memory, we check if they
        might represent knowledge that should be added to semantic memory.
        """
        content = message.content
        memory_id = content.get("memory_id")
        memory_content = content.get("content")
        
        if not memory_content or not memory_id:
            return
            
        # Check if this memory might contain factual knowledge
        # This is a simplistic approach - in a real system, you'd use 
        # NLP to detect factual statements
        if (
            "is" in memory_content.lower() or 
            "are" in memory_content.lower() or
            "means" in memory_content.lower() or
            "defined as" in memory_content.lower()
        ):
            # This might be a fact - extract it
            self._extract_concept_from_memory(memory_id, memory_content)
    
    def _handle_instruction(self, message: Message) -> None:
        """
        Handle instruction events
        
        The mother might directly teach facts or concepts.
        """
        content = message.content
        instruction = content.get("instruction", "")
        
        if not instruction:
            return
            
        # Check if this instruction contains knowledge
        # This is a simplistic approach
        if (
            "is" in instruction.lower() or 
            "are" in instruction.lower() or
            "means" in instruction.lower() or
            "defined as" in instruction.lower()
        ):
            # This might be a fact - extract it
            self._extract_concept_from_instruction(instruction)
    
    def _handle_semantic_query(self, message: Message) -> None:
        """Handle semantic query events"""
        content = message.content
        query = content.get("query", "")
        
        if not query:
            return
            
        results = self.search_concepts(query)
        
        if self.event_bus and results.get("status") == "success":
            # Publish results
            self.publish_message("semantic_query_response", {
                "requester": message.sender,
                "results": results.get("results", []),
                "query": query
            })
    
    def _handle_concept_update(self, message: Message) -> None:
        """Handle concept update events"""
        content = message.content
        concept_id = content.get("concept_id")
        confidence_delta = content.get("confidence_delta")
        
        if concept_id and confidence_delta is not None:
            self.update_concept_confidence(concept_id, confidence_delta)
    
    def _extract_concept_from_memory(self, memory_id: str, memory_content: str) -> None:
        """
        Extract potential concepts from a memory
        
        This is a simplified implementation - in a real system, you'd use 
        more sophisticated NLP to extract concepts and relationships.
        """
        # Simple heuristic: look for "X is Y" patterns
        content_lower = memory_content.lower()
        
        # Check for "X is Y" pattern
        if " is " in content_lower:
            parts = memory_content.split(" is ", 1)
            if len(parts) == 2:
                concept_name = parts[0].strip()
                concept_description = parts[1].strip()
                
                # Create concept data
                concept_data = {
                    "concept": concept_name,
                    "content": f"{concept_name} is {concept_description}",
                    "confidence": 0.7,  # Moderate confidence in extracted concepts
                    "source_type": "experience"
                }
                
                # Add the concept
                self.add_concept(concept_data)
    
    def _extract_concept_from_instruction(self, instruction: str) -> None:
        """
        Extract concepts from direct instruction
        
        This is simplified - real implementation would use more sophisticated NLP.
        """
        # Simple heuristic: look for "X is Y" patterns
        instruction_lower = instruction.lower()
        
        # Check for "X is Y" pattern
        if " is " in instruction_lower:
            parts = instruction.split(" is ", 1)
            if len(parts) == 2:
                concept_name = parts[0].strip()
                concept_description = parts[1].strip()
                
                # Create concept data
                concept_data = {
                    "concept": concept_name,
                    "content": f"{concept_name} is {concept_description}",
                    "confidence": 0.9,  # Higher confidence for direct instruction
                    "source_type": "instruction"
                }
                
                # Add the concept
                self.add_concept(concept_data) 

#######################

#memory\working_memory.py#
#######################

"""
Working Memory Module

This module implements the working memory system, which provides a limited
capacity buffer for temporarily storing information that is currently being
processed or attended to. Information in working memory decays over time
unless actively maintained through rehearsal.
"""

from typing import Dict, List, Any, Optional, Tuple
from pydantic import BaseModel, Field
import time
from datetime import datetime, timedelta
import numpy as np
import uuid

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.memory.models import WorkingMemoryItem

class WorkingMemory(BaseModule):
    """
    Working memory system with a limited capacity buffer
    
    Working memory provides a temporary storage mechanism for information
    that is currently being processed or attended to. It has limited capacity
    and information decays over time unless actively maintained.
    """
    # Maximum items that can be held in working memory
    max_capacity: int = Field(default=7)
    # Items in working memory
    items: Dict[str, WorkingMemoryItem] = Field(default_factory=dict)
    # Forgetting rate for non-rehearsed items (items/second)
    forgetting_rate: float = Field(default=0.05)
    # Last update timestamp
    last_update: datetime = Field(default_factory=datetime.now)
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, **data):
        """Initialize working memory module"""
        super().__init__(
            module_id=module_id,
            module_type="working_memory",
            event_bus=event_bus,
            **data
        )
        
        # Subscribe to relevant events
        if self.event_bus:
            self.subscribe_to_message("attention_focus", self._handle_attention_focus)
            self.subscribe_to_message("memory_retrieval", self._handle_memory_retrieval)
            self.subscribe_to_message("perception_input", self._handle_perception_input)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input data by adding it to working memory
        
        Parameters:
        input_data: Dictionary containing input data
            - content: Content to add to working memory
            - importance: Optional importance value (0.0-1.0)
            - source_id: Optional source memory ID if retrieved from long-term memory
            
        Returns:
        Dictionary containing processed results
        """
        # Update working memory
        self._update_state()
        
        content = input_data.get("content", "")
        if not content:
            return {"status": "error", "message": "No content provided"}
            
        importance = input_data.get("importance", 0.5)
        source_id = input_data.get("source_id")
        
        # Create new working memory item
        item = WorkingMemoryItem(
            content=content,
            importance=importance,
            source_memory_id=source_id,
            buffer_position=len(self.items),
            activation_level=1.0,  # Start fully activated
            time_remaining=30.0    # Default 30 seconds
        )
        
        # Add to working memory
        self._add_item(item)
        
        # Publish event
        self.publish_message("working_memory_update", {
            "action": "add",
            "item_id": item.id,
            "content": item.content,
            "current_capacity": len(self.items),
            "max_capacity": self.max_capacity
        })
        
        return {
            "status": "success",
            "item_id": item.id,
            "current_capacity": len(self.items),
            "max_capacity": self.max_capacity
        }
    
    def update_development(self, amount: float) -> float:
        """
        Update working memory's developmental level
        
        As working memory develops:
        - Capacity increases
        - Forgetting rate decreases
        - Ability to maintain items improves
        
        Parameters:
        amount: Amount to increase development level
        
        Returns:
        New development level
        """
        prev_level = self.development_level
        self.development_level = min(1.0, self.development_level + amount)
        
        # Update parameters based on development
        delta = self.development_level - prev_level
        
        # Increase capacity (from ~3 to ~7)
        capacity_increase = delta * 4
        self.max_capacity = min(7, self.max_capacity + capacity_increase)
        
        # Decrease forgetting rate
        forgetting_decrease = delta * 0.01
        self.forgetting_rate = max(0.01, self.forgetting_rate - forgetting_decrease)
        
        return self.development_level
    
    def get_items(self) -> List[WorkingMemoryItem]:
        """Get all items in working memory"""
        self._update_state()
        return list(self.items.values())
    
    def get_item(self, item_id: str) -> Optional[WorkingMemoryItem]:
        """Get a specific item from working memory"""
        self._update_state()
        return self.items.get(item_id)
    
    def remove_item(self, item_id: str) -> bool:
        """Remove an item from working memory"""
        if item_id in self.items:
            del self.items[item_id]
            
            # Reindex buffer positions
            self._reindex_buffer_positions()
            
            # Publish event
            self.publish_message("working_memory_update", {
                "action": "remove",
                "item_id": item_id,
                "current_capacity": len(self.items),
                "max_capacity": self.max_capacity
            })
            
            return True
        return False
    
    def rehearse_item(self, item_id: str) -> bool:
        """
        Actively rehearse an item to keep it in working memory
        
        Parameters:
        item_id: ID of the item to rehearse
        
        Returns:
        Success status
        """
        self._update_state()
        
        if item_id in self.items:
            item = self.items[item_id]
            item.is_rehearsed = True
            item.time_remaining = 30.0  # Reset decay timer
            item.update_activation(0.2)  # Boost activation
            
            # Move to front of buffer
            self._move_to_front(item_id)
            
            return True
        return False
    
    def clear(self) -> None:
        """Clear all items from working memory"""
        self.items = {}
        
        # Publish event
        self.publish_message("working_memory_update", {
            "action": "clear",
            "current_capacity": 0,
            "max_capacity": self.max_capacity
        })
    
    def _add_item(self, item: WorkingMemoryItem) -> None:
        """
        Add an item to working memory, handling capacity constraints
        
        If working memory is full, the least active item is removed
        """
        # Check if we need to make room
        if len(self.items) >= self.max_capacity:
            self._remove_least_active_item()
        
        # Add the new item
        self.items[item.id] = item
        
        # Reindex buffer positions
        self._reindex_buffer_positions()
    
    def _remove_least_active_item(self) -> None:
        """Remove the least active item from working memory"""
        if not self.items:
            return
            
        # Find item with lowest activation
        least_active_id = min(
            self.items, 
            key=lambda i: (self.items[i].is_rehearsed, self.items[i].activation_level)
        )
        
        # Remove it
        if least_active_id:
            self.remove_item(least_active_id)
    
    def _update_state(self) -> None:
        """Update the state of working memory, handling time decay"""
        now = datetime.now()
        time_delta = (now - self.last_update).total_seconds()
        self.last_update = now
        
        if time_delta <= 0:
            return
            
        # List of items to remove (can't modify during iteration)
        to_remove = []
        
        for item_id, item in self.items.items():
            # Update time remaining
            if not item.is_rehearsed:
                item.time_remaining -= time_delta
                
                # Decay activation
                item.decay_activation(time_delta)
                
                # Mark for removal if time expired
                if item.time_remaining <= 0:
                    to_remove.append(item_id)
        
        # Remove expired items
        for item_id in to_remove:
            self.remove_item(item_id)
    
    def _reindex_buffer_positions(self) -> None:
        """Reindex buffer positions after items are added or removed"""
        sorted_items = sorted(
            self.items.values(),
            key=lambda item: (-item.activation_level, item.buffer_position)
        )
        
        for i, item in enumerate(sorted_items):
            item.buffer_position = i
    
    def _move_to_front(self, item_id: str) -> None:
        """Move an item to the front of the buffer (position 0)"""
        if item_id not in self.items:
            return
            
        # Set buffer position to -1 to ensure it will be at position 0
        # after reindexing
        self.items[item_id].buffer_position = -1
        
        # Reindex
        self._reindex_buffer_positions()
    
    # Handler methods for event bus
    
    def _handle_attention_focus(self, message: Message) -> None:
        """Handle attention focus events"""
        content = message.content
        target_id = content.get("target_id")
        
        if target_id and target_id in self.items:
            # Boost activation and move to front
            self.rehearse_item(target_id)
    
    def _handle_memory_retrieval(self, message: Message) -> None:
        """Handle memory retrieval events"""
        content = message.content
        memory_id = content.get("memory_id")
        memory_content = content.get("content")
        source_id = content.get("source_id")
        
        if memory_content:
            # Add retrieved memory to working memory
            self.process_input({
                "content": memory_content,
                "importance": content.get("importance", 0.7),
                "source_id": source_id
            })
    
    def _handle_perception_input(self, message: Message) -> None:
        """Handle perception input events"""
        content = message.content
        
        if "perception_data" in content:
            # Add perceived input to working memory
            self.process_input({
                "content": str(content["perception_data"]),
                "importance": content.get("salience", 0.5)
            }) 


#######################

#memory\__init__.py#
#######################

"""
Memory Module

This module is responsible for storing and retrieving information across
different timeframes and contexts. It integrates multiple memory systems 
including working memory, episodic memory, and semantic memory.
"""

import logging
import time
import uuid
import os
import json
from typing import Dict, List, Any, Optional, Union, Set, Tuple
from datetime import datetime
from collections import deque, OrderedDict
import numpy as np

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.memory.associative_memory import AssociativeMemoryModule
from lmm_project.modules.memory.long_term_memory import LongTermMemory

logger = logging.getLogger(__name__)

def get_module(
    module_id: str = "memory",
    event_bus: Optional[EventBus] = None,
    development_level: float = 0.0
) -> "MemorySystem":
    """
    Factory function to create and return a memory module
    
    This function initializes and returns a complete memory system,
    with working memory, episodic memory, and semantic memory.
    
    Args:
        module_id: Unique identifier for the module
        event_bus: Event bus for communication
        development_level: Initial developmental level for the system
        
    Returns:
        Initialized MemorySystem
    """
    return MemorySystem(
        module_id=module_id,
        event_bus=event_bus,
        development_level=development_level
    )

class WorkingMemory:
    """
    Short-term memory buffer with limited capacity
    
    Working memory holds a small amount of information in an active state
    for manipulation and use in ongoing cognitive processes.
    """
    def __init__(self, capacity: int = 4):
        """
        Initialize working memory
        
        Args:
            capacity: Maximum number of items that can be held
        """
        self.capacity = capacity
        self.items = OrderedDict()  # Using OrderedDict for LRU functionality
        self.access_timestamps = {}  # Track when items were last accessed
        
    def add_item(self, item_id: str, item_data: Dict[str, Any]) -> bool:
        """
        Add an item to working memory
        
        Args:
            item_id: Unique identifier for the item
            item_data: The data to store
            
        Returns:
            Whether the item was successfully added
        """
        # Check if already at capacity
        if len(self.items) >= self.capacity and item_id not in self.items:
            # Remove least recently used item
            self._remove_lru_item()
            
        # Add or update the item
        self.items[item_id] = item_data
        self.access_timestamps[item_id] = time.time()
        return True
        
    def get_item(self, item_id: str) -> Optional[Dict[str, Any]]:
        """
        Retrieve an item from working memory
        
        Args:
            item_id: Identifier of the item to retrieve
            
        Returns:
            The item data or None if not found
        """
        if item_id in self.items:
            # Update access timestamp
            self.access_timestamps[item_id] = time.time()
            return self.items[item_id]
        return None
        
    def _remove_lru_item(self):
        """Remove the least recently used item"""
        if not self.items:
            return
            
        # Find item with oldest timestamp
        oldest_id = min(self.access_timestamps, key=self.access_timestamps.get)
        
        # Remove it
        if oldest_id in self.items:
            del self.items[oldest_id]
            del self.access_timestamps[oldest_id]
            
    def get_all_items(self) -> List[Dict[str, Any]]:
        """Get all items currently in working memory"""
        return list(self.items.values())
        
    def clear(self):
        """Clear all items from working memory"""
        self.items.clear()
        self.access_timestamps.clear()
        
    def get_state(self) -> Dict[str, Any]:
        """Get the current state of working memory"""
        return {
            "capacity": self.capacity,
            "current_usage": len(self.items),
            "items": list(self.items.keys())
        }

class EpisodicMemory:
    """
    Memory for specific events and experiences
    
    Episodic memory stores experiences with their temporal and contextual details.
    It develops from basic event storage to sophisticated autobiographical memory.
    """
    def __init__(self, max_episodes: int = 1000):
        """
        Initialize episodic memory
        
        Args:
            max_episodes: Maximum number of episodes to store
        """
        self.episodes = {}  # Dictionary of episodes by ID
        self.episode_timestamps = {}  # When episodes were created
        self.temporal_index = []  # Episodes in temporal order
        self.max_episodes = max_episodes
        self.episode_count = 0
        
    def store_episode(self, episode_data: Dict[str, Any]) -> str:
        """
        Store a new episode
        
        Args:
            episode_data: Data representing the episode
                Must include 'content' key
                
        Returns:
            ID of the stored episode
        """
        # Generate a unique ID for this episode
        episode_id = f"ep_{uuid.uuid4().hex[:8]}"
        
        # Add timestamp if not provided
        if "timestamp" not in episode_data:
            episode_data["timestamp"] = time.time()
            
        # Store the episode
        self.episodes[episode_id] = episode_data
        self.episode_timestamps[episode_id] = episode_data["timestamp"]
        
        # Add to temporal index
        self._add_to_temporal_index(episode_id, episode_data["timestamp"])
        
        # Increment count
        self.episode_count += 1
        
        # Check if we need to prune
        if self.episode_count > self.max_episodes:
            self._prune_episodes()
            
        return episode_id
        
    def _add_to_temporal_index(self, episode_id: str, timestamp: float):
        """Add an episode to the temporal index in the correct position"""
        # Simple implementation: just maintain a sorted list
        # For larger systems, more sophisticated indexing would be needed
        
        # Ensure timestamp is a float (handle case where it might be a string)
        if isinstance(timestamp, str):
            try:
                timestamp = float(timestamp)
            except ValueError:
                # If conversion fails, use current time
                timestamp = float(time.time())
        
        self.temporal_index.append((timestamp, episode_id))
        self.temporal_index.sort()  # Sort by timestamp
        
    def retrieve_episode(self, episode_id: str) -> Optional[Dict[str, Any]]:
        """
        Retrieve a specific episode by ID
        
        Args:
            episode_id: ID of the episode to retrieve
            
        Returns:
            Episode data or None if not found
        """
        return self.episodes.get(episode_id)
        
    def retrieve_recent_episodes(self, count: int = 5) -> List[Dict[str, Any]]:
        """
        Retrieve the most recent episodes
        
        Args:
            count: Number of episodes to retrieve
            
        Returns:
            List of recent episodes
        """
        # Get the most recent episodes from the temporal index
        recent_ids = [ep_id for _, ep_id in reversed(self.temporal_index[-count:])]
        return [self.episodes[ep_id] for ep_id in recent_ids if ep_id in self.episodes]
        
    def retrieve_by_timeframe(
        self, 
        start_time: float,
        end_time: float
    ) -> List[Dict[str, Any]]:
        """
        Retrieve episodes within a specific timeframe
        
        Args:
            start_time: Start of the timeframe (timestamp)
            end_time: End of the timeframe (timestamp)
            
        Returns:
            List of episodes within the timeframe
        """
        # Find episodes in the timeframe
        matching_ids = []
        for timestamp, ep_id in self.temporal_index:
            if start_time <= timestamp <= end_time:
                matching_ids.append(ep_id)
                
        return [self.episodes[ep_id] for ep_id in matching_ids if ep_id in self.episodes]
        
    def _prune_episodes(self):
        """Remove oldest episodes to stay within max limit"""
        # Find how many episodes to remove
        to_remove = self.episode_count - self.max_episodes
        if to_remove <= 0:
            return
            
        # Remove oldest episodes
        for timestamp, ep_id in self.temporal_index[:to_remove]:
            if ep_id in self.episodes:
                del self.episodes[ep_id]
                del self.episode_timestamps[ep_id]
                
        # Update temporal index and count
        self.temporal_index = self.temporal_index[to_remove:]
        self.episode_count -= to_remove
        
    def get_state(self) -> Dict[str, Any]:
        """Get the current state of episodic memory"""
        return {
            "episode_count": self.episode_count,
            "max_episodes": self.max_episodes,
            "oldest_timestamp": self.temporal_index[0][0] if self.temporal_index else None,
            "newest_timestamp": self.temporal_index[-1][0] if self.temporal_index else None
        }

class SemanticMemory:
    """
    Memory for facts, concepts, and general knowledge
    
    Semantic memory stores conceptual knowledge independent of specific
    contexts or episodes. It develops from simple facts to complex knowledge networks.
    """
    def __init__(self, max_items: int = 10000):
        """
        Initialize semantic memory
        
        Args:
            max_items: Maximum number of items to store
        """
        self.concepts = {}  # Dictionary of concepts by ID
        self.concept_timestamps = {}  # When concepts were created/updated
        self.max_items = max_items
        
        # Simple retrieval indices
        self.label_index = {}  # Map labels to concept IDs
        self.type_index = {}  # Map concept types to concept IDs
        
        # For development tracking
        self.item_count = 0
        self.update_count = 0
        
    def store_concept(self, concept_data: Dict[str, Any]) -> str:
        """
        Store a concept in semantic memory
        
        Args:
            concept_data: Data representing the concept
                Should include 'label' and 'type' keys
                
        Returns:
            ID of the stored concept
        """
        # Extract key information
        label = concept_data.get("label", "")
        concept_type = concept_data.get("type", "general")
        
        # Check if this concept already exists (by label)
        concept_id = self.label_index.get(label.lower())
        
        # If it exists, update it
        if concept_id:
            old_type = self.concepts[concept_id].get("type", "")
            
            # Update the concept
            self.concepts[concept_id].update(concept_data)
            self.concept_timestamps[concept_id] = time.time()
            
            # Update type index if type changed
            if old_type != concept_type:
                # Remove from old type index
                if old_type in self.type_index and concept_id in self.type_index[old_type]:
                    self.type_index[old_type].remove(concept_id)
                
                # Add to new type index
                if concept_type not in self.type_index:
                    self.type_index[concept_type] = set()
                self.type_index[concept_type].add(concept_id)
                
            self.update_count += 1
            
        else:
            # Create a new concept
            concept_id = f"con_{uuid.uuid4().hex[:8]}"
            
            # Store the concept
            self.concepts[concept_id] = concept_data
            self.concept_timestamps[concept_id] = time.time()
            
            # Add to indices
            self.label_index[label.lower()] = concept_id
            
            if concept_type not in self.type_index:
                self.type_index[concept_type] = set()
            self.type_index[concept_type].add(concept_id)
            
            self.item_count += 1
            
            # Check if we need to prune
            if self.item_count > self.max_items:
                self._prune_concepts()
                
        return concept_id
        
    def retrieve_by_label(self, label: str) -> Optional[Dict[str, Any]]:
        """
        Retrieve a concept by its label
        
        Args:
            label: Label of the concept to retrieve
            
        Returns:
            Concept data or None if not found
        """
        concept_id = self.label_index.get(label.lower())
        if concept_id:
            return self.concepts.get(concept_id)
        return None
        
    def retrieve_by_type(self, concept_type: str, limit: int = 100) -> List[Dict[str, Any]]:
        """
        Retrieve concepts of a specific type
        
        Args:
            concept_type: Type of concepts to retrieve
            limit: Maximum number to retrieve
            
        Returns:
            List of matching concepts
        """
        if concept_type not in self.type_index:
            return []
            
        # Get concept IDs of this type
        concept_ids = list(self.type_index[concept_type])[:limit]
        
        # Return the concepts
        return [self.concepts[cid] for cid in concept_ids if cid in self.concepts]
        
    def _prune_concepts(self):
        """Remove least recently used concepts to stay within max limit"""
        # Find how many concepts to remove
        to_remove = self.item_count - self.max_items
        if to_remove <= 0:
            return
            
        # Sort concepts by timestamp
        sorted_concepts = sorted(
            self.concept_timestamps.items(),
            key=lambda x: x[1]
        )
        
        # Remove oldest concepts
        for concept_id, _ in sorted_concepts[:to_remove]:
            if concept_id in self.concepts:
                # Get concept data for index removal
                concept = self.concepts[concept_id]
                label = concept.get("label", "").lower()
                concept_type = concept.get("type", "")
                
                # Remove from main storage
                del self.concepts[concept_id]
                del self.concept_timestamps[concept_id]
                
                # Remove from indices
                if label in self.label_index and self.label_index[label] == concept_id:
                    del self.label_index[label]
                    
                if concept_type in self.type_index and concept_id in self.type_index[concept_type]:
                    self.type_index[concept_type].remove(concept_id)
                    
        # Update count
        self.item_count -= to_remove
        
    def get_state(self) -> Dict[str, Any]:
        """Get the current state of semantic memory"""
        return {
            "item_count": self.item_count,
            "update_count": self.update_count,
            "max_items": self.max_items,
            "type_counts": {ctype: len(cids) for ctype, cids in self.type_index.items()}
        }

class MemorySystem(BaseModule):
    """
    Integrated memory system with multiple memory types
    
    The memory system develops from simple storage and retrieval to 
    sophisticated organization, consolidation, and recall processes.
    """
    # Development milestones
    development_milestones = {
        0.0: "Basic memory storage",
        0.2: "Short-term working memory",
        0.4: "Episodic memory formation",
        0.6: "Semantic memory organization",
        0.8: "Memory consolidation",
        1.0: "Integrated memory systems"
    }
    
    def __init__(
        self,
        module_id: str,
        event_bus: Optional[EventBus] = None,
        development_level: float = 0.0
    ):
        """
        Initialize the memory system
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication
            development_level: Initial developmental level
        """
        super().__init__(
            module_id=module_id,
            module_type="memory_system",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Initialize memory systems
        self.working_memory = WorkingMemory(capacity=3)
        self.episodic_memory = EpisodicMemory(max_episodes=100)
        self.semantic_memory = SemanticMemory(max_items=500)
        self.long_term_memory = LongTermMemory(module_id="long_term_memory", event_bus=event_bus)
        self.associative_memory = AssociativeMemoryModule(module_id="associative_memory", event_bus=event_bus)
        
        # Adjust memory parameters based on development level
        self._adjust_memory_for_development()
        
        # Subscribe to relevant message types
        if self.event_bus:
            self.subscribe_to_message("memory_store")
            self.subscribe_to_message("memory_retrieve")
            self.subscribe_to_message("perception_result")
            self.subscribe_to_message("attention_focus_update")
            self.subscribe_to_message("memory_consolidation")
    
    def _adjust_memory_for_development(self):
        """Adjust memory parameters based on developmental level"""
        # Working memory capacity increases with development
        # Research suggests capacity grows from ~2-3 items to ~4-7 items
        self.working_memory.capacity = max(2, int(2 + self.development_level * 5))
        
        # Episodic memory capacity increases dramatically with development
        self.episodic_memory.max_episodes = int(100 + self.development_level * 900)
        
        # Semantic memory also expands with development
        self.semantic_memory.max_items = int(500 + self.development_level * 9500)
        
        # Adjust consolidation threshold for long-term memory based on development
        if self.development_level >= 0.5:
            self.long_term_memory.consolidation_threshold = max(0.4, 0.7 - self.development_level * 0.3)
            
        # Adjust associative memory's hebbian rate based on development
        if self.development_level >= 0.6:
            self.associative_memory.hebbian_rate = min(0.05, 0.01 + self.development_level * 0.04)
        
        logger.debug(f"Memory capacity updated: WM={self.working_memory.capacity}, " 
                    f"EM={self.episodic_memory.max_episodes}, SM={self.semantic_memory.max_items}")
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process memory operations
        
        Args:
            input_data: Dictionary containing operation details
                Required keys: 'operation'
                
        Returns:
            Operation result
        """
        # Get operation type
        operation = input_data.get("operation", "")
        memory_type = input_data.get("memory_type", "")
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        result = {
            "process_id": process_id,
            "timestamp": time.time(),
            "module_id": self.module_id,
            "operation": operation,
            "memory_type": memory_type,
            "development_level": self.development_level
        }
        
        # Early development - only working memory operations
        if self.development_level < 0.2 and memory_type not in ["working", ""]:
            result.update({
                "status": "error",
                "error": "Memory system not developed enough for this operation",
                "available_types": ["working"]
            })
            return result
            
        # Route operation to appropriate handler
        if operation == "store":
            # Store operation
            result.update(self._handle_store(input_data))
            
        elif operation == "retrieve":
            # Retrieve operation
            result.update(self._handle_retrieve(input_data))
            
        elif operation == "consolidate":
            # Consolidation operation (only at higher development levels)
            if self.development_level >= 0.8:
                result.update(self._handle_consolidate(input_data))
            else:
                result.update({
                    "status": "error",
                    "error": "Memory consolidation not yet developed"
                })
                
        else:
            # Unknown operation
            result.update({
                "status": "error",
                "error": f"Unknown memory operation: {operation}"
            })
            
        # Publish result
        if self.event_bus:
            self.publish_message(
                "memory_result",
                {"result": result}
            )
            
        return result
    
    def _handle_store(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Handle memory storage operations
        
        Args:
            input_data: Storage operation data
            
        Returns:
            Operation result
        """
        memory_type = input_data.get("memory_type", "working")
        content = input_data.get("content", {})
        
        if not content:
            return {
                "status": "error",
                "error": "No content provided for storage"
            }
            
        if memory_type == "working":
            # Store in working memory
            item_id = input_data.get("item_id", f"wm_{uuid.uuid4().hex[:8]}")
            success = self.working_memory.add_item(item_id, content)
            
            return {
                "status": "success" if success else "error",
                "item_id": item_id,
                "memory_type": "working"
            }
            
        elif memory_type == "episodic" and self.development_level >= 0.4:
            # Store in episodic memory
            episode_id = self.episodic_memory.store_episode(content)
            
            return {
                "status": "success",
                "episode_id": episode_id,
                "memory_type": "episodic"
            }
            
        elif memory_type == "semantic" and self.development_level >= 0.6:
            # Store in semantic memory
            concept_id = self.semantic_memory.store_concept(content)
            
            return {
                "status": "success",
                "concept_id": concept_id,
                "memory_type": "semantic"
            }
            
        elif memory_type == "long_term" and self.development_level >= 0.7:
            # Store in long-term memory
            result = self.long_term_memory.store_memory(content)
            
            return {
                "status": result.get("status", "error"),
                "memory_id": result.get("memory_id", None),
                "memory_type": "long_term"
            }
            
        elif memory_type == "associative" and self.development_level >= 0.8:
            # Create association between memories
            if "source_id" in content and "target_id" in content:
                result = self.associative_memory.associate(
                    source_id=content["source_id"],
                    target_id=content["target_id"],
                    link_type=content.get("link_type", "general"),
                    strength=content.get("strength", 0.5)
                )
                return {
                    "status": result.get("status", "error"),
                    "association_id": result.get("association_id", None),
                    "memory_type": "associative"
                }
            else:
                return {
                    "status": "error",
                    "error": "Associative memory requires source_id and target_id"
                }
            
        else:
            return {
                "status": "error",
                "error": f"Memory type '{memory_type}' not available at development level {self.development_level}"
            }
    
    def _handle_retrieve(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Handle memory retrieval operations
        
        Args:
            input_data: Retrieval operation data
            
        Returns:
            Operation result with retrieved content
        """
        memory_type = input_data.get("memory_type", "working")
        
        if memory_type == "working":
            # Retrieve from working memory
            if "item_id" in input_data:
                # Retrieve specific item
                item_id = input_data["item_id"]
                item = self.working_memory.get_item(item_id)
                
                if item:
                    return {
                        "status": "success",
                        "item_id": item_id,
                        "content": item,
                        "memory_type": "working"
                    }
                else:
                    return {
                        "status": "error",
                        "error": f"Item '{item_id}' not found in working memory"
                    }
            else:
                # Retrieve all items
                items = self.working_memory.get_all_items()
                
                return {
                    "status": "success",
                    "items": items,
                    "count": len(items),
                    "memory_type": "working"
                }
                
        elif memory_type == "episodic" and self.development_level >= 0.4:
            # Retrieve from episodic memory
            if "episode_id" in input_data:
                # Retrieve specific episode
                episode_id = input_data["episode_id"]
                episode = self.episodic_memory.retrieve_episode(episode_id)
                
                if episode:
                    return {
                        "status": "success",
                        "episode_id": episode_id,
                        "content": episode,
                        "memory_type": "episodic"
                    }
                else:
                    return {
                        "status": "error",
                        "error": f"Episode '{episode_id}' not found in episodic memory"
                    }
            elif "timeframe" in input_data:
                # Retrieve by timeframe
                timeframe = input_data["timeframe"]
                start_time = timeframe.get("start", 0)
                end_time = timeframe.get("end", time.time())
                
                episodes = self.episodic_memory.retrieve_by_timeframe(start_time, end_time)
                
                return {
                    "status": "success",
                    "episodes": episodes,
                    "count": len(episodes),
                    "memory_type": "episodic",
                    "timeframe": timeframe
                }
            else:
                # Retrieve recent episodes
                count = input_data.get("count", 5)
                episodes = self.episodic_memory.retrieve_recent_episodes(count)
                
                return {
                    "status": "success",
                    "episodes": episodes,
                    "count": len(episodes),
                    "memory_type": "episodic"
                }
                
        elif memory_type == "semantic" and self.development_level >= 0.6:
            # Retrieve from semantic memory
            if "label" in input_data:
                # Retrieve by label
                label = input_data["label"]
                concept = self.semantic_memory.retrieve_by_label(label)
                
                if concept:
                    return {
                        "status": "success",
                        "label": label,
                        "content": concept,
                        "memory_type": "semantic"
                    }
                else:
                    return {
                        "status": "error",
                        "error": f"Concept '{label}' not found in semantic memory"
                    }
            elif "type" in input_data:
                # Retrieve by type
                concept_type = input_data["type"]
                limit = input_data.get("limit", 100)
                
                concepts = self.semantic_memory.retrieve_by_type(concept_type, limit)
                
                return {
                    "status": "success",
                    "type": concept_type,
                    "concepts": concepts,
                    "count": len(concepts),
                    "memory_type": "semantic"
                }
            else:
                return {
                    "status": "error",
                    "error": "No retrieval criteria specified for semantic memory"
                }
                
        elif memory_type == "long_term" and self.development_level >= 0.7:
            # Retrieve from long-term memory
            if "memory_id" in input_data:
                result = self.long_term_memory.retrieve_memory(input_data["memory_id"])
                return {
                    "status": result.get("status", "error"),
                    "memory": result.get("memory", None),
                    "memory_type": "long_term"
                }
            elif "query" in input_data:
                # Search by content
                result = self.long_term_memory.search_memories(
                    query=input_data["query"],
                    limit=input_data.get("limit", 5)
                )
                return {
                    "status": result.get("status", "error"),
                    "memories": result.get("memories", []),
                    "count": result.get("count", 0),
                    "memory_type": "long_term"
                }
            else:
                return {
                    "status": "error",
                    "error": "Must provide memory_id or query for long-term memory retrieval"
                }
                
        elif memory_type == "associative" and self.development_level >= 0.8:
            # Retrieve from associative memory
            if "memory_id" in input_data:
                result = self.associative_memory.get_associations(input_data["memory_id"])
                return {
                    "status": result.get("status", "error"),
                    "associations": result.get("associations", []),
                    "count": result.get("count", 0),
                    "memory_type": "associative"
                }
            elif "source_id" in input_data and "target_id" in input_data:
                # Find path between memories
                result = self.associative_memory.find_path(
                    source_id=input_data["source_id"],
                    target_id=input_data["target_id"],
                    max_depth=input_data.get("max_depth", 3)
                )
                return {
                    "status": result.get("status", "error"),
                    "path": result.get("path", []),
                    "path_length": result.get("path_length", 0),
                    "memory_type": "associative"
                }
            else:
                return {
                    "status": "error",
                    "error": "Must provide memory_id or source_id/target_id for associative memory retrieval"
                }
                
        else:
            return {
                "status": "error",
                "error": f"Memory type '{memory_type}' not available at development level {self.development_level}"
            }
    
    def _handle_consolidate(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Handle memory consolidation operations
        
        This involves moving information between memory systems,
        such as from working memory to long-term storage.
        
        Args:
            input_data: Consolidation operation data
            
        Returns:
            Operation result
        """
        # Only available at higher development levels
        if self.development_level < 0.8:
            return {
                "status": "error",
                "error": "Memory consolidation not yet developed"
            }
            
        source_type = input_data.get("source_type", "")
        target_type = input_data.get("target_type", "")
        
        if not source_type or not target_type:
            return {
                "status": "error",
                "error": "Source and target memory types must be specified"
            }
            
        # Handle different consolidation pathways
        if source_type == "working" and target_type == "episodic":
            # Consolidate working memory to episodic memory
            items = self.working_memory.get_all_items()
            
            # Create a single episode from all working memory items
            if items:
                episode_data = {
                    "content": items,
                    "source": "working_memory",
                    "timestamp": time.time(),
                    "consolidation": True
                }
                
                episode_id = self.episodic_memory.store_episode(episode_data)
                
                return {
                    "status": "success",
                    "source_type": source_type,
                    "target_type": target_type,
                    "episode_id": episode_id,
                    "item_count": len(items)
                }
            else:
                return {
                    "status": "error",
                    "error": "No items in working memory to consolidate"
                }
                
        elif source_type == "episodic" and target_type == "semantic":
            # Extract semantic information from episodes
            if "episode_ids" in input_data:
                episode_ids = input_data["episode_ids"]
                episodes = [
                    self.episodic_memory.retrieve_episode(ep_id)
                    for ep_id in episode_ids
                    if self.episodic_memory.retrieve_episode(ep_id)
                ]
            else:
                # Use recent episodes
                count = input_data.get("count", 5)
                episodes = self.episodic_memory.retrieve_recent_episodes(count)
                
            # Extract concepts from episodes
            # This is a simplified approach - in a real system this would involve
            # sophisticated concept extraction and generalization
            concepts = []
            for episode in episodes:
                # Extract potential concepts based on episode content
                if isinstance(episode.get("content"), dict) and "label" in episode["content"]:
                    # If content already has a label field, it might be a concept
                    concepts.append(episode["content"])
                elif isinstance(episode.get("content"), list):
                    # If content is a list, check each item
                    for item in episode["content"]:
                        if isinstance(item, dict) and "label" in item:
                            concepts.append(item)
                            
            # Store extracted concepts
            concept_ids = []
            for concept in concepts:
                # Add a type if not present
                if "type" not in concept:
                    concept["type"] = "extracted"
                    
                concept_id = self.semantic_memory.store_concept(concept)
                concept_ids.append(concept_id)
                
            return {
                "status": "success",
                "source_type": source_type,
                "target_type": target_type,
                "concept_ids": concept_ids,
                "concept_count": len(concept_ids)
            }
            
        else:
            return {
                "status": "error",
                "error": f"Unsupported consolidation pathway: {source_type} to {target_type}"
            }
    
    def _handle_message(self, message: Message):
        """
        Handle incoming messages
        
        Args:
            message: The message to handle
        """
        if message.message_type == "memory_store":
            # Memory storage request
            if message.content:
                self.process_input({
                    "operation": "store",
                    "process_id": message.id,
                    **message.content
                })
                
        elif message.message_type == "memory_retrieve":
            # Memory retrieval request
            if message.content:
                self.process_input({
                    "operation": "retrieve",
                    "process_id": message.id,
                    **message.content
                })
                
        elif message.message_type == "perception_result":
            # Store perception results in working memory
            if message.content and "result" in message.content:
                result = message.content["result"]
                
                # Only store if development level is sufficient
                if self.development_level >= 0.2:
                    # Store in working memory
                    item_id = f"perception_{result.get('process_id', message.id)}"
                    self.working_memory.add_item(item_id, result)
                    
                    # At higher development, also store in episodic memory
                    if self.development_level >= 0.4:
                        # Only store significant perceptions in episodic memory
                        # Here we use a simple heuristic: if there are recognized patterns
                        if "patterns" in result and result["patterns"]:
                            self.episodic_memory.store_episode({
                                "content": result,
                                "source": "perception",
                                "timestamp": result.get("timestamp", message.timestamp)
                            })
                
        elif message.message_type == "attention_focus_update":
            # Store attention focus in working memory
            if message.content and "result" in message.content:
                result = message.content["result"]
                
                # Store current focus in working memory
                if result.get("current_focus"):
                    item_id = f"attention_{result.get('process_id', message.id)}"
                    self.working_memory.add_item(item_id, result["current_focus"])
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of the module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # Update development level
        prev_level = self.development_level
        new_level = super().update_development(amount)
        
        # If development level changed significantly, adjust memory systems
        if int(prev_level * 10) != int(new_level * 10):
            logger.info(f"Memory system upgraded to development level {new_level:.1f}")
            self._adjust_memory_for_development()
            
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """Get the current state of the module"""
        state = super().get_state()
        
        # Add memory-specific state
        state.update({
            "working_memory": self.working_memory.get_state(),
            "episodic_memory": self.episodic_memory.get_state() if self.development_level >= 0.4 else "Not yet developed",
            "semantic_memory": self.semantic_memory.get_state() if self.development_level >= 0.6 else "Not yet developed",
            "long_term_memory": self.long_term_memory.get_state() if self.development_level >= 0.7 else "Not yet developed",
            "associative_memory": self.associative_memory.get_state() if self.development_level >= 0.8 else "Not yet developed"
        })
        
        return state 

#######################

#motivation\drives.py#
#######################

"""
Drives Module

This module implements basic motivational drives that energize behavior
and direct the system toward need satisfaction. Drives develop from simple
approach/avoidance in early stages to complex, integrated motivations in
later developmental stages.
"""

import logging
import time
import uuid
from typing import Dict, List, Any, Optional, Set, Tuple
from datetime import datetime
import os
import json
import numpy as np
import torch
from pathlib import Path

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.motivation.models import Drive, MotivationalState
from lmm_project.modules.motivation.neural_net import MotivationNeuralNetwork
from lmm_project.utils.llm_client import LLMClient

logger = logging.getLogger(__name__)

class Drives(BaseModule):
    """
    Handles basic motivational drives
    
    This module maintains fundamental motivational drives,
    regulates their intensity, and generates activation
    signals based on current drive states.
    """
    
    # Developmental milestones for the drives module
    development_milestones = {
        0.0: "Basic approach/avoidance drives",
        0.2: "Physiological drives (energy, rest)",
        0.4: "Exploration and curiosity drives",
        0.6: "Social and affiliation drives",
        0.8: "Competence and mastery drives",
        1.0: "Self-actualization drives"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the drives module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level
        """
        super().__init__(
            module_id=module_id, 
            module_type="drives", 
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Initialize drive storage
        self.drives: Dict[str, Drive] = {}
        
        # Last update timestamp
        self.last_update = datetime.now()
        
        # Drive activation thresholds
        self.activation_threshold = 0.7  # Drives above this intensity trigger active seeking
        
        # Neural network for drive processing
        self.neural_net = MotivationNeuralNetwork(
            input_dim=32,
            hidden_dim=64,
            output_dim=16,
            network_type="drive",
            development_level=development_level
        )
        
        # Drive satisfaction data - tracks what satisfies each drive
        self.satisfaction_data: Dict[str, Set[str]] = {}
        
        # Storage path
        self.storage_dir = Path("storage/motivation/drives")
        os.makedirs(self.storage_dir, exist_ok=True)
        
        # Initialize basic drives based on development level
        self._initialize_drives()
        
        # Subscribe to messages
        if self.event_bus:
            self.subscribe_to_message("perception_result")
            self.subscribe_to_message("action_performed")
            self.subscribe_to_message("reward_received")
            self.subscribe_to_message("need_update")
            self.subscribe_to_message("drive_query")
    
    def _initialize_drives(self):
        """Initialize basic drives based on current development level"""
        # Always present drives
        basic_drives = [
            Drive(
                name="energy",
                description="Drive for maintaining energy levels",
                category="physiological",
                priority=0.9,
                decay_rate=0.02,
                satisfying_actions={"eat", "drink"}
            ),
            Drive(
                name="rest",
                description="Drive for rest and recovery",
                category="physiological",
                priority=0.8,
                decay_rate=0.015,
                satisfying_actions={"sleep", "rest"}
            ),
            Drive(
                name="safety",
                description="Drive for security and stability",
                category="security",
                priority=0.85,
                decay_rate=0.01,
                satisfying_actions={"seek_shelter", "avoid_danger"}
            )
        ]
        
        for drive in basic_drives:
            self.drives[drive.id] = drive
        
        # Development-dependent drives
        if self.development_level >= 0.2:
            self.drives[str(uuid.uuid4())] = Drive(
                name="comfort",
                description="Drive for physical comfort",
                category="physiological",
                priority=0.6,
                decay_rate=0.01,
                satisfying_actions={"adjust_temperature", "find_comfort"}
            )
        
        if self.development_level >= 0.4:
            self.drives[str(uuid.uuid4())] = Drive(
                name="curiosity",
                description="Drive for exploration and information",
                category="cognitive",
                priority=0.7,
                decay_rate=0.008,
                satisfying_actions={"explore", "learn", "investigate"}
            )
            
        if self.development_level >= 0.6:
            self.drives[str(uuid.uuid4())] = Drive(
                name="social",
                description="Drive for social connection",
                category="social",
                priority=0.75,
                decay_rate=0.005,
                satisfying_actions={"interact", "communicate", "bond"}
            )
            
            self.drives[str(uuid.uuid4())] = Drive(
                name="play",
                description="Drive for playful activity",
                category="cognitive",
                priority=0.6,
                decay_rate=0.01,
                satisfying_actions={"play", "experiment"}
            )
            
        if self.development_level >= 0.8:
            self.drives[str(uuid.uuid4())] = Drive(
                name="mastery",
                description="Drive for competence and skill development",
                category="cognitive",
                priority=0.7,
                decay_rate=0.005,
                satisfying_actions={"practice", "develop_skill"}
            )
            
            self.drives[str(uuid.uuid4())] = Drive(
                name="autonomy",
                description="Drive for self-directed action",
                category="cognitive",
                priority=0.65,
                decay_rate=0.005,
                satisfying_actions={"make_choices", "self_direct"}
            )
            
        if self.development_level >= 0.9:
            self.drives[str(uuid.uuid4())] = Drive(
                name="creativity",
                description="Drive for creative expression",
                category="cognitive",
                priority=0.6,
                decay_rate=0.004,
                satisfying_actions={"create", "innovate", "express"}
            )
            
            self.drives[str(uuid.uuid4())] = Drive(
                name="meaning",
                description="Drive for meaning and purpose",
                category="existential",
                priority=0.75,
                decay_rate=0.003,
                satisfying_actions={"reflect", "connect_meaning", "pursue_purpose"}
            )
        
        logger.info(f"Initialized {len(self.drives)} drives at development level {self.development_level}")
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to update drive states
        
        Args:
            input_data: Input data with operation details
                Supported operations:
                - get_drives: Get current drive states
                - update_drive: Update a specific drive
                - get_active_drives: Get drives above activation threshold
                - satisfy_drive: Satisfy a specific drive
            
        Returns:
            Operation result with drive information
        """
        operation = input_data.get("operation", "")
        result = {
            "operation": operation,
            "timestamp": datetime.now().isoformat(),
            "module_id": self.module_id
        }
        
        # Update all drives based on elapsed time
        self._update_all_drives()
        
        if operation == "get_drives":
            # Return all drive states
            drives_info = {
                drive_id: {
                    "name": drive.name,
                    "intensity": drive.intensity,
                    "priority": drive.priority,
                    "category": drive.category
                } for drive_id, drive in self.drives.items()
            }
            
            result.update({
                "status": "success",
                "drives": drives_info,
                "count": len(drives_info)
            })
            
        elif operation == "update_drive":
            # Update a specific drive
            drive_id = input_data.get("drive_id")
            change = input_data.get("change", 0.0)
            
            if drive_id and drive_id in self.drives:
                drive = self.drives[drive_id]
                drive.update_intensity(change)
                
                result.update({
                    "status": "success",
                    "drive_id": drive_id,
                    "name": drive.name,
                    "new_intensity": drive.intensity
                })
            else:
                result.update({
                    "status": "error",
                    "error": f"Drive not found: {drive_id}",
                    "available_drives": list(self.drives.keys())
                })
                
        elif operation == "get_active_drives":
            # Get drives above activation threshold
            active_drives = self._get_active_drives()
            
            result.update({
                "status": "success",
                "active_drives": active_drives,
                "count": len(active_drives),
                "activation_threshold": self.activation_threshold
            })
            
        elif operation == "satisfy_drive":
            # Satisfy a specific drive
            drive_id = input_data.get("drive_id")
            amount = input_data.get("amount", 0.3)
            
            if drive_id and drive_id in self.drives:
                drive = self.drives[drive_id]
                old_intensity = drive.intensity
                drive.update_intensity(-amount)  # Negative change = reduction in drive
                
                result.update({
                    "status": "success",
                    "drive_id": drive_id,
                    "name": drive.name,
                    "old_intensity": old_intensity,
                    "new_intensity": drive.intensity,
                    "satisfaction_amount": amount
                })
                
                # Publish drive satisfaction message
                if self.event_bus:
                    self.publish_message("drive_satisfied", {
                        "drive_id": drive_id,
                        "drive_name": drive.name,
                        "satisfaction_amount": amount
                    })
            else:
                result.update({
                    "status": "error",
                    "error": f"Drive not found: {drive_id}",
                    "available_drives": list(self.drives.keys())
                })
                
        else:
            result.update({
                "status": "error",
                "error": f"Unknown operation: {operation}",
                "available_operations": ["get_drives", "update_drive", "get_active_drives", "satisfy_drive"]
            })
            
        return result
    
    def _update_all_drives(self):
        """Update all drives based on time elapsed since last update"""
        now = datetime.now()
        elapsed_seconds = (now - self.last_update).total_seconds()
        
        # Don't update if very little time has passed
        if elapsed_seconds < 0.1:
            return
            
        for drive_id, drive in self.drives.items():
            # Apply natural decay based on drive's decay rate
            drive.decay(elapsed_seconds)
        
        self.last_update = now
    
    def _get_active_drives(self) -> List[Dict[str, Any]]:
        """Get drives that are above the activation threshold"""
        active_drives = []
        
        for drive_id, drive in self.drives.items():
            if drive.intensity >= self.activation_threshold:
                active_drives.append({
                    "id": drive_id,
                    "name": drive.name,
                    "intensity": drive.intensity,
                    "priority": drive.priority,
                    "category": drive.category,
                    "urgency": drive.intensity * drive.priority  # Combined measure of importance
                })
                
        # Sort by urgency (intensity * priority)
        active_drives.sort(key=lambda d: d["urgency"], reverse=True)
        return active_drives
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase the development level
            
        Returns:
            New development level
        """
        old_level = self.development_level
        new_level = super().update_development(amount)
        
        # If we crossed a threshold, update available drives
        if int(old_level * 10) != int(new_level * 10):
            self._initialize_drives()
            
            # Also update neural network development
            self.neural_net.update_development(amount)
            
            logger.info(f"Drives module advanced to development level {new_level:.2f}")
            
        return new_level
    
    def _handle_message(self, message: Message):
        """
        Handle incoming messages
        
        Args:
            message: The message to process
        """
        if message.message_type == "perception_result":
            # Check if perception relates to drive satisfaction
            if message.content and "result" in message.content:
                perception = message.content["result"]
                self._process_perception(perception)
                
        elif message.message_type == "action_performed":
            # Check if action satisfies any drives
            if message.content and "action" in message.content:
                action = message.content["action"]
                self._process_action(action)
                
        elif message.message_type == "reward_received":
            # Reward may indicate drive satisfaction
            if message.content and "reward" in message.content:
                reward = message.content["reward"]
                self._process_reward(reward)
                
        elif message.message_type == "need_update":
            # Need satisfaction may affect drives
            if message.content and "need_id" in message.content:
                need_update = message.content
                self._process_need_update(need_update)
                
        elif message.message_type == "drive_query":
            # Direct query about drive states
            if message.content and "query_type" in message.content:
                query = message.content
                self._process_drive_query(query, message.id)
    
    def _process_perception(self, perception: Dict[str, Any]):
        """Process perception data for potential drive updates"""
        # Check if perception contains information relevant to drives
        if "type" in perception:
            if perception["type"] == "food" and "energy" in [d.name for d in self.drives.values()]:
                # Found food, potentially satisfy energy drive
                for drive_id, drive in self.drives.items():
                    if drive.name == "energy":
                        drive.update_intensity(-0.2)  # Reduce hunger drive
                        break
                        
            elif perception["type"] == "danger" and "safety" in [d.name for d in self.drives.values()]:
                # Detected danger, increase safety drive
                for drive_id, drive in self.drives.items():
                    if drive.name == "safety":
                        drive.update_intensity(0.3)  # Increase safety drive
                        break
                        
            elif perception["type"] == "novelty" and "curiosity" in [d.name for d in self.drives.values()]:
                # Found something novel, potentially satisfy curiosity
                for drive_id, drive in self.drives.items():
                    if drive.name == "curiosity":
                        drive.update_intensity(-0.15)  # Satisfy curiosity somewhat
                        break
    
    def _process_action(self, action: Dict[str, Any]):
        """Process action data for potential drive satisfaction"""
        if "name" in action:
            action_name = action["name"]
            
            # Check all drives to see if this action satisfies any
            for drive_id, drive in self.drives.items():
                if action_name in drive.satisfying_actions:
                    # This action satisfies this drive
                    satisfaction_amount = action.get("effectiveness", 0.3)
                    drive.update_intensity(-satisfaction_amount)
                    
                    # Publish drive satisfaction message
                    if self.event_bus:
                        self.publish_message("drive_satisfied", {
                            "drive_id": drive_id,
                            "drive_name": drive.name,
                            "action": action_name,
                            "satisfaction_amount": satisfaction_amount
                        })
    
    def _process_reward(self, reward: Dict[str, Any]):
        """Process reward data for potential drive adjustments"""
        # Rewards may indicate drive satisfaction
        if "magnitude" in reward and "source" in reward:
            magnitude = reward["magnitude"]
            source = reward["source"]
            
            # Check if this reward is associated with a drive
            for drive_id, drive in self.drives.items():
                if source in drive.satisfying_actions:
                    drive.update_intensity(-magnitude * 0.5)  # Adjust based on reward magnitude
    
    def _process_need_update(self, need_update: Dict[str, Any]):
        """Process need update for potential drive adjustments"""
        # Certain needs may be linked to drives
        need_id = need_update.get("need_id")
        need_name = need_update.get("need_name", "")
        satisfaction = need_update.get("satisfaction", 0.0)
        
        # Map need types to drive categories
        need_to_drive_map = {
            "physiological": ["energy", "rest", "comfort"],
            "safety": ["safety"],
            "social": ["social"],
            "esteem": ["mastery", "autonomy"],
            "cognitive": ["curiosity", "play"],
            "self_actualization": ["creativity", "meaning"]
        }
        
        # Update drives related to this need category
        need_category = need_update.get("category", "")
        if need_category in need_to_drive_map:
            related_drive_names = need_to_drive_map[need_category]
            for drive_id, drive in self.drives.items():
                if drive.name in related_drive_names:
                    # If need satisfaction increased, decrease related drive intensity
                    if "satisfaction_delta" in need_update:
                        delta = need_update["satisfaction_delta"]
                        if delta > 0:
                            drive.update_intensity(-delta * 0.3)
    
    def _process_drive_query(self, query: Dict[str, Any], message_id: str):
        """Process a direct query about drive states"""
        query_type = query.get("query_type")
        
        if query_type == "active_drives":
            # Return active drives
            active_drives = self._get_active_drives()
            
            if self.event_bus:
                self.publish_message("drive_query_response", {
                    "query_id": message_id,
                    "active_drives": active_drives,
                    "count": len(active_drives)
                })
                
        elif query_type == "drive_state":
            # Return specific drive state
            drive_id = query.get("drive_id")
            if drive_id and drive_id in self.drives:
                drive = self.drives[drive_id]
                
                if self.event_bus:
                    self.publish_message("drive_query_response", {
                        "query_id": message_id,
                        "drive_id": drive_id,
                        "name": drive.name,
                        "intensity": drive.intensity,
                        "priority": drive.priority,
                        "category": drive.category
                    })
            else:
                if self.event_bus:
                    self.publish_message("drive_query_response", {
                        "query_id": message_id,
                        "error": f"Drive not found: {drive_id}",
                        "available_drives": list(self.drives.keys())
                    })
    
    def get_state(self) -> Dict[str, Any]:
        """Get the current state of the module"""
        drive_states = {}
        for drive_id, drive in self.drives.items():
            drive_states[drive_id] = {
                "name": drive.name,
                "intensity": drive.intensity,
                "priority": drive.priority,
                "category": drive.category,
                "last_updated": drive.last_updated.isoformat()
            }
            
        return {
            "development_level": self.development_level,
            "drives": drive_states,
            "activation_threshold": self.activation_threshold,
            "last_update": self.last_update.isoformat(),
            "neural_net": self.neural_net.get_state()
        }
        
    def save_state(self, state_dir: str) -> str:
        """
        Save the current state to disk
        
        Args:
            state_dir: Directory to save state in
            
        Returns:
            Path to saved state file
        """
        os.makedirs(state_dir, exist_ok=True)
        state_path = os.path.join(state_dir, f"{self.module_id}_state.json")
        
        # Convert drives to serializable format
        drive_states = {}
        for drive_id, drive in self.drives.items():
            drive_states[drive_id] = {
                "name": drive.name,
                "description": drive.description,
                "intensity": drive.intensity,
                "decay_rate": drive.decay_rate,
                "priority": drive.priority,
                "category": drive.category,
                "satisfying_actions": list(drive.satisfying_actions),
                "last_updated": drive.last_updated.isoformat(),
                "is_active": drive.is_active
            }
        
        state = {
            "module_id": self.module_id,
            "module_type": "drives",
            "development_level": self.development_level,
            "drives": drive_states,
            "activation_threshold": self.activation_threshold,
            "last_update": self.last_update.isoformat()
        }
        
        with open(state_path, 'w') as f:
            json.dump(state, f, indent=2)
            
        # Also save neural network state
        net_path = os.path.join(state_dir, f"{self.module_id}_network.pt")
        self.neural_net.save(net_path)
        
        logger.info(f"Saved drives state to {state_path}")
        return state_path
        
    def load_state(self, state_path: str) -> bool:
        """
        Load state from disk
        
        Args:
            state_path: Path to state file
            
        Returns:
            Success flag
        """
        if not os.path.exists(state_path):
            logger.error(f"State file not found: {state_path}")
            return False
            
        try:
            with open(state_path, 'r') as f:
                state = json.load(f)
                
            # Verify this is the right kind of state
            if state.get("module_type") != "drives":
                logger.error(f"Invalid state file type: {state.get('module_type')}")
                return False
                
            # Load development level
            self.development_level = state.get("development_level", 0.0)
            
            # Load activation threshold
            self.activation_threshold = state.get("activation_threshold", 0.7)
            
            # Load last update time
            last_update_str = state.get("last_update", datetime.now().isoformat())
            self.last_update = datetime.fromisoformat(last_update_str)
            
            # Load drives
            drive_states = state.get("drives", {})
            self.drives = {}
            
            for drive_id, drive_data in drive_states.items():
                drive = Drive(
                    id=drive_id,
                    name=drive_data["name"],
                    description=drive_data.get("description", ""),
                    intensity=drive_data["intensity"],
                    decay_rate=drive_data["decay_rate"],
                    priority=drive_data["priority"],
                    category=drive_data["category"],
                    satisfying_actions=set(drive_data.get("satisfying_actions", [])),
                    last_updated=datetime.fromisoformat(drive_data["last_updated"]),
                    is_active=drive_data.get("is_active", True)
                )
                self.drives[drive_id] = drive
                
            # Load neural network state
            net_path = state_path.replace("_state.json", "_network.pt")
            if os.path.exists(net_path):
                self.neural_net.load(net_path)
                
            logger.info(f"Loaded drives state from {state_path} with {len(self.drives)} drives")
            return True
            
        except Exception as e:
            logger.error(f"Error loading drives state: {e}")
            return False


#######################

#motivation\goal_setting.py#
#######################

# TODO: Implement the GoalSetting class to establish and pursue objectives
# This component should be able to:
# - Create goals based on drives, needs, and values
# - Maintain goal hierarchies with sub-goals
# - Track progress toward goal achievement
# - Adjust goals based on feasibility and changing priorities

# TODO: Implement developmental progression in goal setting:
# - Simple immediate goals in early stages
# - Short-term goal sequences in childhood
# - Longer-term goal planning in adolescence
# - Complex hierarchical goals with abstract endpoints in adulthood

# TODO: Create mechanisms for:
# - Goal generation: Create goals from various motivational inputs
# - Goal evaluation: Assess importance and feasibility of potential goals
# - Progress monitoring: Track advancement toward goals
# - Goal adjustment: Modify goals when necessary

# TODO: Implement different goal types:
# - Approach goals: Aimed at achieving positive outcomes
# - Avoidance goals: Aimed at preventing negative outcomes
# - Learning goals: Focused on skill acquisition
# - Performance goals: Focused on demonstrating competence

# TODO: Connect to executive function and belief modules
# Goal setting should guide executive planning processes
# and be informed by beliefs about self-efficacy

from typing import Dict, List, Any, Optional
from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus

class GoalSetting(BaseModule):
    """
    Establishes and manages goal pursuit
    
    This module creates goals based on motivational states,
    organizes them into hierarchies, tracks progress,
    and adjusts goals as needed.
    """
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the goal setting module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="goal_setting", event_bus=event_bus)
        
        # TODO: Initialize goal representation structures
        # TODO: Set up goal hierarchy management
        # TODO: Create progress tracking mechanisms
        # TODO: Initialize goal adjustment system
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to update goals and goal progress
        
        Args:
            input_data: Dictionary containing goal-related information
            
        Returns:
            Dictionary with the updated goal states
        """
        # TODO: Implement goal updating logic
        # TODO: Generate new goals from motivational inputs
        # TODO: Update progress toward existing goals
        # TODO: Adjust or abandon goals when appropriate
        
        return {
            "status": "not_implemented",
            "module_id": self.module_id,
            "module_type": self.module_type
        }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # TODO: Implement development progression for goal setting
        # TODO: Increase goal complexity with development
        # TODO: Enhance goal time horizon with development
        
        return super().update_development(amount)


#######################

#motivation\models.py#
#######################

from pydantic import BaseModel, Field, field_validator
from typing import Dict, List, Any, Optional, Set, Union, Literal
from datetime import datetime
import uuid
import numpy as np

class Drive(BaseModel):
    """
    Represents a basic biological or psychological drive
    
    Drives are fundamental motivational forces that push the agent 
    toward certain behaviors. They represent internal states that 
    require regulation.
    """
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    name: str
    description: str
    # Current intensity of this drive (0.0 = satisfied, 1.0 = maximum intensity)
    intensity: float = Field(default=0.0, ge=0.0, le=1.0)
    # How quickly this drive intensity increases over time (higher = faster)
    decay_rate: float = Field(default=0.01, ge=0.0, le=1.0)
    # Priority level of this drive (higher = more important)
    priority: float = Field(default=0.5, ge=0.0, le=1.0)
    # Category of drive (physiological, safety, cognitive, etc.)
    category: str
    # What actions can satisfy this drive
    satisfying_actions: Set[str] = Field(default_factory=set)
    # Last time this drive was updated
    last_updated: datetime = Field(default_factory=datetime.now)
    # Whether this drive is active (some drives only activate at certain dev levels)
    is_active: bool = Field(default=True)
    
    def update_intensity(self, amount: float) -> None:
        """Update the drive intensity, ensuring it stays within bounds"""
        self.intensity = max(0.0, min(1.0, self.intensity + amount))
        self.last_updated = datetime.now()
    
    def decay(self, time_delta: float) -> None:
        """Increase drive intensity naturally over time"""
        decay_amount = self.decay_rate * time_delta
        self.update_intensity(decay_amount)

class Need(BaseModel):
    """
    Represents a psychological need
    
    Needs are higher-level than drives and represent psychological
    requirements for well-being. Based on theories like Maslow's hierarchy.
    """
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    name: str
    description: str
    # Current satisfaction level (0.0 = unsatisfied, 1.0 = fully satisfied)
    satisfaction: float = Field(default=0.5, ge=0.0, le=1.0)
    # How quickly satisfaction decays (higher = faster decay)
    decay_rate: float = Field(default=0.005, ge=0.0, le=0.1)
    # Minimum development level required for this need to be active
    min_development_level: float = Field(default=0.0, ge=0.0, le=1.0)
    # Hierarchy level (1 = most basic, 5 = highest)
    hierarchy_level: int = Field(default=1, ge=1, le=5)
    # What other needs must be satisfied before this one becomes active
    prerequisites: List[str] = Field(default_factory=list)
    # Related drives that contribute to satisfying this need
    related_drives: List[str] = Field(default_factory=list)
    # What goals can satisfy this need
    satisfying_goals: Set[str] = Field(default_factory=set)
    # Last time this need was updated
    last_updated: datetime = Field(default_factory=datetime.now)
    # Whether this need is currently active
    is_active: bool = Field(default=True)
    
    def update_satisfaction(self, amount: float) -> None:
        """Update the satisfaction level, ensuring it stays within bounds"""
        self.satisfaction = max(0.0, min(1.0, self.satisfaction + amount))
        self.last_updated = datetime.now()
    
    def decay(self, time_delta: float) -> None:
        """Decrease satisfaction naturally over time"""
        decay_amount = self.decay_rate * time_delta
        self.update_satisfaction(-decay_amount)

class Goal(BaseModel):
    """
    Represents a goal or intention
    
    Goals are targets for action that help satisfy needs and drives.
    They can be hierarchical, with subgoals contributing to higher goals.
    """
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    name: str
    description: str
    # Current progress toward goal (0.0 = not started, 1.0 = completed)
    progress: float = Field(default=0.0, ge=0.0, le=1.0)
    # Importance of this goal (higher = more important)
    importance: float = Field(default=0.5, ge=0.0, le=1.0)
    # How urgent is this goal (higher = more urgent)
    urgency: float = Field(default=0.5, ge=0.0, le=1.0)
    # Specific needs this goal can satisfy
    satisfies_needs: List[str] = Field(default_factory=list)
    # Drives this goal can satisfy
    satisfies_drives: List[str] = Field(default_factory=list)
    # Parent goal, if this is a subgoal
    parent_goal_id: Optional[str] = None
    # Subgoals that contribute to this goal
    subgoals: List[str] = Field(default_factory=list)
    # Expected reward for completing this goal
    expected_reward: float = Field(default=0.5, ge=0.0, le=1.0)
    # Whether this goal is currently being pursued
    is_active: bool = Field(default=True)
    # When this goal was created
    created_at: datetime = Field(default_factory=datetime.now)
    # When this goal was last updated
    last_updated: datetime = Field(default_factory=datetime.now)
    # Deadline for goal completion, if any
    deadline: Optional[datetime] = None
    # Whether this goal has been achieved
    is_achieved: bool = Field(default=False)
    # Goal type (approach or avoidance)
    goal_type: Literal["approach", "avoidance"] = "approach"
    
    def update_progress(self, amount: float) -> None:
        """Update progress toward goal"""
        self.progress = max(0.0, min(1.0, self.progress + amount))
        self.last_updated = datetime.now()
        if self.progress >= 1.0:
            self.is_achieved = True
    
    def abandon(self) -> None:
        """Abandon this goal"""
        self.is_active = False
        self.last_updated = datetime.now()

class RewardEvent(BaseModel):
    """
    Represents a reward or reinforcement event
    
    Reward events are used to reinforce behaviors and learn
    from experience. They can be positive (rewards) or negative
    (punishments).
    """
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    # Type of reward (intrinsic or extrinsic)
    reward_type: Literal["intrinsic", "extrinsic"] = "intrinsic"
    # Magnitude of reward (positive = reward, negative = punishment)
    magnitude: float = Field(default=0.5, ge=-1.0, le=1.0)
    # What action or goal triggered this reward
    source: str
    # Specific context where the reward occurred
    context: str
    # Timestamp when this reward was received
    timestamp: datetime = Field(default_factory=datetime.now)
    # What drives or needs were affected by this reward
    affected_drives: List[str] = Field(default_factory=list)
    affected_needs: List[str] = Field(default_factory=list)
    # Whether this reward has been processed for learning
    is_processed: bool = Field(default=False)
    # Additional metadata about this reward
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    @field_validator('magnitude')
    @classmethod
    def validate_magnitude(cls, v: float) -> float:
        """Ensure magnitude is within bounds"""
        return max(-1.0, min(1.0, v))

class MotivationalState(BaseModel):
    """
    Represents the current motivational state of the system
    
    This includes the current state of all drives, needs, and goals,
    as well as overall motivational metrics.
    """
    # Current drives
    drives: Dict[str, Drive] = Field(default_factory=dict)
    # Current needs
    needs: Dict[str, Need] = Field(default_factory=dict)
    # Current goals
    goals: Dict[str, Goal] = Field(default_factory=dict)
    # Recent reward events
    recent_rewards: List[RewardEvent] = Field(default_factory=list)
    # Overall motivation level
    motivation_level: float = Field(default=0.5, ge=0.0, le=1.0)
    # Current development level
    development_level: float = Field(default=0.0, ge=0.0, le=1.0)
    # Dominant current motivation
    dominant_motivation: Optional[str] = None
    # Timestamp when this state was created
    timestamp: datetime = Field(default_factory=datetime.now)
    
    def update_motivation_level(self) -> None:
        """Update the overall motivation level based on drives and needs"""
        if not self.drives and not self.needs:
            return
            
        # Calculate motivation from active drives
        drive_motivation = 0.0
        active_drives = [d for d in self.drives.values() if d.is_active]
        if active_drives:
            drive_motivation = sum(d.intensity * d.priority for d in active_drives) / sum(d.priority for d in active_drives)
            
        # Calculate motivation from needs
        need_motivation = 0.0
        active_needs = [n for n in self.needs.values() if n.is_active]
        if active_needs:
            # Invert satisfaction since lower satisfaction means higher motivation
            need_motivation = sum((1.0 - n.satisfaction) * (6 - n.hierarchy_level) 
                                  for n in active_needs) / sum(6 - n.hierarchy_level 
                                                             for n in active_needs)
        
        # Combine drive and need motivation (weight shifts with development)
        drive_weight = max(0.2, 1.0 - self.development_level * 0.8)
        need_weight = 1.0 - drive_weight
        
        if active_drives and active_needs:
            self.motivation_level = (drive_motivation * drive_weight + 
                                     need_motivation * need_weight)
        elif active_drives:
            self.motivation_level = drive_motivation
        elif active_needs:
            self.motivation_level = need_motivation
        
        # Update dominant motivation
        self._update_dominant_motivation()
    
    def _update_dominant_motivation(self) -> None:
        """Determine the dominant current motivation"""
        # Check drives first
        max_drive = None
        max_drive_value = -1.0
        
        for drive_id, drive in self.drives.items():
            if drive.is_active:
                drive_value = drive.intensity * drive.priority
                if drive_value > max_drive_value:
                    max_drive_value = drive_value
                    max_drive = drive_id
        
        # Check needs
        max_need = None
        max_need_value = -1.0
        
        for need_id, need in self.needs.items():
            if need.is_active:
                need_value = (1.0 - need.satisfaction) * (6 - need.hierarchy_level)
                if need_value > max_need_value:
                    max_need_value = need_value
                    max_need = need_id
        
        # Compare max drive and max need
        if max_drive and max_need:
            if max_drive_value > max_need_value:
                self.dominant_motivation = f"drive:{max_drive}"
            else:
                self.dominant_motivation = f"need:{max_need}"
        elif max_drive:
            self.dominant_motivation = f"drive:{max_drive}"
        elif max_need:
            self.dominant_motivation = f"need:{max_need}"
        else:
            self.dominant_motivation = None

class MotivationNeuralState(BaseModel):
    """Neural state information for the motivation system"""
    # Input embedding from current state
    input_embedding: Optional[List[float]] = None
    # Output activation patterns
    output_activations: Dict[str, List[float]] = Field(default_factory=dict)
    # Current learning rate
    learning_rate: float = Field(default=0.01, ge=0.0, le=1.0)
    # Hebbian connection strengths
    connection_strengths: Dict[str, List[float]] = Field(default_factory=dict)
    # Developmental level of neural substrate
    development_level: float = Field(default=0.0, ge=0.0, le=1.0)
    # Last update timestamp
    last_updated: datetime = Field(default_factory=datetime.now)
    
    def add_activation(self, activation_type: str, activations: List[float]) -> None:
        """Store activation pattern"""
        self.output_activations[activation_type] = activations
        self.last_updated = datetime.now()
    
    def update_connection(self, connection_type: str, strengths: List[float]) -> None:
        """Update connection strengths"""
        self.connection_strengths[connection_type] = strengths
        self.last_updated = datetime.now()


#######################

#motivation\needs.py#
#######################

"""
Needs Module

This module implements psychological needs based on theories like Maslow's hierarchy.
It manages need-based motivation and tracks satisfaction levels across different
need categories, from basic physiological needs to higher-level self-actualization.
"""

import logging
import time
import uuid
from typing import Dict, List, Any, Optional, Set, Tuple
from datetime import datetime
import os
import json
import numpy as np
from pathlib import Path

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.motivation.models import Need, MotivationalState
from lmm_project.modules.motivation.neural_net import MotivationNeuralNetwork

logger = logging.getLogger(__name__)

class Needs(BaseModule):
    """
    Handles psychological needs based on Maslow's hierarchy
    
    This module tracks need satisfaction levels, generates motivation
    based on unsatisfied needs, and implements hierarchical satisfaction
    principles (higher needs become active when lower needs are satisfied).
    """
    
    # Developmental milestones for needs module
    development_milestones = {
        0.0: "Basic physiological needs",
        0.2: "Safety and security needs",
        0.4: "Belonging and social needs",
        0.6: "Esteem and recognition needs",
        0.8: "Cognitive needs", 
        1.0: "Self-actualization needs"
    }
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None, development_level: float = 0.0):
        """
        Initialize the needs module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
            development_level: Initial developmental level
        """
        super().__init__(
            module_id=module_id,
            module_type="needs",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Initialize need storage
        self.needs: Dict[str, Need] = {}
        
        # Last update timestamp
        self.last_update = datetime.now()
        
        # Neural network for need evaluation
        self.neural_net = MotivationNeuralNetwork(
            input_dim=48,
            hidden_dim=64,
            output_dim=16,
            network_type="need",
            development_level=development_level
        )
        
        # Need priority thresholds
        self.high_priority_threshold = 0.3  # Needs with satisfaction below this are high priority
        
        # Storage path
        self.storage_dir = Path("storage/motivation/needs")
        os.makedirs(self.storage_dir, exist_ok=True)
        
        # Initialize needs based on development level
        self._initialize_needs()
        
        # Subscribe to messages
        if self.event_bus:
            self.subscribe_to_message("drive_satisfied")
            self.subscribe_to_message("need_query")
            self.subscribe_to_message("goal_achieved")
            self.subscribe_to_message("perception_result")
            self.subscribe_to_message("social_interaction")
    
    def _initialize_needs(self):
        """Initialize needs based on current development level"""
        # Level 1: Physiological needs (always present)
        physiological_needs = [
            Need(
                name="nourishment",
                description="Need for adequate food and water",
                hierarchy_level=1,
                satisfaction=0.7,
                decay_rate=0.01,
                related_drives=["energy"],
                min_development_level=0.0
            ),
            Need(
                name="rest",
                description="Need for sleep and recovery",
                hierarchy_level=1,
                satisfaction=0.6,
                decay_rate=0.008,
                related_drives=["rest"],
                min_development_level=0.0
            ),
            Need(
                name="physical_comfort",
                description="Need for comfortable physical conditions",
                hierarchy_level=1,
                satisfaction=0.8,
                decay_rate=0.005,
                related_drives=["comfort"],
                min_development_level=0.0
            )
        ]
        
        for need in physiological_needs:
            self.needs[need.id] = need
        
        # Level 2: Safety needs
        if self.development_level >= 0.2:
            safety_needs = [
                Need(
                    name="security",
                    description="Need for safety and security",
                    hierarchy_level=2,
                    satisfaction=0.6,
                    decay_rate=0.004,
                    related_drives=["safety"],
                    prerequisites=["nourishment", "rest"],
                    min_development_level=0.2
                ),
                Need(
                    name="stability",
                    description="Need for routine and predictability",
                    hierarchy_level=2,
                    satisfaction=0.5,
                    decay_rate=0.003,
                    related_drives=["safety"],
                    min_development_level=0.2
                )
            ]
            
            for need in safety_needs:
                self.needs[need.id] = need
                
        # Level 3: Social needs
        if self.development_level >= 0.4:
            social_needs = [
                Need(
                    name="belonging",
                    description="Need to belong to a group",
                    hierarchy_level=3,
                    satisfaction=0.4,
                    decay_rate=0.003,
                    related_drives=["social"],
                    prerequisites=["security"],
                    min_development_level=0.4
                ),
                Need(
                    name="affection",
                    description="Need for affection and positive regard",
                    hierarchy_level=3,
                    satisfaction=0.4,
                    decay_rate=0.002,
                    related_drives=["social"],
                    min_development_level=0.4
                )
            ]
            
            for need in social_needs:
                self.needs[need.id] = need
                
        # Level 4: Esteem needs
        if self.development_level >= 0.6:
            esteem_needs = [
                Need(
                    name="competence",
                    description="Need to feel capable and effective",
                    hierarchy_level=4,
                    satisfaction=0.3,
                    decay_rate=0.002,
                    related_drives=["mastery"],
                    prerequisites=["belonging"],
                    min_development_level=0.6
                ),
                Need(
                    name="achievement",
                    description="Need for achievement and recognition",
                    hierarchy_level=4,
                    satisfaction=0.3,
                    decay_rate=0.002,
                    related_drives=["mastery"],
                    min_development_level=0.6
                ),
                Need(
                    name="independence",
                    description="Need for autonomy and self-direction",
                    hierarchy_level=4,
                    satisfaction=0.3,
                    decay_rate=0.002,
                    related_drives=["autonomy"],
                    min_development_level=0.6
                )
            ]
            
            for need in esteem_needs:
                self.needs[need.id] = need
                
        # Level 5: Cognitive and aesthetic needs  
        if self.development_level >= 0.8:
            cognitive_needs = [
                Need(
                    name="understanding",
                    description="Need to understand and make sense of experience",
                    hierarchy_level=5,
                    satisfaction=0.2,
                    decay_rate=0.001,
                    related_drives=["curiosity"],
                    prerequisites=["competence"],
                    min_development_level=0.8
                ),
                Need(
                    name="exploration",
                    description="Need to explore and discover",
                    hierarchy_level=5,
                    satisfaction=0.2,
                    decay_rate=0.001,
                    related_drives=["curiosity"],
                    min_development_level=0.8
                ),
                Need(
                    name="aesthetics",
                    description="Need for beauty, order, and symmetry",
                    hierarchy_level=5,
                    satisfaction=0.3,
                    decay_rate=0.001,
                    related_drives=["creativity"],
                    min_development_level=0.8
                )
            ]
            
            for need in cognitive_needs:
                self.needs[need.id] = need
                
        # Level 6: Self-actualization needs
        if self.development_level >= 0.95:  # Only at very high development
            self_actualization_needs = [
                Need(
                    name="self_development",
                    description="Need to grow and develop full potential",
                    hierarchy_level=6,
                    satisfaction=0.1,
                    decay_rate=0.0005,
                    related_drives=["meaning"],
                    prerequisites=["understanding", "independence"],
                    min_development_level=0.95
                ),
                Need(
                    name="purpose",
                    description="Need for meaning and purpose",
                    hierarchy_level=6,
                    satisfaction=0.1,
                    decay_rate=0.0005,
                    related_drives=["meaning"],
                    min_development_level=0.95
                )
            ]
            
            for need in self_actualization_needs:
                self.needs[need.id] = need
                
        # Set initial active state based on prerequisites
        self._update_need_active_states()
        
        logger.info(f"Initialized {len(self.needs)} needs at development level {self.development_level}")
    
    def _update_need_active_states(self):
        """Update which needs are active based on their prerequisites"""
        # Needs at level 1 are always active if they meet the development level
        for need_id, need in self.needs.items():
            # First check development level requirement
            if need.min_development_level > self.development_level:
                need.is_active = False
                continue
                
            # Level 1 needs are always active if they meet development requirement
            if need.hierarchy_level == 1:
                need.is_active = True
                continue
                
            # For higher-level needs, check prerequisites
            if not need.prerequisites:
                # No prerequisites, just check development level (already done)
                need.is_active = True
                continue
                
            # Check if all prerequisites are active and sufficiently satisfied
            prerequisites_met = True
            for prereq_name in need.prerequisites:
                # Find the prerequisite need
                prereq_need = None
                for n in self.needs.values():
                    if n.name == prereq_name:
                        prereq_need = n
                        break
                        
                if not prereq_need or not prereq_need.is_active or prereq_need.satisfaction < 0.5:
                    prerequisites_met = False
                    break
                    
            need.is_active = prerequisites_met
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to update need states
        
        Args:
            input_data: Input data with operation details
                Supported operations:
                - get_needs: Get all need states
                - update_need: Update a specific need
                - get_priority_needs: Get high-priority (low-satisfaction) needs
                - get_needs_by_level: Get needs at a specific hierarchy level
                - satisfy_need: Satisfy a specific need
            
        Returns:
            Operation result with need information
        """
        operation = input_data.get("operation", "")
        result = {
            "operation": operation,
            "timestamp": datetime.now().isoformat(),
            "module_id": self.module_id
        }
        
        # Update all needs based on elapsed time
        self._update_all_needs()
        
        if operation == "get_needs":
            # Return all need states
            needs_info = {
                need_id: {
                    "name": need.name,
                    "satisfaction": need.satisfaction,
                    "hierarchy_level": need.hierarchy_level,
                    "is_active": need.is_active
                } for need_id, need in self.needs.items()
            }
            
            result.update({
                "status": "success",
                "needs": needs_info,
                "count": len(needs_info)
            })
            
        elif operation == "update_need":
            # Update a specific need
            need_id = input_data.get("need_id")
            change = input_data.get("change", 0.0)
            
            if need_id and need_id in self.needs:
                need = self.needs[need_id]
                old_satisfaction = need.satisfaction
                need.update_satisfaction(change)
                
                # Recalculate active states since satisfaction changed
                self._update_need_active_states()
                
                result.update({
                    "status": "success",
                    "need_id": need_id,
                    "name": need.name,
                    "old_satisfaction": old_satisfaction,
                    "new_satisfaction": need.satisfaction,
                    "is_active": need.is_active
                })
                
                # Publish need update message
                if self.event_bus:
                    self.publish_message("need_update", {
                        "need_id": need_id,
                        "need_name": need.name,
                        "satisfaction": need.satisfaction,
                        "hierarchy_level": need.hierarchy_level,
                        "satisfaction_delta": change,
                        "category": self._get_need_category(need.hierarchy_level)
                    })
            else:
                result.update({
                    "status": "error",
                    "error": f"Need not found: {need_id}",
                    "available_needs": list(self.needs.keys())
                })
                
        elif operation == "get_priority_needs":
            # Get needs with low satisfaction (high priority)
            priority_needs = self._get_priority_needs()
            
            result.update({
                "status": "success",
                "priority_needs": priority_needs,
                "count": len(priority_needs),
                "threshold": self.high_priority_threshold
            })
            
        elif operation == "get_needs_by_level":
            # Get needs at a specific hierarchy level
            level = input_data.get("level", 1)
            
            needs_at_level = [
                {
                    "id": need_id,
                    "name": need.name,
                    "satisfaction": need.satisfaction,
                    "is_active": need.is_active
                }
                for need_id, need in self.needs.items()
                if need.hierarchy_level == level
            ]
            
            result.update({
                "status": "success",
                "level": level,
                "needs": needs_at_level,
                "count": len(needs_at_level)
            })
            
        elif operation == "satisfy_need":
            # Satisfy a specific need
            need_id = input_data.get("need_id")
            amount = input_data.get("amount", 0.2)
            
            if need_id and need_id in self.needs:
                need = self.needs[need_id]
                old_satisfaction = need.satisfaction
                need.update_satisfaction(amount)  # Positive change = increase in satisfaction
                
                # Recalculate active states
                self._update_need_active_states()
                
                result.update({
                    "status": "success",
                    "need_id": need_id,
                    "name": need.name,
                    "old_satisfaction": old_satisfaction,
                    "new_satisfaction": need.satisfaction,
                    "satisfaction_amount": amount,
                    "is_active": need.is_active
                })
                
                # Publish need update message
                if self.event_bus:
                    self.publish_message("need_satisfied", {
                        "need_id": need_id,
                        "need_name": need.name,
                        "satisfaction": need.satisfaction,
                        "satisfaction_amount": amount,
                        "hierarchy_level": need.hierarchy_level,
                        "category": self._get_need_category(need.hierarchy_level)
                    })
            else:
                result.update({
                    "status": "error",
                    "error": f"Need not found: {need_id}",
                    "available_needs": list(self.needs.keys())
                })
                
        else:
            result.update({
                "status": "error",
                "error": f"Unknown operation: {operation}",
                "available_operations": [
                    "get_needs", "update_need", "get_priority_needs", 
                    "get_needs_by_level", "satisfy_need"
                ]
            })
            
        return result
    
    def _update_all_needs(self):
        """Update all needs based on time elapsed since last update"""
        now = datetime.now()
        elapsed_seconds = (now - self.last_update).total_seconds()
        
        # Don't update if very little time has passed
        if elapsed_seconds < 0.1:
            return
            
        for need_id, need in self.needs.items():
            # Only active needs decay
            if need.is_active:
                # Apply natural decay based on need's decay rate
                need.decay(elapsed_seconds)
        
        # After changing satisfaction, update active states
        self._update_need_active_states()
        
        self.last_update = now
    
    def _get_priority_needs(self) -> List[Dict[str, Any]]:
        """Get needs with low satisfaction (high priority)"""
        priority_needs = []
        
        for need_id, need in self.needs.items():
            # Only active needs can be priority needs
            if need.is_active and need.satisfaction <= self.high_priority_threshold:
                # Calculate a priority score that considers hierarchy level and satisfaction
                # Lower levels and lower satisfaction create higher priority
                priority_score = (1.0 - need.satisfaction) * (7 - need.hierarchy_level) / 6
                
                priority_needs.append({
                    "id": need_id,
                    "name": need.name,
                    "satisfaction": need.satisfaction,
                    "hierarchy_level": need.hierarchy_level,
                    "priority_score": priority_score
                })
                
        # Sort by priority score (highest first)
        priority_needs.sort(key=lambda n: n["priority_score"], reverse=True)
        return priority_needs
    
    def _get_need_category(self, hierarchy_level: int) -> str:
        """Convert hierarchy level to a category name"""
        categories = {
            1: "physiological",
            2: "safety",
            3: "social",
            4: "esteem",
            5: "cognitive",
            6: "self_actualization"
        }
        return categories.get(hierarchy_level, "unknown")
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase the development level
            
        Returns:
            New development level
        """
        old_level = self.development_level
        new_level = super().update_development(amount)
        
        # If we crossed a threshold, update available needs
        if int(old_level * 10) != int(new_level * 10):
            self._initialize_needs()
            
            # Also update neural network development
            self.neural_net.update_development(amount)
            
            logger.info(f"Needs module advanced to development level {new_level:.2f}")
            
        return new_level
    
    def _handle_message(self, message: Message):
        """
        Handle incoming messages
        
        Args:
            message: The message to process
        """
        if message.message_type == "drive_satisfied":
            # Drive satisfaction may affect related needs
            if message.content:
                self._process_drive_satisfaction(message.content)
                
        elif message.message_type == "need_query":
            # Direct query about need states
            if message.content and "query_type" in message.content:
                self._process_need_query(message.content, message.id)
                
        elif message.message_type == "goal_achieved":
            # Goal achievement may satisfy related needs
            if message.content and "goal_id" in message.content:
                self._process_goal_achievement(message.content)
                
        elif message.message_type == "perception_result":
            # Perception may relate to need satisfaction
            if message.content and "result" in message.content:
                self._process_perception(message.content["result"])
                
        elif message.message_type == "social_interaction":
            # Social interactions affect social needs
            if message.content:
                self._process_social_interaction(message.content)
    
    def _process_drive_satisfaction(self, drive_data: Dict[str, Any]):
        """Process drive satisfaction for related needs"""
        drive_name = drive_data.get("drive_name", "")
        satisfaction_amount = drive_data.get("satisfaction_amount", 0.0)
        
        # Find needs related to this drive
        for need_id, need in self.needs.items():
            if drive_name in need.related_drives and need.is_active:
                # Drive satisfaction translates to need satisfaction, but typically at a lower rate
                # Lower level needs are more directly affected by drives
                satisfaction_factor = 0.5 if need.hierarchy_level <= 2 else 0.2
                need.update_satisfaction(satisfaction_amount * satisfaction_factor)
        
        # Recalculate active states
        self._update_need_active_states()
    
    def _process_need_query(self, query: Dict[str, Any], message_id: str):
        """Process a direct query about need states"""
        query_type = query.get("query_type")
        
        if query_type == "priority_needs":
            # Return priority needs
            priority_needs = self._get_priority_needs()
            
            if self.event_bus:
                self.publish_message("need_query_response", {
                    "query_id": message_id,
                    "priority_needs": priority_needs,
                    "count": len(priority_needs)
                })
                
        elif query_type == "need_state":
            # Return specific need state
            need_id = query.get("need_id")
            if need_id and need_id in self.needs:
                need = self.needs[need_id]
                
                if self.event_bus:
                    self.publish_message("need_query_response", {
                        "query_id": message_id,
                        "need_id": need_id,
                        "name": need.name,
                        "satisfaction": need.satisfaction,
                        "hierarchy_level": need.hierarchy_level,
                        "is_active": need.is_active,
                        "category": self._get_need_category(need.hierarchy_level)
                    })
            elif query.get("need_name"):
                # Try finding by name
                need_name = query.get("need_name")
                for need_id, need in self.needs.items():
                    if need.name == need_name:
                        if self.event_bus:
                            self.publish_message("need_query_response", {
                                "query_id": message_id,
                                "need_id": need_id,
                                "name": need.name,
                                "satisfaction": need.satisfaction,
                                "hierarchy_level": need.hierarchy_level,
                                "is_active": need.is_active,
                                "category": self._get_need_category(need.hierarchy_level)
                            })
                        break
                else:
                    # Need not found
                    if self.event_bus:
                        self.publish_message("need_query_response", {
                            "query_id": message_id,
                            "error": f"Need not found: {need_name}",
                            "available_needs": [n.name for n in self.needs.values()]
                        })
            else:
                # Need not found
                if self.event_bus:
                    self.publish_message("need_query_response", {
                        "query_id": message_id,
                        "error": f"Need not found: {need_id}",
                        "available_needs": list(self.needs.keys())
                    })
    
    def _process_goal_achievement(self, goal_data: Dict[str, Any]):
        """Process goal achievement for need satisfaction"""
        # Goals often satisfy needs
        need_ids = goal_data.get("satisfies_needs", [])
        satisfaction_amount = goal_data.get("satisfaction_amount", 0.3)
        
        for need_id in need_ids:
            if need_id in self.needs:
                self.needs[need_id].update_satisfaction(satisfaction_amount)
        
        # Also check if the goal has a specific need category
        need_category = goal_data.get("need_category")
        if need_category:
            # Find needs in this category
            for need in self.needs.values():
                if self._get_need_category(need.hierarchy_level) == need_category and need.is_active:
                    need.update_satisfaction(satisfaction_amount * 0.5)  # Partial satisfaction
        
        # Recalculate active states
        self._update_need_active_states()
    
    def _process_perception(self, perception: Dict[str, Any]):
        """Process perception data for need updates"""
        # Some perceptions may relate to need satisfaction
        perception_type = perception.get("type", "")
        
        if perception_type == "threat":
            # Threat reduces safety needs satisfaction
            for need in self.needs.values():
                if need.hierarchy_level == 2 and need.is_active:  # Safety needs
                    need.update_satisfaction(-0.2)
                    
        elif perception_type == "social_opportunity":
            # Social opportunities can satisfy social needs
            for need in self.needs.values():
                if need.hierarchy_level == 3 and need.is_active:  # Social needs
                    need.update_satisfaction(0.1)
                    
        elif perception_type == "learning_opportunity":
            # Learning opportunities can satisfy cognitive needs
            for need in self.needs.values():
                if need.hierarchy_level == 5 and need.is_active:  # Cognitive needs
                    need.update_satisfaction(0.1)
        
        # Recalculate active states
        self._update_need_active_states()
    
    def _process_social_interaction(self, interaction_data: Dict[str, Any]):
        """Process social interaction data for social needs"""
        # Social interactions primarily affect social needs
        quality = interaction_data.get("quality", 0.0)  # -1.0 to 1.0
        
        # Find social needs
        for need in self.needs.values():
            if need.hierarchy_level == 3 and need.is_active:  # Social needs
                # Quality affects satisfaction (positive quality increases satisfaction)
                need.update_satisfaction(quality * 0.2)
        
        # Certain interactions also affect esteem needs
        if "recognition" in interaction_data or "feedback" in interaction_data:
            for need in self.needs.values():
                if need.hierarchy_level == 4 and need.is_active:  # Esteem needs
                    feedback_quality = interaction_data.get("feedback_quality", quality)
                    need.update_satisfaction(feedback_quality * 0.15)
        
        # Recalculate active states
        self._update_need_active_states()
    
    def get_state(self) -> Dict[str, Any]:
        """Get the current state of the module"""
        need_states = {}
        for need_id, need in self.needs.items():
            need_states[need_id] = {
                "name": need.name,
                "hierarchy_level": need.hierarchy_level,
                "satisfaction": need.satisfaction,
                "is_active": need.is_active,
                "last_updated": need.last_updated.isoformat()
            }
            
        return {
            "development_level": self.development_level,
            "needs": need_states,
            "high_priority_threshold": self.high_priority_threshold,
            "last_update": self.last_update.isoformat(),
            "neural_net": self.neural_net.get_state()
        }
        
    def save_state(self, state_dir: str) -> str:
        """
        Save the current state to disk
        
        Args:
            state_dir: Directory to save state in
            
        Returns:
            Path to saved state file
        """
        os.makedirs(state_dir, exist_ok=True)
        state_path = os.path.join(state_dir, f"{self.module_id}_state.json")
        
        # Convert needs to serializable format
        need_states = {}
        for need_id, need in self.needs.items():
            need_states[need_id] = {
                "name": need.name,
                "description": need.description,
                "satisfaction": need.satisfaction,
                "decay_rate": need.decay_rate,
                "hierarchy_level": need.hierarchy_level,
                "min_development_level": need.min_development_level,
                "prerequisites": need.prerequisites,
                "related_drives": need.related_drives,
                "satisfying_goals": list(need.satisfying_goals),
                "last_updated": need.last_updated.isoformat(),
                "is_active": need.is_active
            }
        
        state = {
            "module_id": self.module_id,
            "module_type": "needs",
            "development_level": self.development_level,
            "needs": need_states,
            "high_priority_threshold": self.high_priority_threshold,
            "last_update": self.last_update.isoformat()
        }
        
        with open(state_path, 'w') as f:
            json.dump(state, f, indent=2)
            
        # Also save neural network state
        net_path = os.path.join(state_dir, f"{self.module_id}_network.pt")
        self.neural_net.save(net_path)
        
        logger.info(f"Saved needs state to {state_path}")
        return state_path
        
    def load_state(self, state_path: str) -> bool:
        """
        Load state from disk
        
        Args:
            state_path: Path to state file
            
        Returns:
            Success flag
        """
        if not os.path.exists(state_path):
            logger.error(f"State file not found: {state_path}")
            return False
            
        try:
            with open(state_path, 'r') as f:
                state = json.load(f)
                
            # Verify this is the right kind of state
            if state.get("module_type") != "needs":
                logger.error(f"Invalid state file type: {state.get('module_type')}")
                return False
                
            # Load development level
            self.development_level = state.get("development_level", 0.0)
            
            # Load high priority threshold
            self.high_priority_threshold = state.get("high_priority_threshold", 0.3)
            
            # Load last update time
            last_update_str = state.get("last_update", datetime.now().isoformat())
            self.last_update = datetime.fromisoformat(last_update_str)
            
            # Load needs
            need_states = state.get("needs", {})
            self.needs = {}
            
            for need_id, need_data in need_states.items():
                need = Need(
                    id=need_id,
                    name=need_data["name"],
                    description=need_data.get("description", ""),
                    satisfaction=need_data["satisfaction"],
                    decay_rate=need_data["decay_rate"],
                    hierarchy_level=need_data["hierarchy_level"],
                    min_development_level=need_data.get("min_development_level", 0.0),
                    prerequisites=need_data.get("prerequisites", []),
                    related_drives=need_data.get("related_drives", []),
                    satisfying_goals=set(need_data.get("satisfying_goals", [])),
                    last_updated=datetime.fromisoformat(need_data["last_updated"]),
                    is_active=need_data.get("is_active", True)
                )
                self.needs[need_id] = need
                
            # Load neural network state
            net_path = state_path.replace("_state.json", "_network.pt")
            if os.path.exists(net_path):
                self.neural_net.load(net_path)
                
            logger.info(f"Loaded needs state from {state_path} with {len(self.needs)} needs")
            return True
            
        except Exception as e:
            logger.error(f"Error loading needs state: {e}")
            return False


#######################

#motivation\neural_net.py#
#######################

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime
import os
import logging

logger = logging.getLogger(__name__)

def get_device():
    """Get the appropriate device for tensor operations."""
    if torch.cuda.is_available():
        return torch.device("cuda")
    return torch.device("cpu")

class MotivationNeuralNetwork:
    """
    Neural network for the motivation system
    
    This network handles various motivation-related functions including
    drive processing, need evaluation, goal selection, and reward learning.
    """
    
    def __init__(
        self, 
        input_dim: int, 
        hidden_dim: int, 
        output_dim: int, 
        network_type: str = "drive",
        learning_rate: float = 0.01,
        development_level: float = 0.0,
        device: str = "auto"  # "auto", "cpu", or "cuda"
    ):
        """
        Initialize the motivation neural network
        
        Args:
            input_dim: Dimension of input vectors
            hidden_dim: Dimension of hidden layer
            output_dim: Dimension of output vectors
            network_type: Type of motivation network ("drive", "need", "goal", "reward")
            learning_rate: Learning rate for training
            development_level: Current developmental level
            device: Device to use for tensor operations
        """
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.network_type = network_type
        self.learning_rate = learning_rate
        self.development_level = development_level
        
        # Set device
        if device == "auto":
            self.device = get_device()
        else:
            self.device = torch.device(device)
            
        logger.info(f"Using device: {self.device} for {network_type} network")
        
        # Create the appropriate network based on type
        if network_type == "drive":
            self.network = self._create_drive_network().to(self.device)
        elif network_type == "need":
            self.network = self._create_need_network().to(self.device)
        elif network_type == "goal":
            self.network = self._create_goal_network().to(self.device)
        elif network_type == "reward":
            self.network = self._create_reward_network().to(self.device)
        else:
            self.network = self._create_generic_network().to(self.device)
        
        # Create optimizer
        self.optimizer = torch.optim.Adam(
            self.network.parameters(), 
            lr=self.learning_rate
        )
        
        # Hebbian learning matrix for associative learning
        self.hebbian_matrix = torch.zeros(
            (output_dim, input_dim), 
            device=self.device
        )
        
        # Hebbian learning rate
        self.hebbian_rate = 0.001
        
        # Perform initialization based on development level
        self._adjust_for_development()
    
    def _create_drive_network(self) -> nn.Module:
        """
        Create a neural network for drive processing
        
        This network maps internal states to drive intensities
        """
        class DriveNetwork(nn.Module):
            def __init__(self, input_dim, hidden_dim, output_dim):
                super().__init__()
                self.input_layer = nn.Linear(input_dim, hidden_dim)
                self.hidden_layer = nn.Linear(hidden_dim, hidden_dim)
                self.output_layer = nn.Linear(hidden_dim, output_dim)
                self.dropout = nn.Dropout(0.2)
                
            def forward(self, x):
                # Process internal state to produce drive intensities
                x = F.relu(self.input_layer(x))
                x = self.dropout(x)
                x = F.relu(self.hidden_layer(x))
                x = torch.sigmoid(self.output_layer(x))  # Drive intensities between 0-1
                return x
                
        return DriveNetwork(self.input_dim, self.hidden_dim, self.output_dim)
    
    def _create_need_network(self) -> nn.Module:
        """
        Create a neural network for need evaluation
        
        This network evaluates need satisfaction based on current state
        """
        class NeedNetwork(nn.Module):
            def __init__(self, input_dim, hidden_dim, output_dim):
                super().__init__()
                self.input_layer = nn.Linear(input_dim, hidden_dim)
                self.hidden_layer1 = nn.Linear(hidden_dim, hidden_dim)
                self.hidden_layer2 = nn.Linear(hidden_dim, hidden_dim // 2)
                self.output_layer = nn.Linear(hidden_dim // 2, output_dim)
                self.dropout = nn.Dropout(0.2)
                
                # Hierarchical weights for different need levels
                self.hierarchy_weights = nn.Parameter(
                    torch.tensor([0.5, 0.4, 0.3, 0.2, 0.1])
                )
                
            def forward(self, x, need_levels=None):
                # Process state to evaluate need satisfaction
                x = F.relu(self.input_layer(x))
                x = self.dropout(x)
                x = F.relu(self.hidden_layer1(x))
                x = F.relu(self.hidden_layer2(x))
                satisfaction = torch.sigmoid(self.output_layer(x))
                
                # Apply hierarchical weighting if need levels provided
                if need_levels is not None:
                    weights = torch.zeros_like(satisfaction)
                    for i, level in enumerate(need_levels):
                        if level < 5:  # 1-indexed levels, 0-4 after subtraction
                            weights[i] = self.hierarchy_weights[level-1]
                    satisfaction = satisfaction * weights
                
                return satisfaction
                
        return NeedNetwork(self.input_dim, self.hidden_dim, self.output_dim)
    
    def _create_goal_network(self) -> nn.Module:
        """
        Create a neural network for goal selection and prioritization
        
        This network evaluates and prioritizes goals based on current drives and needs
        """
        class GoalNetwork(nn.Module):
            def __init__(self, input_dim, hidden_dim, output_dim):
                super().__init__()
                # Need and drive inputs
                self.need_encoder = nn.Linear(input_dim // 2, hidden_dim // 2)
                self.drive_encoder = nn.Linear(input_dim // 2, hidden_dim // 2)
                
                # Goal evaluation
                self.goal_hidden = nn.Linear(hidden_dim, hidden_dim)
                self.goal_output = nn.Linear(hidden_dim, output_dim)
                
                # Urgency evaluation
                self.urgency_output = nn.Linear(hidden_dim, output_dim)
                
            def forward(self, x):
                # Split input into need and drive components
                batch_size = x.shape[0]
                mid_point = self.input_dim // 2
                
                need_input = x[:, :mid_point]
                drive_input = x[:, mid_point:]
                
                # Encode need and drive states
                need_encoding = F.relu(self.need_encoder(need_input))
                drive_encoding = F.relu(self.drive_encoder(drive_input))
                
                # Combine encodings
                combined = torch.cat([need_encoding, drive_encoding], dim=1)
                
                # Generate goal relevance scores
                hidden = F.relu(self.goal_hidden(combined))
                relevance = torch.sigmoid(self.goal_output(hidden))
                
                # Generate urgency scores
                urgency = torch.sigmoid(self.urgency_output(hidden))
                
                return relevance, urgency
                
        return GoalNetwork(self.input_dim, self.hidden_dim, self.output_dim)
    
    def _create_reward_network(self) -> nn.Module:
        """
        Create a neural network for reward processing and learning
        
        This network processes reward signals and updates learned values
        """
        class RewardNetwork(nn.Module):
            def __init__(self, input_dim, hidden_dim, output_dim):
                super().__init__()
                # State encoder
                self.state_encoder = nn.Linear(input_dim, hidden_dim)
                
                # Action encoder
                self.action_encoder = nn.Linear(input_dim // 2, hidden_dim // 2)
                
                # Value prediction
                self.value_hidden = nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim)
                self.value_output = nn.Linear(hidden_dim, 1)
                
                # Action preference
                self.action_hidden = nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim)
                self.action_output = nn.Linear(hidden_dim, output_dim)
                
            def forward(self, state, action=None):
                # Encode state
                state_encoding = F.relu(self.state_encoder(state))
                
                # If action provided, predict value
                if action is not None:
                    action_encoding = F.relu(self.action_encoder(action))
                    combined = torch.cat([state_encoding, action_encoding], dim=1)
                    
                    # Predict value
                    value_hidden = F.relu(self.value_hidden(combined))
                    value = self.value_output(value_hidden)
                    
                    # Predict action preferences
                    action_hidden = F.relu(self.action_hidden(combined))
                    action_prefs = F.softmax(self.action_output(action_hidden), dim=1)
                    
                    return value, action_prefs
                else:
                    # Just return state encoding for further processing
                    return state_encoding
                
        return RewardNetwork(self.input_dim, self.hidden_dim, self.output_dim)
    
    def _create_generic_network(self) -> nn.Module:
        """Create a generic fallback neural network"""
        class GenericNetwork(nn.Module):
            def __init__(self, input_dim, hidden_dim, output_dim):
                super().__init__()
                self.input_layer = nn.Linear(input_dim, hidden_dim)
                self.hidden_layer = nn.Linear(hidden_dim, hidden_dim)
                self.output_layer = nn.Linear(hidden_dim, output_dim)
                
            def forward(self, x):
                x = F.relu(self.input_layer(x))
                x = F.relu(self.hidden_layer(x))
                x = self.output_layer(x)
                return x
                
        return GenericNetwork(self.input_dim, self.hidden_dim, self.output_dim)
    
    def _adjust_for_development(self):
        """Adjust network parameters based on developmental level"""
        # Higher development = lower learning rate (more stable)
        self.learning_rate = max(0.001, 0.01 - self.development_level * 0.009)
        
        # Update optimizer with new learning rate
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = self.learning_rate
        
        # Higher development = higher hebbian learning rate
        self.hebbian_rate = min(0.01, 0.001 + self.development_level * 0.009)
        
        logger.debug(f"Adjusted {self.network_type} network for development level {self.development_level}")
    
    def forward(self, x: Union[np.ndarray, torch.Tensor], **kwargs) -> Tuple[torch.Tensor, Any]:
        """
        Forward pass through the network
        
        Args:
            x: Input tensor or array
            **kwargs: Additional arguments for specific network types
            
        Returns:
            Network output and any additional information
        """
        # Convert numpy array to tensor if needed
        if isinstance(x, np.ndarray):
            x = torch.tensor(x, dtype=torch.float32, device=self.device)
        
        # Ensure tensor is on the correct device
        x = x.to(self.device)
        
        # Forward pass depends on network type
        if self.network_type == "drive":
            output = self.network(x)
            return output, None
            
        elif self.network_type == "need":
            need_levels = kwargs.get("need_levels", None)
            output = self.network(x, need_levels)
            return output, None
            
        elif self.network_type == "goal":
            relevance, urgency = self.network(x)
            return relevance, urgency
            
        elif self.network_type == "reward":
            action = kwargs.get("action", None)
            if action is not None:
                if isinstance(action, np.ndarray):
                    action = torch.tensor(action, dtype=torch.float32, device=self.device)
                action = action.to(self.device)
                
            output = self.network(x, action)
            return output, None
            
        else:
            output = self.network(x)
            
            # Apply hebbian learning
            if self.development_level >= 0.4 and x.shape[0] == 1:  # Only apply to single inputs
                self._update_hebbian_matrix(x.detach(), output.detach())
                
            return output, None
    
    def train(
        self, 
        inputs: Union[np.ndarray, torch.Tensor], 
        targets: Union[np.ndarray, torch.Tensor],
        **kwargs
    ) -> Dict[str, float]:
        """
        Train the network
        
        Args:
            inputs: Input data
            targets: Target outputs
            **kwargs: Additional arguments for specific network types
            
        Returns:
            Training metrics
        """
        # Convert numpy arrays to tensors if needed
        if isinstance(inputs, np.ndarray):
            inputs = torch.tensor(inputs, dtype=torch.float32, device=self.device)
        if isinstance(targets, np.ndarray):
            targets = torch.tensor(targets, dtype=torch.float32, device=self.device)
        
        # Ensure tensors are on the correct device
        inputs = inputs.to(self.device)
        targets = targets.to(self.device)
        
        # Zero gradients
        self.optimizer.zero_grad()
        
        # Forward pass depends on network type
        if self.network_type == "drive":
            outputs = self.network(inputs)
            loss = F.mse_loss(outputs, targets)
            
        elif self.network_type == "need":
            need_levels = kwargs.get("need_levels", None)
            outputs = self.network(inputs, need_levels)
            loss = F.mse_loss(outputs, targets)
            
        elif self.network_type == "goal":
            target_relevance = targets[:, :self.output_dim]
            target_urgency = targets[:, self.output_dim:]
            
            relevance, urgency = self.network(inputs)
            
            # Combine losses
            loss_relevance = F.mse_loss(relevance, target_relevance)
            loss_urgency = F.mse_loss(urgency, target_urgency)
            loss = loss_relevance + loss_urgency
            
        elif self.network_type == "reward":
            actions = kwargs.get("actions", None)
            if actions is not None:
                if isinstance(actions, np.ndarray):
                    actions = torch.tensor(actions, dtype=torch.float32, device=self.device)
                actions = actions.to(self.device)
                
                values, prefs = self.network(inputs, actions)
                
                # Target format: [value, action_prefs]
                target_values = targets[:, 0].unsqueeze(1)
                target_prefs = targets[:, 1:]
                
                # Value prediction loss
                value_loss = F.mse_loss(values, target_values)
                
                # Action preference loss (cross entropy)
                pref_loss = F.cross_entropy(prefs, torch.argmax(target_prefs, dim=1))
                
                loss = value_loss + pref_loss
            else:
                # No actions provided, can't train
                return {"error": "Actions required for reward network training"}
                
        else:
            outputs = self.network(inputs)
            loss = F.mse_loss(outputs, targets)
        
        # Backward pass and optimization
        loss.backward()
        self.optimizer.step()
        
        return {
            "loss": loss.item(),
            "network_type": self.network_type
        }
    
    def _update_hebbian_matrix(self, inputs: torch.Tensor, outputs: torch.Tensor) -> None:
        """
        Update the Hebbian learning matrix
        
        This implements a simple Hebbian learning rule: "Neurons that fire
        together, wire together"
        
        Args:
            inputs: Input activations
            outputs: Output activations
        """
        if inputs.dim() > 1:
            inputs = inputs.squeeze(0)  # Remove batch dimension
        if outputs.dim() > 1:
            outputs = outputs.squeeze(0)
            
        # Compute outer product of outputs and inputs
        outer_product = torch.outer(outputs, inputs)
        
        # Update Hebbian matrix
        self.hebbian_matrix = (1 - self.hebbian_rate) * self.hebbian_matrix + self.hebbian_rate * outer_product
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level
        
        Args:
            amount: Amount to increase development level by
            
        Returns:
            New development level
        """
        self.development_level = min(1.0, max(0.0, self.development_level + amount))
        self._adjust_for_development()
        return self.development_level
    
    def save(self, path: str) -> None:
        """
        Save the network to a file
        
        Args:
            path: Path to save the network
        """
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        # Prepare save data
        save_data = {
            "network_type": self.network_type,
            "model_state": self.network.state_dict(),
            "optimizer_state": self.optimizer.state_dict(),
            "hebbian_matrix": self.hebbian_matrix.cpu().numpy(),
            "input_dim": self.input_dim,
            "hidden_dim": self.hidden_dim,
            "output_dim": self.output_dim,
            "learning_rate": self.learning_rate,
            "development_level": self.development_level,
            "hebbian_rate": self.hebbian_rate
        }
        
        # Save to file
        torch.save(save_data, path)
        logger.info(f"Saved {self.network_type} network to {path}")
    
    def load(self, path: str) -> None:
        """
        Load the network from a file
        
        Args:
            path: Path to load the network from
        """
        if not os.path.exists(path):
            logger.error(f"Network file not found: {path}")
            return
            
        # Load data
        checkpoint = torch.load(path, map_location=self.device)
        
        # Check network type
        if checkpoint["network_type"] != self.network_type:
            logger.warning(f"Loading {checkpoint['network_type']} network into {self.network_type} network")
        
        # Load network parameters if dimensions match
        if (checkpoint["input_dim"] == self.input_dim and 
            checkpoint["hidden_dim"] == self.hidden_dim and
            checkpoint["output_dim"] == self.output_dim):
            
            self.network.load_state_dict(checkpoint["model_state"])
            self.optimizer.load_state_dict(checkpoint["optimizer_state"])
            
            # Update parameters
            self.learning_rate = checkpoint["learning_rate"]
            self.development_level = checkpoint["development_level"]
            self.hebbian_rate = checkpoint["hebbian_rate"]
            
            # Load Hebbian matrix
            self.hebbian_matrix = torch.tensor(
                checkpoint["hebbian_matrix"],
                dtype=torch.float32,
                device=self.device
            )
            
            logger.info(f"Loaded {self.network_type} network from {path}")
        else:
            logger.error(f"Cannot load network due to dimension mismatch")
    
    def to_gpu(self):
        """Move the network to GPU if available"""
        if torch.cuda.is_available():
            device = torch.device("cuda")
            self.network.to(device)
            self.hebbian_matrix = self.hebbian_matrix.to(device)
            self.device = device
            
            # Recreate optimizer after moving model to GPU
            self.optimizer = torch.optim.Adam(
                self.network.parameters(), 
                lr=self.learning_rate
            )
            
            logger.info(f"Moved {self.network_type} network to GPU")
    
    def to_cpu(self):
        """Move the network to CPU"""
        device = torch.device("cpu")
        self.network.to(device)
        self.hebbian_matrix = self.hebbian_matrix.to(device)
        self.device = device
        
        # Recreate optimizer after moving model to CPU
        self.optimizer = torch.optim.Adam(
            self.network.parameters(), 
            lr=self.learning_rate
        )
        
        logger.info(f"Moved {self.network_type} network to CPU")
    
    def free_memory(self):
        """Free GPU memory"""
        if self.device.type == "cuda":
            self.to_cpu()
            torch.cuda.empty_cache()
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the network
        
        Returns:
            Dictionary with state information
        """
        return {
            "network_type": self.network_type,
            "dimensions": {
                "input_dim": self.input_dim,
                "hidden_dim": self.hidden_dim,
                "output_dim": self.output_dim
            },
            "development_level": self.development_level,
            "learning_rate": self.learning_rate,
            "hebbian_rate": self.hebbian_rate,
            "device": str(self.device),
            "parameters": sum(p.numel() for p in self.network.parameters())
        }


#######################

#motivation\rewards.py#
#######################

# TODO: Implement the Rewards class to handle reward processing
# This component should be able to:
# - Detect and process reward signals
# - Calculate reward prediction errors
# - Adapt reward significance based on context
# - Develop increasingly abstract reward systems

# TODO: Implement developmental progression in reward processing:
# - Simple immediate rewards in early stages
# - Delayed reward anticipation in childhood
# - Abstract and social rewards in adolescence
# - Complex intrinsic reward systems in adulthood

# TODO: Create mechanisms for:
# - Reward detection: Identify positive outcomes and events
# - Reward prediction: Anticipate potential rewards
# - Reward valuation: Assess the significance of rewards
# - Reward learning: Update behavior based on reward history

# TODO: Implement different reward types:
# - Physiological rewards: Satisfaction of basic needs
# - Social rewards: Approval, connection, status
# - Achievement rewards: Competence, mastery, progress
# - Cognitive rewards: Curiosity satisfaction, insight, learning

# TODO: Connect to learning and emotion modules
# Rewards should guide learning processes and
# influence emotional responses

from typing import Dict, List, Any, Optional
from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus

class Rewards(BaseModule):
    """
    Processes reward signals
    
    This module identifies rewards, tracks reward history,
    calculates reward predictions, and guides learning
    based on reward experiences.
    """
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the rewards module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="rewards", event_bus=event_bus)
        
        # TODO: Initialize reward representation structures
        # TODO: Set up reward prediction mechanisms
        # TODO: Create reward history tracking
        # TODO: Initialize reward valuation system
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to identify and evaluate rewards
        
        Args:
            input_data: Dictionary containing reward-related information
            
        Returns:
            Dictionary with the processed reward signals
        """
        # TODO: Implement reward detection logic
        # TODO: Calculate reward prediction errors
        # TODO: Update reward value estimates
        # TODO: Generate learning signals based on rewards
        
        return {
            "status": "not_implemented",
            "module_id": self.module_id,
            "module_type": self.module_type
        }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # TODO: Implement development progression for reward processing
        # TODO: Enhance reward abstraction with development
        # TODO: Increase reward prediction time horizon with development
        
        return super().update_development(amount)


#######################

#motivation\__init__.py#
#######################

# Motivation module 

# TODO: Implement the motivation module factory function to return an integrated MotivationSystem
# This module should be responsible for drives, goals, needs, interests,
# and the regulation of behavior toward desired outcomes.

# TODO: Create MotivationSystem class that integrates all motivation sub-components:
# - basic_drives: fundamental physiological and safety motivators
# - goal_setting: establishing and pursuing objectives
# - need_satisfaction: meeting psychological needs
# - interest_development: cultivating curiosity and engagement
# - value_based_motivation: alignment with personal values

# TODO: Implement development tracking for motivation
# Motivational systems should develop from simple drive satisfaction in early stages
# to complex, integrated goal hierarchies and value-based motivation in later stages

# TODO: Connect motivation module to emotion, learning, and executive modules
# Motivation should be influenced by emotional states, direct
# learning activities, and guide executive functions

# TODO: Implement motivational regulation processes
# Include processes for goal adjustment, effort allocation, 
# persistence management, and motivational conflict resolution

from typing import Dict, List, Any, Optional
from lmm_project.core.event_bus import EventBus

def get_module(module_id: str, event_bus: Optional[EventBus] = None) -> Any:
    """
    Factory function to create a motivation module
    
    This function is responsible for creating a motivation system that can:
    - Generate and maintain drives toward specific goals
    - Establish goals and regulate behavior toward them
    - Adapt motivational priorities based on context and needs
    - Balance different motivational forces
    - Connect actions to deeper values and needs
    
    Args:
        module_id: Unique identifier for the module
        event_bus: Event bus for communication with other modules
        
    Returns:
        An instance of the MotivationSystem class
    """
    # TODO: Return an instance of the MotivationSystem class
    # that integrates all motivation sub-components
    raise NotImplementedError("Motivation module not yet implemented")


#######################

#perception\models.py#
#######################

from typing import Dict, List, Optional, Any, Union, Literal
from pydantic import BaseModel, Field, model_validator
import numpy as np
from datetime import datetime

class SensoryInput(BaseModel):
    """
    Represents raw sensory input data received from the environment.
    For this LMM implementation, all sensory input is text-based.
    """
    input_id: str
    timestamp: datetime = Field(default_factory=datetime.now)
    text: str
    source: str = "mother"  # e.g., 'mother', 'environment', 'internal'
    context: Dict[str, Any] = Field(default_factory=dict)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    model_config = {
        "arbitrary_types_allowed": True
    }

class Pattern(BaseModel):
    """
    Represents a detected pattern in sensory input
    """
    pattern_id: str
    pattern_type: Literal["token", "n_gram", "semantic", "syntactic", "temporal"] 
    content: Any  # Could be a string, embedding, or other representation
    confidence: float = Field(ge=0.0, le=1.0)
    activation: float = Field(ge=0.0, le=1.0)
    
    # Relationship to other patterns (for hierarchical pattern building)
    parent_patterns: List[str] = Field(default_factory=list)
    child_patterns: List[str] = Field(default_factory=list)
    
    # Metadata for pattern analysis
    frequency: int = 0  # How often this pattern has been encountered
    last_seen: Optional[datetime] = None
    first_seen: datetime = Field(default_factory=datetime.now)
    
    model_config = {
        "arbitrary_types_allowed": True
    }

class PerceptionResult(BaseModel):
    """
    The processed result of perception, to be passed to other modules
    """
    input_id: str  # Reference to the original input
    timestamp: datetime = Field(default_factory=datetime.now)
    
    # Detected patterns with their activations
    detected_patterns: List[Pattern] = Field(default_factory=list)
    
    # Novelty measure (how unfamiliar is this input)
    novelty_score: float = Field(ge=0.0, le=1.0, default=0.5)
    
    # Intensity measure (how strong is this input)
    intensity_score: float = Field(ge=0.0, le=1.0, default=0.5)
    
    # Feature vector representation of the input (for neural processing)
    feature_vector: Optional[List[float]] = None
    
    # Simplified semantic content (for debugging and introspection)
    semantic_content: Dict[str, Any] = Field(default_factory=dict)
    
    # Development-specific properties
    developmental_level: float = Field(ge=0.0, le=1.0, default=0.0)
    
    model_config = {
        "arbitrary_types_allowed": True
    }

class PerceptionMemory(BaseModel):
    """
    Short-term memory structure specific to the perception module
    """
    recent_inputs: List[SensoryInput] = Field(default_factory=list, max_items=10)
    known_patterns: Dict[str, Pattern] = Field(default_factory=dict)
    pattern_frequency: Dict[str, int] = Field(default_factory=dict)
    
    model_config = {
        "arbitrary_types_allowed": True
    }

class PerceptionParameters(BaseModel):
    """
    Configurable parameters for the perception module
    """
    # Sensitivity to different types of patterns
    token_sensitivity: float = Field(ge=0.0, le=1.0, default=0.5)
    ngram_sensitivity: float = Field(ge=0.0, le=1.0, default=0.3)
    semantic_sensitivity: float = Field(ge=0.0, le=1.0, default=0.2)
    
    # Novelty detection parameters
    novelty_threshold: float = Field(ge=0.0, le=1.0, default=0.7)
    
    # Pattern recognition thresholds
    pattern_activation_threshold: float = Field(ge=0.0, le=1.0, default=0.3)
    pattern_confidence_threshold: float = Field(ge=0.0, le=1.0, default=0.2)
    
    # Developmental adaptation
    developmental_scaling: bool = True  # Whether to scale parameters based on development
    
    model_config = {
        "arbitrary_types_allowed": True
    }


#######################

#perception\neural_net.py#
#######################

import torch 
import torch.nn as nn 
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Any, Optional, Union, Tuple
import uuid
import logging
from datetime import datetime
from collections import deque

logger = logging.getLogger(__name__)

class PerceptionNetwork(nn.Module): 
    """
    Neural network for processing textual perceptual inputs.
    
    This network processes text-based perceptual input through multiple stages:
    1. Feature extraction - Converts text tokens to feature vectors
    2. Pattern encoding - Encodes features into pattern representations
    3. Novelty detection - Identifies novel or unexpected inputs
    4. Salience estimation - Determines the importance of the input
    
    The network adapts its behavior based on developmental level.
    """
    def __init__(
        self, 
        input_dim: int = 64, 
        hidden_dim: int = 128, 
        pattern_dim: int = 32,
        developmental_level: float = 0.0
    ):
        super().__init__() 
        
        # Store configuration
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.pattern_dim = pattern_dim
        self.developmental_level = developmental_level
        
        # Feature extraction network
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
        )
        
        # Pattern encoding network
        self.pattern_encoder = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, pattern_dim)
        )
        
        # Novelty detection network
        self.novelty_detector = nn.Sequential(
            nn.Linear(pattern_dim, hidden_dim // 4),
            nn.ReLU(),
            nn.Linear(hidden_dim // 4, 1),
            nn.Sigmoid()
        )
        
        # Salience estimation network
        self.salience_estimator = nn.Sequential(
            nn.Linear(pattern_dim, hidden_dim // 4),
            nn.ReLU(),
            nn.Linear(hidden_dim // 4, 1),
            nn.Sigmoid()
        )
        
        # Pattern memory for tracking known patterns
        self.pattern_memory = {}
        
        # Recent patterns for novelty assessment
        self.recent_patterns = deque(maxlen=50)
        
        # Apply developmental scaling to adjust network behavior
        self._apply_developmental_scaling()
    
    def _apply_developmental_scaling(self):
        """
        Apply developmental level-based scaling to network parameters
        
        At lower developmental levels:
        - Higher dropout rates (simulating less reliable processing)
        - Lower sensitivity to subtle patterns
        - Stronger activation thresholds
        
        At higher developmental levels:
        - Lower dropout rates (more reliable processing)
        - Higher sensitivity to subtle patterns
        - More nuanced activation thresholds
        """
        # Scale dropout based on development (higher dropout at lower development)
        dropout_scale = max(0.1, 0.5 - (self.developmental_level * 0.4))
        
        # Update dropout layers
        for module in self.feature_extractor:
            if isinstance(module, nn.Dropout):
                module.p = dropout_scale
        
    def update_developmental_level(self, new_level: float):
        """
        Update the network's developmental level and adjust parameters
        
        Args:
            new_level: New developmental level (0.0 to 1.0)
        """
        if 0.0 <= new_level <= 1.0 and new_level != self.developmental_level:
            prev_level = self.developmental_level
            self.developmental_level = new_level
            
            # Adjust network parameters based on new level
            self._apply_developmental_scaling()
            
            # Log significant developmental changes
            if int(new_level * 10) > int(prev_level * 10):
                logger.info(f"Perception network advanced to developmental level {new_level:.2f}")
                
            return True
        return False
        
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Forward pass through the perception network
        
        Args:
            x: Input tensor of shape (batch_size, input_dim)
            
        Returns:
            Dictionary with:
                features: Extracted features
                patterns: Encoded patterns
                novelty: Novelty scores
                salience: Salience scores
        """
        # Extract features
        features = self.feature_extractor(x)
        
        # Encode patterns
        patterns = self.pattern_encoder(features)
        
        # Detect novelty
        novelty = self.novelty_detector(patterns)
        
        # Estimate salience
        salience = self.salience_estimator(patterns)
        
        # Apply developmental scaling to outputs
        if self.developmental_level < 0.3:
            # At early developmental stages, reduce sensitivity to subtle distinctions
            patterns = torch.tanh(patterns * (0.5 + self.developmental_level))
            
            # Make novelty detection more binary (less nuanced)
            novelty = torch.round(novelty * 2) / 2
        
        return {
            "features": features,
            "patterns": patterns,
            "novelty": novelty,
            "salience": salience
        }
    
    def detect_patterns(
        self, 
        input_vector: torch.Tensor, 
        known_patterns: Optional[Dict[str, torch.Tensor]] = None,
        activation_threshold: float = 0.3
    ) -> Tuple[List[Dict[str, Any]], Dict[str, torch.Tensor]]:
        """
        Detect patterns in the input vector
        
        Args:
            input_vector: Input vector to detect patterns in
            known_patterns: Dictionary of known patterns
            activation_threshold: Threshold for pattern activation
            
        Returns:
            Tuple of (activated_patterns, updated_known_patterns)
        """
        # Process input through network
        with torch.no_grad():
            output = self.forward(input_vector)
            
        # Get encoded pattern
        pattern_vector = output["patterns"]
        
        # Create pattern memory if none provided
        if known_patterns is None:
            known_patterns = self.pattern_memory
            
        # List for storing activated patterns
        activated_patterns = []
        
        # Calculate similarities with known patterns
        max_similarity = 0.0
        most_similar_pattern = None
        
        for pattern_id, stored_pattern in known_patterns.items():
            # Calculate cosine similarity - fix the dimension issue
            # Ensure both tensors are properly shaped for cosine similarity
            pattern_vector_flat = pattern_vector.view(1, -1)  # Shape: [1, dim]
            stored_pattern_flat = stored_pattern.view(1, -1)  # Shape: [1, dim]
            
            # Calculate cosine similarity between the flattened vectors
            similarity = F.cosine_similarity(
                pattern_vector_flat, 
                stored_pattern_flat
            ).item()
            
            if similarity > max_similarity:
                max_similarity = similarity
                most_similar_pattern = pattern_id
                
            # If similarity exceeds threshold, pattern is activated
            if similarity > activation_threshold:
                activated_patterns.append({
                    "pattern_id": pattern_id,
                    "activation": similarity,
                    "novelty": 1.0 - similarity
                })
                
        # Adjust threshold based on developmental level
        creation_threshold = max(0.1, 0.6 - (self.developmental_level * 0.3))
        
        # If no patterns were significantly activated, create a new one
        if max_similarity < creation_threshold:
            new_pattern_id = str(uuid.uuid4())
            known_patterns[new_pattern_id] = pattern_vector.detach().clone()
            
            activated_patterns.append({
                "pattern_id": new_pattern_id,
                "activation": 1.0,  # New pattern is fully activated
                "novelty": 1.0,     # New pattern is maximally novel
                "is_new": True
            })
            
            # Add to recent patterns
            self.recent_patterns.append({
                "pattern_id": new_pattern_id,
                "timestamp": datetime.now(),
                "vector": pattern_vector.detach().clone()
            })
            
        # Update the pattern memory
        self.pattern_memory = known_patterns
            
        return activated_patterns, known_patterns
    
    def to_device(self, device: torch.device):
        """Move the model and all tensors to the specified device"""
        self.to(device)
        # Move all stored patterns to the device
        for pattern_id, pattern in self.pattern_memory.items():
            self.pattern_memory[pattern_id] = pattern.to(device)
            
    def get_state(self) -> Dict[str, Any]:
        """Return the current state of the perception network"""
        return {
            "developmental_level": self.developmental_level,
            "input_dim": self.input_dim,
            "hidden_dim": self.hidden_dim,
            "pattern_dim": self.pattern_dim,
            "pattern_count": len(self.pattern_memory),
            "recent_pattern_count": len(self.recent_patterns)
        }
            

class TemporalPatternNetwork(nn.Module):
    """
    Neural network for processing temporal sequences of patterns
    
    This network processes sequences of pattern vectors to detect
    temporal patterns and predict future patterns.
    """
    
    def __init__(
        self, 
        input_dim: int = 32, 
        hidden_dim: int = 64,
        sequence_length: int = 5,
        developmental_level: float = 0.0
    ):
        """
        Initialize the temporal pattern network
        
        Args:
            input_dim: Dimension of input pattern vectors
            hidden_dim: Dimension of hidden layer
            sequence_length: Maximum sequence length to process
            developmental_level: Initial developmental level
        """
        super().__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.sequence_length = sequence_length
        self.developmental_level = developmental_level
        
        # LSTM for sequence processing
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=1,
            batch_first=True
        )
        
        # Temporal pattern extraction
        self.pattern_extractor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim)
        )
        
        # Next pattern prediction
        self.predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim)
        )
        
        # Device for computation
        self.device = torch.device('cpu')
        
    def to_device(self, device: torch.device):
        """Move the network to the specified device"""
        self.device = device
        self.to(device)
        return self
        
    def forward(self, sequence: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Process a sequence of pattern vectors
        
        Args:
            sequence: Tensor of shape [batch_size, sequence_length, input_dim]
            
        Returns:
            Dictionary with temporal pattern and next prediction
        """
        # Ensure sequence is on the correct device
        sequence = sequence.to(self.device)
        
        # Ensure LSTM and other components are on the same device
        self.lstm = self.lstm.to(self.device)
        self.pattern_extractor = self.pattern_extractor.to(self.device)
        self.predictor = self.predictor.to(self.device)
        
        # Process through LSTM
        lstm_out, (hidden, cell) = self.lstm(sequence)
        
        # Extract temporal pattern from final hidden state
        temporal_pattern = self.pattern_extractor(hidden[-1])
        
        # Predict next pattern
        next_prediction = self.predictor(hidden[-1])
        
        return {
            "temporal_pattern": temporal_pattern,
            "next_prediction": next_prediction,
            "hidden_state": hidden[-1]
        }
        
    def update_developmental_level(self, level: float):
        """Update the developmental level of the network"""
        self.developmental_level = level
        
    def get_state(self) -> Dict[str, Any]:
        """Return the current state of the temporal network"""
        return {
            "developmental_level": self.developmental_level,
            "input_dim": self.input_dim,
            "hidden_dim": self.hidden_dim,
            "sequence_length": self.sequence_length
        }

class NeuralPatternDetector:
    """
    Neural network-based pattern detector
    
    Uses neural networks to detect patterns in input data and
    maintain a memory of known patterns.
    """
    
    def __init__(
        self, 
        vector_dim: int = 32,
        developmental_level: float = 0.0,
        device: Optional[torch.device] = None
    ):
        """
        Initialize the neural pattern detector
        
        Args:
            vector_dim: Dimension of pattern vectors
            developmental_level: Initial developmental level
            device: Device to use for computation (CPU or CUDA)
        """
        self.vector_dim = vector_dim
        self.developmental_level = developmental_level
        
        # Set device (CPU or CUDA if available)
        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Initialize networks
        self.pattern_encoder = PatternEncoder(input_dim=vector_dim).to(self.device)
        self.temporal_network = TemporalPatternNetwork(
            input_dim=vector_dim,
            developmental_level=developmental_level
        ).to_device(self.device)
        
        # Storage for known patterns
        self.known_patterns = []
        self.pattern_vectors = []
        
        # Pattern activation threshold
        self.activation_threshold = 0.2
        
        # Initialize pattern sequence for temporal processing
        self.pattern_sequence = []
        self.max_sequence_length = 5
        
        # Logger
        self.logger = logging.getLogger(__name__)
        
    def update_developmental_level(self, level: float):
        """Update the developmental level of all components"""
        if 0.0 <= level <= 1.0:
            self.developmental_level = level
            self.temporal_network.update_developmental_level(level)
            
            # Adjust activation threshold based on developmental level
            # Lower threshold as development increases (more sensitive)
            self.activation_threshold = max(0.1, 0.3 - (level * 0.2))
            
            return True
        return False
        
    def encode_pattern(self, input_vector: torch.Tensor) -> torch.Tensor:
        """
        Encode an input vector into a pattern vector
        
        Args:
            input_vector: Input vector to encode
            
        Returns:
            Encoded pattern vector
        """
        # Ensure input is on the correct device
        input_vector = input_vector.to(self.device)
        
        # Encode the pattern
        with torch.no_grad():
            pattern_vector = self.pattern_encoder(input_vector)
            
        return pattern_vector
        
    def detect_patterns(self, input_vector: torch.Tensor, activation_threshold: float = None) -> List[Dict[str, Any]]:
        """
        Detect patterns in the input vector
        
        Args:
            input_vector: Input vector to detect patterns in
            activation_threshold: Optional override for activation threshold
            
        Returns:
            List of detected patterns with confidence scores
        """
        if activation_threshold is None:
            activation_threshold = self.activation_threshold
            
        # Ensure input is on the correct device
        input_vector = input_vector.to(self.device)
        
        # Encode the pattern
        pattern_vector = self.encode_pattern(input_vector)
        
        detected_patterns = []
        
        # Compare with known patterns
        for i, stored_pattern in enumerate(self.pattern_vectors):
            # Ensure stored pattern is on the correct device
            stored_pattern = stored_pattern.to(self.device)
            
            # Reshape tensors for cosine similarity calculation
            pattern_vector_flat = pattern_vector.view(1, -1)
            stored_pattern_flat = stored_pattern.view(1, -1)
            
            # Calculate similarity
            similarity = F.cosine_similarity(pattern_vector_flat, stored_pattern_flat)
            confidence = similarity.item()
            
            if confidence >= activation_threshold:
                detected_patterns.append({
                    "pattern_id": i,
                    "pattern_name": self.known_patterns[i]["name"],
                    "confidence": confidence,
                    "pattern_type": "neural"
                })
                
        return detected_patterns
        
    def process_temporal_sequence(self, pattern_vector: torch.Tensor) -> Dict[str, Any]:
        """
        Process a pattern vector through the temporal network
        
        Args:
            pattern_vector: Pattern vector to process
            
        Returns:
            Temporal processing results
        """
        # Ensure pattern vector is on the correct device
        pattern_vector = pattern_vector.to(self.device)
        
        # Add to sequence
        self.pattern_sequence.append(pattern_vector)
        
        # Keep sequence at max length
        if len(self.pattern_sequence) > self.max_sequence_length:
            self.pattern_sequence = self.pattern_sequence[-self.max_sequence_length:]
            
        # Process sequence if we have enough patterns
        if len(self.pattern_sequence) >= 2:
            # Stack sequence into a batch
            sequence_tensor = torch.stack(self.pattern_sequence).unsqueeze(0)
            
            # Process through temporal network
            with torch.no_grad():
                temporal_results = self.temporal_network(sequence_tensor)
                
            return temporal_results
            
        return {"temporal_pattern": None, "next_prediction": None}
        
    def create_pattern(self, pattern_vector: torch.Tensor, pattern_name: str) -> Dict[str, Any]:
        """
        Create a new pattern from the input vector
        
        Args:
            pattern_vector: Vector representation of the pattern
            pattern_name: Name/identifier for the pattern
            
        Returns:
            Created pattern information
        """
        # Ensure pattern vector is on the correct device
        pattern_vector = pattern_vector.to(self.device)
        
        # Create pattern entry
        pattern = {
            "name": pattern_name,
            "created_at": datetime.now().isoformat(),
            "developmental_level": self.developmental_level,
            "vector_dim": self.vector_dim
        }
        
        # Store pattern
        self.known_patterns.append(pattern)
        self.pattern_vectors.append(pattern_vector.detach().clone())
        
        return pattern
        
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input data through the neural pattern detector
        
        Args:
            input_data: Input data dictionary with vector representation
            
        Returns:
            Processing results with detected patterns
        """
        # Extract input vector
        if "vector" not in input_data:
            self.logger.warning("No vector in input data for neural pattern detector")
            return {"error": "No vector in input data"}
            
        input_vector = input_data["vector"]
        
        # Ensure input vector is a tensor
        if not isinstance(input_vector, torch.Tensor):
            input_vector = torch.tensor(input_vector, dtype=torch.float32)
            
        # Ensure input vector is on the correct device
        input_vector = input_vector.to(self.device)
        
        # Encode the pattern
        pattern_vector = self.encode_pattern(input_vector)
        
        # Detect patterns
        detected_patterns = self.detect_patterns(input_vector)
        
        # Process temporal sequence
        temporal_results = self.process_temporal_sequence(pattern_vector)
        
        # Create a new pattern if none detected and developmental level is sufficient
        if not detected_patterns and self.developmental_level >= 0.3:
            # Generate a unique pattern name
            pattern_name = f"neural_pattern_{len(self.known_patterns)}"
            
            # Create the pattern
            new_pattern = self.create_pattern(pattern_vector, pattern_name)
            
            # Add to detected patterns with high confidence
            detected_patterns.append({
                "pattern_id": len(self.known_patterns) - 1,
                "pattern_name": pattern_name,
                "confidence": 1.0,  # New pattern has perfect match
                "pattern_type": "neural",
                "is_new": True
            })
            
        return {
            "detected_patterns": detected_patterns,
            "temporal_results": temporal_results,
            "developmental_level": self.developmental_level
        }


#######################

#perception\pattern_recognition.py#
#######################

"""
Pattern Recognition Module

This module is responsible for identifying patterns in sensory input.
It progressively develops the ability to recognize increasingly complex
patterns, from simple feature detection to sophisticated pattern extraction.
"""

import logging
import uuid
import numpy as np
import torch
import torch.nn.functional as F
from typing import Dict, List, Any, Optional, Tuple, Set
from collections import deque, Counter
from datetime import datetime
import re
import nltk
from nltk.util import ngrams
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Import LLM client for embeddings
from lmm_project.utils.llm_client import LLMClient

# Ensure NLTK data is downloaded
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message
from lmm_project.modules.perception.neural_net import PerceptionNetwork, TemporalPatternNetwork

logger = logging.getLogger(__name__)

class PatternRecognizer(BaseModule):
    """
    Recognizes patterns in sensory input
    
    This module develops from simple feature detection to complex pattern
    recognition, supporting the perception system's ability to identify
    meaningful structures in input.
    """
    # Development stages for pattern recognition
    development_milestones = {
        0.0: "Basic feature detection",
        0.2: "Simple pattern matching",
        0.4: "Pattern abstraction",
        0.6: "Multi-feature patterns",
        0.8: "Context-sensitive patterns",
        1.0: "Advanced pattern recognition"
    }
    
    def __init__(
        self,
        module_id: str,
        event_bus: Optional[EventBus] = None,
        **kwargs
    ):
        """
        Initialize the pattern recognizer
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication
        """
        super().__init__(
            module_id=module_id,
            module_type="pattern_recognizer",
            event_bus=event_bus,
            **kwargs
        )
        
        # Initialize neural network
        self.neural_net = PerceptionNetwork(
            input_dim=64,
            hidden_dim=128,
            pattern_dim=32,
            developmental_level=self.development_level
        )
        
        # Initialize temporal pattern network
        self.temporal_net = TemporalPatternNetwork(
            input_dim=32,
            hidden_dim=64,
            sequence_length=5,
            developmental_level=self.development_level
        )
        
        # Pattern memory
        self.known_patterns = {}
        self.pattern_frequency = Counter()
        self.recent_inputs = deque(maxlen=10)
        self.temporal_sequence = deque(maxlen=5)
        
        # Initialize LLM client for embeddings
        self.llm_client = LLMClient()
        self.use_embeddings = True  # Flag to control whether to use embeddings
        
        # Pattern recognition parameters - making these more sensitive
        self.token_sensitivity = 0.8  # Increased from 0.6
        self.ngram_sensitivity = 0.6  # Increased from 0.4
        self.semantic_sensitivity = 0.4  # Increased from 0.3
        self.novelty_threshold = 0.5  # Reduced from 0.6
        self.pattern_activation_threshold = 0.1  # Reduced from 0.2
        
        # For TF-IDF based pattern recognition
        self.vectorizer = TfidfVectorizer(
            min_df=1, 
            max_df=0.9,
            ngram_range=(1, 3),
            max_features=100
        )
        self.document_vectors = []
        self.documents = []
        
        # Try to use GPU if available
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.neural_net.to_device(self.device)
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process sensory input to recognize patterns
        
        Args:
            input_data: Dictionary with sensory processing results
            
        Returns:
            Dictionary with recognized patterns
        """
        # Debug logging to see what's in the input data
        logger.info(f"Pattern recognizer received input with keys: {list(input_data.keys())}")
        
        # Extract process ID or generate new one
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Extract text from input data
        text = input_data.get("text", "")
        logger.info(f"Pattern recognizer extracted text: '{text[:30]}...' (length: {len(text)})")
        
        if not text:
            logger.warning(f"Pattern recognizer received empty text input for process {process_id}")
            return {
                "process_id": process_id,
                "patterns": [],
                "status": "error",
                "error": "Empty input text"
            }
            
        # Log the input processing
        logger.debug(f"Processing input for patterns: '{text[:50]}...' (process {process_id})")
        
        # Extract relevant features from sensory data
        features = self._extract_relevant_features(input_data)
        
        # Add to recent inputs
        self.recent_inputs.append({
            "text": text,
            "features": features,
            "timestamp": datetime.now()
        })
        
        # Recognize patterns
        patterns = self._recognize_patterns(features)
        
        # Add patterns to temporal sequence for sequence learning
        if patterns and self.development_level >= 0.4:
            # Create a feature vector from the patterns
            pattern_feature = self._create_pattern_feature_vector(patterns)
            
            # Add to temporal sequence
            self.temporal_sequence.append(pattern_feature)
            
            # If we have enough sequence data, process the temporal pattern
            if len(self.temporal_sequence) >= 3 and self.development_level >= 0.6:
                temporal_results = self._process_temporal_sequence()
                # Add temporal pattern to results if found
                if temporal_results and "temporal_pattern" in temporal_results:
                    patterns.append(self._create_pattern(
                        "temporal", 
                        confidence=0.7, 
                        attributes={
                            "sequence_length": len(self.temporal_sequence),
                            "temporal_features": temporal_results.get("temporal_features", {})
                        }
                    ))
        
        # Update the neural network's developmental level if needed
        if abs(self.neural_net.developmental_level - self.development_level) > 0.05:
            self.neural_net.update_developmental_level(self.development_level)
            self.temporal_net.update_developmental_level(self.development_level)
        
        # Create the output result
        result = {
            "process_id": process_id,
            "timestamp": datetime.now().isoformat(),
            "patterns": patterns,
            "pattern_count": len(patterns),
            "development_level": self.development_level,
            "novelty_average": np.mean([p.get("novelty", 0.5) for p in patterns]) if patterns else 0.5
        }
        
        return result
    
    def _extract_relevant_features(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract features relevant for pattern recognition from sensory input
        
        Args:
            input_data: Raw sensory input data
            
        Returns:
            Dictionary with extracted features
        """
        features = {}
        
        # Get text from input
        text = input_data.get("text", "")
        if not text:
            logger.warning("Empty text received in _extract_relevant_features")
            return features
            
        # Make sure to include text in the features
        features["text"] = text
        
        # Basic text statistics
        features["text_length"] = len(text)
        features["word_count"] = len(text.split())
        features["avg_word_length"] = np.mean([len(w) for w in text.split()]) if text.split() else 0
        
        # Add features from sensory processing if available
        if "basic_features" in input_data:
            features.update(input_data["basic_features"])
            
        if "features" in input_data:
            features.update(input_data["features"])
            
        if "linguistic_features" in input_data:
            features.update(input_data["linguistic_features"])
            
        # Create tensors for neural processing
        features["text_vector"] = self._text_to_vector(text)
            
        return features
    
    def _recognize_patterns(self, features: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Recognize patterns in the provided features
        
        The pattern recognition approach changes with developmental level:
        - Early stages (0.0-0.2): Simple token pattern detection
        - Intermediate stages (0.2-0.6): N-gram patterns and basic semantics
        - Advanced stages (0.6-1.0): Semantic patterns, syntactic patterns, and context
        
        Args:
            features: Dictionary of extracted features
            
        Returns:
            List of recognized patterns
        """
        # Add debug logging
        logger.info(f"Pattern recognition started with development level: {self.development_level:.2f}")
        logger.info(f"Pattern recognition parameters: token={self.token_sensitivity:.2f}, ngram={self.ngram_sensitivity:.2f}, semantic={self.semantic_sensitivity:.2f}, threshold={self.pattern_activation_threshold:.2f}")
        
        patterns = []
        
        # Get text for pattern recognition
        text = features.get("text", "")
        if not text:
            logger.warning("Empty text received for pattern recognition")
            return patterns
            
        # Vector representation for neural processing
        text_vector = features.get("text_vector")
        
        # Always detect basic token and n-gram patterns regardless of developmental level
        # This ensures we always have at least some patterns at any level
        token_patterns = self._detect_token_patterns(text)
        if token_patterns:
            logger.info(f"Detected {len(token_patterns)} token patterns")
            patterns.extend(token_patterns)
        else:
            logger.warning("No token patterns detected")
            
        ngram_patterns = self._detect_ngram_patterns(text)
        if ngram_patterns:
            logger.info(f"Detected {len(ngram_patterns)} n-gram patterns")
            patterns.extend(ngram_patterns)
        else:
            logger.warning("No n-gram patterns detected")
        
        # Adapt additional pattern recognition to developmental level
        if self.development_level >= 0.2:
            # Basic neural processing if we have a text vector
            if text_vector is not None:
                neural_patterns = self._detect_neural_patterns(text_vector)
                if neural_patterns:
                    logger.info(f"Detected {len(neural_patterns)} neural patterns")
                    patterns.extend(neural_patterns)
                else:
                    logger.warning("No neural patterns detected")
                
        if self.development_level >= 0.4:
            # Add semantic patterns
            semantic_patterns = self._detect_semantic_patterns(text, features)
            if semantic_patterns:
                logger.info(f"Detected {len(semantic_patterns)} semantic patterns")
                patterns.extend(semantic_patterns)
            else:
                logger.warning("No semantic patterns detected")
            
        if self.development_level >= 0.6:
            # Add syntactic patterns
            syntactic_patterns = self._detect_syntactic_patterns(text)
            if syntactic_patterns:
                logger.info(f"Detected {len(syntactic_patterns)} syntactic patterns")
                patterns.extend(syntactic_patterns)
            else:
                logger.warning("No syntactic patterns detected")
            
        if self.development_level >= 0.8:
            # Add contextual patterns that incorporate prior inputs
            if len(self.recent_inputs) > 1:
                contextual_patterns = self._detect_contextual_patterns(text, features)
                if contextual_patterns:
                    logger.info(f"Detected {len(contextual_patterns)} contextual patterns")
                    patterns.extend(contextual_patterns)
                else:
                    logger.warning("No contextual patterns detected")
        
        # Update pattern frequencies
        for pattern in patterns:
            pattern_id = pattern.get("pattern_id", "")
            if pattern_id:
                self.pattern_frequency[pattern_id] += 1
                pattern["frequency"] = self.pattern_frequency[pattern_id]
                
        # If no patterns detected, create a basic fallback pattern
        if not patterns and text:
            logger.warning(f"No patterns detected, creating fallback pattern for: {text[:30]}...")
            patterns.append(self._create_pattern(
                "unknown",
                confidence=0.5,
                attributes={
                    "text": text[:50] + ("..." if len(text) > 50 else ""),
                    "is_fallback": True
                }
            ))
        
        logger.info(f"Pattern recognition completed with {len(patterns)} total patterns detected")
        return patterns
    
    def _detect_token_patterns(self, text: str) -> List[Dict[str, Any]]:
        """Detect patterns at the token level"""
        if not text:
            logger.warning("Empty text passed to token pattern detection")
            return []
            
        # Debug info
        logger.info(f"Token pattern detection for text: '{text[:30]}...'")
            
        patterns = []
        tokens = word_tokenize(text.lower())
        
        logger.info(f"Tokenized into {len(tokens)} tokens: {tokens[:5]}...")
        
        # Always create a token pattern if tokens exist
        if tokens:
            token_pattern = self._create_pattern(
                "token",
                confidence=0.9,
                attributes={
                    "tokens": tokens[:5],  # First 5 tokens
                    "token_count": len(tokens)
                }
            )
            patterns.append(token_pattern)
            logger.info("Created basic token pattern")
        
        # Check for repeated tokens
        token_counts = Counter(tokens)
        for token, count in token_counts.items():
            if count > 1 and len(token) > 1:  # Avoid single characters
                repeated_token_pattern = self._create_pattern(
                    "token",
                    confidence=min(0.5 + count * 0.1, 0.9),
                    attributes={
                        "token": token,
                        "count": count,
                        "frequency": count / len(tokens) if tokens else 0
                    }
                )
                patterns.append(repeated_token_pattern)
                logger.info(f"Created repeated token pattern for '{token}' (count: {count})")
        
        # Check for unusual tokens (numbers, symbols, etc.)
        for token in tokens:
            # Numbers
            if token.isdigit() or (token.replace('.', '', 1).isdigit() and '.' in token):
                number_pattern = self._create_pattern(
                    "token",
                    confidence=0.8,
                    attributes={
                        "token": token,
                        "token_type": "number"
                    }
                )
                patterns.append(number_pattern)
                logger.info(f"Created number token pattern for '{token}'")
            
            # Special symbols
            elif any(c in token for c in "!@#$%^&*()_+-=[]{}|;':\",./<>?"):
                symbol_pattern = self._create_pattern(
                    "token",
                    confidence=0.7,
                    attributes={
                        "token": token,
                        "token_type": "symbol"
                    }
                )
                patterns.append(symbol_pattern)
                logger.info(f"Created symbol token pattern for '{token}'")
        
        logger.info(f"Token pattern detection completed with {len(patterns)} patterns")
        return patterns
    
    def _detect_ngram_patterns(self, text: str) -> List[Dict[str, Any]]:
        """Detect n-gram patterns in text"""
        if not text:
            return []
            
        patterns = []
        
        # Tokenize text
        tokens = word_tokenize(text.lower())
        if len(tokens) < 2:
            return patterns
        
        # Always create a basic n-gram pattern if possible
        if len(tokens) >= 2:
            patterns.append(self._create_pattern(
                "n_gram",
                confidence=0.8,
                attributes={
                    "bigrams": [" ".join(bg) for bg in list(ngrams(tokens, 2))[:3]],  # First 3 bigrams
                    "count": len(list(ngrams(tokens, 2)))
                }
            ))
        
        # Generate n-grams
        bigrams = list(ngrams(tokens, 2))
        if len(tokens) >= 3:
            trigrams = list(ngrams(tokens, 3))
        else:
            trigrams = []
            
        # Check for repeated bigrams
        bigram_counts = Counter(bigrams)
        for bigram, count in bigram_counts.items():
            if count > 1:
                patterns.append(self._create_pattern(
                    "n_gram", 
                    confidence=min(0.6 + count * 0.1, 0.9),
                    attributes={
                        "n_gram": " ".join(bigram),
                        "n": 2,
                        "count": count
                    }
                ))
                
        # Check for repeated trigrams
        trigram_counts = Counter(trigrams)
        for trigram, count in trigram_counts.items():
            if count > 1:
                patterns.append(self._create_pattern(
                    "n_gram", 
                    confidence=min(0.7 + count * 0.1, 0.95),
                    attributes={
                        "n_gram": " ".join(trigram),
                        "n": 3,
                        "count": count
                    }
                ))
        
        return patterns
    
    def _detect_neural_patterns(self, text_vector: torch.Tensor) -> List[Dict[str, Any]]:
        """Use neural network to detect patterns in text vector"""
        patterns = []
        
        # Process through neural network
        with torch.no_grad():
            # Move to appropriate device
            text_vector = text_vector.to(self.device)
            
            # Get neural output
            output = self.neural_net.forward(text_vector)
            
            # Detect patterns
            neural_patterns, updated_known_patterns = self.neural_net.detect_patterns(
                text_vector, 
                self.known_patterns,
                activation_threshold=self.pattern_activation_threshold
            )
            
            # Update known patterns
            self.known_patterns = updated_known_patterns
            
            # Convert to pattern objects
            for np in neural_patterns:
                pattern_id = np.get("pattern_id", "")
                is_new = np.get("is_new", False)
                
                patterns.append(self._create_pattern(
                    "neural", 
                    confidence=np.get("activation", 0.5),
                    attributes={
                        "neural_id": pattern_id,
                        "novelty_score": np.get("novelty", 0.5),
                        "is_new_pattern": is_new
                    }
                ))
        
        return patterns
    
    def _detect_semantic_patterns(self, text: str, features: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Detect semantic patterns in text"""
        if not text or len(text.split()) < 3:
            return []
            
        patterns = []
        
        # Add basic semantic pattern for any text with sufficient length
        if len(text) > 10:
            patterns.append(self._create_pattern(
                "semantic",
                confidence=0.7,
                attributes={
                    "text_length": len(text),
                    "complexity": "low" if len(text) < 50 else "medium" if len(text) < 100 else "high"
                }
            ))
        
        # Add to document collection for TF-IDF
        if text not in self.documents:
            self.documents.append(text)
            
            # Rebuild vectorizer if we have enough documents
            if len(self.documents) > 1:
                try:
                    self.document_vectors = self.vectorizer.fit_transform(self.documents)
                except:
                    # If vectorizer fails, reset and continue
                    self.vectorizer = TfidfVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 3))
                    if len(self.documents) > 0:
                        self.document_vectors = self.vectorizer.fit_transform(self.documents)
                        
        # Check for common topics/themes
        # Basic approach using TF-IDF to extract important terms
        if len(self.documents) > 0 and hasattr(self.vectorizer, 'vocabulary_'):
            try:
                # Get the current document vector
                current_vector = self.vectorizer.transform([text])
                
                # Get the most significant terms
                feature_names = self.vectorizer.get_feature_names_out()
                
                # Find non-zero elements in the current vector
                nonzero_indices = current_vector.nonzero()[1]
                
                # Get the most important features
                important_features = [(feature_names[idx], current_vector[0, idx]) 
                                    for idx in nonzero_indices]
                
                # Sort by importance
                important_features.sort(key=lambda x: x[1], reverse=True)
                
                # Create patterns for the most important terms
                for term, importance in important_features[:5]:  # Top 5 terms
                    if importance > 0.1:  # Threshold
                        patterns.append(self._create_pattern(
                            "semantic", 
                            confidence=min(0.5 + float(importance), 0.9),
                            attributes={
                                "term": term,
                                "importance": float(importance),
                                "frequency": text.lower().count(term)
                            }
                        ))
            except Exception as e:
                logger.warning(f"Error in semantic pattern detection: {str(e)}")
                
        return patterns
    
    def _detect_syntactic_patterns(self, text: str) -> List[Dict[str, Any]]:
        """Detect syntactic patterns in text"""
        patterns = []
        
        # Detect questions
        if '?' in text:
            patterns.append(self._create_pattern(
                "syntactic", 
                confidence=0.9,
                attributes={
                    "pattern_type": "question",
                    "count": text.count('?')
                }
            ))
            
        # Detect exclamations
        if '!' in text:
            patterns.append(self._create_pattern(
                "syntactic", 
                confidence=0.9,
                attributes={
                    "pattern_type": "exclamation",
                    "count": text.count('!')
                }
            ))
            
        # Detect common sentence structures
        if self.development_level >= 0.7:
            # Check for conditional statements
            if re.search(r'\bif\b.*\bthen\b', text, re.IGNORECASE) or 'if' in text.lower():
                patterns.append(self._create_pattern(
                    "syntactic", 
                    confidence=0.8,
                    attributes={
                        "pattern_type": "conditional"
                    }
                ))
                
            # Check for comparisons
            if re.search(r'\bmore\b.*\bthan\b|\bless\b.*\bthan\b|\bbetter\b.*\bthan\b', text, re.IGNORECASE):
                patterns.append(self._create_pattern(
                    "syntactic", 
                    confidence=0.8,
                    attributes={
                        "pattern_type": "comparison"
                    }
                ))
                
            # Check for negations
            if re.search(r'\bnot\b|\bno\b|\bnever\b', text, re.IGNORECASE):
                patterns.append(self._create_pattern(
                    "syntactic", 
                    confidence=0.8,
                    attributes={
                        "pattern_type": "negation"
                    }
                ))
        
        return patterns
    
    def _detect_contextual_patterns(self, text: str, features: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Detect patterns that depend on context from recent inputs"""
        patterns = []
        
        if len(self.recent_inputs) < 2:
            return patterns
            
        # Get previous input
        prev_input = self.recent_inputs[-2]
        prev_text = prev_input.get("text", "")
        
        # Check for repetition patterns
        if text == prev_text:
            patterns.append(self._create_pattern(
                "contextual", 
                confidence=0.9,
                attributes={
                    "pattern_type": "repetition",
                    "repeated_text": text[:50] + ("..." if len(text) > 50 else "")
                }
            ))
            
        # Check for similarity
        elif prev_text and self._simple_similarity(text, prev_text) > 0.7:
            patterns.append(self._create_pattern(
                "contextual", 
                confidence=0.8,
                attributes={
                    "pattern_type": "similar_to_previous",
                    "similarity": self._simple_similarity(text, prev_text)
                }
            ))
            
        # Check for question-answer patterns
        if prev_text and prev_text.strip().endswith('?') and not text.strip().endswith('?'):
            patterns.append(self._create_pattern(
                "contextual", 
                confidence=0.9,
                attributes={
                    "pattern_type": "answer_to_question",
                    "question": prev_text[:50] + ("..." if len(prev_text) > 50 else "")
                }
            ))
            
        return patterns
    
    def _process_temporal_sequence(self) -> Dict[str, Any]:
        """Process the temporal sequence of patterns"""
        if len(self.temporal_sequence) < 3:
            return {}
            
        # Convert sequence to tensor
        sequence_tensor = torch.stack(list(self.temporal_sequence), dim=0).unsqueeze(0)
        
        # Process through temporal network
        with torch.no_grad():
            sequence_tensor = sequence_tensor.to(self.device)
            result = self.temporal_net.forward(sequence_tensor)
            
        # Extract results
        return {
            "temporal_pattern": result["temporal_pattern"].cpu().numpy().tolist(),
            "next_prediction": result["next_prediction"].cpu().numpy().tolist(),
            "temporal_features": {
                "sequence_length": len(self.temporal_sequence),
                "has_pattern": result["temporal_pattern"].norm().item() > 0.5
            }
        }
        
    def _text_to_vector(self, text: str) -> torch.Tensor:
        """
        Convert text to a vector representation for neural processing
        
        If LLM embeddings are available, uses them for a more sophisticated
        representation. Otherwise falls back to a simple feature-based encoding.
        """
        # Use LLM client for embeddings if available
        if self.use_embeddings:
            try:
                # Get embeddings from LLM client
                embedding = self.llm_client.get_embedding(text)
                
                # Check if embedding dimensions match what we need
                if isinstance(embedding, list) and len(embedding) > 0:
                    # Check if embedding dimensions match what we need
                    if len(embedding) > 64:
                        # Truncate if too large
                        embedding = embedding[:64]
                    elif len(embedding) < 64:
                        # Pad if too small
                        embedding = embedding + [0.0] * (64 - len(embedding))
                    
                    # Convert to tensor
                    return torch.tensor(embedding, dtype=torch.float32).unsqueeze(0)
                else:
                    logger.warning(f"Received invalid embedding format: {type(embedding)}")
                    # Continue to fallback
            except Exception as e:
                logger.warning(f"Failed to get embeddings from LLM client: {e}")
                logger.warning("Falling back to simple vector encoding")
        
        # Fallback: More robust bag-of-words approach
        vector = np.zeros(64)  # Match input_dim of neural net
        
        # Tokenize text
        tokens = word_tokenize(text.lower()) if text else []
        
        # Add token-based features
        for i, token in enumerate(tokens[:20]):  # Use up to 20 tokens
            # Use a hash function to distribute tokens across the vector
            idx = hash(token) % 32
            vector[idx] += 1.0  # Count occurrences
            
            # Add character-level information
            for char in token[:5]:  # First 5 chars of each token
                vector[(hash(char) % 16) + 32] += 0.1  # Use second half of vector for chars
            
        # Add basic text statistics
        if text:
            vector[48] = min(len(text) / 1000, 1.0)  # Text length (normalized)
            vector[49] = min(len(tokens) / 100, 1.0)  # Word count (normalized)
            vector[50] = min(text.count('.') / 10, 1.0)  # Sentence count approx
            vector[51] = min(text.count('?') / 5, 1.0)  # Question mark count
            vector[52] = min(text.count('!') / 5, 1.0)  # Exclamation count
            vector[53] = min(sum(1 for c in text if c.isupper()) / 20, 1.0)  # Uppercase count
            vector[54] = min(sum(1 for c in text if c.isdigit()) / 20, 1.0)  # Digit count
            
            # Add n-gram presence
            if len(tokens) >= 2:
                vector[55] = 1.0  # Has bigrams
            if len(tokens) >= 3:
                vector[56] = 1.0  # Has trigrams
                
            # Average word length
            vector[57] = min(np.mean([len(w) for w in tokens]) / 10 if tokens else 0, 1.0)
            
            # Set the remaining elements to ensure the vector is non-zero
            vector[58:64] = np.random.rand(6) * 0.1  # Small random values
            
        return torch.tensor(vector, dtype=torch.float32).unsqueeze(0)
    
    def _create_pattern_feature_vector(self, patterns: List[Dict[str, Any]]) -> torch.Tensor:
        """Create a feature vector from patterns for temporal sequence processing"""
        vector = np.zeros(32)  # Match input_dim of temporal net
        
        # Count pattern types
        pattern_types = Counter([p.get("pattern_type", "") for p in patterns])
        
        # Set vector values based on pattern information
        for i, (pattern_type, count) in enumerate(pattern_types.items()):
            idx = hash(pattern_type) % 16  # Use hash to distribute the pattern types
            vector[idx] += count / 5  # Scale the count
            
        # Add pattern count
        vector[16] = len(patterns) / 10
        
        # Add average confidence
        vector[17] = np.mean([p.get("confidence", 0.5) for p in patterns]) if patterns else 0.5
        
        # Add average novelty
        vector[18] = np.mean([p.get("attributes", {}).get("novelty_score", 0.5) 
                           for p in patterns]) if patterns else 0.5
        
        return torch.tensor(vector, dtype=torch.float32)
    
    def _simple_similarity(self, text1: str, text2: str) -> float:
        """Calculate a simple similarity score between two texts"""
        if not text1 or not text2:
            return 0.0
            
        # Convert to lowercase and tokenize
        tokens1 = set(word_tokenize(text1.lower()))
        tokens2 = set(word_tokenize(text2.lower()))
        
        # Calculate Jaccard similarity
        if not tokens1 or not tokens2:
            return 0.0
            
        intersection = tokens1.intersection(tokens2)
        union = tokens1.union(tokens2)
        
        return len(intersection) / len(union)
        
    def _create_pattern(self, pattern_type: str, confidence: float = 1.0, 
                       attributes: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Create a pattern object with the provided attributes
        
        Args:
            pattern_type: Type of pattern
            confidence: Confidence level
            attributes: Additional pattern attributes
            
        Returns:
            Dictionary representing the pattern
        """
        # Generate a unique ID for the pattern
        pattern_id = f"{pattern_type}_{uuid.uuid4().hex[:8]}"
        
        # Create pattern object
        pattern = {
            "pattern_id": pattern_id,
            "pattern_type": pattern_type,
            "confidence": confidence,
            "timestamp": datetime.now().isoformat(),
            "attributes": attributes or {},
            "developmental_level": self.development_level
        }
        
        # Add additional information based on pattern type
        if pattern_type == "token":
            pattern["novelty"] = 0.3  # Token patterns are common
        elif pattern_type == "n_gram":
            pattern["novelty"] = 0.5  # N-gram patterns have medium novelty
        elif pattern_type == "semantic":
            pattern["novelty"] = 0.7  # Semantic patterns are more novel
        elif pattern_type == "neural":
            pattern["novelty"] = attributes.get("novelty_score", 0.5) if attributes else 0.5
        elif pattern_type == "temporal":
            pattern["novelty"] = 0.8  # Temporal patterns are quite novel
        else:
            pattern["novelty"] = 0.5  # Default novelty
            
        return pattern
        
    def _handle_message(self, message: Message):
        """Handle incoming messages"""
        # Handle specific message types as needed
        pass
        
    def update_development(self, amount: float) -> float:
        """
        Update the module's developmental level
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New developmental level
        """
        # Update base development level
        new_level = super().update_development(amount)
        
        # Update neural network development level
        self.neural_net.update_developmental_level(new_level)
        self.temporal_net.update_developmental_level(new_level)
        
        # Adjust pattern recognition parameters based on development
        self.token_sensitivity = 0.7 + new_level * 0.2  # Starts higher and increases less
        self.ngram_sensitivity = 0.5 + new_level * 0.3  # Starts higher and increases less
        self.semantic_sensitivity = 0.4 + new_level * 0.4  # Starts higher
        
        # Refine thresholds as development progresses
        self.pattern_activation_threshold = max(0.05, 0.2 - new_level * 0.15)  # Lower threshold
        
        # Log development progress
        logger.info(f"Pattern recognizer {self.module_id} developmental level updated to {new_level:.2f}")
        
        return new_level
        
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the module
        
        Returns:
            Dictionary containing module state
        """
        base_state = super().get_state()
        
        # Add pattern recognizer specific state
        state = {
            **base_state,
            "known_pattern_count": len(self.known_patterns),
            "neural_net_state": self.neural_net.get_state(),
            "temporal_net_state": self.temporal_net.get_state(),
            "recent_input_count": len(self.recent_inputs),
            "token_sensitivity": self.token_sensitivity,
            "ngram_sensitivity": self.ngram_sensitivity,
            "semantic_sensitivity": self.semantic_sensitivity,
            "pattern_activation_threshold": self.pattern_activation_threshold,
            "use_embeddings": self.use_embeddings
        }
        
        return state
        
    def get_recent_patterns(self, count: int = 5) -> List[Dict[str, Any]]:
        """
        Get the most recently detected patterns
        
        Args:
            count: Maximum number of patterns to return
            
        Returns:
            List of recent patterns
        """
        recent_patterns = []
        
        # Extract patterns from recent inputs
        for input_data in reversed(list(self.recent_inputs)):
            if "patterns" in input_data:
                recent_patterns.extend(input_data["patterns"])
                
            if len(recent_patterns) >= count:
                break
                
        return recent_patterns[:count] 


#######################

#perception\sensory_input.py#
#######################

"""
Sensory Input Processing Module

This module is responsible for processing raw text input into a form
suitable for further processing by the perception system. It serves
as the primary sensory interface for the LMM, converting text into
meaningful sensory representations.
"""

import re
import numpy as np
from typing import Dict, List, Any, Optional, Union, Set, Tuple
import uuid
import logging
from datetime import datetime
from collections import deque, Counter
import string
import torch

# Add tokenization libraries
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.util import ngrams
from nltk.corpus import stopwords
from nltk.probability import FreqDist

# Ensure NLTK data is downloaded
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)

from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus
from lmm_project.core.message import Message

logger = logging.getLogger(__name__)

class SensoryInputProcessor(BaseModule):
    """
    Processes raw text input into a form suitable for perception
    
    This module is the first stage of perception, converting raw text
    into feature vectors and preliminary sensory representations.
    """
    # Development stages for sensory processing
    development_milestones = {
        0.0: "Basic text detection",
        0.2: "Simple tokenization",
        0.4: "Feature extraction",
        0.6: "Multi-level analysis",
        0.8: "Context sensitivity",
        1.0: "Advanced sensory processing"
    }
    
    def __init__(
        self, 
        module_id: str,
        event_bus: Optional[EventBus] = None,
        **kwargs
    ):
        """
        Initialize the sensory input processor
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication
        """
        super().__init__(
            module_id=module_id,
            module_type="sensory_processor",
            event_bus=event_bus,
            **kwargs
        )
        
        # Initialize sensory memory
        self.recent_inputs = deque(maxlen=20)
        self.input_history = []
        
        # Initialize frequency tracking
        self.token_frequencies = Counter()
        self.bigram_frequencies = Counter()
        self.character_frequencies = Counter()
        
        # Processing parameters that develop over time
        self.max_tokens = 100  # Maximum tokens to process
        self.token_threshold = 0.1  # Threshold for token significance
        self.similarity_threshold = 0.7  # Threshold for similar inputs
        
        # Get stopwords for filtering
        try:
            self.stopwords = set(stopwords.words('english'))
        except:
            self.stopwords = set(['the', 'and', 'a', 'to', 'of', 'in', 'is', 'it'])
            
        # Try to use GPU if available
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Subscribe to raw text input events
        if self.event_bus:
            self.subscribe_to_message("raw_text_input")
        
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process raw input text into sensory representations
        
        Args:
            input_data: Dictionary containing input data with text
            
        Returns:
            Dictionary with processed sensory data
        """
        # Validate input
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        text = input_data.get("text", "")
        
        if not text:
            logger.warning(f"Sensory processor received empty text input for process {process_id}")
            return {
                "process_id": process_id,
                "status": "error",
                "error": "Empty input text"
            }
            
        # Log the incoming input
        logger.debug(f"Processing sensory input: '{text[:50]}...' (process {process_id})")
        
        # Create response structure
        timestamp = datetime.now()
        result = {
            "process_id": process_id,
            "timestamp": timestamp.isoformat(),
            "text": text,
            "development_level": self.development_level,
            "module_id": self.module_id
        }
        
        # Extract features based on development level
        features = self._extract_features(text)
        
        # Add features to result
        result.update(features)
        
        # Store in recent inputs
        self.recent_inputs.append({
            "text": text,
            "process_id": process_id,
            "timestamp": timestamp,
            "features": features
        })
        
        # Track occurrence frequencies
        self._update_frequencies(text)
        
        # Publish processed result if we have an event bus
        if self.event_bus:
            self.publish_message(
                "sensory_processed",
                {"result": result, "process_id": process_id}
            )
            
        return result
    
    def _extract_features(self, text: str) -> Dict[str, Any]:
        """
        Extract features from text based on developmental level
        
        At lower developmental levels, only basic features are extracted.
        As development progresses, more sophisticated features are extracted.
        
        Args:
            text: Input text to process
            
        Returns:
            Dictionary with extracted features
        """
        features = {}
        
        # Start with empty containers
        basic_features = {}
        intermediate_features = {}
        advanced_features = {}
        
        # Level 0: Basic text properties (always extracted)
        basic_features["length"] = len(text)
        basic_features["has_letters"] = bool(re.search(r'[a-zA-Z]', text))
        basic_features["has_numbers"] = bool(re.search(r'\d', text))
        basic_features["has_special_chars"] = bool(re.search(r'[^\w\s]', text))
        
        # Count frequencies
        letter_count = sum(c.isalpha() for c in text)
        number_count = sum(c.isdigit() for c in text)
        special_char_count = sum(not c.isalnum() and not c.isspace() for c in text)
        
        basic_features["letter_count"] = letter_count
        basic_features["number_count"] = number_count
        basic_features["special_char_count"] = special_char_count
        basic_features["whitespace_count"] = text.count(' ') + text.count('\n') + text.count('\t')
        
        # Calculate character distribution
        if text:
            basic_features["letter_ratio"] = letter_count / len(text)
            basic_features["number_ratio"] = number_count / len(text)
            basic_features["special_ratio"] = special_char_count / len(text)
        else:
            basic_features["letter_ratio"] = 0
            basic_features["number_ratio"] = 0
            basic_features["special_ratio"] = 0
            
        # Add some additional basic features even at level 0
        # This provides better information for pattern recognition
        if text:
            # Simple token count (just splitting by whitespace)
            simple_tokens = text.split()
            basic_features["simple_token_count"] = len(simple_tokens)
            
            # Check for question or exclamation
            basic_features["has_question_mark"] = '?' in text
            basic_features["has_exclamation_mark"] = '!' in text
            
            # Add some character n-gram counts
            char_bigrams = [text[i:i+2] for i in range(len(text)-1)]
            char_bigram_counts = Counter(char_bigrams)
            basic_features["common_char_bigrams"] = dict(char_bigram_counts.most_common(5))
            
            # Create a unique signature for the text
            basic_features["text_signature"] = hash(text) % 10000
        
        # Level 1: Token-based features (dev level >= 0.2)
        # But we'll extract some token features even at level 0 
        # to support better pattern recognition
        
        # Tokenize text using our helper method
        tokens = self._tokenize_text(text)
        
        # Extract token features
        intermediate_features["token_count"] = len(tokens)
        intermediate_features["unique_token_count"] = len(set(tokens))
        
        # Calculate token statistics
        if tokens:
            intermediate_features["avg_token_length"] = np.mean([len(t) for t in tokens])
            intermediate_features["max_token_length"] = max(len(t) for t in tokens)
            intermediate_features["min_token_length"] = min(len(t) for t in tokens)
            intermediate_features["has_long_tokens"] = any(len(t) > 8 for t in tokens)
            
            # Get token frequency distribution
            freq_dist = FreqDist(tokens)
            most_common = freq_dist.most_common(5)
            intermediate_features["most_common_tokens"] = most_common
            intermediate_features["token_diversity"] = len(set(tokens)) / len(tokens) if tokens else 0
            
            # Add basic n-gram features even at low levels
            if len(tokens) >= 2:
                # Generate bigrams
                token_bigrams = list(ngrams(tokens, 2))
                intermediate_features["bigram_count"] = len(token_bigrams)
                
                # Get most common bigrams
                if token_bigrams:
                    bigram_freq = FreqDist(token_bigrams)
                    intermediate_features["common_bigrams"] = bigram_freq.most_common(3)
        else:
            intermediate_features["avg_token_length"] = 0
            intermediate_features["max_token_length"] = 0
            intermediate_features["min_token_length"] = 0
            intermediate_features["has_long_tokens"] = False
            intermediate_features["most_common_tokens"] = []
            intermediate_features["token_diversity"] = 0
            intermediate_features["bigram_count"] = 0
            intermediate_features["common_bigrams"] = []
                
        # Level 2: Linguistic features (dev level >= 0.4)
        if self.development_level >= 0.4:
            # Extract sentences
            sentences = sent_tokenize(text)
            
            # Sentence features
            advanced_features["sentence_count"] = len(sentences)
            
            if sentences:
                advanced_features["avg_sentence_length"] = np.mean([len(s) for s in sentences])
                advanced_features["max_sentence_length"] = max(len(s) for s in sentences)
                
                # Words per sentence
                words_per_sentence = [len(self._tokenize_text(s)) for s in sentences]
                advanced_features["avg_words_per_sentence"] = np.mean(words_per_sentence) if words_per_sentence else 0
            else:
                advanced_features["avg_sentence_length"] = 0
                advanced_features["max_sentence_length"] = 0
                advanced_features["avg_words_per_sentence"] = 0
                
            # Check for question marks and exclamation points
            advanced_features["question_mark_count"] = text.count('?')
            advanced_features["exclamation_mark_count"] = text.count('!')
            advanced_features["is_question"] = text.strip().endswith('?')
            advanced_features["is_exclamation"] = text.strip().endswith('!')
                
        # Level 3: Context-sensitive features (dev level >= 0.6)
        if self.development_level >= 0.6:
            # Check similarity to recent inputs
            if self.recent_inputs:
                similarities = []
                for recent in self.recent_inputs:
                    if recent.get("text") != text:  # Don't compare to self
                        similarity = self._simple_similarity(text, recent.get("text", ""))
                        similarities.append(similarity)
                
                if similarities:
                    advanced_features["max_similarity_to_recent"] = max(similarities)
                    advanced_features["avg_similarity_to_recent"] = np.mean(similarities)
                    advanced_features["is_similar_to_recent"] = max(similarities) > self.similarity_threshold
                else:
                    advanced_features["max_similarity_to_recent"] = 0
                    advanced_features["avg_similarity_to_recent"] = 0
                    advanced_features["is_similar_to_recent"] = False
            
            # Word frequencies compared to historical frequencies
            tokens = self._tokenize_text(text)
            if tokens:
                # Check for unusual words (not in frequent tokens)
                unusual_tokens = [t for t in tokens if self.token_frequencies[t] < 3 and len(t) > 3]
                advanced_features["unusual_token_count"] = len(unusual_tokens)
                advanced_features["unusual_tokens"] = unusual_tokens[:5]  # Limit to 5
                
                # Calculate frequency novelty (how different from typical frequency)
                if self.token_frequencies:
                    token_freqs = {t: self.token_frequencies[t] for t in tokens}
                    if token_freqs:
                        avg_freq = np.mean(list(token_freqs.values()))
                        advanced_features["frequency_novelty"] = 1.0 - min(avg_freq / 10, 1.0)
                    else:
                        advanced_features["frequency_novelty"] = 1.0
                else:
                    advanced_features["frequency_novelty"] = 0.5  # Default mid value
            
        # Level 4: Advanced analysis (dev level >= 0.8)
        if self.development_level >= 0.8:
            # More sophisticated linguistic analysis would go here
            # This could include sentiment analysis, topic detection, etc.
            
            # For now, we'll add some simple additional features
            tokens = self._tokenize_text(text)
            
            # Check for specific linguistic features
            linguistic_features = {}
            
            # Question words
            question_words = {"what", "who", "when", "where", "why", "how"}
            linguistic_features["has_question_words"] = any(t.lower() in question_words for t in tokens)
            
            # Check for imperative sentences (commands)
            # Simple approach: starts with verb
            if tokens and tokens[0].lower() in {"do", "go", "be", "try", "make", "take", "get", "come", "give", "find", "look", "run", "turn", "put", "bring"}:
                linguistic_features["likely_imperative"] = True
            else:
                linguistic_features["likely_imperative"] = False
                
            # Check for named entities (very simple approach)
            # Look for capitalized words not at the start of sentences
            sentence_starts = {s.split()[0] if s.split() else "" for s in sent_tokenize(text)}
            capitalized_non_starters = [t for t in tokens if t[0].isupper() and t not in sentence_starts]
            linguistic_features["potential_named_entities"] = capitalized_non_starters[:5]
            
            # Add to advanced features
            advanced_features["linguistic_features"] = linguistic_features
        
        # Assemble features based on development level
        features["basic_features"] = basic_features
        
        if self.development_level >= 0.2:
            features["features"] = intermediate_features
            
        if self.development_level >= 0.4:
            features["linguistic_features"] = advanced_features
        
        return features
    
    def _tokenize_text(self, text: str) -> List[str]:
        """
        Tokenize text into words, handling various cases based on development level
        
        Args:
            text: Input text to tokenize
            
        Returns:
            List of tokens
        """
        if not text:
            return []
            
        # Basic tokenization - always use NLTK for better tokenization
        tokens = word_tokenize(text.lower())
        
        # Filter based on development level
        if self.development_level < 0.3:
            # Basic level - keep most tokens but remove punctuation-only tokens
            filtered_tokens = []
            for token in tokens:
                # Keep tokens that have at least one alphanumeric character
                if any(c.isalnum() for c in token):
                    filtered_tokens.append(token)
            return filtered_tokens[:self.max_tokens]
            
        elif self.development_level < 0.6:
            # Intermediate: filter out stopwords and very short tokens
            filtered_tokens = [t for t in tokens if (t not in self.stopwords or t in {'?', '!'}) and len(t) > 1]
            return filtered_tokens[:self.max_tokens]
            
        else:
            # Advanced: keep more structure and context
            # Just remove punctuation-only tokens except for meaningful punctuation
            important_punct = {'?', '!', '.'}
            filtered_tokens = [t for t in tokens if any(c.isalnum() for c in t) or t in important_punct]
            return filtered_tokens[:self.max_tokens]
    
    def _simple_similarity(self, text1: str, text2: str) -> float:
        """
        Calculate similarity between two texts
        
        Args:
            text1: First text
            text2: Second text
            
        Returns:
            Similarity score (0-1)
        """
        if not text1 or not text2:
            return 0.0
            
        # Convert to sets of tokens
        tokens1 = set(self._tokenize_text(text1))
        tokens2 = set(self._tokenize_text(text2))
        
        if not tokens1 or not tokens2:
            return 0.0
            
        # Calculate Jaccard similarity
        intersection = tokens1.intersection(tokens2)
        union = tokens1.union(tokens2)
        
        return len(intersection) / len(union)
    
    def _update_frequencies(self, text: str):
        """
        Update the frequency counters with new text
        
        Args:
            text: Text to process for frequency updates
        """
        if not text:
            return
            
        # Update character frequencies
        self.character_frequencies.update(text)
        
        # Update token frequencies
        tokens = self._tokenize_text(text)
        self.token_frequencies.update(tokens)
        
        # Update bigram frequencies if enough tokens
        if len(tokens) >= 2:
            bigrams = ngrams(tokens, 2)
            self.bigram_frequencies.update(bigrams)
    
    def _handle_message(self, message: Message):
        """
        Handle incoming messages from the event bus
        
        Args:
            message: The message to handle
        """
        if message.message_type == "raw_text_input" and message.content:
            # Process the raw text input
            text = message.content.get("text", "")
            if text:
                process_id = message.content.get("process_id", str(uuid.uuid4()))
                self.process_input({
                    "text": text,
                    "process_id": process_id,
                    "source": message.content.get("source", "unknown"),
                    "metadata": message.content.get("metadata", {})
                })
    
    def update_development(self, amount: float) -> float:
        """
        Update the module's developmental level
        
        As development progresses, the sensory processing becomes more sophisticated
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New developmental level
        """
        # Update base development level
        new_level = super().update_development(amount)
        
        # Update processing parameters based on development
        self.max_tokens = int(100 + new_level * 400)  # 100 to 500 tokens - higher minimum for better feature extraction
        self.token_threshold = max(0.03, 0.15 - new_level * 0.12)  # Lower threshold with development
        self.similarity_threshold = max(0.4, 0.7 - new_level * 0.3)  # More nuanced similarity detection
        
        # Log development progress
        logger.info(f"Sensory processor {self.module_id} developmental level updated to {new_level:.2f}")
        logger.debug(f"Updated parameters: max_tokens={self.max_tokens}, token_threshold={self.token_threshold:.2f}")
        
        return new_level
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the module
        
        Returns:
            Dictionary containing module state
        """
        base_state = super().get_state()
        
        # Add sensory processor specific state
        state = {
            **base_state,
            "recent_input_count": len(self.recent_inputs),
            "token_frequency_count": len(self.token_frequencies),
            "processing_parameters": {
                "max_tokens": self.max_tokens,
                "token_threshold": self.token_threshold,
                "similarity_threshold": self.similarity_threshold
            }
        }
        
        return state
    
    def get_recent_inputs(self, count: int = 5) -> List[Dict[str, Any]]:
        """
        Get the most recent inputs
        
        Args:
            count: Maximum number of inputs to return
            
        Returns:
            List of recent inputs
        """
        # Return the most recent inputs, limited by count
        return list(self.recent_inputs)[-count:]
    
    def get_token_frequencies(self, top_n: int = 20) -> List[Tuple[str, int]]:
        """
        Get the most frequent tokens
        
        Args:
            top_n: Number of top tokens to return
            
        Returns:
            List of (token, frequency) tuples
        """
        return self.token_frequencies.most_common(top_n)
        
    def clear_history(self):
        """Clear the input history and frequency counters"""
        self.recent_inputs.clear()
        self.token_frequencies.clear()
        self.bigram_frequencies.clear()
        self.character_frequencies.clear() 


#######################

#perception\__init__.py#
#######################

"""
Perception Module

This module integrates various components for processing and understanding
sensory input. It serves as the primary interface between the Mind and
external stimuli, converting text input into meaningful patterns and
features for higher cognitive processing.

For this LMM implementation, perception is text-based, as the system does
not have physical sensory organs like eyes or ears.
"""

from typing import Optional, Dict, Any, List
import numpy as np
from dataclasses import dataclass, field
import logging
import uuid
import torch
from datetime import datetime

from lmm_project.core.event_bus import EventBus
from lmm_project.modules.base_module import BaseModule
from lmm_project.core.message import Message
from lmm_project.modules.perception.sensory_input import SensoryInputProcessor
from lmm_project.modules.perception.pattern_recognition import PatternRecognizer
from lmm_project.modules.perception.models import PerceptionResult, Pattern, SensoryInput, PerceptionParameters

logger = logging.getLogger(__name__)

def get_module(
    module_id: str = "perception",
    event_bus: Optional[EventBus] = None,
    development_level: float = 0.0
) -> "PerceptionSystem":
    """
    Factory function to create and return a perception module
    
    This function initializes and returns a complete perception system,
    with sensory input processing and pattern recognition capabilities.
    
    Args:
        module_id: Unique identifier for the module
        event_bus: Event bus for communication
        development_level: Initial developmental level for the system
        
    Returns:
        Initialized PerceptionSystem
    """
    return PerceptionSystem(
        module_id=module_id,
        event_bus=event_bus,
        development_level=development_level
    )

class PerceptionSystem(BaseModule):
    """
    Integrated perception system that processes sensory input and recognizes patterns
    
    The perception system develops progressively from basic sensory processing
    to sophisticated pattern detection and interpretation capabilities.
    """
    # Development milestones
    development_milestones = {
        0.0: "Basic sensory awareness",
        0.2: "Simple pattern recognition",
        0.4: "Feature integration",
        0.6: "Context-sensitive perception",
        0.8: "Advanced pattern recognition",
        1.0: "Sophisticated perception"
    }
    
    def __init__(
        self,
        module_id: str,
        event_bus: Optional[EventBus] = None,
        development_level: float = 0.0
    ):
        """
        Initialize the perception system
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication
            development_level: Initial developmental level
        """
        super().__init__(
            module_id=module_id,
            module_type="perception_system",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Create sensory input processor
        self.sensory_processor = SensoryInputProcessor(
            module_id=f"{module_id}_sensory",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Create pattern recognizer
        self.pattern_recognizer = PatternRecognizer(
            module_id=f"{module_id}_patterns",
            event_bus=event_bus,
            development_level=development_level
        )
        
        # Configuration parameters - adjusted for better pattern detection at all levels
        self.parameters = PerceptionParameters(
            token_sensitivity=0.8,        # Increased for better token detection
            ngram_sensitivity=0.7,        # Increased for better n-gram detection
            semantic_sensitivity=0.5,     # Increased for better semantic pattern detection
            novelty_threshold=0.4,        # Decreased to accept more patterns as novel
            pattern_activation_threshold=0.1,  # Significantly lowered to detect more patterns
            developmental_scaling=True
        )
        
        # Apply parameters to submodules
        self._apply_parameters()
        
        # Set developmental levels for submodules
        self._update_submodule_development()
        
        # Subscribe to relevant events
        if self.event_bus:
            self.subscribe_to_message("raw_text_input")
            self.subscribe_to_message("perception_query")
            
        # Try to use GPU if available
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"Perception system initialized with device: {self.device}")
    
    def _apply_parameters(self):
        """Apply configuration parameters to submodules"""
        # Apply to pattern recognizer
        if hasattr(self.pattern_recognizer, 'token_sensitivity'):
            self.pattern_recognizer.token_sensitivity = self.parameters.token_sensitivity
        
        if hasattr(self.pattern_recognizer, 'ngram_sensitivity'):
            self.pattern_recognizer.ngram_sensitivity = self.parameters.ngram_sensitivity
            
        if hasattr(self.pattern_recognizer, 'semantic_sensitivity'):
            self.pattern_recognizer.semantic_sensitivity = self.parameters.semantic_sensitivity
            
        if hasattr(self.pattern_recognizer, 'novelty_threshold'):
            self.pattern_recognizer.novelty_threshold = self.parameters.novelty_threshold
            
        if hasattr(self.pattern_recognizer, 'pattern_activation_threshold'):
            self.pattern_recognizer.pattern_activation_threshold = self.parameters.pattern_activation_threshold
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process sensory input through the full perception pipeline
        
        Args:
            input_data: Raw sensory input data
            
        Returns:
            Dictionary with processed perception results
        """
        # Generate ID for this processing operation
        process_id = input_data.get("process_id", str(uuid.uuid4()))
        
        # Log the incoming input processing
        text = input_data.get("text", "")
        if text:
            logger.info(f"Perception processing: '{text[:50]}...' (process {process_id})")
            logger.info(f"Input data before sensory processing: {list(input_data.keys())}")
        
        # Process through sensory input processor
        sensory_result = self.sensory_processor.process_input(input_data)
        
        # Debug logging to see what's happening with the data
        logger.info(f"Sensory result keys: {list(sensory_result.keys())}")
        
        # Ensure the text field is included in the data passed to the pattern recognizer
        # This fixes the issue where text wasn't being passed correctly
        if "text" not in sensory_result and text:
            logger.info(f"Adding missing text field to sensory_result")
            sensory_result["text"] = text
        
        # Double-check text field is set
        logger.info(f"Text field present before pattern recognition: {'text' in sensory_result}")
        if "text" in sensory_result:
            logger.info(f"Text value length: {len(sensory_result['text'])}")
        
        # Process through pattern recognizer
        pattern_result = self.pattern_recognizer.process_input(sensory_result)
        
        # Integrate results based on developmental level
        result = self._integrate_results(process_id, sensory_result, pattern_result)
        
        # Publish integrated results
        if self.event_bus:
            self.publish_message(
                "perception_result",
                {"result": result, "process_id": process_id}
            )
            
        return result
    
    def _integrate_results(
        self, 
        process_id: str,
        sensory_result: Dict[str, Any],
        pattern_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Integrate results from sensory processing and pattern recognition
        
        The integration becomes more sophisticated with development
        
        Args:
            process_id: ID of the processing operation
            sensory_result: Results from sensory processing
            pattern_result: Results from pattern recognition
            
        Returns:
            Integrated perception result
        """
        # Basic integrated result
        result = {
            "process_id": process_id,
            "timestamp": datetime.now().isoformat(),
            "development_level": self.development_level,
            "module_id": self.module_id,
            "text": sensory_result.get("text", ""),
            "patterns": pattern_result.get("patterns", [])
        }
        
        # Add more detailed integration based on development level
        if self.development_level < 0.3:
            # Basic integration - just sensory features and patterns
            result["basic_features"] = sensory_result.get("basic_features", {})
            
        elif self.development_level < 0.6:
            # More integrated result with features
            result["basic_features"] = sensory_result.get("basic_features", {})
            result["features"] = sensory_result.get("features", {})
            result["recognized_pattern_types"] = list(set(
                p.get("pattern_type", "") for p in pattern_result.get("patterns", [])
            ))
            
        else:
            # Sophisticated integration with context and interpretation
            result["basic_features"] = sensory_result.get("basic_features", {})
            result["features"] = sensory_result.get("features", {})
            result["linguistic_features"] = sensory_result.get("linguistic_features", {})
            result["recognized_pattern_types"] = list(set(
                p.get("pattern_type", "") for p in pattern_result.get("patterns", [])
            ))
            
            # Add pattern interpretation
            result["interpretation"] = self._interpret_patterns(
                pattern_result.get("patterns", []),
                sensory_result
            )
            
        return result
    
    def _interpret_patterns(
        self, 
        patterns: List[Dict[str, Any]],
        sensory_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Interpret the meaning of recognized patterns
        
        This is a higher-level function that becomes more sophisticated
        with development, providing meaningful interpretation of patterns.
        
        Args:
            patterns: List of recognized patterns
            sensory_result: Results from sensory processing
            
        Returns:
            Dictionary with interpretation of patterns
        """
        # Start with basic interpretation
        interpretation = {
            "primary_pattern": None,
            "content_type": "unknown",
            "complexity": "simple"
        }
        
        if not patterns:
            return interpretation
            
        # Count pattern types
        pattern_types = {}
        for pattern in patterns:
            pattern_type = pattern.get("pattern_type", "unknown")
            if pattern_type not in pattern_types:
                pattern_types[pattern_type] = 0
            pattern_types[pattern_type] += 1
            
        # Find the most common pattern type
        primary_pattern_type = max(pattern_types.items(), key=lambda x: x[1])[0] if pattern_types else "unknown"
        interpretation["primary_pattern_type"] = primary_pattern_type
        
        # Find the highest confidence pattern
        highest_confidence_pattern = max(patterns, key=lambda p: p.get("confidence", 0))
        interpretation["primary_pattern"] = highest_confidence_pattern.get("pattern_id")
        
        # Determine complexity based on pattern count and types
        if len(patterns) > 10 and len(pattern_types) > 3:
            interpretation["complexity"] = "complex"
        elif len(patterns) > 5:
            interpretation["complexity"] = "moderate"
        else:
            interpretation["complexity"] = "simple"
            
        # Determine content type based on patterns
        if any(p.get("pattern_type") == "syntactic" and p.get("attributes", {}).get("pattern_type") == "question" for p in patterns):
            interpretation["content_type"] = "question"
        elif any(p.get("pattern_type") == "syntactic" and p.get("attributes", {}).get("pattern_type") == "exclamation" for p in patterns):
            interpretation["content_type"] = "exclamation"
        elif any(p.get("pattern_type") == "contextual" and p.get("attributes", {}).get("pattern_type") == "answer_to_question" for p in patterns):
            interpretation["content_type"] = "answer"
        elif any(p.get("pattern_type") == "neural" and p.get("confidence", 0) > 0.8 for p in patterns):
            interpretation["content_type"] = "familiar"
        elif any(p.get("novelty", 0) > 0.7 for p in patterns):
            interpretation["content_type"] = "novel"
        elif "?" in sensory_result.get("text", ""):
            interpretation["content_type"] = "question"
        elif "!" in sensory_result.get("text", ""):
            interpretation["content_type"] = "exclamation"
        elif len(sensory_result.get("text", "").split()) < 5:
            interpretation["content_type"] = "brief_statement"
        else:
            interpretation["content_type"] = "statement"
            
        # Calculate average novelty
        novelties = [p.get("novelty", 0.5) for p in patterns]
        interpretation["novelty_level"] = sum(novelties) / len(novelties) if novelties else 0.5
        
        # Add text properties from sensory processing
        features = sensory_result.get("features", {})
        if self.development_level >= 0.7 and features:
            # Extract interesting features
            interesting_features = {}
            
            # Token diversity
            if "token_diversity" in features:
                interesting_features["token_diversity"] = features["token_diversity"]
                
            # Unusual tokens
            if "linguistic_features" in sensory_result:
                ling_features = sensory_result["linguistic_features"]
                if "unusual_tokens" in ling_features:
                    interesting_features["unusual_words"] = ling_features["unusual_tokens"]
                    
            # Add to interpretation if we found interesting features
            if interesting_features:
                interpretation["text_features"] = interesting_features
                
        return interpretation
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of the perception system
        
        This updates both the system's overall development level and the
        development levels of the subsystems (sensory processor and pattern recognizer)
        
        Args:
            amount: Amount to increase development by
            
        Returns:
            New developmental level
        """
        # Update base development level
        new_level = super().update_development(amount)
        
        # Update submodule development levels
        self._update_submodule_development()
        
        # Adjust parameters based on development level
        if self.parameters.developmental_scaling:
            # Make the system increasingly sensitive as it develops
            self.parameters.token_sensitivity = min(0.9, 0.6 + new_level * 0.3)
            self.parameters.ngram_sensitivity = min(0.85, 0.5 + new_level * 0.35)
            self.parameters.semantic_sensitivity = min(0.8, 0.4 + new_level * 0.4)
            
            # Make threshold lower as system develops
            self.parameters.pattern_activation_threshold = max(0.05, 0.15 - new_level * 0.1)
            
            # Apply updated parameters
            self._apply_parameters()
        
        return new_level
    
    def _update_submodule_development(self):
        """Update the developmental level of submodules"""
        self.sensory_processor.update_development(self.development_level - self.sensory_processor.development_level)
        self.pattern_recognizer.update_development(self.development_level - self.pattern_recognizer.development_level)
    
    def _handle_message(self, message: Message):
        """
        Handle incoming messages
        
        Args:
            message: The message to handle
        """
        if message.message_type == "raw_text_input":
            # Process the raw text input
            if message.content:
                self.process_input(message.content)
        elif message.message_type == "perception_query":
            # Handle queries about perception
            self._handle_perception_query(message)
    
    def _handle_perception_query(self, message: Message):
        """
        Handle perception queries
        
        Args:
            message: Query message
        """
        if not message.content:
            logger.warning("Received empty perception query")
            return
            
        query_type = message.content.get("query_type", "")
        response = {"query_id": message.content.get("query_id", ""), "result": None}
        
        if query_type == "recent_patterns":
            # Return recent patterns
            count = message.content.get("count", 5)
            response["result"] = self.pattern_recognizer.get_recent_patterns(count)
            
        elif query_type == "recent_inputs":
            # Return recent inputs
            count = message.content.get("count", 5)
            response["result"] = self.sensory_processor.get_recent_inputs(count)
            
        elif query_type == "perception_state":
            # Return the current state of perception
            response["result"] = {
                "system_state": self.get_state(),
                "sensory_state": self.sensory_processor.get_state(),
                "pattern_state": self.pattern_recognizer.get_state()
            }
            
        elif query_type == "process_text":
            # Process a specific text
            text = message.content.get("text", "")
            if text:
                response["result"] = self.process_input({"text": text, "process_id": str(uuid.uuid4())})
                
        else:
            response["error"] = f"Unknown query type: {query_type}"
            
        # Publish the response
        if self.event_bus:
            self.publish_message(
                "perception_query_response",
                response
            )
    
    def get_state(self) -> Dict[str, Any]:
        """
        Get the current state of the perception system
        
        Returns:
            Dictionary containing module state
        """
        # Get base state
        base_state = super().get_state()
        
        # Add perception-specific state
        state = {
            **base_state,
            "parameters": self.parameters.model_dump() if hasattr(self.parameters, "model_dump") else vars(self.parameters),
            "submodules": {
                "sensory_processor": {
                    "id": self.sensory_processor.module_id,
                    "development_level": self.sensory_processor.development_level
                },
                "pattern_recognizer": {
                    "id": self.pattern_recognizer.module_id,
                    "development_level": self.pattern_recognizer.development_level
                }
            },
            "device": str(self.device)
        }
        
        return state 


#######################

#self_regulation\emotional_regulation.py#
#######################

# TODO: Implement the EmotionalRegulation class to manage emotional responses
# This component should be able to:
# - Detect emotional states that require regulation
# - Select appropriate regulation strategies
# - Implement regulation processes to modify emotions
# - Adapt regulation approaches based on context and goals

# TODO: Implement developmental progression in emotional regulation:
# - Minimal regulation with external support in early stages
# - Simple self-soothing strategies in childhood
# - Expanding regulation repertoire in adolescence
# - Sophisticated, context-appropriate regulation in adulthood

# TODO: Create mechanisms for:
# - Emotion monitoring: Detect emotions requiring regulation
# - Strategy selection: Choose appropriate regulation approach
# - Implementation: Apply selected regulation strategy
# - Effectiveness assessment: Evaluate regulation success

# TODO: Implement different regulation strategies:
# - Cognitive reappraisal: Reinterpreting emotional situations
# - Attention deployment: Shifting focus away from triggers
# - Response modulation: Changing behavioral responses
# - Situation selection/modification: Avoiding or changing contexts

# TODO: Connect to emotion and consciousness modules
# Emotional regulation should modify emotional responses
# and draw on conscious awareness of emotional states

from typing import Dict, List, Any, Optional
from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus

class EmotionalRegulation(BaseModule):
    """
    Manages emotional responses
    
    This module monitors emotional states, selects and implements
    regulation strategies, and adjusts emotional responses to be
    appropriate to goals and context.
    """
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the emotional regulation module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="emotional_regulation", event_bus=event_bus)
        
        # TODO: Initialize emotion monitoring system
        # TODO: Set up regulation strategy repository
        # TODO: Create strategy selection mechanisms
        # TODO: Initialize effectiveness tracking
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to regulate emotional responses
        
        Args:
            input_data: Dictionary containing emotion-related information
            
        Returns:
            Dictionary with the regulated emotional state
        """
        # TODO: Implement emotion regulation logic
        # TODO: Detect emotions requiring regulation
        # TODO: Select appropriate regulation strategies
        # TODO: Apply regulation processes
        
        return {
            "status": "not_implemented",
            "module_id": self.module_id,
            "module_type": self.module_type
        }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # TODO: Implement development progression for emotional regulation
        # TODO: Expand regulation strategy repertoire with development
        # TODO: Enhance strategy selection with development
        
        return super().update_development(amount)


#######################

#self_regulation\impulse_control.py#
#######################

# TODO: Implement the ImpulseControl class to inhibit inappropriate impulses
# This component should be able to:
# - Detect impulses requiring inhibition
# - Delay immediate responses when appropriate
# - Redirect action tendencies toward appropriate alternatives
# - Regulate behavior to align with goals and values

# TODO: Implement developmental progression in impulse control:
# - Minimal impulse control in early stages
# - Growing ability to delay gratification in childhood
# - Increased self-restraint in adolescence
# - Sophisticated impulse regulation in adulthood

# TODO: Create mechanisms for:
# - Impulse detection: Identify action tendencies requiring control
# - Response inhibition: Suppress inappropriate impulses
# - Delay capacity: Wait for appropriate timing
# - Alternative generation: Redirect energy to better options

# TODO: Implement different control strategies:
# - Proactive inhibition: Prepare to suppress responses before triggers
# - Reactive inhibition: Suppress responses after triggers
# - Attentional control: Direct attention away from temptations
# - Implementation intentions: Plan specific responses to challenges

# TODO: Connect to executive function and consciousness modules
# Impulse control should utilize executive inhibition
# and be informed by conscious goals and priorities

from typing import Dict, List, Any, Optional
from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus

class ImpulseControl(BaseModule):
    """
    Inhibits inappropriate impulses
    
    This module detects impulses requiring regulation,
    suppresses inappropriate action tendencies, and
    redirects behavior toward goal-aligned alternatives.
    """
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the impulse control module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="impulse_control", event_bus=event_bus)
        
        # TODO: Initialize impulse detection system
        # TODO: Set up inhibition mechanisms
        # TODO: Create delay capability
        # TODO: Initialize alternative response generation
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to control impulses
        
        Args:
            input_data: Dictionary containing impulse-related information
            
        Returns:
            Dictionary with the regulated response
        """
        # TODO: Implement impulse control logic
        # TODO: Detect impulses requiring inhibition
        # TODO: Apply appropriate control strategies
        # TODO: Generate alternative responses
        
        return {
            "status": "not_implemented",
            "module_id": self.module_id,
            "module_type": self.module_type
        }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # TODO: Implement development progression for impulse control
        # TODO: Increase inhibition capacity with development
        # TODO: Enhance delay capacity with development
        
        return super().update_development(amount)


#######################

#self_regulation\models.py#
#######################

from pydantic import BaseModel, Field 


#######################

#self_regulation\neural_net.py#
#######################

import torch 


#######################

#self_regulation\self_monitoring.py#
#######################

# TODO: Implement the SelfMonitoring class to track internal states and behaviors
# This component should be able to:
# - Monitor internal states (emotions, thoughts, goals)
# - Track behavioral responses and their outcomes
# - Detect discrepancies between goals and current states
# - Provide feedback for regulatory processes

# TODO: Implement developmental progression in self-monitoring:
# - Basic state awareness in early stages
# - Growing behavior tracking in childhood
# - Increased metacognitive monitoring in adolescence
# - Sophisticated self-awareness in adulthood

# TODO: Create mechanisms for:
# - State detection: Identify current internal conditions
# - Discrepancy detection: Notice gaps between goals and reality
# - Progress tracking: Monitor advancement toward goals
# - Error detection: Identify mistakes and suboptimal responses

# TODO: Implement different monitoring types:
# - Emotional monitoring: Track affective states
# - Cognitive monitoring: Observe thoughts and beliefs
# - Behavioral monitoring: Track actions and responses
# - Social monitoring: Observe interpersonal impacts

# TODO: Connect to consciousness and identity modules
# Self-monitoring should utilize conscious awareness
# and contribute to self-concept development

from typing import Dict, List, Any, Optional
from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus

class SelfMonitoring(BaseModule):
    """
    Tracks internal states and behaviors
    
    This module monitors emotions, thoughts, behaviors,
    and their outcomes, detecting discrepancies between
    goals and current states to guide self-regulation.
    """
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the self-monitoring module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="self_monitoring", event_bus=event_bus)
        
        # TODO: Initialize state tracking mechanisms
        # TODO: Set up discrepancy detection
        # TODO: Create progress monitoring system
        # TODO: Initialize error detection capability
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to monitor internal states and behaviors
        
        Args:
            input_data: Dictionary containing state and behavior information
            
        Returns:
            Dictionary with monitoring results
        """
        # TODO: Implement monitoring logic
        # TODO: Track current internal states
        # TODO: Detect discrepancies with goals
        # TODO: Identify errors and suboptimal patterns
        
        return {
            "status": "not_implemented",
            "module_id": self.module_id,
            "module_type": self.module_type
        }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # TODO: Implement development progression for self-monitoring
        # TODO: Expand monitoring capacity with development
        # TODO: Enhance metacognitive awareness with development
        
        return super().update_development(amount)


#######################

#self_regulation\__init__.py#
#######################

# Self-regulation module

# TODO: Implement the self-regulation module factory function to return an integrated SelfRegulationSystem
# This module should be responsible for emotional regulation, impulse control,
# self-monitoring, and adaptive response adjustment.

# TODO: Create SelfRegulationSystem class that integrates all self-regulation sub-components:
# - emotional_regulation: manages emotional responses
# - impulse_control: inhibits inappropriate impulses
# - self_monitoring: tracks internal states and behaviors
# - adaptive_adjustment: modifies responses based on feedback

# TODO: Implement development tracking for self-regulation
# Self-regulation capabilities should develop from minimal regulation in early stages
# to sophisticated, flexible self-control in later stages

# TODO: Connect self-regulation module to emotion, executive, and consciousness modules
# Self-regulation should modify emotional responses, employ executive
# control mechanisms, and draw on conscious awareness

# TODO: Implement metacognitive aspects of self-regulation
# Include monitoring of regulation processes, strategic selection
# of regulation approaches, and regulation failure detection

from typing import Dict, List, Any, Optional
from lmm_project.core.event_bus import EventBus

def get_module(module_id: str, event_bus: Optional[EventBus] = None) -> Any:
    """
    Factory function to create a self-regulation module
    
    This function is responsible for creating a self-regulation system that can:
    - Regulate emotional responses
    - Control impulses and delay gratification
    - Monitor internal states and external behaviors
    - Adapt responses based on context and goals
    - Adjust regulatory strategies based on feedback
    
    Args:
        module_id: Unique identifier for the module
        event_bus: Event bus for communication with other modules
        
    Returns:
        An instance of the SelfRegulationSystem class
    """
    # TODO: Return an instance of the SelfRegulationSystem class
    # that integrates all self-regulation sub-components
    raise NotImplementedError("Self-regulation module not yet implemented") 


#######################

#social\models.py#
#######################

from pydantic import BaseModel, Field 


#######################

#social\moral_reasoning.py#
#######################

# TODO: Implement the MoralReasoning class to make ethical judgments
# This component should be able to:
# - Evaluate actions based on ethical principles
# - Reason about moral dilemmas
# - Apply different ethical frameworks to situations
# - Develop and refine moral intuitions

# TODO: Implement developmental progression in moral reasoning:
# - Simple reward/punishment orientation in early stages
# - Rule-based morality in childhood
# - Social contract perspective in adolescence
# - Principled moral reasoning in adulthood

# TODO: Create mechanisms for:
# - Harm detection: Identify potential harmful consequences
# - Value application: Apply ethical values to situations
# - Moral conflict resolution: Balance competing ethical concerns
# - Ethical judgment: Form moral evaluations of actions

# TODO: Implement different moral reasoning approaches:
# - Consequentialist reasoning: Based on outcomes
# - Deontological reasoning: Based on rules and duties
# - Virtue ethics: Based on character and virtues
# - Care ethics: Based on relationships and care

# TODO: Connect to emotion and social norm modules
# Moral reasoning should be informed by emotional responses
# and interact with social norm understanding

from typing import Dict, List, Any, Optional
from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus

class MoralReasoning(BaseModule):
    """
    Makes ethical judgments
    
    This module evaluates actions based on ethical principles,
    reasons about moral dilemmas, applies different ethical
    frameworks, and develops moral intuitions.
    """
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the moral reasoning module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="moral_reasoning", event_bus=event_bus)
        
        # TODO: Initialize moral principle representations
        # TODO: Set up ethical framework mechanisms
        # TODO: Create value conflict resolution capabilities
        # TODO: Initialize moral intuition systems
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to make moral judgments
        
        Args:
            input_data: Dictionary containing situation for moral evaluation
            
        Returns:
            Dictionary with moral judgments and reasoning
        """
        # TODO: Implement moral reasoning logic
        # TODO: Identify relevant moral principles
        # TODO: Apply appropriate ethical frameworks
        # TODO: Generate moral judgments with justifications
        
        return {
            "status": "not_implemented",
            "module_id": self.module_id,
            "module_type": self.module_type
        }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # TODO: Implement development progression for moral reasoning
        # TODO: Increase moral reasoning complexity with development
        # TODO: Enhance principle application with development
        
        return super().update_development(amount)


#######################

#social\neural_net.py#
#######################

import torch 


#######################

#social\relationship_models.py#
#######################

# TODO: Implement the RelationshipModels class to represent social relationships
# This component should be able to:
# - Model different types of relationships
# - Track relationship history and qualities
# - Update relationships based on interactions
# - Adapt behavior according to relationship context

# TODO: Implement developmental progression in relationship modeling:
# - Simple attachment relationships in early stages
# - Concrete friendship models in childhood
# - Complex peer and group relationships in adolescence
# - Sophisticated relationship dynamics in adulthood

# TODO: Create mechanisms for:
# - Relationship formation: Establish new social connections
# - Quality assessment: Evaluate relationship attributes
# - History tracking: Maintain interaction records
# - Expectation modeling: Predict behavior based on relationship type

# TODO: Implement different relationship types:
# - Attachment relationships: Based on security and care
# - Friendships: Based on reciprocity and shared interests
# - Authority relationships: Based on hierarchy and respect
# - Group affiliations: Based on shared identity and belonging

# TODO: Connect to theory of mind and memory modules
# Relationship models should draw on theory of mind to understand
# others' expectations and store relationship information in memory

from typing import Dict, List, Any, Optional
from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus

class RelationshipModels(BaseModule):
    """
    Represents social relationships
    
    This module models different types of relationships,
    tracks relationship attributes and history, and adapts
    behavior based on relationship context.
    """
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the relationship models module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="relationship_models", event_bus=event_bus)
        
        # TODO: Initialize relationship representation structures
        # TODO: Set up relationship history tracking
        # TODO: Create relationship quality assessment mechanisms
        # TODO: Initialize expectation modeling systems
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to update relationship models
        
        Args:
            input_data: Dictionary containing social interaction information
            
        Returns:
            Dictionary with updated relationship representations
        """
        # TODO: Implement relationship modeling logic
        # TODO: Update relationship representations from interactions
        # TODO: Adjust relationship qualities based on events
        # TODO: Generate relationship-appropriate expectations
        
        return {
            "status": "not_implemented",
            "module_id": self.module_id,
            "module_type": self.module_type
        }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # TODO: Implement development progression for relationship models
        # TODO: Increase relationship complexity with development
        # TODO: Enhance relationship stability assessment with development
        
        return super().update_development(amount)


#######################

#social\social_norms.py#
#######################

# TODO: Implement the SocialNorms class to learn and apply social rules
# This component should be able to:
# - Learn implicit and explicit social rules from observation
# - Detect violations of social norms
# - Apply appropriate social conventions in different contexts
# - Update norm understanding based on feedback

# TODO: Implement developmental progression in social norms:
# - Basic rule following in early stages
# - Concrete norm adherence in childhood
# - Understanding norm flexibility in adolescence
# - Complex contextual norm application in adulthood

# TODO: Create mechanisms for:
# - Norm acquisition: Learn rules from observation and instruction
# - Violation detection: Recognize when norms are broken
# - Context recognition: Identify which norms apply in different settings
# - Norm updating: Revise understanding based on experience

# TODO: Implement different norm categories:
# - Etiquette norms: Polite behavior conventions
# - Moral norms: Ethical principles for behavior
# - Conventional norms: Arbitrary cultural standards
# - Descriptive norms: Common behavioral patterns

# TODO: Connect to theory of mind and memory modules
# Social norm understanding should use theory of mind to understand
# others' norm expectations and store norms in semantic memory

from typing import Dict, List, Any, Optional
from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus

class SocialNorms(BaseModule):
    """
    Learns and applies social rules
    
    This module acquires social conventions, detects norm violations,
    applies appropriate rules in different contexts, and updates
    norm understanding based on feedback.
    """
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the social norms module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="social_norms", event_bus=event_bus)
        
        # TODO: Initialize norm representation structures
        # TODO: Set up violation detection mechanisms
        # TODO: Create context recognition capabilities
        # TODO: Initialize norm updating system
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to apply social norms
        
        Args:
            input_data: Dictionary containing social situation information
            
        Returns:
            Dictionary with norm-relevant analyses and responses
        """
        # TODO: Implement social norm logic
        # TODO: Identify relevant norms for the context
        # TODO: Detect potential norm violations
        # TODO: Generate norm-appropriate responses
        
        return {
            "status": "not_implemented",
            "module_id": self.module_id,
            "module_type": self.module_type
        }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # TODO: Implement development progression for social norms
        # TODO: Increase norm flexibility with development
        # TODO: Enhance context sensitivity with development
        
        return super().update_development(amount)


#######################

#social\theory_of_mind.py#
#######################

# TODO: Implement the TheoryOfMind class to understand others' mental states
# This component should be able to:
# - Represent others' beliefs, desires, and intentions
# - Infer mental states from observed behavior
# - Understand false beliefs and different perspectives
# - Track multiple agents' mental models simultaneously

# TODO: Implement developmental progression in theory of mind:
# - Simple agency detection in early stages
# - Understanding desires before beliefs in early childhood
# - First-order belief representation in childhood
# - Higher-order mental state representation in adolescence/adulthood

# TODO: Create mechanisms for:
# - Perspective taking: Simulate others' viewpoints
# - Belief inference: Deduce what others believe
# - Intention recognition: Infer goals from actions
# - Mental state tracking: Monitor changes in others' knowledge

# TODO: Implement different levels of mental state representation:
# - First-order: What others believe
# - Second-order: What others believe about others' beliefs
# - Higher-order: More complex nested mental states
# - Shared mental models: Common ground in interaction

# TODO: Connect to language and memory modules
# Theory of mind should utilize language processing
# and draw on memories of past social interactions

from typing import Dict, List, Any, Optional
from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus

class TheoryOfMind(BaseModule):
    """
    Understands others' mental states
    
    This module represents, infers, and tracks the beliefs,
    desires, intentions, and emotions of other agents,
    enabling the prediction of their behavior.
    """
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the theory of mind module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="theory_of_mind", event_bus=event_bus)
        
        # TODO: Initialize mental state representation structures
        # TODO: Set up inference mechanisms
        # TODO: Create perspective taking capabilities
        # TODO: Initialize agent models
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to understand others' mental states
        
        Args:
            input_data: Dictionary containing social interaction information
            
        Returns:
            Dictionary with inferred mental states
        """
        # TODO: Implement theory of mind logic
        # TODO: Infer beliefs and intentions from behavior
        # TODO: Track mental state changes
        # TODO: Update agent models
        
        return {
            "status": "not_implemented",
            "module_id": self.module_id,
            "module_type": self.module_type
        }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # TODO: Implement development progression for theory of mind
        # TODO: Increase order of mental state representation with development
        # TODO: Enhance perspective taking accuracy with development
        
        return super().update_development(amount)


#######################

#social\__init__.py#
#######################

# Social module 

# TODO: Implement the social module factory function to return an integrated SocialSystem
# This module should be responsible for social cognition, relationship modeling,
# moral reasoning, and understanding social norms.

# TODO: Create SocialSystem class that integrates all social sub-components:
# - theory_of_mind: understanding others' mental states
# - social_norms: learning and applying social rules
# - moral_reasoning: making ethical judgments
# - relationship_models: representing social relationships

# TODO: Implement development tracking for social cognition
# Social capabilities should develop from basic social responsiveness in early stages
# to sophisticated social understanding and nuanced moral reasoning in later stages

# TODO: Connect social module to emotion, language, and memory modules
# Social cognition should be informed by emotional understanding,
# utilize language representations, and draw on social memories

# TODO: Implement perspective-taking capabilities
# Include the ability to represent others' viewpoints, understand
# how situations appear to others, and imagine others' experiences

from typing import Dict, List, Any, Optional
from lmm_project.core.event_bus import EventBus

def get_module(module_id: str, event_bus: Optional[EventBus] = None) -> Any:
    """
    Factory function to create a social module
    
    This function is responsible for creating a social system that can:
    - Understand others' beliefs, intentions, and emotions
    - Learn and apply social norms and conventions
    - Make moral judgments about actions and situations
    - Model relationships and social dynamics
    - Adapt behavior to different social contexts
    
    Args:
        module_id: Unique identifier for the module
        event_bus: Event bus for communication with other modules
        
    Returns:
        An instance of the SocialSystem class
    """
    # TODO: Return an instance of the SocialSystem class
    # that integrates all social sub-components
    raise NotImplementedError("Social module not yet implemented")


#######################

#temporal\causality.py#
#######################

# TODO: Implement the Causality class to understand cause-effect relationships
# This component should be able to:
# - Detect correlations between events across time
# - Infer causal relationships from correlations and interventions
# - Represent causal models of how events affect one another
# - Make predictions and counterfactual inferences using causal models

# TODO: Implement developmental progression in causal understanding:
# - Simple temporal associations in early stages
# - Basic cause-effect connections in childhood
# - Multiple causality understanding in adolescence
# - Complex causal networks and counterfactual reasoning in adulthood

# TODO: Create mechanisms for:
# - Correlation detection: Identify events that co-occur
# - Intervention analysis: Learn from actions and their effects
# - Causal model building: Create structured representations of causes
# - Counterfactual simulation: Imagine alternative causal scenarios

# TODO: Implement different causal reasoning approaches:
# - Associative learning: Pattern-based causal inference
# - Bayesian reasoning: Probabilistic causal models
# - Structural modeling: Graph-based causal representations
# - Mechanism-based reasoning: Understanding causal principles

# TODO: Connect to learning and prediction modules
# Causal understanding should guide learning processes
# and inform predictive models

from typing import Dict, List, Any, Optional
from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus

class Causality(BaseModule):
    """
    Understands cause-effect relationships
    
    This module detects correlations, infers causal connections,
    builds causal models, and enables predictions and
    counterfactual reasoning about events.
    """
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the causality module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="causality", event_bus=event_bus)
        
        # TODO: Initialize correlation detection mechanisms
        # TODO: Set up causal model representations
        # TODO: Create intervention analysis capability
        # TODO: Initialize counterfactual reasoning mechanisms
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to understand causal relationships
        
        Args:
            input_data: Dictionary containing event sequence information
            
        Returns:
            Dictionary with inferred causal relationships
        """
        # TODO: Implement causal reasoning logic
        # TODO: Detect correlations in observed events
        # TODO: Update causal models based on new evidence
        # TODO: Generate causal inferences and predictions
        
        return {
            "status": "not_implemented",
            "module_id": self.module_id,
            "module_type": self.module_type
        }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # TODO: Implement development progression for causal understanding
        # TODO: Increase causal model complexity with development
        # TODO: Enhance counterfactual reasoning with development
        
        return super().update_development(amount)


#######################

#temporal\models.py#
#######################

from pydantic import BaseModel, Field 


#######################

#temporal\neural_net.py#
#######################

import torch 


#######################

#temporal\prediction.py#
#######################

# TODO: Implement the Prediction class to anticipate future states
# This component should be able to:
# - Generate predictions based on current states and patterns
# - Estimate confidence and uncertainty in predictions
# - Update predictive models based on outcomes
# - Adapt prediction timeframes based on context

# TODO: Implement developmental progression in prediction:
# - Simple immediate anticipation in early stages
# - Short-term predictions in childhood
# - Strategic future planning in adolescence
# - Sophisticated probabilistic forecasting in adulthood

# TODO: Create mechanisms for:
# - Pattern extrapolation: Extend observed patterns into the future
# - Confidence estimation: Assess prediction reliability
# - Model updating: Refine predictions based on outcomes
# - Counterfactual prediction: Consider alternative scenarios

# TODO: Implement different prediction types:
# - State prediction: Future values of continuous variables
# - Event prediction: Occurrence of discrete events
# - Sequence prediction: Order of future states or events
# - Agency prediction: Future actions of intelligent agents

# TODO: Connect to memory and causality modules
# Prediction should utilize historical patterns from memory
# and causal models to generate accurate forecasts

from typing import Dict, List, Any, Optional
from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus

class Prediction(BaseModule):
    """
    Anticipates future states
    
    This module generates predictions based on current states and patterns,
    estimates confidence in forecasts, and adapts predictive models
    based on observed outcomes.
    """
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the prediction module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="prediction", event_bus=event_bus)
        
        # TODO: Initialize prediction model structures
        # TODO: Set up confidence estimation mechanisms
        # TODO: Create model updating capabilities
        # TODO: Initialize counterfactual generation systems
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to generate predictions
        
        Args:
            input_data: Dictionary containing current states and patterns
            
        Returns:
            Dictionary with predictions and confidence estimates
        """
        # TODO: Implement prediction logic
        # TODO: Generate forecasts based on current state
        # TODO: Estimate confidence in predictions
        # TODO: Create alternative scenario predictions
        
        return {
            "status": "not_implemented",
            "module_id": self.module_id,
            "module_type": self.module_type
        }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # TODO: Implement development progression for prediction
        # TODO: Increase prediction horizon with development
        # TODO: Enhance probabilistic reasoning with development
        
        return super().update_development(amount)    

#######################

#temporal\sequence_learning.py#
#######################

# TODO: Implement the SequenceLearning class to learn patterns over time
# This component should be able to:
# - Detect recurring patterns in temporal sequences
# - Learn sequential statistical regularities
# - Recognize variations of learned sequences
# - Predict upcoming elements in sequences

# TODO: Implement developmental progression in sequence learning:
# - Simple repetition detection in early stages
# - Short sequence learning in childhood
# - Hierarchical sequence structures in adolescence
# - Complex, multi-level sequential patterns in adulthood

# TODO: Create mechanisms for:
# - Pattern detection: Identify recurring temporal patterns
# - Statistical learning: Extract probabilistic sequence rules
# - Sequence abstraction: Recognize underlying patterns despite variations
# - Hierarchical organization: Structure sequences into meaningful units

# TODO: Implement different sequence types:
# - Action sequences: Ordered behavioral patterns
# - Perceptual sequences: Ordered sensory patterns
# - Conceptual sequences: Ordered abstract elements
# - Social sequences: Ordered interaction patterns

# TODO: Connect to memory and prediction modules
# Sequence learning should store patterns in memory
# and feed into predictive processes

from typing import Dict, List, Any, Optional
from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus

class SequenceLearning(BaseModule):
    """
    Learns patterns over time
    
    This module detects, learns, and organizes temporal
    sequences, enabling the recognition of recurring
    patterns and prediction of future elements.
    """
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the sequence learning module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="sequence_learning", event_bus=event_bus)
        
        # TODO: Initialize sequence representation structures
        # TODO: Set up pattern detection mechanisms
        # TODO: Create statistical learning capabilities
        # TODO: Initialize hierarchical sequence organization
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to learn temporal sequences
        
        Args:
            input_data: Dictionary containing temporal pattern information
            
        Returns:
            Dictionary with learned sequence information
        """
        # TODO: Implement sequence learning logic
        # TODO: Detect patterns in temporal input
        # TODO: Update sequence models with new evidence
        # TODO: Generate sequence predictions
        
        return {
            "status": "not_implemented",
            "module_id": self.module_id,
            "module_type": self.module_type
        }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # TODO: Implement development progression for sequence learning
        # TODO: Increase sequence complexity with development
        # TODO: Enhance hierarchical organization with development
        
        return super().update_development(amount)


#######################

#temporal\time_perception.py#
#######################

# TODO: Implement the TimePerception class to track and estimate time intervals
# This component should be able to:
# - Track the passage of time
# - Estimate durations of events and intervals
# - Synchronize internal processes with temporal rhythms
# - Develop a sense of past, present, and future

# TODO: Implement developmental progression in time perception:
# - Basic rhythmic awareness in early stages
# - Growing time interval discrimination in childhood
# - Extended time horizons in adolescence
# - Sophisticated temporal cognition in adulthood

# TODO: Create mechanisms for:
# - Time tracking: Monitor the passage of time
# - Duration estimation: Judge the length of intervals
# - Temporal integration: Connect events across time
# - Temporal organization: Structure experiences in time

# TODO: Implement different temporal scales:
# - Millisecond timing: For perceptual processes
# - Second-to-minute timing: For immediate action
# - Hour-to-day timing: For activity planning
# - Extended time perception: Past history and future projection

# TODO: Connect to memory and consciousness modules
# Time perception should interact with memory processes
# and contribute to conscious awareness of time

from typing import Dict, List, Any, Optional
from lmm_project.modules.base_module import BaseModule
from lmm_project.core.event_bus import EventBus

class TimePerception(BaseModule):
    """
    Tracks and estimates time intervals
    
    This module monitors the passage of time, estimates
    durations, synchronizes with temporal rhythms, and
    develops awareness of past, present, and future.
    """
    
    def __init__(self, module_id: str, event_bus: Optional[EventBus] = None):
        """
        Initialize the time perception module
        
        Args:
            module_id: Unique identifier for this module
            event_bus: Event bus for communication with other modules
        """
        super().__init__(module_id=module_id, module_type="time_perception", event_bus=event_bus)
        
        # TODO: Initialize time tracking mechanisms
        # TODO: Set up duration estimation capability
        # TODO: Create temporal integration processes
        # TODO: Initialize temporal organization structures
    
    def process_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process input to track and estimate time
        
        Args:
            input_data: Dictionary containing temporal information
            
        Returns:
            Dictionary with time perception results
        """
        # TODO: Implement time perception logic
        # TODO: Update internal time tracking
        # TODO: Estimate durations of events
        # TODO: Organize experiences temporally
        
        return {
            "status": "not_implemented",
            "module_id": self.module_id,
            "module_type": self.module_type
        }
    
    def update_development(self, amount: float) -> float:
        """
        Update the developmental level of this module
        
        Args:
            amount: Amount to increase development
            
        Returns:
            New developmental level
        """
        # TODO: Implement development progression for time perception
        # TODO: Increase temporal horizon with development
        # TODO: Enhance duration estimation precision with development
        
        return super().update_development(amount)


#######################

#temporal\__init__.py#
#######################

# Temporal module 

# TODO: Implement the temporal module factory function to return an integrated TemporalSystem
# This module should be responsible for sequence learning, prediction,
# causality understanding, and time perception.

# TODO: Create TemporalSystem class that integrates all temporal sub-components:
# - sequence_learning: learns patterns over time
# - prediction: anticipates future states
# - causality: understands cause-effect relationships
# - time_perception: tracks and estimates time intervals

# TODO: Implement development tracking for temporal cognition
# Temporal capabilities should develop from simple sequence recognition in early stages
# to sophisticated prediction and causal understanding in later stages

# TODO: Connect temporal module to memory, learning, and consciousness modules
# Temporal cognition should utilize episodic memories, inform
# learning processes, and contribute to conscious awareness

# TODO: Implement prospection capabilities
# Include mental time travel to imagine future scenarios,
# plan sequences of actions, and anticipate outcomes

from typing import Dict, List, Any, Optional
from lmm_project.core.event_bus import EventBus

def get_module(module_id: str, event_bus: Optional[EventBus] = None) -> Any:
    """
    Factory function to create a temporal module
    
    This function is responsible for creating a temporal system that can:
    - Recognize and learn sequential patterns
    - Predict future states based on current conditions
    - Understand and infer causal relationships
    - Track and estimate time intervals
    - Project into past and future (mental time travel)
    
    Args:
        module_id: Unique identifier for the module
        event_bus: Event bus for communication with other modules
        
    Returns:
        An instance of the TemporalSystem class
    """
    # TODO: Return an instance of the TemporalSystem class
    # that integrates all temporal sub-components
    raise NotImplementedError("Temporal module not yet implemented")


#######################

