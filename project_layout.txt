#######################
Project Layout:

agendev/
├── app.py
├── requirements.txt
└── setup.py
├── cache/
├── src/
│   ├── agendev/
│   │   ├── __init__.py
│   │   ├── context_management.py
│   │   ├── core.py
│   │   ├── llm_integration.py
│   │   ├── llm_module.py
│   │   ├── parameter_controller.py
│   │   ├── probability_modeling.py
│   │   ├── search_algorithms.py
│   │   ├── snapshot_engine.py
│   │   ├── test_generation.py
│   │   ├── tts_module.py
│   │   └── tts_notification.py
│   │   ├── agents/
│   │   ├── models/
│   │   │   ├── __init__.py
│   │   │   ├── planning_models.py
│   │   │   └── task_models.py
│   │   ├── utils/
│   │   │   ├── __init__.py
│   │   │   ├── fs_utils.py
│   │   │   └── visualization.py
│   ├── agendev.egg-info/
│   │   ├── SOURCES.txt
│   │   ├── dependency_links.txt
│   │   ├── entry_points.txt
│   │   ├── requires.txt
│   │   └── top_level.txt
├── workspace/
│   ├── artifacts/
│   │   ├── audio/
│   │   ├── models/
│   │   ├── snapshots/
│   ├── planning/
│   │   ├── pathfinding/
│   │   ├── search_trees/
│   │   ├── simulations/
│   ├── progress/
│   │   ├── history/
│   ├── quality/
│   │   ├── benchmarks/
│   │   ├── reviews/
│   │   ├── tests/
│   ├── src/

#######################

Codebase:
#######################
#app.py#
#######################

import dash
from dash import dcc, html, Input, Output, State, callback, ctx
import dash_bootstrap_components as dbc
from dash.exceptions import PreventUpdate
import json
import time
from datetime import datetime

# Initialize the Dash app with dark theme
app = dash.Dash(
    __name__,
    external_stylesheets=[
        dbc.themes.DARKLY,
        "https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css",
    ],
    suppress_callback_exceptions=True,
    meta_tags=[
        {"name": "viewport", "content": "width=device-width, initial-scale=1.0"}
    ],
)

# Custom CSS for styling to match the screenshots
app.index_string = '''
<!DOCTYPE html>
<html>
    <head>
        {%metas%}
        <title>AgenDev - Intelligent Agentic Development System</title>
        {%favicon%}
        {%css%}
        <style>
            :root {
                --primary-color: #333;
                --secondary-color: #444;
                --text-color: #f8f9fa;
                --accent-color: #61dafb;
                --success-color: #28a745;
                --danger-color: #dc3545;
                --warning-color: #ffc107;
            }
            
            body {
                background-color: #1a1a1a;
                color: var(--text-color);
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                overflow: hidden;
                margin: 0;
                padding: 0;
                height: 100vh;
            }
            
            .landing-page {
                display: flex;
                flex-direction: column;
                align-items: center;
                justify-content: center;
                height: 100vh;
            }
            
            .main-container {
                display: flex;
                height: 100vh;
                width: 100%;
                overflow: hidden;
            }
            
            .chat-container {
                width: 50%;
                height: 100%;
                overflow-y: auto;
                padding: 20px;
                background-color: #1a1a1a;
                border-right: 1px solid #333;
            }
            
            .view-container {
                width: 50%;
                height: 100%;
                overflow: hidden;
                display: flex;
                flex-direction: column;
                background-color: #1a1a1a;
            }
            
            .view-header {
                background-color: #333;
                padding: 10px;
                display: flex;
                justify-content: space-between;
                align-items: center;
            }
            
            .view-content {
                flex-grow: 1;
                overflow: auto;
                padding: 0;
                background-color: #2a2a2a;
            }
            
            .view-controls {
                background-color: #333;
                padding: 10px;
                display: flex;
                justify-content: space-between;
            }
            
            .chat-message {
                margin-bottom: 20px;
                animation: fadeIn 0.3s ease;
            }
            
            @keyframes fadeIn {
                from { opacity: 0; transform: translateY(10px); }
                to { opacity: 1; transform: translateY(0); }
            }
            
            .system-message {
                padding: 15px;
                background-color: #333;
                border-radius: 8px;
                margin-bottom: 20px;
            }
            
            .user-message {
                padding: 15px;
                background-color: #2a2a2a;
                border-radius: 8px;
                margin-bottom: 20px;
            }
            
            .collapsible-header {
                display: flex;
                align-items: center;
                padding: 10px;
                background-color: #333;
                border-radius: 4px;
                cursor: pointer;
                margin-bottom: 10px;
            }
            
            .collapsible-header:hover {
                background-color: #444;
            }
            
            .collapsible-content {
                padding: 10px;
                background-color: #2a2a2a;
                border-radius: 4px;
                margin-bottom: 15px;
                margin-left: 15px;
                border-left: 2px solid #61dafb;
            }
            
            .command-element {
                background-color: #2a2a2a;
                padding: 8px 12px;
                border-radius: 4px;
                margin: 5px 0;
                font-family: 'Consolas', 'Courier New', monospace;
                border-left: 3px solid #61dafb;
            }
            
            .status-element {
                display: flex;
                align-items: center;
                margin: 5px 0;
            }
            
            .status-icon {
                margin-right: 10px;
            }
            
            .terminal-view {
                background-color: #1e1e1e;
                color: #ddd;
                font-family: 'Consolas', 'Courier New', monospace;
                padding: 10px;
                height: 100%;
                overflow: auto;
            }
            
            .editor-view {
                background-color: #1e1e1e;
                height: 100%;
                overflow: auto;
            }
            
            .editor-header {
                background-color: #2d2d2d;
                padding: 5px 10px;
                border-bottom: 1px solid #444;
                display: flex;
                justify-content: space-between;
            }
            
            .editor-content {
                padding: 10px;
                font-family: 'Consolas', 'Courier New', monospace;
                color: #ddd;
                min-height: calc(100% - 40px);
            }
            
            .browser-view {
                background-color: #fff;
                height: 100%;
                overflow: auto;
            }
            
            .file-path {
                font-family: 'Consolas', 'Courier New', monospace;
                color: #888;
                font-size: 0.85em;
                margin-bottom: 5px;
            }
            
            .function-tag {
                background-color: #61dafb;
                color: #000;
                padding: 2px 6px;
                border-radius: 4px;
                margin-right: 5px;
                font-size: 0.8em;
            }
            
            .status-tag {
                padding: 2px 6px;
                border-radius: 4px;
                margin-right: 5px;
                font-size: 0.8em;
            }
            
            .status-tag.success {
                background-color: #28a745;
                color: #fff;
            }
            
            .status-tag.in-progress {
                background-color: #ffc107;
                color: #000;
            }
            
            .status-tag.error {
                background-color: #dc3545;
                color: #fff;
            }
            
            .progress-controls {
                display: flex;
                align-items: center;
            }
            
            .time-indicator {
                font-size: 0.8em;
                color: #888;
                margin-left: 10px;
            }
            
            .btn-control {
                background: none;
                border: none;
                color: #888;
                font-size: 1em;
                cursor: pointer;
                padding: 5px;
                transition: color 0.2s;
            }
            
            .btn-control:hover {
                color: #fff;
            }
            
            .code-content {
                border-radius: 4px;
                background-color: #2d2d2d;
                padding: 10px;
                font-family: 'Consolas', 'Courier New', monospace;
                overflow-x: auto;
            }
            
            .input-prompt {
                width: 80%;
                max-width: 800px;
                padding: 20px;
                background-color: #333;
                border-radius: 8px;
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            }
            
            .input-heading {
                font-size: 1.5rem;
                margin-bottom: 20px;
                text-align: center;
            }
            
            .brand-logo {
                margin-bottom: 30px;
                font-size: 2.5rem;
                font-weight: bold;
                color: #61dafb;
            }
            
            .brand-slogan {
                font-size: 1rem;
                color: #888;
                margin-bottom: 30px;
                text-align: center;
            }
        </style>
    </head>
    <body>
        {%app_entry%}
        <footer>
            {%config%}
            {%scripts%}
            {%renderer%}
        </footer>
    </body>
</html>
'''

# Landing page layout with centered input
landing_page = html.Div(
    id="landing-page",
    className="landing-page",
    children=[
        html.Div(className="brand-logo", children=["AgenDev"]),
        html.Div(
            className="brand-slogan", 
            children=["An Intelligent Agentic Development System"]
        ),
        html.Div(
            className="input-prompt",
            children=[
                html.H2("What would you like to develop today?", className="input-heading"),
                dcc.Textarea(
                    id="initial-prompt",
                    placeholder="Describe your project or what you'd like help with...",
                    style={
                        "width": "100%",
                        "height": "120px",
                        "borderRadius": "4px",
                        "padding": "10px",
                        "marginBottom": "15px",
                        "backgroundColor": "#2a2a2a",
                        "color": "#fff",
                        "border": "1px solid #444"
                    }
                ),
                html.Button(
                    "Submit",
                    id="submit-button",
                    style={
                        "width": "100%",
                        "padding": "10px",
                        "borderRadius": "4px",
                        "backgroundColor": "#61dafb",
                        "color": "#000",
                        "border": "none",
                        "cursor": "pointer",
                        "fontWeight": "bold"
                    }
                )
            ]
        )
    ]
)

# Terminal view component
def create_terminal_view(content):
    return html.Div(
        className="terminal-view",
        children=[
            html.Pre(content)
        ]
    )

# Editor view component
def create_editor_view(filename, content, language="text"):
    syntax_highlighting = {
        "python": {
            "keywords": ["def", "class", "import", "from", "return", "if", "else", "elif", "for", "while", "try", "except", "with"],
            "keyword_color": "#569CD6",
            "string_color": "#CE9178",
            "comment_color": "#6A9955",
            "function_color": "#DCDCAA",
            "variable_color": "#9CDCFE"
        },
        "json": {
            "keywords": ["null", "true", "false"],
            "keyword_color": "#569CD6",
            "string_color": "#CE9178",
            "number_color": "#B5CEA8",
            "punctuation_color": "#D4D4D4"
        },
        "text": {
            "color": "#D4D4D4"
        }
    }
    
    return html.Div(
        className="editor-view",
        children=[
            html.Div(
                className="editor-header",
                children=[
                    html.Div(filename),
                    html.Div([
                        html.Button("Diff", className="btn-control"),
                        html.Button("Original", className="btn-control"),
                        html.Button("Modified", className="btn-control", style={"color": "#fff"}),
                    ])
                ]
            ),
            html.Pre(
                content,
                className="editor-content",
                style={"whiteSpace": "pre-wrap"}
            )
        ]
    )

# Collapsible section component
def create_collapsible_section(id_prefix, header_content, content, is_open=True):
    return html.Div([
        html.Div(
            className="collapsible-header",
            id=f"{id_prefix}-header",
            children=[
                html.I(
                    className="fas fa-chevron-down mr-2",
                    style={"marginRight": "10px"}
                ),
                header_content
            ]
        ),
        html.Div(
            id=f"{id_prefix}-content",
            className="collapsible-content",
            style={"display": "block" if is_open else "none"},
            children=content
        )
    ])

# Command execution component
def create_command_element(command, status="completed"):
    icon_class = "fas fa-check-circle text-success" if status == "completed" else "fas fa-spinner fa-spin text-warning"
    return html.Div(
        className="status-element",
        children=[
            html.Span(className=f"status-icon {icon_class}"),
            html.Span("Executing command", style={"marginRight": "10px"}),
            html.Code(command, className="command-element")
        ]
    )

# File creation/editing component
def create_file_operation(operation, filepath, status="completed"):
    icon_class = "fas fa-check-circle text-success" if status == "completed" else "fas fa-spinner fa-spin text-warning"
    
    return html.Div(
        className="status-element",
        children=[
            html.Span(className=f"status-icon {icon_class}"),
            html.Span(f"{operation} file", style={"marginRight": "10px"}),
            html.Code(filepath, className="file-path")
        ]
    )

# Main split view layout
main_view = html.Div(
    id="main-container",
    className="main-container",
    style={"display": "none"},  # Hidden initially
    children=[
        # Left side - Chat interface
        html.Div(
            className="chat-container",
            id="chat-container",
            children=[
                # Header with current task
                html.Div(
                    className="system-message",
                    children=[
                        html.Div(
                            style={"display": "flex", "alignItems": "center", "marginBottom": "15px"},
                            children=[
                                html.I(className="fas fa-robot", style={"fontSize": "24px", "marginRight": "10px", "color": "#61dafb"}),
                                html.Span("AgenDev", style={"fontSize": "24px", "fontWeight": "bold"})
                            ]
                        ),
                        html.H3(id="project-title", children="Python Snake Game Development", style={"margin": "0 0 10px 0"})
                    ]
                ),
                
                # Example of a task section
                create_collapsible_section(
                    "task1",
                    html.Div([
                        html.I(className="fas fa-check-circle", style={"marginRight": "10px", "color": "#28a745"}),
                        html.Span("Continue: Create Next.js application for political debate simulation")
                    ]),
                    [
                        html.P("Creating a Next.js application for the Zelenskyy-Trump-Vance interactive simulation."),
                        create_command_element("cd /home/ubuntu && create_nextjs_app zelenskyy_debate_sim"),
                        html.P("Successfully created Next.js application for the Zelenskyy-Trump-Vance interactive simulation."),
                        create_file_operation("Creating", "zelenskyy_debate_sim/src/app/data/scenarios.json")
                    ]
                ),
                
                # Example of a task section
                create_collapsible_section(
                    "task2",
                    html.Div([
                        html.I(className="fas fa-check-circle", style={"marginRight": "10px", "color": "#28a745"}),
                        html.Span("Develop dialogue system and political scenario scripts")
                    ]),
                    [
                        html.P("Implementing dialogue system and scenarios for the Zelenskyy-Trump-Vance interactive simulation."),
                        create_file_operation("Editing", "zelenskyy_debate_sim/src/app/page.tsx"),
                        html.P("Implementing dialogue system and scenarios for the Zelenskyy-Trump-Vance interactive simulation."),
                        create_file_operation("Creating", "zelenskyy_debate_sim/src/app/simulation/page.tsx"),
                        html.P("Implementing dialogue system and scenarios for the Zelenskyy-Trump-Vance interactive simulation."),
                        create_file_operation("Editing", "zelenskyy_debate_sim/src/app/globals.css")
                    ]
                ),
                
                # Example of a task section
                create_collapsible_section(
                    "task3",
                    html.Div([
                        html.I(className="fas fa-spinner fa-spin", style={"marginRight": "10px", "color": "#ffc107"}),
                        html.Span("Design and implement user interface with styling")
                    ]),
                    [
                        html.P("Moving to add additional user interface elements and styling to the Zelenskyy-Trump-Vance interactive simulation."),
                        create_file_operation("Creating", "zelenskyy_debate_sim/src/components/CharacterPortrait.tsx"),
                        html.P("Adding user interface components and styling to enhance the Zelenskyy-Trump-Vance interactive simulation."),
                        create_file_operation("Creating", "zelenskyy_debate_sim/src/components/DialogueBubble.tsx"),
                        html.P("Adding user interface components and styling to enhance the Zelenskyy-Trump-Vance interactive simulation."),
                        create_file_operation("Creating", "zelenskyy_debate_sim/src/components/ResponseOption.tsx")
                    ]
                ),
                
                # Thinking indicator
                html.Div(
                    className="chat-message",
                    children=[
                        html.Div(
                            style={
                                "display": "flex",
                                "alignItems": "center",
                                "color": "#888"
                            },
                            children=[
                                html.I(className="fas fa-circle-notch fa-spin", style={"marginRight": "10px"}),
                                html.Span("Thinking")
                            ]
                        )
                    ]
                )
            ]
        ),
        
        # Right side - Dynamic view (Terminal, Editor, Browser)
        html.Div(
            className="view-container",
            children=[
                # Header
                html.Div(
                    className="view-header",
                    children=[
                        html.Div("AgenDev's Computer"),
                        html.Button(
                            html.I(className="fas fa-expand"),
                            className="btn-control"
                        )
                    ]
                ),
                
                # View type indicator
                html.Div(
                    style={
                        "padding": "5px 10px",
                        "backgroundColor": "#2d2d2d",
                        "borderBottom": "1px solid #444",
                        "display": "flex",
                        "alignItems": "center"
                    },
                    children=[
                        html.Span("AgenDev is using", style={"color": "#888", "marginRight": "5px"}),
                        html.Span("Editor"),
                        html.Div(
                            style={
                                "marginLeft": "20px",
                                "display": "flex",
                                "alignItems": "center",
                                "color": "#888",
                                "fontSize": "0.85em"
                            },
                            children=[
                                html.Span("Creating file"),
                                html.Code(
                                    "zelenskyy_debate_sim/src/app/data/scenarios.json",
                                    style={
                                        "marginLeft": "5px",
                                        "backgroundColor": "transparent",
                                        "padding": "0"
                                    }
                                )
                            ]
                        )
                    ]
                ),
                
                # Content area (can be terminal, editor, or browser)
                html.Div(
                    className="view-content",
                    id="view-content",
                    children=[
                        # Default to editor view
                        create_editor_view(
                            "scenarios.json",
                            '''
{
  "scenarios": [
    {
      "id": 1,
      "title": "Opening Remarks",
      "description": "President Trump welcomes you to the White House. The meeting has just begun with initial pleasantries.",
      "trumpDialogue": "President Trump welcomes you to the White House. We're going to have a great discussion today about ending this terrible war. I hope I'm going to be remembered as a peacemaker.",
      "vanceDialogue": "",
      "options": [
        {
          "id": "1a",
          "text": "Thank you, Mr. President. Ukraine is grateful for America's support. We look forward to discussing how we can achieve a just peace that ensures Ukraine's security.",
          "type": "diplomatic",
          "trumpReaction": "positive",
          "vanceReaction": "neutral",
          "nextScenario": 2
        },
        {
          "id": "1b",
          "text": "Thank you for meeting with me. I must emphasize that Ukraine needs more than just words - we need continued military support and security guarantees to end this war.",
          "type": "assertive",
          "trumpReaction": "neutral",
          "vanceReaction": "negative",
          "nextScenario": 2
        },
        ...
      ]
    }
  ]
}''',
                            "json"
                        )
                    ]
                ),
                
                # Controls
                html.Div(
                    className="view-controls",
                    children=[
                        html.Div(
                            className="progress-controls",
                            children=[
                                html.Button(
                                    html.I(className="fas fa-step-backward"),
                                    className="btn-control",
                                    id="playback-backward"
                                ),
                                html.Button(
                                    html.I(className="fas fa-play"),
                                    className="btn-control",
                                    id="playback-play"
                                ),
                                html.Button(
                                    html.I(className="fas fa-step-forward"),
                                    className="btn-control",
                                    id="playback-forward"
                                ),
                                html.Div(
                                    dcc.Slider(
                                        id="playback-slider",
                                        min=0,
                                        max=100,
                                        value=50,
                                        updatemode="drag",
                                        marks=None,
                                        tooltip={"always_visible": False},
                                        className="timeline-slider"
                                    ),
                                    style={"width": "300px", "marginLeft": "10px", "marginRight": "10px"}
                                )
                            ]
                        ),
                        html.Div(
                            className="status-indicator",
                            children=[
                                html.Span(
                                    html.I(className="fas fa-check-circle"),
                                    className="status-tag success",
                                    style={"marginRight": "5px"}
                                ),
                                html.Span("Deploy simulation to a public URL for permanent access"),
                                html.Span("9/9", className="time-indicator")
                            ]
                        )
                    ]
                )
            ]
        )
    ]
)

# Store of playback data with timeline steps
def create_demo_playback_data():
    return {
        "total_steps": 10,
        "current_step": 0,
        "is_playing": False,
        "steps": [
            {
                "type": "terminal",
                "content": "ubuntu@sandbox:~ $ cd /home/ubuntu && cd /home/ubuntu\nubuntu@sandbox:~ $ create_nextjs_app python_snake_game\nStarting setup...\nCreating Next.js app for development: python_snake_game\nInstalling dependencies...\nInitializing git repository...",
                "timestamp": "00:00"
            },
            {
                "type": "terminal",
                "content": "ubuntu@sandbox:~ $ cd /home/ubuntu && cd /home/ubuntu\nubuntu@sandbox:~ $ create_nextjs_app python_snake_game\nStarting setup...\nCreating Next.js app for development: python_snake_game\nInstalling dependencies...\nInitializing git repository...\nCreated new Next.js app python_snake_game at /home/ubuntu/python_snake_game\n--- Project Structure ---\n|— migrations/\n|   └— 0001_initial.sql      # DB migration script\n|— src/\n|   |— app/                 # Next.js pages\n|   |   └— counter.ts       # Example component\n|   |— components/\n|   |— hooks/\n|   |— lib/\n|   └— wrangler.toml        # Cloudflare config",
                "timestamp": "00:10"
            },
            {
                "type": "editor",
                "filename": "game_engine.py",
                "content": "# game_engine.py\n\nclass SnakeGame:\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n        self.snake = [(width // 2, height // 2)]\n        self.direction = 'RIGHT'\n        self.food = None\n        self.score = 0\n        self.game_over = False\n        self._place_food()\n    \n    def _place_food(self):\n        # Logic to place food\n        import random\n        while True:\n            x = random.randint(0, self.width - 1)\n            y = random.randint(0, self.height - 1)\n            if (x, y) not in self.snake:\n                self.food = (x, y)\n                break",
                "timestamp": "00:30"
            },
            {
                "type": "editor",
                "filename": "snake_game.py",
                "content": "# snake_game.py\nimport pygame\nimport sys\nfrom game_engine import SnakeGame\n\nclass SnakeGameUI:\n    def __init__(self, width=20, height=20, cell_size=20):\n        self.width = width\n        self.height = height\n        self.cell_size = cell_size\n        self.game = SnakeGame(width, height)\n        \n        # Initialize pygame\n        pygame.init()\n        self.screen = pygame.display.set_mode(\n            (width * cell_size, height * cell_size)\n        )\n        pygame.display.set_caption('Python Snake Game')\n        \n        # Colors\n        self.colors = {\n            'background': (15, 15, 15),\n            'snake': (0, 255, 0),\n            'food': (255, 0, 0),\n            'text': (255, 255, 255)\n        }\n        \n        # Game clock\n        self.clock = pygame.time.Clock()\n        self.speed = 10  # FPS",
                "timestamp": "00:45"
            },
            {
                "type": "editor",
                "filename": "snake_game.py",
                "content": "# snake_game.py\nimport pygame\nimport sys\nfrom game_engine import SnakeGame\n\nclass SnakeGameUI:\n    def __init__(self, width=20, height=20, cell_size=20):\n        self.width = width\n        self.height = height\n        self.cell_size = cell_size\n        self.game = SnakeGame(width, height)\n        \n        # Initialize pygame\n        pygame.init()\n        self.screen = pygame.display.set_mode(\n            (width * cell_size, height * cell_size)\n        )\n        pygame.display.set_caption('Python Snake Game')\n        \n        # Colors\n        self.colors = {\n            'background': (15, 15, 15),\n            'snake': (0, 255, 0),\n            'food': (255, 0, 0),\n            'text': (255, 255, 255)\n        }\n        \n        # Game clock\n        self.clock = pygame.time.Clock()\n        self.speed = 10  # FPS\n        \n    def draw(self):\n        # Clear screen\n        self.screen.fill(self.colors['background'])\n        \n        # Draw snake\n        for segment in self.game.snake:\n            pygame.draw.rect(\n                self.screen,\n                self.colors['snake'],\n                pygame.Rect(\n                    segment[0] * self.cell_size,\n                    segment[1] * self.cell_size,\n                    self.cell_size,\n                    self.cell_size\n                )\n            )\n        \n        # Draw food\n        pygame.draw.rect(\n            self.screen,\n            self.colors['food'],\n            pygame.Rect(\n                self.game.food[0] * self.cell_size,\n                self.game.food[1] * self.cell_size,\n                self.cell_size,\n                self.cell_size\n            )\n        )\n        \n        # Update display\n        pygame.display.flip()",
                "timestamp": "01:05"
            },
            {
                "type": "editor",
                "filename": "main.py",
                "content": "# main.py\nfrom snake_game import SnakeGameUI\nimport pygame\nimport sys\n\ndef main():\n    # Create game instance\n    game_ui = SnakeGameUI(width=20, height=20, cell_size=30)\n    \n    # Main game loop\n    while not game_ui.game.game_over:\n        # Process events\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                pygame.quit()\n                sys.exit()\n            elif event.type == pygame.KEYDOWN:\n                if event.key == pygame.K_UP and game_ui.game.direction != 'DOWN':\n                    game_ui.game.direction = 'UP'\n                elif event.key == pygame.K_DOWN and game_ui.game.direction != 'UP':\n                    game_ui.game.direction = 'DOWN'\n                elif event.key == pygame.K_LEFT and game_ui.game.direction != 'RIGHT':\n                    game_ui.game.direction = 'LEFT'\n                elif event.key == pygame.K_RIGHT and game_ui.game.direction != 'LEFT':\n                    game_ui.game.direction = 'RIGHT'\n        \n        # Update game state\n        game_ui.game.update()\n        \n        # Draw game\n        game_ui.draw()\n        \n        # Control game speed\n        game_ui.clock.tick(game_ui.speed)\n    \n    # Game over screen\n    game_ui.show_game_over()\n    \n    # Wait for quit event\n    waiting = True\n    while waiting:\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                waiting = False\n    \n    pygame.quit()\n\nif __name__ == \"__main__\":\n    main()",
                "timestamp": "01:30"
            },
            {
                "type": "terminal",
                "content": "ubuntu@sandbox:~ $ cd /home/ubuntu/python_snake_game\nubuntu@sandbox:~/python_snake_game $ python main.py\nTraceback (most recent call last):\n  File \"main.py\", line 31, in <module>\n    main()\n  File \"main.py\", line 19, in main\n    game_ui.game.update()\nAttributeError: 'SnakeGame' object has no attribute 'update'\n",
                "timestamp": "02:00"
            },
            {
                "type": "editor",
                "filename": "game_engine.py",
                "content": "# game_engine.py\n\nclass SnakeGame:\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n        self.snake = [(width // 2, height // 2)]\n        self.direction = 'RIGHT'\n        self.food = None\n        self.score = 0\n        self.game_over = False\n        self._place_food()\n    \n    def _place_food(self):\n        # Logic to place food\n        import random\n        while True:\n            x = random.randint(0, self.width - 1)\n            y = random.randint(0, self.height - 1)\n            if (x, y) not in self.snake:\n                self.food = (x, y)\n                break\n                \n    def update(self):\n        # Move snake based on current direction\n        head_x, head_y = self.snake[0]\n        \n        if self.direction == 'UP':\n            head_y -= 1\n        elif self.direction == 'DOWN':\n            head_y += 1\n        elif self.direction == 'LEFT':\n            head_x -= 1\n        elif self.direction == 'RIGHT':\n            head_x += 1\n            \n        # Check for game over conditions\n        if (head_x < 0 or head_x >= self.width or\n            head_y < 0 or head_y >= self.height or\n            (head_x, head_y) in self.snake):\n            self.game_over = True\n            return\n            \n        # Check if snake ate food\n        if (head_x, head_y) == self.food:\n            self.score += 1\n            self._place_food()\n        else:\n            # Remove tail if snake didn't eat\n            self.snake.pop()\n            \n        # Add new head\n        self.snake.insert(0, (head_x, head_y))",
                "timestamp": "02:30"
            },
            {
                "type": "terminal",
                "content": "ubuntu@sandbox:~ $ cd /home/ubuntu/python_snake_game\nubuntu@sandbox:~/python_snake_game $ python main.py\n[Game is now running successfully in a pygame window]",
                "timestamp": "03:00"
            },
            {
                "type": "editor",
                "filename": "README.md",
                "content": "# Python Snake Game\n\nA classic snake game implemented in Python using Pygame.\n\n## Features\n\n- Clean, modular code structure with separation of game logic and UI\n- Smooth controls using arrow keys\n- Score tracking\n- Game over detection\n\n## Requirements\n\n- Python 3.6+\n- Pygame\n\n## Installation\n\n```bash\npip install pygame\n```\n\n## How to Run\n\n```bash\npython main.py\n```\n\n## Controls\n\n- Arrow keys to change direction\n- Esc to quit\n\n## Project Structure\n\n- `main.py` - Entry point for the game\n- `game_engine.py` - Core game logic\n- `snake_game.py` - UI and rendering logic\n\n## Future Improvements\n\n- Add pause functionality\n- Add high score tracking\n- Implement difficulty levels\n- Add sound effects",
                "timestamp": "03:15"
            }
        ]
    }

# Add a Store component to manage playback state
playback_store = dcc.Store(
    id='playback-data',
    data=create_demo_playback_data()
)

# Add app state store
app_state_store = dcc.Store(
    id='app-state',
    data={"view": "landing", "initial_prompt": ""}
)

# Set the app layout - this is what was missing
app.layout = html.Div([
    app_state_store,
    playback_store,
    landing_page,
    main_view
])

# Callback to transition from landing page to main view
@app.callback(
    [Output("app-state", "data"),
     Output("landing-page", "style"),
     Output("main-container", "style"),
     Output("project-title", "children")],
    [Input("submit-button", "n_clicks")],
    [State("initial-prompt", "value"),
     State("app-state", "data")],
    prevent_initial_call=True
)
def transition_to_main_view(n_clicks, prompt_value, current_state):
    if not n_clicks:
        raise PreventUpdate
    
    # Update state
    current_state["view"] = "main"
    current_state["initial_prompt"] = prompt_value
    
    # Generate a title based on the prompt
    title = "New Project"
    if prompt_value:
        # Simple algorithm to extract a title
        if "create" in prompt_value.lower() and "snake" in prompt_value.lower() and "python" in prompt_value.lower():
            title = "Python Snake Game Development"
        elif "todo" in prompt_value.lower() or "task" in prompt_value.lower() or "list" in prompt_value.lower():
            title = "Todo List Application"
        elif "dashboard" in prompt_value.lower() or "data" in prompt_value.lower() or "visualization" in prompt_value.lower():
            title = "Data Visualization Dashboard"
        elif "web" in prompt_value.lower() or "site" in prompt_value.lower() or "app" in prompt_value.lower():
            title = "Web Application Development"
        elif "game" in prompt_value.lower():
            title = "Game Development Project"
        elif "api" in prompt_value.lower() or "backend" in prompt_value.lower() or "server" in prompt_value.lower():
            title = "API Development Project"
        else:
            # Extract key words for a generic title
            words = prompt_value.split()
            if len(words) > 3:
                # Take a few significant words from the middle of the prompt
                middle_index = len(words) // 2
                title_words = words[max(0, middle_index-1):min(len(words), middle_index+2)]
                title = " ".join(word.capitalize() for word in title_words) + " Project"
            else:
                # For short prompts, use the whole thing
                title = prompt_value.capitalize()
    
    # Hide landing page, show main container
    landing_style = {"display": "none"}
    main_style = {"display": "flex"}
    
    return current_state, landing_style, main_style, title

# Callback for task1 collapsible section
@app.callback(
    Output("task1-content", "style"),
    Input("task1-header", "n_clicks"),
    State("task1-content", "style"),
    prevent_initial_call=True
)
def toggle_section_task1(n_clicks, current_style):
    if not n_clicks:
        raise PreventUpdate
    
    is_visible = current_style.get("display") == "block"
    new_style = {"display": "none" if is_visible else "block"}
    return new_style

# Callback for task2 collapsible section  
@app.callback(
    Output("task2-content", "style"),
    Input("task2-header", "n_clicks"),
    State("task2-content", "style"),
    prevent_initial_call=True
)
def toggle_section_task2(n_clicks, current_style):
    if not n_clicks:
        raise PreventUpdate
    
    is_visible = current_style.get("display") == "block"
    new_style = {"display": "none" if is_visible else "block"}
    return new_style

# Callback for task3 collapsible section
@app.callback(
    Output("task3-content", "style"),
    Input("task3-header", "n_clicks"),
    State("task3-content", "style"),
    prevent_initial_call=True
)
def toggle_section_task3(n_clicks, current_style):
    if not n_clicks:
        raise PreventUpdate
    
    is_visible = current_style.get("display") == "block"
    new_style = {"display": "none" if is_visible else "block"}
    return new_style

# Run the server
if __name__ == "__main__":
    app.run_server(debug=True)

#######################

#requirements.txt#
#######################

# Dependencies 
requests 
pydantic 
numpy 
soundfile 
sounddevice 
dash 
dash-bootstrap-components 
plotly 
pandas 


#######################

#setup.py#
#######################

from setuptools import setup, find_packages

setup(
    name="agendev",
    version="0.1.0",
    description="An Intelligent Agentic Development System",
    author="RenDev",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    install_requires=[
        "requests",
        "pydantic",
        "numpy",
        "soundfile",
        "sounddevice",
        # Add dash for the web interface
        "dash",
        "dash-bootstrap-components",
        "plotly",
        "pandas",
    ],
    python_requires=">=3.9",
)

#######################

#src\agendev\context_management.py#
#######################

# context_management.py
"""Embedding-based code representation and relationship mapping."""

from __future__ import annotations
from typing import Dict, List, Optional, Set, Union, Any, Tuple
from uuid import UUID, uuid4
from datetime import datetime
from pathlib import Path
import os
import json
import numpy as np
from pydantic import BaseModel, Field, model_validator

from .utils.fs_utils import (
    resolve_path, 
    load_json, 
    save_json, 
    safe_save_json, 
    save_pickle, 
    load_pickle
)
from .llm_module import LLMClient, Message

class ContextElement(BaseModel):
    """Represents a single element in the context system."""
    id: UUID = Field(default_factory=uuid4)
    element_type: str  # "code", "comment", "function", "class", "file", etc.
    content: str
    source_file: Optional[str] = None
    source_line_start: Optional[int] = None
    source_line_end: Optional[int] = None
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)
    
    # Embedding vector (stored separately for efficiency)
    embedding_id: Optional[str] = None
    
    # Relationships
    parent_id: Optional[UUID] = None
    children_ids: List[UUID] = Field(default_factory=list)
    related_ids: Dict[str, List[UUID]] = Field(default_factory=dict)
    
    # Metadata
    metadata: Dict[str, Any] = Field(default_factory=dict)
    tags: List[str] = Field(default_factory=list)
    
    def add_child(self, child_id: UUID) -> None:
        """Add a child to this element."""
        if child_id not in self.children_ids:
            self.children_ids.append(child_id)
            self.updated_at = datetime.now()
    
    def add_relationship(self, relation_type: str, target_id: UUID) -> None:
        """Add a relationship to another element."""
        if relation_type not in self.related_ids:
            self.related_ids[relation_type] = []
        
        if target_id not in self.related_ids[relation_type]:
            self.related_ids[relation_type].append(target_id)
            self.updated_at = datetime.now()
    
    def add_tag(self, tag: str) -> None:
        """Add a tag to this element."""
        if tag not in self.tags:
            self.tags.append(tag)
            self.updated_at = datetime.now()

class CodeEmbedding(BaseModel):
    """Stores embedding vectors for context elements."""
    id: str
    vector: List[float]
    element_id: UUID
    created_at: datetime = Field(default_factory=datetime.now)
    
    @model_validator(mode='after')
    def validate_vector(self) -> 'CodeEmbedding':
        """Ensure vector is properly formatted."""
        if not self.vector:
            raise ValueError("Embedding vector cannot be empty")
        return self

class ContextManager:
    """Manages code context using embeddings and relationships."""
    
    def __init__(self, 
                embedding_model: str = "text-embedding-nomic-embed-text-v1.5@q4_k_m",
                llm_base_url: str = "http://192.168.2.12:1234",
                context_dir: str = "artifacts/models/context"):
        """
        Initialize the context manager.
        
        Args:
            embedding_model: Model to use for generating embeddings
            llm_base_url: Base URL for the LLM service
            context_dir: Directory to store context data in
        """
        self.embedding_model = embedding_model
        self.llm_client = LLMClient(base_url=llm_base_url)
        self.context_dir = resolve_path(context_dir, create_parents=True)
        
        # Load existing elements and embeddings
        self.elements: Dict[UUID, ContextElement] = {}
        self.embeddings: Dict[str, CodeEmbedding] = {}
        
        self._load_context()
    
    def _load_context(self) -> None:
        """Load context data from disk."""
        elements_path = self.context_dir / "elements.json"
        if elements_path.exists():
            elements_data = load_json(elements_path)
            for element_dict in elements_data.get("elements", []):
                try:
                    element = ContextElement.model_validate(element_dict)
                    self.elements[element.id] = element
                except Exception as e:
                    print(f"Error loading context element: {e}")
        
        embeddings_path = self.context_dir / "embeddings.pkl"
        if embeddings_path.exists():
            self.embeddings = load_pickle(embeddings_path, default={})
    
    def _save_context(self) -> None:
        """Save context data to disk."""
        elements_data = {
            "elements": [element.model_dump() for element in self.elements.values()]
        }
        safe_save_json(elements_data, self.context_dir / "elements.json")
        save_pickle(self.embeddings, self.context_dir / "embeddings.pkl")
        
        # Save individual element files for easier access
        elements_dir = self.context_dir / "elements"
        os.makedirs(elements_dir, exist_ok=True)
        
        for element_id, element in self.elements.items():
            element_path = elements_dir / f"{element_id}.json"
            safe_save_json(element.model_dump(), element_path)
    
    def create_element(self, 
                     content: str, 
                     element_type: str, 
                     source_file: Optional[str] = None,
                     source_line_start: Optional[int] = None,
                     source_line_end: Optional[int] = None,
                     parent_id: Optional[UUID] = None,
                     metadata: Optional[Dict[str, Any]] = None,
                     tags: Optional[List[str]] = None,
                     generate_embedding: bool = True) -> UUID:
        """
        Create a new context element.
        
        Args:
            content: Text content of the element
            element_type: Type of element (code, comment, function, etc.)
            source_file: Path to the source file
            source_line_start: Starting line number in source
            source_line_end: Ending line number in source
            parent_id: UUID of parent element
            metadata: Additional metadata
            tags: List of tags
            generate_embedding: Whether to generate an embedding vector
            
        Returns:
            UUID of the created element
        """
        element = ContextElement(
            element_type=element_type,
            content=content,
            source_file=source_file,
            source_line_start=source_line_start,
            source_line_end=source_line_end,
            parent_id=parent_id,
            metadata=metadata or {},
            tags=tags or []
        )
        
        # Generate embedding if requested
        if generate_embedding:
            embedding_id = self._generate_embedding(element.id, content)
            element.embedding_id = embedding_id
        
        # Add as child to parent if parent exists
        if parent_id and parent_id in self.elements:
            self.elements[parent_id].add_child(element.id)
        
        # Store the element
        self.elements[element.id] = element
        
        # Save context data
        self._save_context()
        
        return element.id
    
    def _generate_embedding(self, element_id: UUID, content: str) -> str:
        """
        Generate an embedding vector for the content.
        
        Args:
            element_id: UUID of the element
            content: Text content to embed
            
        Returns:
            ID of the embedding
        """
        try:
            # Generate embedding using the LLM client
            vector = self.llm_client.get_embedding(content, embedding_model=self.embedding_model)
            
            # Create a unique ID for the embedding
            embedding_id = f"emb_{uuid4().hex[:8]}"
            
            # Store the embedding
            embedding = CodeEmbedding(
                id=embedding_id,
                vector=vector,
                element_id=element_id
            )
            
            self.embeddings[embedding_id] = embedding
            return embedding_id
            
        except Exception as e:
            print(f"Error generating embedding: {e}")
            return ""
    
    def get_element(self, element_id: UUID) -> Optional[ContextElement]:
        """
        Get a context element by ID.
        
        Args:
            element_id: UUID of the element
            
        Returns:
            ContextElement or None if not found
        """
        return self.elements.get(element_id)
    
    def get_elements_by_type(self, element_type: str) -> List[ContextElement]:
        """
        Get all elements of a specific type.
        
        Args:
            element_type: Type of elements to retrieve
            
        Returns:
            List of matching elements
        """
        return [e for e in self.elements.values() if e.element_type == element_type]
    
    def get_elements_by_tag(self, tag: str) -> List[ContextElement]:
        """
        Get all elements with a specific tag.
        
        Args:
            tag: Tag to search for
            
        Returns:
            List of matching elements
        """
        return [e for e in self.elements.values() if tag in e.tags]
    
    def get_element_embedding(self, element_id: UUID) -> Optional[List[float]]:
        """
        Get the embedding vector for an element.
        
        Args:
            element_id: UUID of the element
            
        Returns:
            Embedding vector or None if not found
        """
        element = self.elements.get(element_id)
        if not element or not element.embedding_id:
            return None
        
        embedding = self.embeddings.get(element.embedding_id)
        if not embedding:
            return None
        
        return embedding.vector
    
    def get_children(self, element_id: UUID) -> List[ContextElement]:
        """
        Get all children of an element.
        
        Args:
            element_id: UUID of the parent element
            
        Returns:
            List of child elements
        """
        element = self.elements.get(element_id)
        if not element:
            return []
        
        return [self.elements.get(child_id) for child_id in element.children_ids 
                if child_id in self.elements]
    
    def get_related_elements(self, element_id: UUID, relation_type: Optional[str] = None) -> Dict[str, List[ContextElement]]:
        """
        Get elements related to the specified element.
        
        Args:
            element_id: UUID of the element
            relation_type: Optional type of relationship to filter by
            
        Returns:
            Dictionary mapping relation types to lists of elements
        """
        element = self.elements.get(element_id)
        if not element:
            return {}
        
        if relation_type:
            related_ids = element.related_ids.get(relation_type, [])
            return {relation_type: [self.elements.get(rel_id) for rel_id in related_ids 
                                   if rel_id in self.elements]}
        
        result = {}
        for rel_type, rel_ids in element.related_ids.items():
            result[rel_type] = [self.elements.get(rel_id) for rel_id in rel_ids 
                               if rel_id in self.elements]
        
        return result
    
    def get_top_elements(self, limit: int = 10) -> List[Dict[str, str]]:
        """
        Get the most recently added elements in the context manager.
        
        Args:
            limit: Maximum number of elements to return
            
        Returns:
            List of dictionaries with element information
        """
        # Sort elements by creation time (newest first)
        sorted_elements = sorted(
            self.elements.values(), 
            key=lambda x: x.created_at if hasattr(x, 'created_at') else datetime.now(),
            reverse=True
        )
        
        # Return the first 'limit' elements
        result = []
        for element in sorted_elements[:limit]:
            result.append({
                "type": element.element_type,
                "name": element.metadata.get("name", "Unnamed") if element.metadata else "Unnamed",
                "file": element.source_file or "Unknown"
            })
        
        return result
    
    def find_similar_elements(self, query: str, top_k: int = 5, threshold: float = 0.7) -> List[Tuple[ContextElement, float]]:
        """
        Find elements similar to the query text.
        
        Args:
            query: Text to compare against
            top_k: Maximum number of results to return
            threshold: Minimum similarity score (0-1)
            
        Returns:
            List of tuples containing (element, similarity_score)
        """
        # Generate embedding for the query
        try:
            query_vector = self.llm_client.get_embedding(query, embedding_model=self.embedding_model)
        except Exception as e:
            print(f"Error generating embedding for query: {e}")
            return []
        
        # Calculate similarity with all embeddings
        similarities = []
        for emb_id, embedding in self.embeddings.items():
            element_id = embedding.element_id
            if element_id not in self.elements:
                continue
            
            similarity = self._calculate_similarity(query_vector, embedding.vector)
            if similarity >= threshold:
                similarities.append((self.elements[element_id], similarity))
        
        # Sort by similarity score (descending)
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # Return top_k results
        return similarities[:top_k]
    
    def _calculate_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """
        Calculate cosine similarity between two vectors.
        
        Args:
            vec1: First vector
            vec2: Second vector
            
        Returns:
            Similarity score (0-1)
        """
        # Convert to numpy arrays
        a = np.array(vec1)
        b = np.array(vec2)
        
        # Calculate cosine similarity
        dot_product = np.dot(a, b)
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)
        
        if norm_a == 0 or norm_b == 0:
            return 0.0
            
        return dot_product / (norm_a * norm_b)
    
    def index_file(self, file_path: Union[str, Path], 
                 element_types: List[str] = ["file", "class", "function", "method"]) -> List[UUID]:
        """
        Index a source code file, creating context elements.
        
        Args:
            file_path: Path to the file to index
            element_types: Types of elements to extract
            
        Returns:
            List of UUIDs of created elements
        """
        full_path = resolve_path(file_path)
        if not full_path.exists():
            return []
        
        # Read the file content
        with open(full_path, 'r') as f:
            content = f.read()
        
        # Create a file-level element
        file_element_id = self.create_element(
            content=content,
            element_type="file",
            source_file=str(file_path),
            tags=["file", full_path.name, full_path.suffix[1:]]  # .py -> py
        )
        
        created_elements = [file_element_id]
        
        # Use LLM to extract code elements
        if "class" in element_types or "function" in element_types or "method" in element_types:
            elements = self._extract_code_elements(content, str(file_path), element_types)
            
            for elem in elements:
                element_id = self.create_element(
                    content=elem["content"],
                    element_type=elem["type"],
                    source_file=str(file_path),
                    source_line_start=elem.get("line_start"),
                    source_line_end=elem.get("line_end"),
                    parent_id=file_element_id,
                    metadata=elem.get("metadata", {}),
                    tags=[elem["type"], *elem.get("tags", [])]
                )
                created_elements.append(element_id)
        
        return created_elements
    
    def _extract_code_elements(self, content: str, file_path: str, element_types: List[str]) -> List[Dict]:
        """
        Extract code elements from content using LLM.
        
        Args:
            content: Source code content
            file_path: Path to the source file
            element_types: Types of elements to extract
            
        Returns:
            List of dictionaries describing the extracted elements
        """
        # Prepare the prompt for the LLM
        element_types_str = ", ".join(element_types)
        prompt = f"""
        Extract the following elements from this source code: {element_types_str}.
        For each element, provide:
        1. The element type
        2. The element name
        3. The full content including docstrings and comments
        4. The start and end line numbers
        5. Any relevant tags (e.g., 'class', 'public', 'private', etc.)
        
        Format the output as a JSON array of objects.
        
        Source file: {file_path}
        
        ```
        {content}
        ```
        """
        
        # Ask the LLM to extract elements
        messages = [
            Message(role="system", content="You are a code analysis assistant that extracts elements from source code."),
            Message(role="user", content=prompt)
        ]
        
        try:
            # Define the schema for structured output
            json_schema = {
                "name": "code_elements",
                "strict": "true",
                "schema": {
                    "type": "object",
                    "properties": {
                        "elements": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "type": {"type": "string"},
                                    "name": {"type": "string"},
                                    "content": {"type": "string"},
                                    "line_start": {"type": "integer"},
                                    "line_end": {"type": "integer"},
                                    "tags": {"type": "array", "items": {"type": "string"}},
                                    "metadata": {"type": "object"}
                                },
                                "required": ["type", "name", "content"]
                            }
                        }
                    },
                    "required": ["elements"]
                }
            }
            
            response = self.llm_client.structured_completion(
                messages=messages,
                json_schema=json_schema,
                temperature=0.2,  # Low temperature for more precise extraction
                max_tokens=4000
            )
            
            # Parse the response
            if isinstance(response, dict) and "elements" in response:
                return response["elements"]
            elif isinstance(response, str):
                try:
                    parsed = json.loads(response)
                    if "elements" in parsed:
                        return parsed["elements"]
                except:
                    pass
            
            return []
            
        except Exception as e:
            print(f"Error extracting code elements: {e}")
            return []
    
    def generate_context_for_task(self, task_description: str, top_k: int = 10) -> Dict[str, Any]:
        """
        Generate relevant context for a task.
        
        Args:
            task_description: Description of the task
            top_k: Maximum number of context elements to include
            
        Returns:
            Dictionary with relevant context
        """
        # Find similar elements
        similar_elements = self.find_similar_elements(task_description, top_k=top_k)
        
        context = {
            "task": task_description,
            "elements": [
                {
                    "id": str(elem.id),
                    "type": elem.element_type,
                    "content": elem.content,
                    "source_file": elem.source_file,
                    "relevance": score
                }
                for elem, score in similar_elements
            ]
        }
        
        return context
    
    def explain_code_relationships(self, element_id: UUID) -> str:
        """
        Use LLM to explain relationships between a code element and related elements.
        
        Args:
            element_id: UUID of the element
            
        Returns:
            Explanation of relationships
        """
        element = self.get_element(element_id)
        if not element:
            return "Element not found."
        
        # Get related elements
        related = self.get_related_elements(element_id)
        children = self.get_children(element_id)
        
        # Build context
        context = f"Element: {element.element_type} from {element.source_file}\n\n{element.content}\n\n"
        
        if children:
            context += "Children:\n"
            for child in children:
                context += f"- {child.element_type}: {child.content[:100]}...\n"
        
        for rel_type, elements in related.items():
            context += f"\n{rel_type.title()} relationships:\n"
            for rel_elem in elements:
                context += f"- {rel_elem.element_type}: {rel_elem.content[:100]}...\n"
        
        # Ask the LLM for an explanation
        messages = [
            Message(role="system", content="You are a code analysis assistant that explains relationships between code elements."),
            Message(role="user", content=f"Explain the relationships and dependencies in this code:\n\n{context}")
        ]
        
        try:
            explanation = self.llm_client.chat_completion(
                messages=messages,
                temperature=0.5,
                max_tokens=1000
            )
            return explanation
        except Exception as e:
            return f"Error generating explanation: {e}"

#######################

#src\agendev\core.py#
#######################

# core.py
"""Core orchestration module for AgenDev system."""

from __future__ import annotations
from typing import List, Dict, Optional, Set, Union, Any, Tuple, Literal
from enum import Enum
from pathlib import Path
import os
import time
import logging
from uuid import UUID, uuid4
from datetime import datetime
from pydantic import BaseModel, Field, model_validator

# Import models
from .models.task_models import (
    Task, TaskStatus, TaskPriority, TaskRisk, TaskType, 
    Epic, TaskGraph, Dependency
)
from .models.planning_models import (
    SimulationConfig, PlanSnapshot, PlanningHistory, 
    SearchNodeType, PlanningPhase, SimulationResult
)

# Import components
from .utils.fs_utils import resolve_path, ensure_workspace_structure, save_json, load_json
from .context_management import ContextManager, ContextElement
from .llm_integration import LLMIntegration, LLMConfig
from .tts_notification import NotificationManager, NotificationType, NotificationPriority
from .search_algorithms import MCTSPlanner, AStarPathfinder
from .probability_modeling import TaskProbabilityModel, ProjectRiskModel
from .snapshot_engine import SnapshotEngine
from .parameter_controller import ParameterController
from .test_generation import TestGenerator, TestType

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ProjectState(str, Enum):
    """States of the project lifecycle."""
    INITIALIZED = "initialized"
    PLANNING = "planning"
    IMPLEMENTING = "implementing"
    TESTING = "testing"
    COMPLETED = "completed"

class AgenDevConfig(BaseModel):
    """Configuration for the AgenDev system."""
    project_name: str
    workspace_dir: Optional[str] = None
    llm_base_url: str = "http://192.168.2.12:1234"
    tts_base_url: str = "http://127.0.0.1:7860"
    default_model: str = "qwen2.5-7b-instruct"
    embedding_model: str = "text-embedding-nomic-embed-text-v1.5@q4_k_m"
    notifications_enabled: bool = True
    auto_save: bool = True
    auto_save_interval_minutes: float = 5.0
    
    @model_validator(mode='after')
    def validate_config(self) -> 'AgenDevConfig':
        """Ensure configuration values are valid."""
        if self.auto_save_interval_minutes <= 0:
            self.auto_save_interval_minutes = 5.0
        return self

class AgenDev:
    """Main orchestration class for the AgenDev system."""
    
    def __init__(self, config: Optional[AgenDevConfig] = None):
        """
        Initialize the AgenDev system.
        
        Args:
            config: Configuration for the system
        """
        self.config = config or AgenDevConfig(project_name="agendev_project")
        self.project_state = ProjectState.INITIALIZED
        self.last_save_time = time.time()
        
        # Set up workspace
        self.workspace_dir = resolve_path(self.config.workspace_dir or "")
        ensure_workspace_structure()
        
        # Initialize components
        self._init_components()
        
        # Set up default models and data structures
        self.task_graph = TaskGraph()
        self.planning_history = PlanningHistory()
        
        # Load project if it exists
        self._load_project_state()
        
        # Announce system initialization
        if self.notification_manager:
            self.notification_manager.info(
                f"AgenDev system initialized for project '{self.config.project_name}'."
            )
    
    def _init_components(self) -> None:
        """Initialize all system components."""
        # Initialize LLM integration
        self.llm = LLMIntegration(
            base_url=self.config.llm_base_url,
            config=LLMConfig(model=self.config.default_model)
        )
        
        # Initialize context management
        self.context_manager = ContextManager(
            embedding_model=self.config.embedding_model,
            llm_base_url=self.config.llm_base_url
        )
        
        # Initialize TTS notification
        if self.config.notifications_enabled:
            self.notification_manager = NotificationManager(
                tts_base_url=self.config.tts_base_url
            )
        else:
            self.notification_manager = None
        
        # Initialize parameter controller
        self.parameter_controller = ParameterController()
        
        # Initialize snapshot engine
        self.snapshot_engine = SnapshotEngine(workspace_dir=self.workspace_dir)
        
        # Initialize test generator
        self.test_generator = TestGenerator(
            llm_integration=self.llm,
            context_manager=self.context_manager
        )
        
        # The following components will be initialized as needed:
        # - MCTSPlanner (needs task graph)
        # - AStarPathfinder (needs task graph)
        # - TaskProbabilityModel (needs task graph)
        # - ProjectRiskModel (needs probability model and task graph)
    
    def _load_project_state(self) -> None:
        """Load project state from disk."""
        tasks_path = resolve_path("planning/tasks.json")
        epics_path = resolve_path("planning/epics.json")
        
        if tasks_path.exists() and epics_path.exists():
            try:
                # Load tasks
                tasks_data = load_json(tasks_path)
                for task_dict in tasks_data.get("tasks", []):
                    task = Task.model_validate(task_dict)
                    self.task_graph.tasks[task.id] = task
                
                # Load epics
                epics_data = load_json(epics_path)
                for epic_dict in epics_data.get("epics", []):
                    epic = Epic.model_validate(epic_dict)
                    self.task_graph.epics[epic.id] = epic
                
                # Load planning history if it exists
                history_path = resolve_path("planning/planning_history.json")
                if history_path.exists():
                    history_data = load_json(history_path)
                    self.planning_history = PlanningHistory.model_validate(history_data)
                
                # Notify about project load
                if self.notification_manager:
                    self.notification_manager.info(
                        f"Project {self.config.project_name} loaded with "
                        f"{len(self.task_graph.tasks)} tasks and {len(self.task_graph.epics)} epics."
                    )
            except Exception as e:
                logger.error(f"Error loading project state: {e}")
    
    def _save_project_state(self) -> bool:
        """
        Save project state to disk.
        
        Returns:
            Whether the save was successful
        """
        try:
            # Save tasks
            tasks_data = {
                "tasks": [task.model_dump() for task in self.task_graph.tasks.values()]
            }
            save_json(tasks_data, resolve_path("planning/tasks.json"))
            
            # Save epics
            epics_data = {
                "epics": [epic.model_dump() for epic in self.task_graph.epics.values()]
            }
            save_json(epics_data, resolve_path("planning/epics.json"))
            
            # Save planning history
            history_data = self.planning_history.model_dump()
            save_json(history_data, resolve_path("planning/planning_history.json"))
            
            # Update last save time
            self.last_save_time = time.time()
            
            return True
        except Exception as e:
            logger.error(f"Error saving project state: {e}")
            if self.notification_manager:
                self.notification_manager.error(f"Failed to save project state: {e}")
            return False
    
    def _check_auto_save(self) -> None:
        """Check if it's time to auto-save the project state."""
        if self.config.auto_save:
            current_time = time.time()
            minutes_since_save = (current_time - self.last_save_time) / 60
            if minutes_since_save >= self.config.auto_save_interval_minutes:
                if self._save_project_state():
                    logger.info("Auto-saved project state")
    
    def create_task(
        self,
        title: str,
        description: str,
        task_type: TaskType = TaskType.IMPLEMENTATION,
        priority: TaskPriority = TaskPriority.MEDIUM,
        risk: TaskRisk = TaskRisk.MEDIUM,
        estimated_duration_hours: float = 1.0,
        epic_id: Optional[UUID] = None,
        dependencies: Optional[List[UUID]] = None
    ) -> UUID:
        """
        Create a new task.
        
        Args:
            title: Task title
            description: Task description
            task_type: Type of task
            priority: Task priority
            risk: Risk level
            estimated_duration_hours: Estimated duration in hours
            epic_id: Optional epic to associate with
            dependencies: Optional task dependencies
            
        Returns:
            ID of the created task
        """
        # Create task object
        task = Task(
            title=title,
            description=description,
            task_type=task_type,
            priority=priority,
            risk=risk,
            estimated_duration_hours=estimated_duration_hours,
            epic_id=epic_id,
            dependencies=dependencies or []
        )
        
        # Add to task graph
        task_id = self.task_graph.add_task(task)
        
        # Add dependencies if provided
        if dependencies:
            for dep_id in dependencies:
                if dep_id in self.task_graph.tasks:
                    dependency = Dependency(
                        source_id=dep_id,
                        target_id=task_id,
                        dependency_type="blocks"
                    )
                    self.task_graph.add_dependency(dependency)
        
        # Update task statuses
        self.task_graph.update_task_statuses()
        
        # Notify about task creation
        if self.notification_manager:
            self.notification_manager.info(f"Task '{title}' created.")
        
        # Auto-save if needed
        self._check_auto_save()
        
        return task_id
    
    def create_epic(
        self,
        title: str,
        description: str,
        priority: TaskPriority = TaskPriority.MEDIUM,
        risk: TaskRisk = TaskRisk.MEDIUM
    ) -> UUID:
        """
        Create a new epic.
        
        Args:
            title: Epic title
            description: Epic description
            priority: Epic priority
            risk: Risk level
            
        Returns:
            ID of the created epic
        """
        # Create epic object
        epic = Epic(
            title=title,
            description=description,
            priority=priority,
            risk=risk
        )
        
        # Add to task graph
        epic_id = self.task_graph.add_epic(epic)
        
        # Notify about epic creation
        if self.notification_manager:
            self.notification_manager.info(f"Epic '{title}' created.")
        
        # Auto-save if needed
        self._check_auto_save()
        
        return epic_id
    
    def generate_implementation_plan(self, max_iterations: int = 1000) -> PlanSnapshot:
        """
        Generate an implementation plan using MCTS.
        
        Args:
            max_iterations: Maximum MCTS iterations
            
        Returns:
            Plan snapshot with the generated plan
        """
        # Update project state
        self.project_state = ProjectState.PLANNING
        
        # Initialize probability model
        probability_model = TaskProbabilityModel(
            task_graph=self.task_graph, 
            llm_integration=self.llm
        )
        
        # Initialize MCTS planner
        planner = MCTSPlanner(
            task_graph=self.task_graph,
            llm_integration=self.llm,
            config=SimulationConfig(max_iterations=max_iterations)
        )
        
        # Notify about planning start
        if self.notification_manager:
            self.notification_manager.info(f"Generating implementation plan...")
        
        # Run simulation
        simulation_results = planner.run_simulation(iterations=max_iterations)
        
        # Get best sequence
        task_sequence = simulation_results["best_sequence"]
        
        # Calculate expected duration
        total_duration = sum(
            self.task_graph.tasks[task_id].estimated_duration_hours
            for task_id in task_sequence
            if task_id in self.task_graph.tasks
        )
        
        # Create risk assessment
        risk_model = ProjectRiskModel(task_probability_model=probability_model, task_graph=self.task_graph)
        risk_hotspots = risk_model.identify_risk_hotspots()
        risk_assessment = {
            str(item["task_id"]): item["success_probability"]
            for item in risk_hotspots
        }
        
        # Create plan snapshot
        plan = PlanSnapshot(
            plan_version=len(self.planning_history.snapshots) + 1,
            task_sequence=task_sequence,
            expected_duration_hours=total_duration,
            confidence_score=simulation_results["success_rate"],
            risk_assessment=risk_assessment,
            simulation_id=UUID(simulation_results["session_id"]),
            generated_by="mcts",
            description=f"Plan generated with {max_iterations} iterations, {simulation_results['success_rate']:.2f} success rate"
        )
        
        # Add to planning history
        plan_id = self.planning_history.add_snapshot(plan)
        
        # Notify about plan generation
        if self.notification_manager:
            self.notification_manager.success(
                f"Implementation plan generated with {len(task_sequence)} tasks, "
                f"estimated duration: {total_duration:.1f} hours, "
                f"confidence: {simulation_results['success_rate']:.0%}"
            )
        
        # Save project state
        self._save_project_state()
        
        # Update project state
        self.project_state = ProjectState.IMPLEMENTING
        
        return plan
    
    def optimize_task_sequence(self) -> List[UUID]:
        """
        Optimize the task sequence using A* pathfinding.
        
        Returns:
            Optimized task sequence
        """
        # Initialize A* pathfinder
        pathfinder = AStarPathfinder(task_graph=self.task_graph)
        
        # Find optimal path
        path, metadata = pathfinder.find_path()
        
        # Notify about optimization
        if self.notification_manager and path:
            self.notification_manager.info(
                f"Task sequence optimized with {len(path)} tasks, "
                f"estimated duration: {metadata.get('estimated_duration', 0):.1f} hours."
            )
        
        return path
    
    def implement_task(self, task_id: UUID) -> Dict[str, Any]:
        """
        Implement a task using LLM.
        
        Args:
            task_id: ID of the task to implement
            
        Returns:
            Dictionary with implementation details
        """
        if task_id not in self.task_graph.tasks:
            error_msg = f"Task not found: {task_id}"
            if self.notification_manager:
                self.notification_manager.error(error_msg)
            return {"error": error_msg}
        
        task = self.task_graph.tasks[task_id]
        
        # Notify about implementation start
        if self.notification_manager:
            self.notification_manager.info(f"Implementing task: {task.title}")
        
        # Get optimal parameters for this task
        llm_config = self.parameter_controller.get_llm_config(task)
        
        # Generate implementation context
        if self.context_manager:
            context = self.context_manager.generate_context_for_task(task.description, top_k=5)
            context_str = "\n\n".join(
                f"File: {elem['source_file']}\n```\n{elem['content']}\n```"
                for elem in context.get("elements", [])
            )
        else:
            context_str = ""
        
        # Generate implementation
        prompt = f"""
        Task: {task.title}
        Description: {task.description}
        
        {context_str}
        
        Implement this task according to the description. 
        Generate clean, well-structured, production-ready code.
        Include detailed comments explaining the code.
        """
        
        # Use LLM to generate implementation
        implementation = self.llm.query(prompt, config=llm_config)
        
        # Extract code from implementation (removing any markdown formatting)
        code = implementation
        if "```" in implementation:
            import re
            code_blocks = re.findall(r"```(?:\w+)?\n(.*?)```", implementation, re.DOTALL)
            if code_blocks:
                code = "\n\n".join(code_blocks)
        
        # Create safe file name from task title
        import re
        safe_filename = re.sub(r'[^\w\-_\.]', '_', task.title.lower().replace(' ', '_'))
        file_path = f"src/{safe_filename}.py"
        
        try:
            # Make sure the src directory exists
            src_dir = resolve_path("src", create_parents=True)
            
            # Create a snapshot
            snapshot_metadata = self.snapshot_engine.create_snapshot(
                file_path=file_path,
                content=code,
                commit_message=f"Implementation of {task.title}",
                tags=[task.task_type.value]
            )
            
            # Actually save the implementation file
            full_path = resolve_path(file_path, create_parents=True)
            with open(full_path, 'w') as f:
                f.write(code)
                
            # Update task status
            old_status = task.status
            task.status = TaskStatus.COMPLETED
            task.completion_percentage = 100.0
            task.actual_duration_hours = task.estimated_duration_hours  # In real system, we'd track actual time
            task.artifact_paths.append(file_path)
            
            # Notify about task completion
            if self.notification_manager:
                self.notification_manager.task_status_update(task, old_status)
            
            # Update dependencies
            self.task_graph.update_task_statuses()
            
            # Auto-save if needed
            self._save_project_state()
            
            return {
                "success": True,
                "task_id": str(task_id),
                "implementation": implementation,
                "file_path": file_path,
                "snapshot_id": snapshot_metadata.snapshot_id
            }
            
        except Exception as e:
            error_msg = f"Failed to implement task '{task.title}': {str(e)}"
            print(f"Implementation error: {error_msg}")
            
            # Update task status to failed
            old_status = task.status
            task.status = TaskStatus.FAILED
            
            # Notify about failure
            if self.notification_manager:
                self.notification_manager.error(error_msg)
            
            # Save state even on failure
            self._save_project_state()
            
            return {
                "success": False,
                "task_id": str(task_id),
                "error": error_msg
            }
    
    def get_project_status(self) -> Dict[str, Any]:
        """
        Get the current status of the project.
        
        Returns:
            Dictionary with project status details
        """
        # Count tasks by status
        status_counts = {}
        for task in self.task_graph.tasks.values():
            status_counts[task.status.value] = status_counts.get(task.status.value, 0) + 1
        
        # Calculate progress
        total_tasks = len(self.task_graph.tasks)
        completed_tasks = status_counts.get(TaskStatus.COMPLETED.value, 0)
        progress_percentage = (completed_tasks / total_tasks * 100) if total_tasks > 0 else 0
        
        # Get current plan
        current_plan = self.planning_history.get_current_plan()
        
        # Get risk assessment
        risk_assessment = {}
        if current_plan:
            risk_assessment = current_plan.risk_assessment
        
        # Compile status
        status = {
            "project_name": self.config.project_name,
            "state": self.project_state.value,
            "tasks": {
                "total": total_tasks,
                "by_status": status_counts
            },
            "epics": {
                "total": len(self.task_graph.epics),
                "completed": sum(1 for epic in self.task_graph.epics.values() 
                                if epic.milestone_percentage >= 100)
            },
            "progress": {
                "percentage": progress_percentage,
                "estimated_remaining_hours": sum(task.estimated_duration_hours 
                                               for task in self.task_graph.tasks.values()
                                               if task.status != TaskStatus.COMPLETED)
            },
            "current_plan": {
                "id": str(current_plan.id) if current_plan else None,
                "confidence": current_plan.confidence_score if current_plan else None,
                "remaining_tasks": len(current_plan.task_sequence) if current_plan else 0
            },
            "risk_assessment": risk_assessment
        }
        
        return status
    
    def summarize_progress(self, voice_summary: bool = True) -> str:
        """
        Generate a summary of project progress.
        
        Args:
            voice_summary: Whether to generate a voice summary
            
        Returns:
            Text summary of progress
        """
        # Get project status
        status = self.get_project_status()
        
        # Create summary text
        summary = f"""
        Project: {status['project_name']}
        State: {status['state']}
        
        Progress: {status['progress']['percentage']:.1f}% complete
        Tasks: {status['tasks']['total']} total, {status['tasks'].get('by_status', {}).get(TaskStatus.COMPLETED.value, 0)} completed
        Epics: {status['epics']['total']} total, {status['epics']['completed']} completed
        
        Estimated remaining work: {status['progress']['estimated_remaining_hours']:.1f} hours
        """
        
        # Generate voice summary if requested
        if voice_summary and self.notification_manager:
            self.notification_manager.progress(
                f"Project {status['project_name']} is {status['progress']['percentage']:.0f}% complete with "
                f"{status['tasks'].get('by_status', {}).get(TaskStatus.COMPLETED.value, 0)} of {status['tasks']['total']} "
                f"tasks completed. Estimated remaining work is {status['progress']['estimated_remaining_hours']:.1f} hours."
            )
        
        return summary.strip()
    
    def shutdown(self) -> None:
        """Save state and shut down the system."""
        # Save project state
        self._save_project_state()
        
        # Notify about shutdown
        if self.notification_manager:
            self.notification_manager.info(f"AgenDev system shutting down, project state saved.")
            
        logger.info(f"AgenDev system shutdown complete for project '{self.config.project_name}'")

#######################

#src\agendev\llm_integration.py#
#######################

# llm_integration.py
"""Enhanced LLM client for diverse query types and context management."""

from typing import List, Dict, Union, Optional, Any, Tuple, Callable
from dataclasses import dataclass
from pydantic import BaseModel, Field
import time
import json
import logging
from pathlib import Path

from .llm_module import LLMClient, Message
from .models.task_models import Task, TaskType
from .utils.fs_utils import save_json, load_json, resolve_path, save_snapshot

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LLMConfig(BaseModel):
    """Configuration for LLM requests."""
    model: str = "qwen2.5-7b-instruct"
    temperature: float = Field(0.7, ge=0.0, le=1.0)
    max_tokens: int = Field(2000, ge=0)
    stream: bool = False
    context_window_size: int = Field(16384, ge=0)
    
    # Retry configuration
    max_retries: int = 3
    retry_delay_seconds: float = 2.0
    
    # Context management
    include_history: bool = True
    max_history_messages: int = 10

class ContextWindow(BaseModel):
    """Manages conversation history and context."""
    system_message: Optional[str] = None
    messages: List[Dict[str, str]] = Field(default_factory=list)
    max_messages: int = 20
    
    def add_message(self, role: str, content: str) -> None:
        """Add a message to the context window."""
        self.messages.append({"role": role, "content": content})
        # Trim history if necessary
        if len(self.messages) > self.max_messages:
            # Keep system message if present, otherwise trim oldest messages
            if self.messages[0]["role"] == "system":
                self.messages = [self.messages[0]] + self.messages[-(self.max_messages-1):]
            else:
                self.messages = self.messages[-self.max_messages:]
    
    def get_messages(self) -> List[Dict[str, str]]:
        """Get all messages in the context window."""
        return self.messages.copy()
    
    def clear(self, keep_system: bool = True) -> None:
        """Clear all messages except optionally the system message."""
        if keep_system and self.messages and self.messages[0]["role"] == "system":
            self.messages = [self.messages[0]]
        else:
            self.messages = []
    
    def as_message_list(self) -> List[Message]:
        """Convert to a list of Message objects for LLMClient."""
        return [Message(role=msg["role"], content=msg["content"]) for msg in self.messages]

class LLMIntegration:
    """Enhanced LLM client for diverse query types and context management."""
    
    def __init__(
        self,
        base_url: str = "http://192.168.2.12:1234",
        config: Optional[LLMConfig] = None,
        system_message: Optional[str] = None
    ):
        """
        Initialize the LLM integration layer.
        
        Args:
            base_url: URL for the LLM API
            config: Configuration for LLM requests
            system_message: Default system message for all conversations
        """
        self.llm_client = LLMClient(base_url=base_url)
        self.config = config or LLMConfig()
        self.context = ContextWindow(max_messages=self.config.max_history_messages)
        
        # Set system message if provided
        if system_message:
            self.set_system_message(system_message)
            
        # Store task-specific configurations
        self.task_configs: Dict[TaskType, LLMConfig] = {}
        
        # Initialize with some defaults for different task types
        self._initialize_task_configs()
    
    def _initialize_task_configs(self) -> None:
        """Initialize default configurations for different task types."""
        self.task_configs = {
            TaskType.IMPLEMENTATION: LLMConfig(temperature=0.7, max_tokens=2000),
            TaskType.REFACTOR: LLMConfig(temperature=0.5, max_tokens=2000),
            TaskType.BUGFIX: LLMConfig(temperature=0.2, max_tokens=1500),
            TaskType.TEST: LLMConfig(temperature=0.4, max_tokens=2000),
            TaskType.DOCUMENTATION: LLMConfig(temperature=0.8, max_tokens=1500),
            TaskType.PLANNING: LLMConfig(temperature=0.9, max_tokens=3000),
        }
    
    def set_system_message(self, message: str) -> None:
        """Set or update the system message."""
        if self.context.messages and self.context.messages[0]["role"] == "system":
            self.context.messages[0]["content"] = message
        else:
            self.context.messages.insert(0, {"role": "system", "content": message})
    
    def query(
        self,
        prompt: str,
        config: Optional[LLMConfig] = None,
        clear_context: bool = False,
        save_to_context: bool = True
    ) -> str:
        """
        Send a query to the LLM and optionally update context.
        
        Args:
            prompt: The prompt to send
            config: Optional configuration override
            clear_context: Whether to clear context before sending
            save_to_context: Whether to save the interaction to context
            
        Returns:
            Response from the LLM
        """
        # Use provided config or default
        cfg = config or self.config
        
        # Optionally clear context
        if clear_context:
            self.context.clear()
        
        # Add user message to context if requested
        if save_to_context:
            self.context.add_message("user", prompt)
        
        # Prepare message list
        messages = self.context.as_message_list() if cfg.include_history else [Message(role="user", content=prompt)]
        
        # Add system message if not present
        if not messages or messages[0].role != "system":
            if self.context.system_message:
                messages.insert(0, Message(role="system", content=self.context.system_message))
        
        # Execute with retry logic
        response = self._execute_with_retry(
            lambda: self.llm_client.chat_completion(
                messages=messages,
                model=cfg.model,
                temperature=cfg.temperature,
                max_tokens=cfg.max_tokens,
                stream=cfg.stream
            )
        )
        
        # Save response to context if requested
        if save_to_context:
            self.context.add_message("assistant", response)
        
        return response
    
    def structured_query(
        self,
        prompt: str,
        json_schema: Dict,
        config: Optional[LLMConfig] = None,
        clear_context: bool = False,
        save_to_context: bool = True
    ) -> Dict:
        """
        Send a structured query to the LLM and get a JSON response.
        
        Args:
            prompt: The prompt to send
            json_schema: Schema for the structured output
            config: Optional configuration override
            clear_context: Whether to clear context before sending
            save_to_context: Whether to save the interaction to context
            
        Returns:
            Structured response from the LLM
        """
        # Use provided config or default
        cfg = config or self.config
        
        # Optionally clear context
        if clear_context:
            self.context.clear()
        
        # Add user message to context if requested
        if save_to_context:
            self.context.add_message("user", prompt)
        
        # Prepare message list
        messages = self.context.as_message_list() if cfg.include_history else [Message(role="user", content=prompt)]
        
        # Add system message if not present
        if not messages or messages[0].role != "system":
            if self.context.system_message:
                messages.insert(0, Message(role="system", content=self.context.system_message))
        
        # Execute with retry logic
        response = self._execute_with_retry(
            lambda: self.llm_client.structured_completion(
                messages=messages,
                json_schema=json_schema,
                model=cfg.model,
                temperature=cfg.temperature,
                max_tokens=cfg.max_tokens,
                stream=cfg.stream
            )
        )
        
        # Save string representation of response to context if requested
        if save_to_context:
            response_str = json.dumps(response, indent=2) if isinstance(response, dict) else str(response)
            self.context.add_message("assistant", response_str)
        
        return response
    
    def generate_embedding(self, content: str, model: Optional[str] = None) -> List[float]:
        """
        Generate an embedding for the given content.
        
        Args:
            content: Text to embed
            model: Optional embedding model override
            
        Returns:
            Embedding vector
        """
        return self._execute_with_retry(
            lambda: self.llm_client.get_embedding(
                texts=content,
                embedding_model=model or "text-embedding-nomic-embed-text-v1.5@q4_k_m"
            )
        )
    
    def clear_context(self, keep_system: bool = True) -> None:
        """
        Clear the context window.
        
        Args:
            keep_system: Whether to keep the system message
        """
        self.context.clear(keep_system=keep_system)
    
    def generate_code(
        self,
        prompt: str,
        language: str,
        file_path: Optional[str] = None,
        include_comments: bool = True,
        config: Optional[LLMConfig] = None,
        create_snapshot: bool = True
    ) -> str:
        """
        Generate code with an enhanced prompt for better structure preservation.
        
        Args:
            prompt: Description of the code to generate
            language: Programming language for the code
            file_path: Optional path to save the code
            include_comments: Whether to include comments in the generated code
            config: Optional configuration override
            create_snapshot: Whether to create a snapshot of the generated code
            
        Returns:
            Generated code
        """
        # Use lower temperature for code generation
        code_config = config or LLMConfig(temperature=0.4, max_tokens=3000)
        
        # Set system message for code generation
        system_message = f"""You are an expert {language} developer. 
        Generate clean, well-structured {language} code that follows best practices.
        {'Include detailed comments explaining the code.' if include_comments else 'Minimize comments, focus on clean code.'}
        Output ONLY the code with no additional text or explanations outside of code comments."""
        
        # Build the structured prompt
        structured_prompt = f"""
        Task: Generate {language} code
        
        Requirements:
        {prompt}
        
        Please generate the complete code. Do not include markdown code blocks or any explanatory text outside the code.
        """
        
        # Generate the code
        code = self.query(
            prompt=structured_prompt,
            config=code_config,
            clear_context=True,
            save_to_context=False
        )
        
        # Strip any markdown code blocks if present
        code = code.strip()
        if code.startswith("```") and code.endswith("```"):
            lines = code.split("\n")
            if len(lines) >= 2:
                code = "\n".join(lines[1:-1])
        
        # Save to file if path provided
        if file_path:
            resolved_path = resolve_path(file_path, create_parents=True)
            with open(resolved_path, 'w') as f:
                f.write(code)
            
            # Create snapshot if requested
            if create_snapshot:
                save_snapshot(code, file_path, metadata={"language": language})
        
        return code
    
    def get_task_config(self, task: Task) -> LLMConfig:
        """
        Get an appropriate configuration for a specific task.
        
        Args:
            task: The task to get configuration for
            
        Returns:
            LLM configuration tailored to the task
        """
        # Start with the default for this task type
        base_config = self.task_configs.get(task.task_type, self.config)
        
        # Override with task-specific parameters if available
        return LLMConfig(
            model=base_config.model,
            temperature=task.temperature if task.temperature is not None else base_config.temperature,
            max_tokens=task.max_tokens if task.max_tokens is not None else base_config.max_tokens,
            stream=base_config.stream,
            context_window_size=base_config.context_window_size,
            max_retries=base_config.max_retries,
            retry_delay_seconds=base_config.retry_delay_seconds,
            include_history=base_config.include_history,
            max_history_messages=base_config.max_history_messages
        )
    
    def update_task_config(self, task_type: TaskType, config: LLMConfig) -> None:
        """
        Update the configuration for a specific task type.
        
        Args:
            task_type: The task type to update configuration for
            config: The new configuration
        """
        self.task_configs[task_type] = config
    
    def _execute_with_retry(self, operation: Callable, max_retries: Optional[int] = None) -> Any:
        """
        Execute an operation with retry logic.
        
        Args:
            operation: Function to execute
            max_retries: Maximum number of retries
            
        Returns:
            Result of the operation
            
        Raises:
            Exception: If all retries fail
        """
        retries = max_retries or self.config.max_retries
        last_error = None
        
        for attempt in range(retries):
            try:
                return operation()
            except Exception as e:
                last_error = e
                if attempt < retries - 1:
                    logger.warning(f"Attempt {attempt + 1} failed: {str(e)}. Retrying in {self.config.retry_delay_seconds}s...")
                    time.sleep(self.config.retry_delay_seconds)
                else:
                    logger.error(f"All {retries} attempts failed.")
        
        raise last_error

#######################

#src\agendev\llm_module.py#
#######################

# llm_module.py
import requests
import json
from typing import List, Dict, Union
from dataclasses import dataclass

@dataclass
class Message:
    role: str
    content: str

class LLMClient:
    def __init__(self, base_url: str = "http://192.168.2.12:1234"):
        self.base_url = base_url
        self.headers = {"Content-Type": "application/json"}

    # -------------------------
    # Chat Completion Methods
    # -------------------------
    def chat_completion(
        self,
        messages: List[Message],
        model: str = "qwen2.5-7b-instruct",
        temperature: float = 0.7,
        max_tokens: int = -1,
        stream: bool = False
    ) -> Union[str, requests.Response]:
        endpoint = f"{self.base_url}/v1/chat/completions"
        payload = {
            "model": model,
            "messages": [{"role": msg.role, "content": msg.content} for msg in messages],
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": stream
        }
        response = requests.post(endpoint, headers=self.headers, json=payload)
        response.raise_for_status()

        if stream:
            return response
        return response.json()["choices"][0]["message"]["content"]

    # -------------------------
    # Structured JSON Completion
    # -------------------------
    def structured_completion(
        self,
        messages: List[Message],
        json_schema: Dict,
        model: str = "qwen2.5-7b-instruct",
        temperature: float = 0.7,
        max_tokens: int = -1,
        stream: bool = False
    ) -> Union[Dict, requests.Response]:
        endpoint = f"{self.base_url}/v1/chat/completions"
        payload = {
            "model": model,
            "messages": [{"role": msg.role, "content": msg.content} for msg in messages],
            "temperature": temperature,
            "max_tokens": max_tokens,
            "response_format": {
                "type": "json_schema",
                "json_schema": json_schema
            },
            "stream": stream
        }
        response = requests.post(endpoint, headers=self.headers, json=payload)
        response.raise_for_status()
        if stream:
            return response
        else:
            content = response.json()["choices"][0]["message"]["content"]
            # Parse the content as JSON since it should be a valid JSON string
            try:
                return json.loads(content)
            except json.JSONDecodeError as e:
                print(f"Error parsing JSON response: {e}")
                print(f"Raw content: {content}")
                # Return an empty dict as fallback
                return {}

    # -------------------------
    # Embedding Methods
    # -------------------------
    def get_embedding(
        self,
        texts: Union[str, List[str]],
        embedding_model: str = "text-embedding-nomic-embed-text-v1.5@q4_k_m"
    ) -> Union[List[float], List[List[float]]]:
        """Generate embeddings for given input text(s)."""
        endpoint = f"{self.base_url}/v1/embeddings"
        payload = {
            "model": embedding_model,
            "input": texts
        }
        response = requests.post(endpoint, headers=self.headers, json=payload)
        response.raise_for_status()
        embeddings_data = response.json()["data"]

        # Handle single or multiple embeddings
        if isinstance(texts, str):
            return embeddings_data[0]["embedding"]
        else:
            return [item["embedding"] for item in embeddings_data]

    # -------------------------
    # Streaming Helper
    # -------------------------
    def process_stream(self, response: requests.Response) -> str:
        accumulated_text = ""
        for line in response.iter_lines():
            if line:
                try:
                    json_response = json.loads(line.decode('utf-8').replace('data: ', ''))
                    chunk = json_response.get("choices", [{}])[0].get("delta", {}).get("content", "")
                    accumulated_text += chunk
                except json.JSONDecodeError:
                    continue
        return accumulated_text
    
    def stream_generator(self, response: requests.Response):
        """Generator that yields each token as it arrives from the stream"""
        for line in response.iter_lines():
            if line:
                try:
                    line_text = line.decode('utf-8')
                    if line_text.startswith('data: '):
                        json_response = json.loads(line_text.replace('data: ', ''))
                        chunk = json_response.get("choices", [{}])[0].get("delta", {}).get("content", "")
                        if chunk:
                            yield chunk
                except (json.JSONDecodeError, Exception) as e:
                    print(f"Error processing stream chunk: {str(e)}")
                    continue

# -------------------------
# Usage Example (Embedding)
# -------------------------
if __name__ == "__main__":
    client = LLMClient()

    # Chat completion usage example:
    messages = [
        Message(role="system", content="Always speak in rhymes."),
        Message(role="user", content="Tell me about your day.")
    ]
    chat_response = client.chat_completion(messages)
    print("\n\nChat Response:", chat_response)

    json_schema = {
        "name": "joke_response",
        "strict": "true",
        "schema": {
            "type": "object",
            "properties": {
                "joke": {"type": "string"}
            },
            "required": ["joke"] 
        }
    }
    messages = [
        Message(role="system", content="You are a helpful jokester."),
        Message(role="user", content="Tell me a joke.")
    ]

    structured_response = client.structured_completion(messages, json_schema)
    print("\n\nStructured Response:", structured_response)

    # Embedding usage example:
    embedding_response = client.get_embedding(["I feel happy today!"])
    print("\n\nEmbedding Response:", embedding_response)


#######################

#src\agendev\parameter_controller.py#
#######################

# parameter_controller.py
"""Dynamic adjustment of LLM parameters based on task type."""

from __future__ import annotations
from typing import Dict, List, Optional, Set, Union, Any, Tuple
from enum import Enum
from datetime import datetime
import math
import json
import random
from uuid import UUID, uuid4
from pydantic import BaseModel, Field, model_validator

from .models.task_models import Task, TaskType, TaskRisk, TaskPriority
from .llm_integration import LLMConfig
from .utils.fs_utils import resolve_path, load_json, save_json

class ParameterProfile(BaseModel):
    """Configuration profile for a specific task type."""
    task_type: TaskType
    temperature: float = Field(0.7, ge=0.0, le=1.0)
    max_tokens: int = Field(2000, ge=0)
    top_p: float = Field(1.0, ge=0.0, le=1.0)
    frequency_penalty: float = Field(0.0, ge=0.0, le=2.0)
    presence_penalty: float = Field(0.0, ge=0.0, le=2.0)
    
    # Success tracking
    success_count: int = 0
    failure_count: int = 0
    total_uses: int = 0
    
    @property
    def success_rate(self) -> float:
        """Calculate the success rate for this profile."""
        if self.total_uses == 0:
            return 0.5  # Default to 50% if unused
        return self.success_count / self.total_uses
    
    def to_llm_config(self) -> LLMConfig:
        """Convert to LLMConfig object."""
        return LLMConfig(
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )
    
    def record_outcome(self, success: bool) -> None:
        """Record the outcome of using this profile."""
        self.total_uses += 1
        if success:
            self.success_count += 1
        else:
            self.failure_count += 1

class AdjustmentStrategy(str, Enum):
    """Strategies for parameter adjustment."""
    CONSERVATIVE = "conservative"  # Small, gradual changes
    MODERATE = "moderate"          # Medium changes
    AGGRESSIVE = "aggressive"      # Larger changes, faster adaptation
    EXPLORATION = "exploration"    # Occasionally try very different values

class ParameterController:
    """Controls and adapts LLM parameters based on task characteristics and past performance."""
    
    def __init__(self, storage_path: Optional[str] = None):
        """
        Initialize the parameter controller.
        
        Args:
            storage_path: Optional path to store parameter data
        """
        self.storage_path = resolve_path(storage_path or "artifacts/models/parameters.json", create_parents=True)
        
        # Default profiles for each task type
        self.profiles: Dict[TaskType, ParameterProfile] = {}
        
        # Task-specific overrides
        self.task_overrides: Dict[UUID, ParameterProfile] = {}
        
        # Adjustment settings
        self.adjustment_strategy = AdjustmentStrategy.MODERATE
        self.exploration_rate = 0.1  # 10% chance of exploration
        
        # Load existing data
        self._initialize_profiles()
        self._load_profiles()
    
    def _initialize_profiles(self) -> None:
        """Initialize default profiles for all task types."""
        self.profiles = {
            TaskType.IMPLEMENTATION: ParameterProfile(
                task_type=TaskType.IMPLEMENTATION,
                temperature=0.7,
                max_tokens=2000
            ),
            TaskType.REFACTOR: ParameterProfile(
                task_type=TaskType.REFACTOR,
                temperature=0.5,
                max_tokens=2000
            ),
            TaskType.BUGFIX: ParameterProfile(
                task_type=TaskType.BUGFIX,
                temperature=0.2,
                max_tokens=1500
            ),
            TaskType.TEST: ParameterProfile(
                task_type=TaskType.TEST,
                temperature=0.4,
                max_tokens=2000
            ),
            TaskType.DOCUMENTATION: ParameterProfile(
                task_type=TaskType.DOCUMENTATION,
                temperature=0.8,
                max_tokens=1500
            ),
            TaskType.PLANNING: ParameterProfile(
                task_type=TaskType.PLANNING,
                temperature=0.9,
                max_tokens=3000
            )
        }
    
    def _load_profiles(self) -> None:
        """Load profiles from storage."""
        if not self.storage_path.exists():
            return
            
        try:
            data = load_json(self.storage_path)
            
            # Load profiles
            if "profiles" in data:
                for task_type_str, profile_data in data["profiles"].items():
                    try:
                        task_type = TaskType(task_type_str)
                        self.profiles[task_type] = ParameterProfile.model_validate(profile_data)
                    except Exception as e:
                        print(f"Error loading profile for {task_type_str}: {e}")
            
            # Load task overrides
            if "task_overrides" in data:
                for task_id_str, profile_data in data["task_overrides"].items():
                    try:
                        task_id = UUID(task_id_str)
                        self.task_overrides[task_id] = ParameterProfile.model_validate(profile_data)
                    except Exception as e:
                        print(f"Error loading task override for {task_id_str}: {e}")
            
            # Load adjustment settings
            if "adjustment_strategy" in data:
                try:
                    self.adjustment_strategy = AdjustmentStrategy(data["adjustment_strategy"])
                except:
                    pass
                    
            if "exploration_rate" in data:
                self.exploration_rate = float(data["exploration_rate"])
                
        except Exception as e:
            print(f"Error loading parameter profiles: {e}")
    
    def _save_profiles(self) -> None:
        """Save profiles to storage."""
        data = {
            "profiles": {
                task_type.value: profile.model_dump()
                for task_type, profile in self.profiles.items()
            },
            "task_overrides": {
                str(task_id): profile.model_dump()
                for task_id, profile in self.task_overrides.items()
            },
            "adjustment_strategy": self.adjustment_strategy.value,
            "exploration_rate": self.exploration_rate,
            "last_updated": datetime.now().isoformat()
        }
        
        save_json(data, self.storage_path)
    
    def get_profile_for_task(self, task: Task) -> ParameterProfile:
        """
        Get the parameter profile for a specific task.
        
        Args:
            task: The task to get parameters for
            
        Returns:
            Parameter profile for the task
        """
        # Check for task-specific override
        if task.id in self.task_overrides:
            return self.task_overrides[task.id]
            
        # Use task type profile
        if task.task_type in self.profiles:
            profile = self.profiles[task.task_type]
            
            # Apply task-specific adjustments
            adjusted_profile = self._adjust_for_task_properties(profile, task)
            
            # Potentially explore new parameters
            if random.random() < self.exploration_rate and self.adjustment_strategy == AdjustmentStrategy.EXPLORATION:
                adjusted_profile = self._explore_parameters(adjusted_profile)
                
            return adjusted_profile
            
        # Fallback to default
        return ParameterProfile(task_type=task.task_type)
    
    def _adjust_for_task_properties(self, profile: ParameterProfile, task: Task) -> ParameterProfile:
        """
        Adjust parameters based on task properties.
        
        Args:
            profile: Base parameter profile
            task: Task to adjust for
            
        Returns:
            Adjusted parameter profile
        """
        # Create a copy to modify
        adjusted = ParameterProfile.model_validate(profile.model_dump())
        
        # Adjust temperature based on task priority and risk
        if task.priority == TaskPriority.CRITICAL:
            # Lower temperature for critical tasks (more deterministic)
            adjusted.temperature *= 0.8
        elif task.priority == TaskPriority.LOW:
            # Higher temperature for low priority tasks (more creative)
            adjusted.temperature = min(1.0, adjusted.temperature * 1.2)
            
        if task.risk == TaskRisk.HIGH or task.risk == TaskRisk.CRITICAL:
            # Lower temperature for high-risk tasks
            adjusted.temperature *= 0.7
            
        # Adjust based on task complexity
        if task.estimated_complexity > 1.5:
            # Increase max_tokens for complex tasks
            adjusted.max_tokens = int(adjusted.max_tokens * 1.3)
            
        # Ensure parameters are within valid ranges
        adjusted.temperature = max(0.1, min(1.0, adjusted.temperature))
        adjusted.max_tokens = max(500, min(4000, adjusted.max_tokens))
        
        return adjusted
    
    def _explore_parameters(self, profile: ParameterProfile) -> ParameterProfile:
        """
        Explore new parameter values for learning.
        
        Args:
            profile: Base parameter profile
            
        Returns:
            Modified parameter profile with exploration
        """
        # Create a copy
        explored = ParameterProfile.model_validate(profile.model_dump())
        
        # Randomly adjust temperature within a reasonable range
        temp_change = random.uniform(-0.3, 0.3)
        explored.temperature = max(0.1, min(1.0, explored.temperature + temp_change))
        
        # Randomly adjust max_tokens
        token_factor = random.uniform(0.8, 1.5)
        explored.max_tokens = max(500, min(4000, int(explored.max_tokens * token_factor)))
        
        return explored
    
    def record_task_outcome(self, task: Task, success: bool, add_override: bool = False) -> None:
        """
        Record the outcome of a task to improve future parameter selection.
        
        Args:
            task: The task that was completed
            success: Whether the task was successful
            add_override: Whether to add a task-specific override
        """
        # Update the base profile
        if task.task_type in self.profiles:
            self.profiles[task.task_type].record_outcome(success)
            
        # Update task override if it exists
        if task.id in self.task_overrides:
            self.task_overrides[task.id].record_outcome(success)
            
        # Add a new task override if requested
        if add_override and task.id not in self.task_overrides:
            # Create a new profile based on what was likely used
            profile = self.get_profile_for_task(task)
            profile.record_outcome(success)
            self.task_overrides[task.id] = profile
            
        # Save changes
        self._save_profiles()
        
        # Adjust parameters based on outcomes
        self._adapt_parameters()
    
    def _adapt_parameters(self) -> None:
        """Adapt parameters based on outcomes."""
        for task_type, profile in self.profiles.items():
            # Only adapt if we have enough data
            if profile.total_uses < 5:
                continue
                
            # Determine adjustment magnitude based on strategy
            if self.adjustment_strategy == AdjustmentStrategy.CONSERVATIVE:
                magnitude = 0.05
            elif self.adjustment_strategy == AdjustmentStrategy.MODERATE:
                magnitude = 0.1
            elif self.adjustment_strategy == AdjustmentStrategy.AGGRESSIVE:
                magnitude = 0.2
            else:
                magnitude = 0.1
                
            # Calculate success rate
            success_rate = profile.success_rate
            
            # If success rate is poor, adjust parameters
            if success_rate < 0.5:
                # Make more conservative (lower temperature)
                new_temp = profile.temperature - magnitude
                profile.temperature = max(0.1, new_temp)
                
                # Increase max_tokens slightly
                profile.max_tokens = int(profile.max_tokens * 1.1)
            elif success_rate > 0.8 and profile.total_uses > 10:
                # If very successful, we can be slightly more adventurous
                new_temp = profile.temperature + (magnitude / 2)
                profile.temperature = min(1.0, new_temp)
    
    def create_task_specific_profile(self, task: Task, temperature: float, max_tokens: int) -> None:
        """
        Create a task-specific parameter profile.
        
        Args:
            task: The task to create a profile for
            temperature: Temperature value
            max_tokens: Max tokens value
        """
        profile = ParameterProfile(
            task_type=task.task_type,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        self.task_overrides[task.id] = profile
        self._save_profiles()
    
    def get_llm_config(self, task: Task) -> LLMConfig:
        """
        Get LLMConfig for a task.
        
        Args:
            task: The task to get config for
            
        Returns:
            LLMConfig for the task
        """
        profile = self.get_profile_for_task(task)
        return profile.to_llm_config()
    
    def set_adjustment_strategy(self, strategy: AdjustmentStrategy) -> None:
        """
        Set the adjustment strategy.
        
        Args:
            strategy: Strategy to use
        """
        self.adjustment_strategy = strategy
        self._save_profiles()
    
    def set_exploration_rate(self, rate: float) -> None:
        """
        Set the exploration rate.
        
        Args:
            rate: Exploration rate (0-1)
        """
        self.exploration_rate = max(0.0, min(1.0, rate))
        self._save_profiles()
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """
        Get performance statistics for different parameters.
        
        Returns:
            Dictionary of performance statistics
        """
        stats = {
            "task_types": {},
            "overall_success_rate": 0.0,
            "total_tasks": 0
        }
        
        total_success = 0
        total_tasks = 0
        
        for task_type, profile in self.profiles.items():
            if profile.total_uses > 0:
                stats["task_types"][task_type.value] = {
                    "success_rate": profile.success_rate,
                    "total_uses": profile.total_uses,
                    "temperature": profile.temperature,
                    "max_tokens": profile.max_tokens
                }
                
                total_success += profile.success_count
                total_tasks += profile.total_uses
        
        if total_tasks > 0:
            stats["overall_success_rate"] = total_success / total_tasks
            
        stats["total_tasks"] = total_tasks
        stats["adjustment_strategy"] = self.adjustment_strategy.value
        stats["exploration_rate"] = self.exploration_rate
        
        return stats

#######################

#src\agendev\probability_modeling.py#
#######################

# probability_modeling.py
"""Bayesian models for task completion probability."""

from __future__ import annotations
from typing import List, Dict, Optional, Set, Union, Any, Tuple
import math
import random
import time
import logging
import json
from uuid import UUID, uuid4
from datetime import datetime
import numpy as np
from pathlib import Path

from .models.task_models import Task, TaskStatus, TaskPriority, TaskRisk, TaskGraph
from .models.planning_models import SimulationResult
from .llm_integration import LLMIntegration, LLMConfig, Message
from .utils.fs_utils import safe_save_json, resolve_path, load_json

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ProbabilityDistribution:
    """Base class for probability distributions."""
    
    def sample(self) -> float:
        """Sample a value from the distribution."""
        raise NotImplementedError("Subclasses must implement this method")
    
    def mean(self) -> float:
        """Get the mean of the distribution."""
        raise NotImplementedError("Subclasses must implement this method")
    
    def variance(self) -> float:
        """Get the variance of the distribution."""
        raise NotImplementedError("Subclasses must implement this method")
    
    def confidence_interval(self, confidence: float = 0.95) -> Tuple[float, float]:
        """Get the confidence interval for the distribution."""
        raise NotImplementedError("Subclasses must implement this method")

class BetaDistribution(ProbabilityDistribution):
    """Beta distribution for modeling probabilities."""
    
    def __init__(self, alpha: float = 1.0, beta: float = 1.0):
        """
        Initialize the beta distribution.
        
        Args:
            alpha: Alpha parameter (successes + 1)
            beta: Beta parameter (failures + 1)
        """
        self.alpha = max(0.01, alpha)  # Avoid zero
        self.beta = max(0.01, beta)    # Avoid zero
    
    def sample(self) -> float:
        """Sample a value from the beta distribution."""
        return np.random.beta(self.alpha, self.beta)
    
    def mean(self) -> float:
        """Get the mean of the beta distribution."""
        return self.alpha / (self.alpha + self.beta)
    
    def variance(self) -> float:
        """Get the variance of the beta distribution."""
        denominator = (self.alpha + self.beta) ** 2 * (self.alpha + self.beta + 1)
        return (self.alpha * self.beta) / denominator
    
    def confidence_interval(self, confidence: float = 0.95) -> Tuple[float, float]:
        """
        Get the confidence interval for the beta distribution.
        
        Args:
            confidence: Confidence level (0-1)
            
        Returns:
            Tuple of (lower_bound, upper_bound)
        """
        # Use percentile method
        lower_percentile = (1 - confidence) / 2
        upper_percentile = 1 - lower_percentile
        
        try:
            from scipy import stats
            lower = stats.beta.ppf(lower_percentile, self.alpha, self.beta)
            upper = stats.beta.ppf(upper_percentile, self.alpha, self.beta)
            return (lower, upper)
        except ImportError:
            # Fallback if SciPy is not available
            samples = [self.sample() for _ in range(1000)]
            samples.sort()
            lower_idx = int(lower_percentile * 1000)
            upper_idx = int(upper_percentile * 1000)
            return (samples[lower_idx], samples[upper_idx])
    
    def update(self, successes: int, failures: int) -> None:
        """
        Update the distribution with new observations.
        
        Args:
            successes: Number of successes observed
            failures: Number of failures observed
        """
        self.alpha += successes
        self.beta += failures
    
    def to_dict(self) -> Dict[str, float]:
        """Convert to dictionary representation."""
        return {
            "alpha": self.alpha,
            "beta": self.beta,
            "mean": self.mean(),
            "variance": self.variance()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, float]) -> BetaDistribution:
        """Create from dictionary representation."""
        return cls(
            alpha=data.get("alpha", 1.0),
            beta=data.get("beta", 1.0)
        )

class TaskProbabilityModel:
    """Models task completion probabilities using Bayesian reasoning."""
    
    def __init__(
        self,
        task_graph: Optional[TaskGraph] = None,
        llm_integration: Optional[LLMIntegration] = None,
        model_id: Optional[str] = None
    ):
        """
        Initialize the task probability model.
        
        Args:
            task_graph: Optional task graph
            llm_integration: Optional LLM integration for generating priors
            model_id: Optional identifier for the model
        """
        self.task_graph = task_graph
        self.llm_integration = llm_integration
        self.model_id = model_id or f"model_{int(time.time())}"
        
        # Probability distributions for each task
        self.task_distributions: Dict[UUID, BetaDistribution] = {}
        
        # Dependency risk factors
        self.dependency_risks: Dict[Tuple[UUID, UUID], float] = {}
        
        # Statistics tracking
        self.simulation_results: Dict[UUID, List[SimulationResult]] = {}
        self.task_attempts: Dict[UUID, int] = {}
        self.task_successes: Dict[UUID, int] = {}
        
        # Initialize model directory
        self.model_dir = resolve_path(f"planning/simulations/{self.model_id}", create_parents=True)
        
        # Initialize distributions if task graph is provided
        if task_graph:
            self._initialize_distributions()
    
    def _initialize_distributions(self) -> None:
        """Initialize probability distributions for all tasks."""
        if not self.task_graph:
            return
            
        for task_id, task in self.task_graph.tasks.items():
            # Create initial distribution based on task metadata
            self.task_distributions[task_id] = self._create_prior_distribution(task)
            
            # Initialize statistics
            self.simulation_results[task_id] = []
            self.task_attempts[task_id] = 0
            self.task_successes[task_id] = 0
    
    def _create_prior_distribution(self, task: Task) -> BetaDistribution:
        """
        Create a prior distribution for a task based on its metadata.
        
        Args:
            task: The task to create a prior for
            
        Returns:
            Beta distribution representing prior beliefs
        """
        # Start with neutral prior
        alpha = 1.0
        beta = 1.0
        
        # Adjust based on risk level
        if task.risk == TaskRisk.LOW:
            alpha += 3.0  # More likely to succeed
            beta += 1.0
        elif task.risk == TaskRisk.MEDIUM:
            alpha += 2.0
            beta += 2.0
        elif task.risk == TaskRisk.HIGH:
            alpha += 1.0
            beta += 2.0
        elif task.risk == TaskRisk.CRITICAL:
            alpha += 1.0
            beta += 3.0  # More likely to fail
        
        # Adjust based on complexity
        complexity_factor = task.estimated_complexity
        if complexity_factor > 1.0:
            # More complex tasks are harder
            beta += (complexity_factor - 1.0) * 0.5
        
        # If LLM integration is available, refine with LLM
        if self.llm_integration:
            alpha, beta = self._refine_prior_with_llm(task, alpha, beta)
        
        return BetaDistribution(alpha, beta)
    
    def _refine_prior_with_llm(self, task: Task, alpha: float, beta: float) -> Tuple[float, float]:
        """
        Use LLM to refine the prior distribution.
        
        Args:
            task: The task to refine
            alpha: Initial alpha value
            beta: Initial beta value
            
        Returns:
            Tuple of (refined_alpha, refined_beta)
        """
        if not self.llm_integration:
            return alpha, beta
            
        try:
            # Create prompt for the LLM
            prompt = f"""
            Task: {task.title}
            Description: {task.description}
            Risk Level: {task.risk.value}
            Complexity: {task.estimated_complexity}
            Estimated Duration: {task.estimated_duration_hours} hours
            
            Based on the above information, estimate the probability of successful completion of this task.
            Provide your estimate as a percentage between 0 and 100.
            
            Also, estimate your confidence in this prediction on a scale of 1-10, where:
            1 = Very low confidence (high uncertainty)
            10 = Very high confidence (low uncertainty)
            """
            
            # Define the schema for structured output
            json_schema = {
                "name": "task_probability",
                "strict": "true",
                "schema": {
                    "type": "object",
                    "properties": {
                        "success_probability": {
                            "type": "number",
                            "description": "Estimated probability of success (0-100)",
                            "minimum": 0,
                            "maximum": 100
                        },
                        "confidence": {
                            "type": "number",
                            "description": "Confidence in the estimate (1-10)",
                            "minimum": 1,
                            "maximum": 10
                        }
                    },
                    "required": ["success_probability", "confidence"]
                }
            }
            
            # Get structured response from LLM
            response = self.llm_integration.structured_query(
                prompt=prompt,
                json_schema=json_schema,
                clear_context=True,
                save_to_context=False
            )
            
            # Extract values
            success_prob = response.get("success_probability", 50) / 100.0  # Convert to 0-1
            confidence = response.get("confidence", 5) / 10.0  # Convert to 0-1
            
            # Higher confidence = stronger prior
            strength = 1.0 + confidence * 9.0  # Scale to 1-10
            
            # Convert to alpha/beta
            new_alpha = 1.0 + success_prob * strength
            new_beta = 1.0 + (1.0 - success_prob) * strength
            
            # Blend with original prior (giving more weight to LLM)
            alpha = 0.3 * alpha + 0.7 * new_alpha
            beta = 0.3 * beta + 0.7 * new_beta
            
        except Exception as e:
            logger.warning(f"Error refining prior with LLM: {e}")
        
        return alpha, beta
    
    def get_task_probability(self, task_id: UUID) -> float:
        """
        Get the probability of successful completion for a task.
        
        Args:
            task_id: ID of the task
            
        Returns:
            Probability of success (0-1)
        """
        if task_id not in self.task_distributions:
            # If no distribution exists, create one
            if self.task_graph and task_id in self.task_graph.tasks:
                self.task_distributions[task_id] = self._create_prior_distribution(self.task_graph.tasks[task_id])
            else:
                # Default to 0.5 if task not found
                return 0.5
        
        return self.task_distributions[task_id].mean()
    
    def get_confidence_interval(self, task_id: UUID, confidence: float = 0.95) -> Tuple[float, float]:
        """
        Get the confidence interval for task completion probability.
        
        Args:
            task_id: ID of the task
            confidence: Confidence level (0-1)
            
        Returns:
            Tuple of (lower_bound, upper_bound)
        """
        if task_id not in self.task_distributions:
            # If no distribution exists, create one
            if self.task_graph and task_id in self.task_graph.tasks:
                self.task_distributions[task_id] = self._create_prior_distribution(self.task_graph.tasks[task_id])
            else:
                # Default wide interval if task not found
                return (0.25, 0.75)
        
        return self.task_distributions[task_id].confidence_interval(confidence)
    
    def update_from_simulation(self, task_id: UUID, result: SimulationResult) -> None:
        """
        Update the probability model with a simulation result.
        
        Args:
            task_id: ID of the task
            result: Result of the simulation
        """
        if task_id not in self.task_distributions:
            # If no distribution exists, create one
            if self.task_graph and task_id in self.task_graph.tasks:
                self.task_distributions[task_id] = self._create_prior_distribution(self.task_graph.tasks[task_id])
            else:
                # Skip if task not found
                return
        
        # Track the result
        if task_id not in self.simulation_results:
            self.simulation_results[task_id] = []
        self.simulation_results[task_id].append(result)
        
        # Update statistics
        self.task_attempts[task_id] = self.task_attempts.get(task_id, 0) + 1
        
        # Update distribution
        if result == SimulationResult.SUCCESS:
            self.task_successes[task_id] = self.task_successes.get(task_id, 0) + 1
            self.task_distributions[task_id].update(1, 0)
        elif result == SimulationResult.FAILURE:
            self.task_distributions[task_id].update(0, 1)
        else:
            # Partial success counts as half a success
            self.task_successes[task_id] = self.task_successes.get(task_id, 0) + 0.5
            self.task_distributions[task_id].update(0.5, 0.5)
    
    def update_dependency_risk(self, source_id: UUID, target_id: UUID, risk_factor: float) -> None:
        """
        Update the risk factor for a dependency.
        
        Args:
            source_id: ID of the source task
            target_id: ID of the target task
            risk_factor: Risk factor (0-1) where higher means more risk
        """
        self.dependency_risks[(source_id, target_id)] = max(0.0, min(1.0, risk_factor))
    
    def calculate_sequence_probability(self, task_sequence: List[UUID]) -> float:
        """
        Calculate the joint probability of a task sequence succeeding.
        
        Args:
            task_sequence: Sequence of task IDs
            
        Returns:
            Joint probability of sequence success
        """
        if not task_sequence:
            return 1.0
            
        # Initialize with 1.0 (certainty)
        joint_prob = 1.0
        
        # Track completed tasks
        completed_tasks = set()
        
        for task_id in task_sequence:
            # Get base task probability
            task_prob = self.get_task_probability(task_id)
            
            # Adjust for dependencies
            dependency_factor = 1.0
            
            for completed_id in completed_tasks:
                # Check if there is a dependency
                risk = self.dependency_risks.get((completed_id, task_id), 0.0)
                
                # Apply risk factor
                if risk > 0:
                    dependency_factor *= (1.0 - risk * 0.5)
            
            # Apply dependency adjustment
            adjusted_prob = task_prob * dependency_factor
            
            # Update joint probability
            joint_prob *= adjusted_prob
            
            # Add to completed tasks
            completed_tasks.add(task_id)
        
        return joint_prob
    
    def sample_sequence_outcome(self, task_sequence: List[UUID]) -> List[Tuple[UUID, SimulationResult]]:
        """
        Sample outcomes for a task sequence.
        
        Args:
            task_sequence: Sequence of task IDs
            
        Returns:
            List of (task_id, result) tuples
        """
        outcomes = []
        completed_tasks = set()
        
        for task_id in task_sequence:
            # Get base task distribution
            if task_id not in self.task_distributions:
                if self.task_graph and task_id in self.task_graph.tasks:
                    self.task_distributions[task_id] = self._create_prior_distribution(self.task_graph.tasks[task_id])
                else:
                    # Use default
                    self.task_distributions[task_id] = BetaDistribution(1.0, 1.0)
            
            distribution = self.task_distributions[task_id]
            
            # Sample from distribution
            success_prob = distribution.sample()
            
            # Adjust for dependencies
            for completed_id in completed_tasks:
                risk = self.dependency_risks.get((completed_id, task_id), 0.0)
                if risk > 0:
                    success_prob *= (1.0 - risk * 0.5)
            
            # Determine outcome
            random_value = random.random()
            if random_value < success_prob:
                result = SimulationResult.SUCCESS
            elif random_value < success_prob + (1 - success_prob) * 0.5:
                result = SimulationResult.PARTIAL_SUCCESS
            else:
                result = SimulationResult.FAILURE
            
            outcomes.append((task_id, result))
            
            # If task failed, subsequent tasks may be affected
            if result == SimulationResult.FAILURE:
                # Exit early if critical failure
                if self.task_graph and task_id in self.task_graph.tasks:
                    task = self.task_graph.tasks[task_id]
                    if task.risk == TaskRisk.CRITICAL:
                        break
            
            # Add to completed tasks if at least partial success
            if result != SimulationResult.FAILURE:
                completed_tasks.add(task_id)
        
        return outcomes
    
    def save(self) -> str:
        """
        Save the probability model to disk.
        
        Returns:
            Path to the saved model
        """
        model_path = self.model_dir / "model.json"
        
        # Prepare model data
        model_data = {
            "model_id": self.model_id,
            "timestamp": datetime.now().isoformat(),
            "task_distributions": {
                str(task_id): dist.to_dict()
                for task_id, dist in self.task_distributions.items()
            },
            "dependency_risks": {
                f"{source}_{target}": risk
                for (source, target), risk in self.dependency_risks.items()
            },
            "task_attempts": {
                str(task_id): attempts
                for task_id, attempts in self.task_attempts.items()
            },
            "task_successes": {
                str(task_id): successes
                for task_id, successes in self.task_successes.items()
            }
        }
        
        # Save to disk
        safe_save_json(model_data, model_path)
        
        # Save simulation results
        results_path = self.model_dir / "simulation_results.json"
        results_data = {
            str(task_id): [result.value for result in results]
            for task_id, results in self.simulation_results.items()
        }
        safe_save_json(results_data, results_path)
        
        return str(model_path)
    
    @classmethod
    def load(cls, model_id: str, task_graph: Optional[TaskGraph] = None) -> TaskProbabilityModel:
        """
        Load a probability model from disk.
        
        Args:
            model_id: ID of the model to load
            task_graph: Optional task graph
            
        Returns:
            Loaded probability model
        """
        model_dir = resolve_path(f"planning/simulations/{model_id}")
        model_path = model_dir / "model.json"
        
        if not model_path.exists():
            raise FileNotFoundError(f"Model not found: {model_path}")
        
        # Load model data
        model_data = load_json(model_path)
        
        # Create model
        model = cls(task_graph=task_graph, model_id=model_id)
        
        # Load distributions
        for task_id_str, dist_data in model_data.get("task_distributions", {}).items():
            task_id = UUID(task_id_str)
            model.task_distributions[task_id] = BetaDistribution.from_dict(dist_data)
        
        # Load dependency risks
        for key, risk in model_data.get("dependency_risks", {}).items():
            source_str, target_str = key.split("_")
            model.dependency_risks[(UUID(source_str), UUID(target_str))] = risk
        
        # Load statistics
        for task_id_str, attempts in model_data.get("task_attempts", {}).items():
            model.task_attempts[UUID(task_id_str)] = attempts
            
        for task_id_str, successes in model_data.get("task_successes", {}).items():
            model.task_successes[UUID(task_id_str)] = successes
        
        # Load simulation results
        results_path = model_dir / "simulation_results.json"
        if results_path.exists():
            results_data = load_json(results_path)
            for task_id_str, results in results_data.items():
                model.simulation_results[UUID(task_id_str)] = [
                    SimulationResult(result) for result in results
                ]
        
        return model
    
    def generate_task_report(self, task_id: UUID) -> Dict[str, Any]:
        """
        Generate a detailed report for a specific task.
        
        Args:
            task_id: ID of the task
            
        Returns:
            Dictionary with task report data
        """
        if task_id not in self.task_distributions:
            return {"error": "Task not found in probability model"}
            
        # Get task details
        task_name = "Unknown"
        task_description = ""
        if self.task_graph and task_id in self.task_graph.tasks:
            task = self.task_graph.tasks[task_id]
            task_name = task.title
            task_description = task.description
        
        # Get probability distribution
        distribution = self.task_distributions[task_id]
        success_prob = distribution.mean()
        confidence_interval = distribution.confidence_interval()
        
        # Get simulation statistics
        attempts = self.task_attempts.get(task_id, 0)
        successes = self.task_successes.get(task_id, 0)
        success_rate = successes / max(1, attempts) if attempts > 0 else float('nan')
        
        # Count result types
        results = self.simulation_results.get(task_id, [])
        result_counts = {
            SimulationResult.SUCCESS.value: 0,
            SimulationResult.PARTIAL_SUCCESS.value: 0,
            SimulationResult.FAILURE.value: 0
        }
        for result in results:
            result_counts[result.value] = result_counts.get(result.value, 0) + 1
        
        # Generate report
        report = {
            "task_id": str(task_id),
            "task_name": task_name,
            "task_description": task_description,
            "probability": {
                "success_probability": success_prob,
                "confidence_interval": confidence_interval,
                "alpha": distribution.alpha,
                "beta": distribution.beta,
                "variance": distribution.variance()
            },
            "simulations": {
                "total_attempts": attempts,
                "total_successes": successes,
                "success_rate": success_rate,
                "result_counts": result_counts
            },
            "dependencies": {
                "risk_factors": {
                    str(dep_id): risk
                    for (source, dep_id), risk in self.dependency_risks.items()
                    if source == task_id
                }
            }
        }
        
        return report

class ProjectRiskModel:
    """Models overall project risk using task probability models."""
    
    def __init__(
        self,
        task_probability_model: TaskProbabilityModel,
        task_graph: TaskGraph
    ):
        """
        Initialize the project risk model.
        
        Args:
            task_probability_model: Model for task probabilities
            task_graph: Task dependency graph
        """
        self.task_model = task_probability_model
        self.task_graph = task_graph
        
        # Cache for computed paths
        self.critical_path_cache: Optional[List[UUID]] = None
    
    def calculate_project_success_probability(self) -> float:
        """
        Calculate the overall probability of project success.
        
        Returns:
            Probability of project success (0-1)
        """
        # Get the critical path
        critical_path = self._get_critical_path()
        
        # Calculate the joint probability of the critical path
        return self.task_model.calculate_sequence_probability(critical_path)
    
    def _get_critical_path(self) -> List[UUID]:
        """
        Get the critical path through the task graph.
        
        Returns:
            List of task IDs on the critical path
        """
        if self.critical_path_cache is not None:
            return self.critical_path_cache
            
        self.critical_path_cache = self.task_graph.get_critical_path()
        return self.critical_path_cache
    
    def identify_risk_hotspots(self, threshold: float = 0.7) -> List[Dict[str, Any]]:
        """
        Identify high-risk tasks that could jeopardize the project.
        
        Args:
            threshold: Probability threshold for considering a task high-risk
            
        Returns:
            List of high-risk task details
        """
        critical_path = self._get_critical_path()
        hotspots = []
        
        for task_id in critical_path:
            prob = self.task_model.get_task_probability(task_id)
            
            if prob < threshold:
                # This is a risk hotspot
                task_name = "Unknown"
                if task_id in self.task_graph.tasks:
                    task_name = self.task_graph.tasks[task_id].title
                
                hotspots.append({
                    "task_id": str(task_id),
                    "task_name": task_name,
                    "success_probability": prob,
                    "confidence_interval": self.task_model.get_confidence_interval(task_id),
                    "is_on_critical_path": True
                })
        
        # Also check non-critical-path tasks with high dependencies
        for task_id, task in self.task_graph.tasks.items():
            if task_id in critical_path:
                continue  # Already checked
                
            # Check if this task has many dependents
            if len(task.dependents) >= 2:
                prob = self.task_model.get_task_probability(task_id)
                
                if prob < threshold:
                    hotspots.append({
                        "task_id": str(task_id),
                        "task_name": task.title,
                        "success_probability": prob,
                        "confidence_interval": self.task_model.get_confidence_interval(task_id),
                        "is_on_critical_path": False,
                        "dependent_count": len(task.dependents)
                    })
        
        # Sort by probability (ascending)
        return sorted(hotspots, key=lambda x: x["success_probability"])
    
    def monte_carlo_simulation(self, num_simulations: int = 1000) -> Dict[str, Any]:
        """
        Run Monte Carlo simulation to estimate project completion.
        
        Args:
            num_simulations: Number of simulations to run
            
        Returns:
            Dictionary with simulation results
        """
        completion_times = []
        success_counts = 0
        task_failures = {}
        
        # Get all planned tasks
        planned_tasks = [
            task_id for task_id, task in self.task_graph.tasks.items()
            if task.status == TaskStatus.PLANNED
        ]
        
        # Track the critical path
        critical_path = self._get_critical_path()
        
        for _ in range(num_simulations):
            # Reset for this simulation
            completion_time = 0
            all_succeeded = True
            completed_tasks = set()
            
            # Process tasks in topological order
            remaining_tasks = planned_tasks.copy()
            
            while remaining_tasks:
                # Find tasks that can be executed
                executable = []
                for task_id in remaining_tasks:
                    task = self.task_graph.tasks[task_id]
                    if all(dep_id in completed_tasks for dep_id in task.dependencies):
                        executable.append(task_id)
                
                if not executable:
                    # Deadlock - cannot complete all tasks
                    all_succeeded = False
                    break
                
                # Process executable tasks in parallel
                max_duration = 0
                for task_id in executable:
                    # Sample outcome
                    outcomes = self.task_model.sample_sequence_outcome([task_id])
                    if not outcomes:
                        continue
                        
                    _, result = outcomes[0]
                    
                    if result == SimulationResult.FAILURE:
                        # Task failed
                        all_succeeded = False
                        task_failures[task_id] = task_failures.get(task_id, 0) + 1
                    else:
                        # Task succeeded (fully or partially)
                        completed_tasks.add(task_id)
                        
                        # Add to completion time
                        task = self.task_graph.tasks[task_id]
                        task_duration = task.estimated_duration_hours
                        
                        # Partial success takes longer
                        if result == SimulationResult.PARTIAL_SUCCESS:
                            task_duration *= 1.5
                            
                        max_duration = max(max_duration, task_duration)
                
                # Update total completion time
                completion_time += max_duration
                
                # Remove processed tasks
                for task_id in executable:
                    if task_id in remaining_tasks:
                        remaining_tasks.remove(task_id)
            
            # Record results
            if all_succeeded:
                success_counts += 1
                completion_times.append(completion_time)
        
        # Calculate statistics
        success_rate = success_counts / num_simulations
        
        # Process completion times
        if completion_times:
            completion_times.sort()
            mean_time = sum(completion_times) / len(completion_times)
            p10 = completion_times[int(0.1 * len(completion_times))]
            p50 = completion_times[int(0.5 * len(completion_times))]
            p90 = completion_times[int(0.9 * len(completion_times))]
        else:
            mean_time = p10 = p50 = p90 = 0
        
        # Get failure points
        failure_points = [
            {
                "task_id": str(task_id),
                "task_name": self.task_graph.tasks[task_id].title if task_id in self.task_graph.tasks else "Unknown",
                "failure_count": count,
                "failure_rate": count / num_simulations,
                "is_on_critical_path": task_id in critical_path
            }
            for task_id, count in sorted(task_failures.items(), key=lambda x: x[1], reverse=True)
            if count > 0
        ]
        
        return {
            "simulations": num_simulations,
            "success_rate": success_rate,
            "completion_time": {
                "mean": mean_time,
                "p10": p10,
                "p50": p50,
                "p90": p90
            },
            "failure_points": failure_points[:10]  # Top 10 failure points
        }

#######################

#src\agendev\search_algorithms.py#
#######################

# search_algorithms.py
"""Implementation of MCTS and A* for development planning."""

from __future__ import annotations
from typing import List, Dict, Optional, Set, Union, Any, Tuple, Callable
import math
import random
import time
import logging
from uuid import UUID, uuid4
from datetime import datetime
from heapq import heappush, heappop
import numpy as np
import os

from .models.planning_models import (
    SearchNode, MCTSNode, AStarNode, SimulationConfig, 
    SimulationSession, PlanningPhase, SimulationResult
)
from .models.task_models import Task, TaskStatus, TaskGraph
from .llm_integration import LLMIntegration, LLMConfig
from .utils.fs_utils import safe_save_json, resolve_path

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MCTSPlanner:
    """Monte Carlo Tree Search implementation for development planning."""
    
    def __init__(
        self,
        task_graph: TaskGraph,
        llm_integration: Optional[LLMIntegration] = None,
        config: Optional[SimulationConfig] = None,
        session_id: Optional[UUID] = None
    ):
        """
        Initialize the MCTS planner.
        
        Args:
            task_graph: The graph of tasks and dependencies
            llm_integration: Optional LLM integration for evaluations
            config: Configuration for the simulation
            session_id: Optional ID for the simulation session
        """
        self.task_graph = task_graph
        self.llm_integration = llm_integration
        self.config = config or SimulationConfig()
        
        # Initialize nodes dictionary
        self.nodes: Dict[UUID, MCTSNode] = {}
        
        # Create root node
        self.root_node = self._create_root_node()
        
        # Initialize simulation session
        self.session = SimulationSession(
            id=session_id or uuid4(),
            config=self.config,
            root_node_id=self.root_node.id
        )
        
        # Track statistics
        self.total_simulations = 0
        self.successful_simulations = 0
        
        # Save session directory
        self.session_dir = resolve_path(f"planning/search_trees/{self.session.id}", create_parents=True)
    
    def _create_root_node(self) -> MCTSNode:
        """Create the root node for the search tree."""
        # Get all planned tasks
        planned_tasks = {
            task_id: task for task_id, task in self.task_graph.tasks.items()
            if task.status == TaskStatus.PLANNED
        }
        
        # Create root node with no completed tasks
        root = MCTSNode(
            node_type="task",
            task_sequence=[],
            completed_tasks=set(),
            depth=0
        )
        
        self.nodes[root.id] = root
        return root
    
    def run_simulation(self, iterations: Optional[int] = None, time_limit: Optional[float] = None) -> Dict[str, Any]:
        """
        Run MCTS simulation for a specified number of iterations or time limit.
        
        Args:
            iterations: Maximum number of iterations to run
            time_limit: Maximum time to run in seconds
            
        Returns:
            Simulation results
        """
        max_iterations = iterations or self.config.max_iterations
        max_time = time_limit or self.config.time_limit_seconds
        
        start_time = time.time()
        iteration = 0
        
        while (
            iteration < max_iterations and 
            (time.time() - start_time) < max_time
        ):
            # Run one iteration of MCTS
            result = self._run_iteration()
            iteration += 1
            
            # Update statistics
            self.total_simulations += 1
            if result == SimulationResult.SUCCESS:
                self.successful_simulations += 1
            
            # Update session
            self.session.iteration_count = iteration
            
            # Check if we should move to the next planning phase
            self._update_planning_phase(iteration, max_iterations)
            
            # Periodically save progress
            if iteration % 10 == 0:
                self._save_progress()
        
        # Complete the simulation
        elapsed_time = time.time() - start_time
        success_rate = self.successful_simulations / max(1, self.total_simulations)
        
        self.session.complete_simulation(success_rate)
        self._save_progress()
        
        # Return results
        return {
            "iterations": iteration,
            "elapsed_time": elapsed_time,
            "success_rate": success_rate,
            "best_sequence": self._get_best_sequence(),
            "session_id": str(self.session.id)
        }
    
    def _run_iteration(self) -> SimulationResult:
        """
        Run a single iteration of the MCTS algorithm.
        
        Returns:
            Result of the simulation
        """
        # 1. Selection: Select a promising node
        node = self._select_node(self.root_node)
        
        # 2. Expansion: Expand the selected node
        if node.visits > 0 and len(self._get_available_tasks(node)) > 0:
            node = self._expand_node(node)
        
        # 3. Simulation: Perform a rollout from the node
        result = self._rollout(node)
        
        # 4. Backpropagation: Update the path with the result
        self._backpropagate(node, result)
        
        return result
    
    def _select_node(self, node: MCTSNode) -> MCTSNode:
        """
        Select a promising node for expansion using UCB1.
        
        Args:
            node: Current node
            
        Returns:
            Selected node
        """
        # If node is a leaf node or hasn't been visited, return it
        if not node.children_ids or node.visits == 0:
            return node
        
        # Choose exploration/exploitation balance based on planning phase
        exploration_weight = self.config.exploration_weight
        if self.config.phase == PlanningPhase.EXPLOITATION:
            exploration_weight *= 0.5
        elif self.config.phase == PlanningPhase.FINALIZATION:
            exploration_weight *= 0.2
        
        best_score = float('-inf')
        best_child = None
        
        for child_id in node.children_ids:
            if child_id not in self.nodes:
                continue
                
            child = self.nodes[child_id]
            
            # Update scores with current exploration weight
            child.update_scores(exploration_weight)
            
            if child.value > best_score:
                best_score = child.value
                best_child = child
        
        # If no valid child found, return current node
        if best_child is None:
            return node
        
        # Recursive selection
        return self._select_node(best_child)
    
    def _expand_node(self, node: MCTSNode) -> MCTSNode:
        """
        Expand a node by adding a child.
        
        Args:
            node: Node to expand
            
        Returns:
            Newly created child node
        """
        # Get available tasks
        available_tasks = self._get_available_tasks(node)
        
        if not available_tasks:
            return node
        
        # Choose a random task
        task_id = random.choice(list(available_tasks))
        task = self.task_graph.tasks[task_id]
        
        # Create a new node
        new_task_sequence = node.task_sequence.copy() + [task_id]
        new_completed_tasks = node.completed_tasks.copy()
        new_completed_tasks.add(task_id)
        
        child = MCTSNode(
            node_type="task",
            parent_id=node.id,
            task_id=task_id,
            task_sequence=new_task_sequence,
            completed_tasks=new_completed_tasks,
            depth=node.depth + 1
        )
        
        # Add to nodes dictionary
        self.nodes[child.id] = child
        
        # Add child to parent
        if child.id not in node.children_ids:
            node.children_ids.append(child.id)
        
        return child
    
    def _rollout(self, node: MCTSNode) -> SimulationResult:
        """
        Perform a random rollout from the node.
        
        Args:
            node: Starting node for rollout
            
        Returns:
            Result of the simulation
        """
        # Copy current state
        completed_tasks = node.completed_tasks.copy()
        depth = node.depth
        
        # Continue rollout until we complete all tasks or reach max depth
        while depth < self.config.max_depth:
            # Get available tasks
            available_tasks = {
                task_id: task for task_id, task in self.task_graph.tasks.items()
                if task_id not in completed_tasks and 
                all(dep_id in completed_tasks for dep_id in task.dependencies)
            }
            
            # If no more tasks available
            if not available_tasks:
                # Check if we completed all planned tasks
                all_planned_tasks = {
                    task_id for task_id, task in self.task_graph.tasks.items()
                    if task.status == TaskStatus.PLANNED
                }
                
                if all_planned_tasks.issubset(completed_tasks):
                    return SimulationResult.SUCCESS
                else:
                    return SimulationResult.PARTIAL_SUCCESS
            
            # Choose a random task
            task_id = random.choice(list(available_tasks.keys()))
            
            # Add to completed tasks
            completed_tasks.add(task_id)
            depth += 1
        
        # If we reached max depth without completing all tasks
        return SimulationResult.PARTIAL_SUCCESS
    
    def _backpropagate(self, node: MCTSNode, result: SimulationResult) -> None:
        """
        Backpropagate the result up the tree.
        
        Args:
            node: Starting node for backpropagation
            result: Result of the simulation
        """
        current = node
        while current is not None:
            # Increment visit count
            current.mark_visited()
            
            # Add result
            current.add_simulation_result(result)
            
            # Move to parent
            if current.parent_id:
                current = self.nodes.get(current.parent_id)
            else:
                current = None
    
    def _get_available_tasks(self, node: MCTSNode) -> Dict[UUID, Task]:
        """
        Get tasks that can be executed next given the current state.
        
        Args:
            node: Current node
            
        Returns:
            Dictionary of available tasks
        """
        return {
            task_id: task for task_id, task in self.task_graph.tasks.items()
            if task_id not in node.completed_tasks and
            task.status == TaskStatus.PLANNED and
            all(dep_id in node.completed_tasks for dep_id in task.dependencies)
        }
    
    def _update_planning_phase(self, iteration: int, max_iterations: int) -> None:
        """Update the planning phase based on progress."""
        progress = iteration / max_iterations
        
        # Transition through phases based on progress
        if progress >= self.config.phase_thresholds["exploitation_to_finalization"]:
            self.config.phase = PlanningPhase.FINALIZATION
        elif progress >= self.config.phase_thresholds["exploration_to_exploitation"]:
            self.config.phase = PlanningPhase.EXPLOITATION
        else:
            self.config.phase = PlanningPhase.EXPLORATION
    
    def _get_best_sequence(self) -> List[UUID]:
        """
        Get the best task sequence found by MCTS.
        
        Returns:
            List of task IDs in the recommended sequence
        """
        # Start with the root
        current = self.root_node
        sequence = []
        
        # Follow the path with highest win rate
        while current.children_ids:
            best_win_rate = -1
            best_child = None
            
            for child_id in current.children_ids:
                if child_id not in self.nodes:
                    continue
                    
                child = self.nodes[child_id]
                
                if child.win_rate > best_win_rate:
                    best_win_rate = child.win_rate
                    best_child = child
            
            if best_child is None:
                break
                
            if best_child.task_id:
                sequence.append(best_child.task_id)
                
            current = best_child
        
        return sequence
    
    def _save_progress(self) -> None:
        """Save the current state of the simulation."""
        # Save session data
        session_path = self.session_dir / "session.json"
        safe_save_json(self.session.model_dump(), session_path)
        
        # Save top nodes (root + children)
        nodes_path = self.session_dir / "nodes.json"
        
        # Only save root and its immediate children to avoid huge files
        nodes_to_save = {
            str(self.root_node.id): self.root_node.model_dump()
        }
        
        for child_id in self.root_node.children_ids:
            if child_id in self.nodes:
                nodes_to_save[str(child_id)] = self.nodes[child_id].model_dump()
        
        safe_save_json(nodes_to_save, nodes_path)
        
        # Save best sequence
        sequence_path = self.session_dir / "best_sequence.json"
        sequence_data = {
            "task_ids": [str(task_id) for task_id in self._get_best_sequence()],
            "win_rate": self.successful_simulations / max(1, self.total_simulations),
            "total_simulations": self.total_simulations
        }
        safe_save_json(sequence_data, sequence_path)

    def save_session(self) -> None:
        """Save the current session state to disk."""
        if not self.session_dir.exists():
            os.makedirs(self.session_dir, exist_ok=True)
        
        # Save session metadata
        session_data = {
            "id": self.session.id,
            "timestamp": datetime.now().isoformat(),
            "statistics": {
                "total_simulations": self.total_simulations,
                "successful_simulations": self.successful_simulations
            },
            "config": {
                "max_iterations": self.config.max_iterations,
                "exploration_weight": self.config.exploration_weight
            }
        }
        
        # Use safe_save_json instead of save_json
        safe_save_json(session_data, self.session_dir / "session.json")
        
        # Save tree nodes
        self.save_tree()
    
    def save_tree(self) -> None:
        """Save the current tree state to disk."""
        if not self.session_dir.exists():
            os.makedirs(self.session_dir, exist_ok=True)
        
        # Serialize the nodes
        serialized_nodes = {}
        for node_id, node in self.nodes.items():
            serialized_nodes[str(node_id)] = {
                "id": str(node.id),
                "parent_id": str(node.parent_id) if node.parent_id else None,
                "state": [str(task_id) for task_id in node.state],
                "untried_actions": [str(task_id) for task_id in node.untried_actions],
                "children": [str(child_id) for child_id in node.children],
                "visits": node.visits,
                "value": node.value,
                "heuristic": node.heuristic
            }
        
        # Save nodes
        safe_save_json(serialized_nodes, self.session_dir / "nodes.json")
        
        # Save best sequence if available
        if self._get_best_sequence():
            sequence_data = {
                "sequence": [str(task_id) for task_id in self._get_best_sequence()],
                "value": self.successful_simulations / max(1, self.total_simulations),
                "timestamp": datetime.now().isoformat()
            }
            safe_save_json(sequence_data, self.session_dir / "best_sequence.json")

class AStarPathfinder:
    """A* search implementation for finding optimal task sequences."""
    
    def __init__(
        self,
        task_graph: TaskGraph,
        heuristic_function: Optional[Callable[[UUID, Set[UUID]], float]] = None,
        config: Optional[Dict[str, Any]] = None
    ):
        """
        Initialize the A* pathfinder.
        
        Args:
            task_graph: The graph of tasks and dependencies
            heuristic_function: Optional custom heuristic function
            config: Optional configuration
        """
        self.task_graph = task_graph
        self.heuristic_function = heuristic_function or self._default_heuristic
        self.config = config or {
            "heuristic_weight": 1.0,
            "max_iterations": 1000
        }
        
        # Nodes and results
        self.nodes: Dict[UUID, AStarNode] = {}
        self.came_from: Dict[UUID, UUID] = {}
        self.g_score: Dict[UUID, float] = {}
        self.f_score: Dict[UUID, float] = {}
    
    def find_path(
        self, 
        start_tasks: Optional[List[UUID]] = None,
        goal_condition: Optional[Callable[[Set[UUID]], bool]] = None
    ) -> Tuple[List[UUID], Dict[str, Any]]:
        """
        Find the optimal path through the task graph.
        
        Args:
            start_tasks: Optional list of starting tasks
            goal_condition: Optional custom goal condition
            
        Returns:
            Tuple of (path, metadata)
        """
        # Initialize
        if start_tasks is None:
            # Default to tasks with no dependencies
            start_tasks = [
                task_id for task_id, task in self.task_graph.tasks.items()
                if not task.dependencies and task.status == TaskStatus.PLANNED
            ]
        
        if goal_condition is None:
            # Default goal is to complete all planned tasks
            planned_tasks = {
                task_id for task_id, task in self.task_graph.tasks.items()
                if task.status == TaskStatus.PLANNED
            }
            goal_condition = lambda completed: planned_tasks.issubset(completed)
        
        # Create start node
        start_node = AStarNode(
            node_type="sequence",
            task_sequence=[],
            completed_tasks=set(),
            remaining_tasks={
                task_id for task_id, task in self.task_graph.tasks.items()
                if task.status == TaskStatus.PLANNED
            }
        )
        self.nodes[start_node.id] = start_node
        
        # Open and closed sets
        open_set = [(0, start_node.id)]  # Priority queue (f_score, node_id)
        closed_set = set()
        
        # Initialize scores
        self.g_score = {start_node.id: 0}
        self.f_score = {start_node.id: self._calculate_heuristic(start_node.id)}
        
        # Search variables
        iterations = 0
        path_found = False
        goal_node_id = None
        
        # A* search
        while open_set and iterations < self.config["max_iterations"]:
            iterations += 1
            
            # Get node with lowest f_score
            current_f, current_id = heappop(open_set)
            
            if current_id in closed_set:
                continue
                
            current = self.nodes[current_id]
            
            # Check if we've reached the goal
            if goal_condition(current.completed_tasks):
                path_found = True
                goal_node_id = current_id
                break
            
            # Mark as processed
            closed_set.add(current_id)
            
            # Get available next tasks
            available_tasks = {
                task_id: task for task_id, task in self.task_graph.tasks.items()
                if task_id not in current.completed_tasks and
                task.status == TaskStatus.PLANNED and
                all(dep_id in current.completed_tasks for dep_id in task.dependencies)
            }
            
            # Try each available task
            for task_id, task in available_tasks.items():
                # Create new node
                new_task_sequence = current.task_sequence.copy() + [task_id]
                new_completed_tasks = current.completed_tasks.copy()
                new_completed_tasks.add(task_id)
                new_remaining_tasks = current.remaining_tasks.copy()
                new_remaining_tasks.remove(task_id)
                
                neighbor = AStarNode(
                    node_type="sequence",
                    parent_id=current_id,
                    task_id=task_id,
                    task_sequence=new_task_sequence,
                    completed_tasks=new_completed_tasks,
                    remaining_tasks=new_remaining_tasks,
                    depth=current.depth + 1
                )
                
                # Skip if we've already processed this exact state
                state_key = frozenset(new_completed_tasks)
                if any(n.completed_tasks == new_completed_tasks for n in self.nodes.values() if n.id in closed_set):
                    continue
                
                # Add to nodes dictionary
                self.nodes[neighbor.id] = neighbor
                
                # Calculate scores
                tentative_g_score = self.g_score[current_id] + task.estimated_duration_hours
                
                if neighbor.id not in self.g_score or tentative_g_score < self.g_score[neighbor.id]:
                    # This is a better path
                    self.came_from[neighbor.id] = current_id
                    self.g_score[neighbor.id] = tentative_g_score
                    
                    h_score = self._calculate_heuristic(neighbor.id)
                    self.f_score[neighbor.id] = tentative_g_score + h_score
                    
                    # Update node scores
                    neighbor.update_scores(tentative_g_score, h_score)
                    
                    # Add to open set
                    heappush(open_set, (self.f_score[neighbor.id], neighbor.id))
        
        # Reconstruct path if found
        path = []
        metadata = {
            "iterations": iterations,
            "nodes_explored": len(closed_set),
            "path_found": path_found
        }
        
        if path_found and goal_node_id:
            path = self._reconstruct_path(goal_node_id)
            metadata["path_length"] = len(path)
            metadata["estimated_duration"] = sum(
                self.task_graph.tasks[task_id].estimated_duration_hours
                for task_id in path
            )
        
        return path, metadata
    
    def _reconstruct_path(self, goal_id: UUID) -> List[UUID]:
        """
        Reconstruct the path from start to goal.
        
        Args:
            goal_id: ID of the goal node
            
        Returns:
            List of task IDs in the optimal sequence
        """
        path = []
        current_id = goal_id
        
        while current_id in self.nodes:
            current = self.nodes[current_id]
            if current.task_id:
                path.append(current.task_id)
            
            if current_id not in self.came_from:
                break
                
            current_id = self.came_from[current_id]
        
        return list(reversed(path))
    
    def _calculate_heuristic(self, node_id: UUID) -> float:
        """
        Calculate the heuristic value for a node.
        
        Args:
            node_id: ID of the node
            
        Returns:
            Heuristic value
        """
        node = self.nodes[node_id]
        
        if not node.remaining_tasks:
            return 0.0
            
        # Use the custom heuristic function if provided, otherwise use default
        return self.heuristic_function(node_id, node.remaining_tasks) * self.config["heuristic_weight"]
    
    def _default_heuristic(self, node_id: UUID, remaining_tasks: Set[UUID]) -> float:
        """
        Default heuristic function based on remaining task duration.
        
        Args:
            node_id: ID of the node
            remaining_tasks: Set of remaining task IDs
            
        Returns:
            Heuristic value
        """
        if not remaining_tasks:
            return 0.0
            
        # Sum up the estimated duration of all remaining tasks
        return sum(
            self.task_graph.tasks[task_id].estimated_duration_hours
            for task_id in remaining_tasks
            if task_id in self.task_graph.tasks
        )
    
    def critical_path_heuristic(self, node_id: UUID, remaining_tasks: Set[UUID]) -> float:
        """
        Heuristic based on the critical path through remaining tasks.
        
        Args:
            node_id: ID of the node
            remaining_tasks: Set of remaining task IDs
            
        Returns:
            Heuristic value
        """
        if not remaining_tasks:
            return 0.0
            
        # Create a subgraph of remaining tasks
        subgraph = TaskGraph()
        
        for task_id in remaining_tasks:
            if task_id in self.task_graph.tasks:
                subgraph.tasks[task_id] = self.task_graph.tasks[task_id]
        
        # Find dependencies within the subgraph
        for task_id, task in subgraph.tasks.items():
            for dep_id in task.dependencies:
                if dep_id in subgraph.tasks:
                    # This dependency is within the subgraph
                    dep_task = self.task_graph.tasks[dep_id]
                    
                    # Create dependency object
                    dependency = {
                        "source_id": dep_id,
                        "target_id": task_id,
                        "dependency_type": "blocks",
                        "strength": 1.0
                    }
                    
                    subgraph.dependencies.append(dependency)
        
        # Get the critical path
        critical_path = subgraph.get_critical_path()
        
        # Sum up durations along the critical path
        return sum(
            self.task_graph.tasks[task_id].estimated_duration_hours
            for task_id in critical_path
            if task_id in self.task_graph.tasks
        )

#######################

#src\agendev\snapshot_engine.py#
#######################

# snapshot_engine.py
"""Binary differential storage for local version control."""

from __future__ import annotations
from typing import List, Dict, Optional, Union, Any, Tuple, Iterator
from datetime import datetime
from pathlib import Path
import os
import difflib
import hashlib
from uuid import uuid4
from pydantic import BaseModel, Field, model_validator

from .utils.fs_utils import (
    resolve_path, save_snapshot, list_snapshots, get_latest_snapshot,
    load_json, save_json, safe_save_json, content_hash
)

class SnapshotMetadata(BaseModel):
    """Metadata for a code snapshot."""
    snapshot_id: str = Field(default_factory=lambda: uuid4().hex[:10])
    file_path: str
    timestamp: datetime = Field(default_factory=datetime.now)
    hash: str
    parent_snapshot_id: Optional[str] = None
    commit_message: Optional[str] = None
    tags: List[str] = Field(default_factory=list)
    author: str = "agendev"
    
    @model_validator(mode='after')
    def validate_snapshot(self) -> 'SnapshotMetadata':
        """Ensure snapshot has required fields."""
        if not self.file_path or not self.hash:
            raise ValueError("Snapshot must have file_path and hash")
        return self

class SnapshotDiff(BaseModel):
    """Represents differences between snapshots."""
    source_id: str
    target_id: str
    added_lines: int = 0
    removed_lines: int = 0
    changed_lines: int = 0
    diff_content: str
    
    def summary(self) -> str:
        """Get a human-readable summary of the changes."""
        return f"+{self.added_lines} -{self.removed_lines} ~{self.changed_lines}"

class Branch(BaseModel):
    """Represents a development branch."""
    name: str
    head_snapshot_id: Optional[str] = None
    created_at: datetime = Field(default_factory=datetime.now)
    snapshots: List[str] = Field(default_factory=list)
    
    @model_validator(mode='after')
    def validate_branch(self) -> 'Branch':
        """Validate branch data and set defaults."""
        # Ensure created_at is a valid datetime
        if not self.created_at or isinstance(self.created_at, str) and not self.created_at:
            self.created_at = datetime.now()
        return self

class SnapshotEngine:
    """Manages code snapshots and version history."""
    
    def __init__(self, workspace_dir: Optional[Union[str, Path]] = None):
        """
        Initialize the snapshot engine.
        
        Args:
            workspace_dir: Optional workspace directory override
        """
        self.workspace_dir = resolve_path(workspace_dir or "")
        self.snapshots_dir = resolve_path("artifacts/snapshots", create_parents=True)
        self.metadata_dir = self.snapshots_dir / "metadata"
        self.branches_file = self.snapshots_dir / "branches.json"
        
        # Create directories if they don't exist
        os.makedirs(self.metadata_dir, exist_ok=True)
        
        # Initialize branches
        self.branches: Dict[str, Branch] = {}
        self._load_branches()
        
        # Ensure main branch exists
        if "main" not in self.branches:
            self.create_branch("main")
    
    def _load_branches(self) -> None:
        """Load branch information from disk."""
        if not self.branches_file.exists():
            return
            
        branches_data = load_json(self.branches_file)
        for branch_name, branch_data in branches_data.get("branches", {}).items():
            # Fix empty created_at
            if "created_at" in branch_data and (not branch_data["created_at"] or branch_data["created_at"] == ""):
                branch_data["created_at"] = datetime.now().isoformat()
            try:
                self.branches[branch_name] = Branch.model_validate(branch_data)
            except Exception as e:
                # If validation fails, create a new branch with default values
                self.branches[branch_name] = Branch(
                    name=branch_name,
                    head_snapshot_id=branch_data.get("head_snapshot_id"),
                    snapshots=branch_data.get("snapshots", [])
                )
    
    def _save_branches(self) -> None:
        """Save branch information to disk."""
        branches_data = {
            "branches": {
                name: branch.model_dump() for name, branch in self.branches.items()
            }
        }
        safe_save_json(branches_data, self.branches_file)
    
    def create_snapshot(
        self,
        file_path: Union[str, Path],
        content: str,
        branch: str = "main",
        commit_message: Optional[str] = None,
        tags: Optional[List[str]] = None
    ) -> SnapshotMetadata:
        """
        Create a snapshot of a file.
        
        Args:
            file_path: Path to the file to snapshot
            content: Content of the file
            branch: Branch to create the snapshot on
            commit_message: Optional commit message
            tags: Optional tags for the snapshot
            
        Returns:
            Metadata for the created snapshot
        """
        # Resolve file path
        resolved_path = Path(file_path)
        relative_path = resolved_path.name if resolved_path.is_absolute() else str(resolved_path)
        
        # Ensure branch exists
        if branch not in self.branches:
            self.create_branch(branch)
        
        # Get previous snapshot if any
        parent_id = None
        if self.branches[branch].head_snapshot_id:
            parent_id = self.branches[branch].head_snapshot_id
        
        # Create snapshot
        hash_value, snapshot_path = save_snapshot(
            content=content,
            file_path=file_path,
            metadata={
                "branch": branch,
                "commit_message": commit_message,
                "tags": tags or []
            }
        )
        
        # Create metadata
        metadata = SnapshotMetadata(
            file_path=relative_path,
            hash=hash_value,
            parent_snapshot_id=parent_id,
            commit_message=commit_message,
            tags=tags or []
        )
        
        # Save metadata
        metadata_path = self.metadata_dir / f"{metadata.snapshot_id}.json"
        safe_save_json(metadata.model_dump(), metadata_path)
        
        # Update branch
        branch_obj = self.branches[branch]
        branch_obj.head_snapshot_id = metadata.snapshot_id
        branch_obj.snapshots.append(metadata.snapshot_id)
        self._save_branches()
        
        return metadata
    
    def get_snapshots(self, file_path: Union[str, Path], branch: Optional[str] = None) -> List[SnapshotMetadata]:
        """
        Get snapshots for a file.
        
        Args:
            file_path: Path to the file
            branch: Optional branch to filter by
            
        Returns:
            List of snapshot metadata
        """
        snapshots = []
        file_path_str = str(Path(file_path).name if Path(file_path).is_absolute() else file_path)
        
        # List all snapshot metadata files
        for metadata_file in self.metadata_dir.glob("*.json"):
            try:
                data = load_json(metadata_file)
                metadata = SnapshotMetadata.model_validate(data)
                
                # Filter by file path
                if metadata.file_path == file_path_str:
                    # Filter by branch if specified
                    if branch is None or metadata.snapshot_id in self.branches.get(branch, Branch(name="")).snapshots:
                        snapshots.append(metadata)
            except Exception as e:
                print(f"Error loading snapshot metadata {metadata_file}: {e}")
        
        # Sort by timestamp (newest first)
        return sorted(snapshots, key=lambda x: x.timestamp, reverse=True)
    
    def get_snapshot_content(self, snapshot_id: str) -> Optional[str]:
        """
        Get the content of a snapshot.
        
        Args:
            snapshot_id: ID of the snapshot
            
        Returns:
            Content of the snapshot or None if not found
        """
        # Find metadata for the snapshot
        metadata_path = self.metadata_dir / f"{snapshot_id}.json"
        if not metadata_path.exists():
            return None
            
        metadata_data = load_json(metadata_path)
        metadata = SnapshotMetadata.model_validate(metadata_data)
        
        # Try to find the snapshot file
        snapshot_glob = list(self.snapshots_dir.glob(f"**/*_{metadata.hash[:8]}*"))
        if not snapshot_glob:
            return None
            
        snapshot_path = snapshot_glob[0]
        
        try:
            with open(snapshot_path, 'r') as f:
                return f.read()
        except Exception as e:
            print(f"Error reading snapshot {snapshot_id}: {e}")
            return None
    
    def get_all_snapshots(self, branch: Optional[str] = None) -> List[SnapshotMetadata]:
        """
        Get all snapshots across all files.
        
        Args:
            branch: Optional branch to filter by
            
        Returns:
            List of snapshot metadata
        """
        snapshots = []
        
        # List all snapshot metadata files
        for metadata_file in self.metadata_dir.glob("*.json"):
            try:
                data = load_json(metadata_file)
                metadata = SnapshotMetadata.model_validate(data)
                
                # Filter by branch if specified
                if branch is None or metadata.snapshot_id in self.branches.get(branch, Branch(name="")).snapshots:
                    snapshots.append(metadata)
            except Exception as e:
                print(f"Error loading snapshot metadata {metadata_file}: {e}")
        
        # Sort by timestamp (newest first)
        return sorted(snapshots, key=lambda x: x.timestamp, reverse=True)
    
    def compare_snapshots(self, source_id: str, target_id: str) -> Optional[SnapshotDiff]:
        """
        Compare two snapshots.
        
        Args:
            source_id: ID of the source snapshot
            target_id: ID of the target snapshot
            
        Returns:
            Snapshot diff or None if either snapshot not found
        """
        source_content = self.get_snapshot_content(source_id)
        target_content = self.get_snapshot_content(target_id)
        
        if source_content is None or target_content is None:
            return None
            
        # Generate diff
        source_lines = source_content.splitlines()
        target_lines = target_content.splitlines()
        
        diff = list(difflib.unified_diff(
            source_lines,
            target_lines,
            fromfile=f"snapshot_{source_id}",
            tofile=f"snapshot_{target_id}",
            lineterm=''
        ))
        
        # Count changes
        added_lines = sum(1 for line in diff if line.startswith('+') and not line.startswith('+++'))
        removed_lines = sum(1 for line in diff if line.startswith('-') and not line.startswith('---'))
        changed_lines = min(added_lines, removed_lines)  # Estimate
        added_lines -= changed_lines
        removed_lines -= changed_lines
        
        return SnapshotDiff(
            source_id=source_id,
            target_id=target_id,
            added_lines=added_lines,
            removed_lines=removed_lines,
            changed_lines=changed_lines,
            diff_content='\n'.join(diff)
        )
    
    def create_branch(self, name: str, base_branch: str = "main") -> Branch:
        """
        Create a new branch.
        
        Args:
            name: Name of the branch
            base_branch: Branch to base the new branch on
            
        Returns:
            The created branch
        """
        if name in self.branches:
            return self.branches[name]
            
        head_id = None
        if base_branch in self.branches and self.branches[base_branch].head_snapshot_id:
            head_id = self.branches[base_branch].head_snapshot_id
            
        branch = Branch(name=name, head_snapshot_id=head_id)
        
        if head_id:
            branch.snapshots.append(head_id)
            
        self.branches[name] = branch
        self._save_branches()
        
        return branch
    
    def switch_branch(self, branch: str) -> bool:
        """
        Switch to a branch.
        
        Args:
            branch: Name of the branch to switch to
            
        Returns:
            Whether the switch was successful
        """
        if branch not in self.branches:
            return False
            
        # Nothing special to do here as branches are just pointers
        return True
    
    def get_branch_history(self, branch: str) -> List[SnapshotMetadata]:
        """
        Get the history of a branch.
        
        Args:
            branch: Name of the branch
            
        Returns:
            List of snapshot metadata in chronological order
        """
        if branch not in self.branches:
            return []
            
        snapshots = []
        for snapshot_id in self.branches[branch].snapshots:
            metadata_path = self.metadata_dir / f"{snapshot_id}.json"
            if metadata_path.exists():
                data = load_json(metadata_path)
                snapshots.append(SnapshotMetadata.model_validate(data))
        
        # Sort by timestamp
        return sorted(snapshots, key=lambda x: x.timestamp)
    
    def restore_snapshot(self, snapshot_id: str, output_path: Optional[Union[str, Path]] = None) -> Optional[str]:
        """
        Restore a snapshot to a file.
        
        Args:
            snapshot_id: ID of the snapshot to restore
            output_path: Optional path to restore to
            
        Returns:
            Path to the restored file or None if snapshot not found
        """
        content = self.get_snapshot_content(snapshot_id)
        if content is None:
            return None
            
        # Find metadata for the snapshot
        metadata_path = self.metadata_dir / f"{snapshot_id}.json"
        if not metadata_path.exists():
            return None
            
        metadata_data = load_json(metadata_path)
        metadata = SnapshotMetadata.model_validate(metadata_data)
        
        # Determine output path
        if output_path is None:
            output_path = resolve_path(metadata.file_path)
            
        output_path = Path(output_path)
        os.makedirs(output_path.parent, exist_ok=True)
        
        # Write content
        with open(output_path, 'w') as f:
            f.write(content)
            
        return str(output_path)

#######################

#src\agendev\test_generation.py#
#######################

# test_generation.py
"""Automatic generation of unit and integration tests."""

from __future__ import annotations
from typing import List, Dict, Optional, Set, Union, Any, Tuple
from pathlib import Path
import os
import re
import ast
import inspect
from enum import Enum
import importlib.util
from pydantic import BaseModel, Field, model_validator

from .llm_integration import LLMIntegration, LLMConfig, Message
from .context_management import ContextManager
from .utils.fs_utils import resolve_path, save_json, load_json

class TestType(str, Enum):
    """Types of tests that can be generated."""
    UNIT = "unit"
    INTEGRATION = "integration"
    PROPERTY = "property"
    PERFORMANCE = "performance"

class CodeElement(BaseModel):
    """Represents a code element to test."""
    name: str
    element_type: str  # "function", "class", "method"
    source_file: str
    line_start: int
    line_end: int
    code: str
    imports: List[str] = Field(default_factory=list)
    dependencies: List[str] = Field(default_factory=list)

class TestCase(BaseModel):
    """Represents a test case."""
    id: str = Field(default_factory=lambda: f"test_{id(object)}")
    name: str
    description: str
    test_type: TestType
    target_element: CodeElement
    test_code: str
    created_at: str = Field(default_factory=lambda: __import__('datetime').datetime.now().isoformat())
    
    @model_validator(mode='after')
    def validate_test_case(self) -> 'TestCase':
        """Ensure the test case is valid."""
        if not self.test_code or not self.target_element:
            raise ValueError("Test case must have code and target element")
        return self

class TestSuite(BaseModel):
    """Represents a test suite."""
    name: str
    description: str
    target_module: str
    test_cases: List[TestCase] = Field(default_factory=list)
    imports: List[str] = Field(default_factory=list)
    setup_code: str = ""
    teardown_code: str = ""

class TestGenerator:
    """Generates tests for code."""
    
    def __init__(
        self,
        llm_integration: LLMIntegration,
        context_manager: Optional[ContextManager] = None,
        output_dir: Optional[Union[str, Path]] = None
    ):
        """
        Initialize the test generator.
        
        Args:
            llm_integration: LLM integration for generating tests
            context_manager: Optional context manager for code understanding
            output_dir: Optional directory for test output
        """
        self.llm = llm_integration
        self.context_manager = context_manager
        self.output_dir = resolve_path(output_dir or "quality/tests", create_parents=True)
        
        # Set up system message for test generation
        self.llm.set_system_message(
            "You are an expert Python test engineer specialized in generating high-quality test cases."
            "Your tests should be comprehensive, follow best practices, and use pytest."
        )
    
    def extract_code_elements(self, file_path: Union[str, Path]) -> List[CodeElement]:
        """
        Extract testable code elements from a file.
        
        Args:
            file_path: Path to the file
            
        Returns:
            List of code elements
        """
        elements = []
        file_path = resolve_path(file_path)
        
        if not file_path.exists():
            return []
            
        try:
            # Read the file
            with open(file_path, 'r') as f:
                content = f.read()
                
            # Parse the AST
            tree = ast.parse(content)
            
            # Track imports
            imports = []
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for name in node.names:
                        imports.append(name.name)
                elif isinstance(node, ast.ImportFrom):
                    module = node.module or ""
                    for name in node.names:
                        imports.append(f"{module}.{name.name}")
            
            # Extract functions
            for node in tree.body:
                if isinstance(node, ast.FunctionDef):
                    # Skip private functions
                    if node.name.startswith('_') and not node.name.startswith('__'):
                        continue
                        
                    # Get the function code
                    func_lines = content.splitlines()[node.lineno-1:node.end_lineno]
                    func_code = '\n'.join(func_lines)
                    
                    elements.append(CodeElement(
                        name=node.name,
                        element_type="function",
                        source_file=str(file_path),
                        line_start=node.lineno,
                        line_end=node.end_lineno,
                        code=func_code,
                        imports=imports
                    ))
                    
                elif isinstance(node, ast.ClassDef):
                    # Extract class and its methods
                    class_lines = content.splitlines()[node.lineno-1:node.end_lineno]
                    class_code = '\n'.join(class_lines)
                    
                    elements.append(CodeElement(
                        name=node.name,
                        element_type="class",
                        source_file=str(file_path),
                        line_start=node.lineno,
                        line_end=node.end_lineno,
                        code=class_code,
                        imports=imports
                    ))
                    
                    # Extract methods
                    for method in [n for n in node.body if isinstance(n, ast.FunctionDef)]:
                        # Skip private methods
                        if method.name.startswith('_') and not method.name.startswith('__'):
                            continue
                            
                        method_lines = content.splitlines()[method.lineno-1:method.end_lineno]
                        method_code = '\n'.join(method_lines)
                        
                        elements.append(CodeElement(
                            name=f"{node.name}.{method.name}",
                            element_type="method",
                            source_file=str(file_path),
                            line_start=method.lineno,
                            line_end=method.end_lineno,
                            code=method_code,
                            imports=imports,
                            dependencies=[node.name]  # The class is a dependency
                        ))
            
        except Exception as e:
            print(f"Error extracting code elements from {file_path}: {e}")
            
        return elements
    
    def generate_test_case(
        self,
        element: CodeElement,
        test_type: TestType = TestType.UNIT
    ) -> TestCase:
        """
        Generate a test case for a code element.
        
        Args:
            element: Code element to test
            test_type: Type of test to generate
            
        Returns:
            Generated test case
        """
        # Prepare the prompt
        prompt = self._create_test_prompt(element, test_type)
        
        # Use LLM to generate test
        test_code = self.llm.query(prompt)
        
        # Clean up the code (remove markdown formatting if present)
        test_code = self._clean_code(test_code)
        
        # Create test case
        return TestCase(
            name=f"test_{element.name.lower().replace('.', '_')}",
            description=f"Test for {element.element_type} {element.name}",
            test_type=test_type,
            target_element=element,
            test_code=test_code
        )
    
    def _create_test_prompt(self, element: CodeElement, test_type: TestType) -> str:
        """
        Create a prompt for test generation.
        
        Args:
            element: Code element to test
            test_type: Type of test to generate
            
        Returns:
            Prompt for the LLM
        """
        # Get related context if context manager is available
        related_context = ""
        if self.context_manager:
            # Use context manager to find related code
            similar_elements = self.context_manager.find_similar_elements(element.code, top_k=3)
            if similar_elements:
                related_context = "\n\nRelated code context:\n"
                for elem, score in similar_elements:
                    if elem.content != element.code:  # Skip exact matches
                        related_context += f"\n```python\n{elem.content}\n```\n"
        
        # Build the prompt
        prompt = f"""
        Generate a comprehensive {test_type.value} test for the following Python {element.element_type}:
        
        ```python
        {element.code}
        ```
        
        File: {element.source_file}
        
        Imports in the file:
        {', '.join(element.imports)}
        
        {related_context}
        
        Requirements:
        1. Use pytest for the test framework
        2. Include detailed test cases covering edge cases
        3. Use appropriate mocking for external dependencies
        4. Include docstrings explaining the tests
        5. Follow best practices for Python testing
        
        Only return the Python test code, properly formatted and ready to use.
        Do not include explanations outside of code comments.
        """
        
        return prompt
    
    def _clean_code(self, code: str) -> str:
        """
        Clean generated code.
        
        Args:
            code: Code to clean
            
        Returns:
            Cleaned code
        """
        # Remove markdown code blocks if present
        if code.startswith("```python"):
            code = re.sub(r"^```python\n", "", code)
            code = re.sub(r"\n```$", "", code)
        elif code.startswith("```"):
            code = re.sub(r"^```\n", "", code)
            code = re.sub(r"\n```$", "", code)
            
        return code.strip()
    
    def generate_test_suite(
        self,
        module_path: Union[str, Path],
        test_types: List[TestType] = [TestType.UNIT]
    ) -> TestSuite:
        """
        Generate a test suite for a module.
        
        Args:
            module_path: Path to the module
            test_types: Types of tests to generate
            
        Returns:
            Generated test suite
        """
        module_path = resolve_path(module_path)
        
        if not module_path.exists():
            raise FileNotFoundError(f"Module not found: {module_path}")
            
        # Extract code elements
        elements = self.extract_code_elements(module_path)
        
        # Create test suite
        suite = TestSuite(
            name=f"Test{module_path.stem.capitalize()}",
            description=f"Tests for {module_path.stem}",
            target_module=str(module_path),
            imports=[
                "import pytest",
                f"import {module_path.stem.replace('-', '_')}",
                "from unittest import mock"
            ]
        )
        
        # Generate test cases for each element
        for element in elements:
            for test_type in test_types:
                test_case = self.generate_test_case(element, test_type)
                suite.test_cases.append(test_case)
        
        # Generate setup and teardown code if needed
        if len(suite.test_cases) > 1:
            suite.setup_code = self._generate_setup_code(suite, elements)
            suite.teardown_code = self._generate_teardown_code(suite)
        
        return suite
    
    def _generate_setup_code(self, suite: TestSuite, elements: List[CodeElement]) -> str:
        """
        Generate setup code for a test suite.
        
        Args:
            suite: Test suite
            elements: Code elements
            
        Returns:
            Generated setup code
        """
        # Check if there are classes that might need fixtures
        has_classes = any(e.element_type == "class" for e in elements)
        
        if has_classes:
            return """
@pytest.fixture
def setup_test_environment():
    # Set up any resources needed for tests
    yield
    # Clean up resources after tests
"""
        return ""
    
    def _generate_teardown_code(self, suite: TestSuite) -> str:
        """
        Generate teardown code for a test suite.
        
        Args:
            suite: Test suite
            
        Returns:
            Generated teardown code
        """
        return ""
    
    def save_test_suite(self, suite: TestSuite) -> str:
        """
        Save a test suite to a file.
        
        Args:
            suite: Test suite to save
            
        Returns:
            Path to the saved file
        """
        # Create test file path
        module_name = Path(suite.target_module).stem
        test_file_path = self.output_dir / f"test_{module_name}.py"
        
        # Generate the test file content
        content = self._generate_test_file_content(suite)
        
        # Save the file
        os.makedirs(test_file_path.parent, exist_ok=True)
        with open(test_file_path, 'w') as f:
            f.write(content)
            
        # Save test suite metadata
        metadata_path = self.output_dir / f"test_{module_name}_metadata.json"
        save_json(suite.model_dump(), metadata_path)
        
        return str(test_file_path)
    
    def _generate_test_file_content(self, suite: TestSuite) -> str:
        """
        Generate test file content from a test suite.
        
        Args:
            suite: Test suite
            
        Returns:
            Generated file content
        """
        lines = [
            "# Auto-generated test file",
            f"# Target: {suite.target_module}",
            f"# Generated at: {__import__('datetime').datetime.now().isoformat()}",
            ""
        ]
        
        # Add imports
        for import_line in suite.imports:
            lines.append(import_line)
        lines.append("")
        
        # Add setup code
        if suite.setup_code:
            lines.append(suite.setup_code)
            lines.append("")
        
        # Add test cases
        for test_case in suite.test_cases:
            lines.append(f"# {test_case.description}")
            lines.append(test_case.test_code)
            lines.append("")
        
        # Add teardown code
        if suite.teardown_code:
            lines.append(suite.teardown_code)
        
        return "\n".join(lines)
    
    def generate_tests_for_directory(
        self,
        directory: Union[str, Path],
        test_types: List[TestType] = [TestType.UNIT],
        pattern: str = "*.py"
    ) -> List[str]:
        """
        Generate tests for all matching files in a directory.
        
        Args:
            directory: Directory to scan
            test_types: Types of tests to generate
            pattern: File pattern to match
            
        Returns:
            List of generated test file paths
        """
        directory = resolve_path(directory)
        
        if not directory.exists() or not directory.is_dir():
            raise ValueError(f"Not a valid directory: {directory}")
            
        # Find all Python files
        python_files = list(directory.glob(pattern))
        
        # Generate tests for each file
        test_files = []
        for py_file in python_files:
            # Skip test files and __init__.py
            if py_file.name.startswith("test_") or py_file.name == "__init__.py":
                continue
                
            try:
                suite = self.generate_test_suite(py_file, test_types)
                test_file = self.save_test_suite(suite)
                test_files.append(test_file)
            except Exception as e:
                print(f"Error generating tests for {py_file}: {e}")
        
        return test_files

#######################

#src\agendev\tts_module.py#
#######################

# tts_module.py
import os
import json
import time
import tempfile
import shutil
from typing import List, Optional, Dict, Any, Literal, Union
from pathlib import Path
from uuid import uuid4
import threading

import requests
from pydantic import BaseModel, Field, field_validator

# For audio playback
import soundfile as sf
import sounddevice as sd

# Output directory for generated audio files
OUTPUT_DIRECTORY = "generated"
DEFAULT_FILENAME = "output_voice.wav"

# Global variable to track if audio is playing
is_audio_playing = False

class GenerateAudioRequest(BaseModel):
    text: str
    voice: str = Field(default="af_nicole")
    speed: float = Field(default=1.0, ge=0.1, le=2.0)

    @field_validator('text')
    @classmethod
    def validate_text(cls, v: str) -> str:
        if not v.strip():
            raise ValueError("Text cannot be empty")
        return v

class TTSClient:
    def __init__(self, base_url: str = "http://127.0.0.1:7860"):
        self.base_url = base_url.rstrip('/')
        self.session = requests.Session()
        
        try:
            self.session.get(f"{self.base_url}", timeout=5).raise_for_status()
        except requests.exceptions.RequestException as e:
            raise ConnectionError(f"Could not connect to TTS API at {self.base_url}: {e}")
    
    def _wait_for_completion(self, file_path: str, max_wait_time: int = 120) -> bool:
        if not os.path.exists(file_path):
            return False
            
        start_time = time.time()
        last_size = 0
        
        while time.time() - start_time < max_wait_time:
            try:
                current_size = os.path.getsize(file_path)
                if current_size > 0 and current_size == last_size:
                    with open(file_path, 'rb') as f:
                        header = f.read(44)
                        if header[:4] == b'RIFF' and header[8:12] == b'WAVE':
                            time.sleep(1.0)
                            return True
                last_size = current_size
            except:
                pass
            
            time.sleep(0.5)
            
        return os.path.exists(file_path) and os.path.getsize(file_path) > 0
    
    def generate_audio(self, request: GenerateAudioRequest, save_to: Optional[str] = None) -> Dict[str, Any]:
        """
        Generate audio from text using the TTS API
        
        Parameters:
        request: GenerateAudioRequest - The request parameters
        save_to: Optional[str] - Path to save the audio file
        
        Returns:
        Dict containing audio_path and phoneme_sequence
        """
        api_data = [
            request.text,
            request.voice,
            request.speed
        ]
        
        # Use the correct endpoint from your API call example
        endpoint = "/gradio_api/call/generate_first"
        
        response = self.session.post(
            f"{self.base_url}{endpoint}",
            json={"data": api_data},
            headers={"Content-Type": "application/json"}
        )
        response.raise_for_status()
        
        response_json = response.json()
        event_id = response_json.get("event_id")
        
        if not event_id:
            raise ValueError("No event_id in response")
            
        stream_url = f"{self.base_url}{endpoint}/{event_id}"
        stream_response = self.session.get(stream_url, stream=True)
        stream_response.raise_for_status()
        
        data_content = None
        
        for line in stream_response.iter_lines():
            if not line:
                continue
                
            decoded_line = line.decode('utf-8')
            
            if decoded_line.startswith('data:'):
                data_json = decoded_line[5:].strip()
                try:
                    data_content = json.loads(data_json)
                    break
                except:
                    continue
        
        if not data_content or not isinstance(data_content, list) or len(data_content) < 2:
            raise ValueError(f"Invalid data content: {data_content}")
            
        audio_info = data_content[0] 
        phoneme_sequence = data_content[1]
        
        result = {
            "audio_info": audio_info,
            "phoneme_sequence": phoneme_sequence
        }
        
        if isinstance(audio_info, dict) and 'path' in audio_info:
            audio_path = audio_info['path']
            
            if save_to:
                output_path = save_to
            else:
                temp_dir = tempfile.gettempdir()
                temp_filename = f"tts_audio_{uuid4()}.wav"
                output_path = os.path.join(temp_dir, temp_filename)
            
            if os.path.exists(audio_path):
                self._wait_for_completion(audio_path)
            
            if os.path.exists(audio_path):
                shutil.copy2(audio_path, output_path)
                result["audio_path"] = output_path
            
            elif audio_path.startswith(('http://', 'https://')) or audio_info.get('url'):
                url = audio_path if audio_path.startswith(('http://', 'https://')) else audio_info.get('url')
                
                try:
                    audio_response = self.session.get(url, stream=True)
                    audio_response.raise_for_status()
                    
                    with open(output_path, 'wb') as f:
                        for chunk in audio_response.iter_content(chunk_size=8192):
                            f.write(chunk)
                            
                    result["audio_path"] = output_path
                except Exception as e:
                    print(f"Error downloading audio: {e}")
            
            result["audio_path"] = audio_path if os.path.exists(audio_path) else output_path
        
        return result

def get_output_path(filename: Optional[str] = None) -> str:
    """Create output directory and return file path"""
    os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)
    
    if not filename:
        filename = DEFAULT_FILENAME
    
    return os.path.join(OUTPUT_DIRECTORY, filename)

def play_audio(file_path: str):
    """
    Play audio file using sounddevice and soundfile in a non-blocking way
    
    Parameters:
    file_path: str - Path to the audio file to play
    """
    global is_audio_playing
    
    # If audio is already playing, don't start a new one
    if is_audio_playing:
        print("Audio is already playing, not starting a new playback")
        return
        
    def _play_in_background():
        global is_audio_playing
        try:
            # Set flag to indicate audio is playing
            is_audio_playing = True
            
            # Load audio file
            data, samplerate = sf.read(file_path)
            
            # Play audio
            sd.play(data, samplerate)
            
            # Wait until file is done playing
            sd.wait()
        except Exception as e:
            print(f"Error playing audio: {e}")
        finally:
            # Reset flag when done
            is_audio_playing = False
    
    # Start audio playback in a separate thread
    audio_thread = threading.Thread(target=_play_in_background)
    audio_thread.daemon = True  # Thread will exit when main program exits
    audio_thread.start()

def text_to_speech(
    text: str, 
    voice: str = "af_nicole", 
    speed: float = 1.0,
    output_path: Optional[str] = None,
    auto_play: bool = True
) -> Dict[str, Any]:
    """
    Convert text to speech using the TTS API
    
    Parameters:
    text: str - The text to convert to speech
    voice: str - The voice to use (e.g. "af_nicole", "af_heart")
    speed: float - The speech speed (0.1 to 2.0)
    output_path: Optional[str] - Path to save the audio file
    auto_play: bool - Whether to automatically play the audio after generation
    
    Returns:
    Dict containing audio_path and phoneme_sequence
    """
    global is_audio_playing
    
    client = TTSClient()
    
    if output_path is None:
        output_path = get_output_path()
    
    request = GenerateAudioRequest(
        text=text,
        voice=voice,
        speed=speed
    )
    
    result = client.generate_audio(request, save_to=output_path)
    
    if auto_play and "audio_path" in result and os.path.exists(result["audio_path"]):
        # Only play if no audio is currently playing
        if not is_audio_playing:
            play_audio(result["audio_path"])
    
    return result

def get_available_voices() -> List[str]:
    """
    Return a list of example voices that we know work with the API
    Note: The actual list may be different based on your TTS backend
    
    Returns:
    List of voice IDs
    """
    return ['af_heart', 'af_bella', 'af_nicole', 'af_aoede', 'af_kore', 'af_sarah', 'af_nova', 'af_sky', 'af_alloy', 'af_jessica', 'af_river', 'am_michael', 'am_fenrir', 'am_puck', 'am_echo', 'am_eric', 'am_liam', 'am_onyx', 'am_santa', 'am_adam', 'bf_emma', 'bf_isabella', 'bf_alice', 'bf_lily', 'bm_george', 'bm_fable', 'bm_lewis', 'bm_daniel', 'pf_dora', 'pm_alex', 'pm_santa']

def tips_for_better_speech():
    """
    Return tips for better speech synthesis
    """
    return """
    Tips for Better Results
    
    Improve Speech Quality:
    - Add punctuation: Proper punctuation helps create natural pauses and intonation
    - Use complete sentences: The model performs better with grammatically complete phrases
    - Try different speeds: Some voices sound more natural at slightly faster or slower speeds
    - Consider voice-content match: Choose voices that match the tone of your content
    
    Handling Special Content:
    - Numbers: Write out numbers as words for better pronunciation of important figures
    - Acronyms: Add periods between letters (like "U.S.A.") or write them out
    - Foreign words: The model handles common foreign words, but may struggle with uncommon ones
    - Technical terms: For domain-specific terminology, test different voices
    
    Performance Tips:
    - For longer texts: Break into smaller chunks for better processing
    """

if __name__ == "__main__":
    # Example usage
    # result = text_to_speech("This is another voice from this local Text-to-Speech model. It's more on the soft and ASMR side.")
    #print(f"Audio saved to: {result['audio_path']}")
    # print(f"Phoneme sequence: {result['phoneme_sequence']}")
    
    # Test with different voice and speed (without auto-play)
    result = text_to_speech(
        #"Hello, let me introduce myself. I am Bella. The mother for the Neural Child project that is currently in development.",
        "Hey Chris, this is pretty fast right? Do you see how fast my voice was generated based on this text?",
        voice="af_bella",
        speed=0.9,
        auto_play=True
    )
    print(f"Audio saved to: {result['audio_path']}")
    
    # You can manually play it later if needed
    # play_audio(result["audio_path"])

#######################

#src\agendev\tts_notification.py#
#######################

# tts_notification.py
"""Voice-based notification system using TTSClient."""

from typing import List, Dict, Optional, Union, Any, Literal, Callable
from enum import Enum
from pathlib import Path
import os
import time
import logging
from pydantic import BaseModel, Field
import json

from .tts_module import TTSClient, GenerateAudioRequest, play_audio, get_output_path
from .models.task_models import Task, TaskStatus, TaskPriority, Epic
from .utils.fs_utils import resolve_path, append_to_file, safe_save_json, load_json

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NotificationPriority(str, Enum):
    """Priority levels for voice notifications."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class NotificationType(str, Enum):
    """Types of voice notifications."""
    INFO = "info"
    SUCCESS = "success"
    WARNING = "warning"
    ERROR = "error"
    MILESTONE = "milestone"
    PROGRESS = "progress"

class VoiceProfile(BaseModel):
    """Configuration for a voice profile."""
    voice_id: str = "af_nicole"
    speed: float = Field(1.0, ge=0.1, le=2.0)
    auto_play: bool = True
    
    # Optional path to save audio files
    output_dir: Optional[str] = "artifacts/audio"
    
    # Text formatting templates
    templates: Dict[NotificationType, str] = Field(
        default_factory=lambda: {
            NotificationType.INFO: "Information: {message}",
            NotificationType.SUCCESS: "Success! {message}",
            NotificationType.WARNING: "Warning: {message}",
            NotificationType.ERROR: "Error: {message}",
            NotificationType.MILESTONE: "Milestone achieved: {message}",
            NotificationType.PROGRESS: "Progress update: {message}"
        }
    )

class NotificationConfig(BaseModel):
    """Configuration for the notification system."""
    enabled: bool = True
    
    # Default voice profiles for different notification priorities
    voices: Dict[NotificationPriority, VoiceProfile] = Field(
        default_factory=lambda: {
            NotificationPriority.LOW: VoiceProfile(voice_id="af_bella", speed=1.0),
            NotificationPriority.MEDIUM: VoiceProfile(voice_id="af_bella", speed=1.0),
            NotificationPriority.HIGH: VoiceProfile(voice_id="af_bella", speed=0.9),
            NotificationPriority.CRITICAL: VoiceProfile(voice_id="af_bella", speed=0.8),
        }
    )
    
    # Notification history
    history_enabled: bool = True
    max_history_items: int = 100
    history_file: str = "artifacts/audio/notification_history.json"
    
    # Rate limiting
    min_interval_seconds: float = 5.0  # Minimum time between notifications
    
    # Silent periods
    silent_mode: bool = False

class NotificationHistory(BaseModel):
    """History of notifications sent."""
    notifications: List[Dict[str, Any]] = Field(default_factory=list)
    
    def add(self, notification_data: Dict[str, Any]) -> None:
        """Add a notification to history."""
        self.notifications.append(notification_data)
        # Trim if exceeding max length
        if len(self.notifications) > 100:  # Hardcoded for safety
            self.notifications = self.notifications[-100:]

class NotificationManager:
    """Manages voice notifications using TTS."""
    
    def __init__(
        self,
        tts_base_url: str = "http://127.0.0.1:7860",
        config: Optional[NotificationConfig] = None
    ):
        """
        Initialize the notification manager.
        
        Args:
            tts_base_url: URL for the TTS API
            config: Configuration for notifications
        """
        self.tts_client = TTSClient(base_url=tts_base_url)
        self.config = config or NotificationConfig()
        self.history = NotificationHistory()
        self.last_notification_time = 0
        
        # Load history if available
        self._load_history()
    
    def _load_history(self) -> None:
        """Load notification history from file."""
        if self.config.history_enabled:
            history_path = resolve_path(self.config.history_file)
            if os.path.exists(history_path):
                try:
                    history_data = load_json(history_path)
                    self.history = NotificationHistory.model_validate(history_data)
                except Exception as e:
                    logger.error(f"Error loading notification history: {e}")
    
    def _save_history(self) -> None:
        """Save notification history to file."""
        if self.config.history_enabled:
            try:
                # Use direct file writing instead of safe_save_json
                history_path = resolve_path(self.config.history_file, create_parents=True)
                
                # Convert to dictionary first
                history_data = self.history.model_dump()
                
                # Write directly to file
                with open(history_path, 'w') as f:
                    json.dump(history_data, f, indent=2, default=str)
            except Exception as e:
                logger.error(f"Error saving notification history: {e}")
    
    def notify(
        self,
        message: str,
        notification_type: NotificationType = NotificationType.INFO,
        priority: NotificationPriority = NotificationPriority.MEDIUM,
        auto_play: Optional[bool] = None,
        save_to_file: bool = True,
        filename: Optional[str] = None,
        voice_profile: Optional[VoiceProfile] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Optional[str]:
        """
        Send a voice notification.
        
        Args:
            message: The message to speak
            notification_type: Type of notification
            priority: Priority level
            auto_play: Whether to play the audio immediately
            save_to_file: Whether to save the audio to a file
            filename: Optional filename to use
            voice_profile: Optional voice profile override
            metadata: Additional metadata to store with the notification
            
        Returns:
            Path to the generated audio file or None
        """
        # Check if notifications are enabled
        if not self.config.enabled or self.config.silent_mode:
            logger.info(f"Notification suppressed (silent mode): {message}")
            return None
        
        # Apply rate limiting
        current_time = time.time()
        if current_time - self.last_notification_time < self.config.min_interval_seconds:
            logger.info(f"Notification rate-limited: {message}")
            return None
        
        # Get the appropriate voice profile
        profile = voice_profile or self.config.voices.get(priority, self.config.voices[NotificationPriority.MEDIUM])
        
        # Apply message template
        template = profile.templates.get(notification_type, "{message}")
        formatted_message = template.format(message=message)
        
        # Determine output path if saving to file
        output_path = None
        if save_to_file:
            if filename:
                output_path = resolve_path(f"{profile.output_dir}/{filename}", create_parents=True)
            else:
                timestamp = int(time.time())
                output_path = resolve_path(
                    f"{profile.output_dir}/{notification_type.value}_{timestamp}.wav", 
                    create_parents=True
                )
        
        # Determine whether to auto-play
        should_play = auto_play if auto_play is not None else profile.auto_play
        
        try:
            # Create TTS request
            request = GenerateAudioRequest(
                text=formatted_message,
                voice=profile.voice_id,
                speed=profile.speed
            )
            
            # Generate audio
            result = self.tts_client.generate_audio(request, save_to=str(output_path) if output_path else None)
            
            # Play if requested
            if should_play and "audio_path" in result:
                play_audio(result["audio_path"])
            
            # Update last notification time
            self.last_notification_time = current_time
            
            # Record in history if enabled
            if self.config.history_enabled:
                notification_data = {
                    "timestamp": current_time,
                    "message": message,
                    "formatted_message": formatted_message,
                    "type": notification_type.value,
                    "priority": priority.value,
                    "voice": profile.voice_id,
                    "audio_path": result.get("audio_path", ""),
                    "metadata": metadata or {}
                }
                self.history.add(notification_data)
                self._save_history()
            
            return result.get("audio_path")
        
        except Exception as e:
            logger.error(f"Error generating notification: {e}")
            return None
    
    def info(self, message: str, **kwargs) -> Optional[str]:
        """Send an informational notification."""
        return self.notify(message, NotificationType.INFO, NotificationPriority.LOW, **kwargs)
    
    def success(self, message: str, **kwargs) -> Optional[str]:
        """Send a success notification."""
        return self.notify(message, NotificationType.SUCCESS, NotificationPriority.MEDIUM, **kwargs)
    
    def warning(self, message: str, **kwargs) -> Optional[str]:
        """Send a warning notification."""
        return self.notify(message, NotificationType.WARNING, NotificationPriority.HIGH, **kwargs)
    
    def error(self, message: str, **kwargs) -> Optional[str]:
        """Send an error notification."""
        return self.notify(message, NotificationType.ERROR, NotificationPriority.CRITICAL, **kwargs)
    
    def milestone(self, message: str, **kwargs) -> Optional[str]:
        """Send a milestone notification."""
        return self.notify(message, NotificationType.MILESTONE, NotificationPriority.HIGH, **kwargs)
    
    def progress(self, message: str, **kwargs) -> Optional[str]:
        """Send a progress update notification."""
        return self.notify(message, NotificationType.PROGRESS, NotificationPriority.LOW, **kwargs)
    
    def task_status_update(self, task: Task, old_status: Optional[TaskStatus] = None) -> Optional[str]:
        """
        Notify about a task status change.
        
        Args:
            task: The task that changed status
            old_status: The previous status (if any)
            
        Returns:
            Path to the generated audio file or None
        """
        if old_status is None or old_status == task.status:
            message = f"Task {task.title} is {task.status.value}."
        else:
            message = f"Task {task.title} changed from {old_status.value} to {task.status.value}."
        
        # Map task status to notification type
        notification_type = NotificationType.INFO
        if task.status == TaskStatus.COMPLETED:
            notification_type = NotificationType.SUCCESS
        elif task.status == TaskStatus.BLOCKED:
            notification_type = NotificationType.WARNING
        elif task.status == TaskStatus.FAILED:
            notification_type = NotificationType.ERROR
        
        # Map task priority to notification priority
        notification_priority = NotificationPriority.MEDIUM
        if task.priority == TaskPriority.CRITICAL:
            notification_priority = NotificationPriority.CRITICAL
        elif task.priority == TaskPriority.HIGH:
            notification_priority = NotificationPriority.HIGH
        elif task.priority == TaskPriority.LOW:
            notification_priority = NotificationPriority.LOW
        
        # Include task metadata
        metadata = {
            "task_id": str(task.id),
            "task_title": task.title,
            "old_status": old_status.value if old_status else None,
            "new_status": task.status.value
        }
        
        return self.notify(
            message=message,
            notification_type=notification_type,
            priority=notification_priority,
            metadata=metadata
        )
    
    def epic_progress(self, epic: Epic, progress: float) -> Optional[str]:
        """
        Notify about epic progress.
        
        Args:
            epic: The epic to report on
            progress: Current progress percentage
            
        Returns:
            Path to the generated audio file or None
        """
        # Format progress as percentage
        progress_pct = round(progress)
        
        # Create appropriate message based on progress
        if progress_pct == 100:
            message = f"Epic {epic.title} is now complete!"
            notification_type = NotificationType.SUCCESS
            priority = NotificationPriority.HIGH
        elif progress_pct >= 75:
            message = f"Epic {epic.title} is {progress_pct}% complete. Getting close to completion."
            notification_type = NotificationType.PROGRESS
            priority = NotificationPriority.MEDIUM
        elif progress_pct >= 50:
            message = f"Epic {epic.title} is {progress_pct}% complete. Making good progress."
            notification_type = NotificationType.PROGRESS
            priority = NotificationPriority.MEDIUM
        elif progress_pct >= 25:
            message = f"Epic {epic.title} is {progress_pct}% complete. Moving forward."
            notification_type = NotificationType.PROGRESS
            priority = NotificationPriority.LOW
        else:
            message = f"Epic {epic.title} is {progress_pct}% complete. Just getting started."
            notification_type = NotificationType.PROGRESS
            priority = NotificationPriority.LOW
        
        # Include epic metadata
        metadata = {
            "epic_id": str(epic.id),
            "epic_title": epic.title,
            "progress": progress_pct
        }
        
        return self.notify(
            message=message,
            notification_type=notification_type,
            priority=priority,
            metadata=metadata
        )
    
    def silence(self, enable_silent_mode: bool) -> None:
        """
        Enable or disable silent mode for notifications.
        
        Args:
            enable_silent_mode: Whether to enable silent mode
        """
        logger.info(f"{'Enabling' if enable_silent_mode else 'Disabling'} silent mode")
        self.config.silent_mode = enable_silent_mode
    
    def get_history(self, limit: int = 10, notification_type: Optional[NotificationType] = None) -> List[Dict[str, Any]]:
        """
        Get recent notification history.
        
        Args:
            limit: Maximum number of items to return
            notification_type: Optional filter by notification type
            
        Returns:
            List of notification history items
        """
        if not self.config.history_enabled:
            return []
        
        # Filter by type if specified
        if notification_type:
            filtered = [n for n in self.history.notifications if n.get("type") == notification_type.value]
        else:
            filtered = self.history.notifications
        
        # Return most recent first, limited by count
        return sorted(filtered, key=lambda x: x.get("timestamp", 0), reverse=True)[:limit]

#######################

#src\agendev\__init__.py#
#######################

# AgenDev Package 
"""AgenDev: An Intelligent Agentic Development System.""" 


#######################

#src\agendev\models\planning_models.py#
#######################

# planning_models.py
"""Pydantic models for planning and simulation data."""

from __future__ import annotations
from typing import List, Dict, Optional, Set, Union, Any, Tuple
from datetime import datetime
from uuid import UUID, uuid4
from enum import Enum
from pydantic import BaseModel, Field, field_validator, model_validator

class SearchNodeType(str, Enum):
    """Types of nodes in search algorithms."""
    TASK = "task"
    SEQUENCE = "sequence"
    DECISION = "decision"
    SCENARIO = "scenario"

class PlanningPhase(str, Enum):
    """Phases of the planning process."""
    INITIALIZATION = "initialization"
    EXPLORATION = "exploration" 
    EXPLOITATION = "exploitation"
    FINALIZATION = "finalization"

class SimulationResult(str, Enum):
    """Possible outcomes of a simulation."""
    SUCCESS = "success"
    PARTIAL_SUCCESS = "partial_success"
    FAILURE = "failure"
    UNKNOWN = "unknown"

class SearchNode(BaseModel):
    """Base model for search algorithm nodes."""
    id: UUID = Field(default_factory=uuid4)
    node_type: SearchNodeType
    parent_id: Optional[UUID] = None
    children_ids: List[UUID] = Field(default_factory=list)
    created_at: datetime = Field(default_factory=datetime.now)
    
    # Search algorithm metrics
    visits: int = 0
    depth: int = 0
    
    def mark_visited(self) -> None:
        """Increment the visit count for this node."""
        self.visits += 1

class MCTSNode(SearchNode):
    """Monte Carlo Tree Search node with UCB1 exploration."""
    wins: int = 0
    losses: int = 0
    draws: int = 0
    
    # Node values and scores
    value: float = 0.0
    exploration_score: float = 0.0
    exploitation_score: float = 0.0
    
    # Task-specific information
    task_id: Optional[UUID] = None
    task_sequence: List[UUID] = Field(default_factory=list)
    completed_tasks: Set[UUID] = Field(default_factory=set)
    
    @property
    def total_simulations(self) -> int:
        """Total number of simulations run from this node."""
        return self.wins + self.losses + self.draws
    
    @property
    def win_rate(self) -> float:
        """Win rate for this node."""
        return self.wins / self.total_simulations if self.total_simulations > 0 else 0.0
    
    def update_scores(self, exploration_weight: float = 1.0) -> None:
        """Update exploitation and exploration scores."""
        # Exploitation score is win rate
        self.exploitation_score = self.win_rate
        
        # UCB1 exploration formula
        parent_visits = 1  # Avoid division by zero
        if hasattr(self, 'parent') and self.parent is not None:
            parent_visits = max(1, self.parent.visits)
            
        if self.visits == 0:
            self.exploration_score = float('inf')
        else:
            import math
            self.exploration_score = exploration_weight * math.sqrt(
                math.log(parent_visits) / self.visits
            )
            
        # Combined score for node selection
        self.value = self.exploitation_score + self.exploration_score
        
    def add_simulation_result(self, result: SimulationResult) -> None:
        """Add a simulation result to this node."""
        if result == SimulationResult.SUCCESS:
            self.wins += 1
        elif result == SimulationResult.FAILURE:
            self.losses += 1
        else:
            self.draws += 1
        
        self.update_scores()

class AStarNode(SearchNode):
    """A* search algorithm node."""
    g_score: float = Field(0.0, description="Cost from start to current node")
    h_score: float = Field(0.0, description="Heuristic estimate to goal")
    f_score: float = Field(0.0, description="Total estimated cost (g + h)")
    
    # Task-specific information
    task_id: Optional[UUID] = None
    task_sequence: List[UUID] = Field(default_factory=list)
    completed_tasks: Set[UUID] = Field(default_factory=set)
    remaining_tasks: Set[UUID] = Field(default_factory=set)
    
    def update_scores(self, new_g_score: float, new_h_score: float) -> None:
        """Update the node's f, g, and h scores."""
        self.g_score = new_g_score
        self.h_score = new_h_score
        self.f_score = new_g_score + new_h_score

class SimulationConfig(BaseModel):
    """Configuration for a planning simulation."""
    max_iterations: int = 1000
    exploration_weight: float = 1.414  # UCT constant (sqrt(2))
    max_depth: int = 20
    time_limit_seconds: float = 30.0
    task_priorities: Dict[UUID, float] = Field(default_factory=dict)
    risk_weights: Dict[UUID, float] = Field(default_factory=dict)
    
    # A* parameters
    heuristic_weight: float = 1.0
    
    # Planning phases
    phase: PlanningPhase = PlanningPhase.INITIALIZATION
    phase_thresholds: Dict[str, float] = Field(
        default_factory=lambda: {
            "exploration_to_exploitation": 0.5,
            "exploitation_to_finalization": 0.9
        }
    )

class SimulationStep(BaseModel):
    """Represents a single step in a simulation."""
    id: UUID = Field(default_factory=uuid4)
    simulation_id: UUID
    step_number: int
    node_id: UUID
    action_taken: Dict[str, Any]
    state_before: Dict[str, Any]
    state_after: Dict[str, Any]
    reward: float = 0.0
    timestamp: datetime = Field(default_factory=datetime.now)

class SimulationSession(BaseModel):
    """Records a complete simulation session."""
    id: UUID = Field(default_factory=uuid4)
    config: SimulationConfig
    root_node_id: UUID
    iteration_count: int = 0
    start_time: datetime = Field(default_factory=datetime.now)
    end_time: Optional[datetime] = None
    total_duration_seconds: float = 0.0
    success_rate: float = 0.0
    
    steps: List[SimulationStep] = Field(default_factory=list)
    
    def record_step(self, node_id: UUID, action: Dict[str, Any], 
                   state_before: Dict[str, Any], state_after: Dict[str, Any],
                   reward: float = 0.0) -> UUID:
        """Record a step in the simulation."""
        step = SimulationStep(
            simulation_id=self.id,
            step_number=len(self.steps) + 1,
            node_id=node_id,
            action_taken=action,
            state_before=state_before,
            state_after=state_after,
            reward=reward
        )
        self.steps.append(step)
        return step.id
    
    def complete_simulation(self, success_rate: float) -> None:
        """Mark the simulation as complete and record results."""
        self.end_time = datetime.now()
        self.total_duration_seconds = (self.end_time - self.start_time).total_seconds()
        self.success_rate = success_rate
        self.iteration_count = len(self.steps)

class PlanSnapshot(BaseModel):
    """A snapshot of the current development plan."""
    id: UUID = Field(default_factory=uuid4)
    timestamp: datetime = Field(default_factory=datetime.now)
    plan_version: int
    task_sequence: List[UUID]
    expected_duration_hours: float
    confidence_score: float = Field(0.0, ge=0.0, le=1.0)
    risk_assessment: Dict[str, float]
    simulation_id: Optional[UUID] = None
    
    generated_by: str = "mcts"  # Algorithm used to generate this plan
    description: str = ""
    
    @model_validator(mode='after')
    def validate_plan(self) -> 'PlanSnapshot':
        """Validate the plan structure."""
        if not self.task_sequence:
            raise ValueError("Plan must include at least one task")
        return self

class PlanningHistory(BaseModel):
    """Tracks the evolution of development plans over time."""
    snapshots: List[PlanSnapshot] = Field(default_factory=list)
    current_plan_id: Optional[UUID] = None
    
    def add_snapshot(self, snapshot: PlanSnapshot) -> UUID:
        """Add a plan snapshot and set it as current if it's the first one."""
        self.snapshots.append(snapshot)
        if self.current_plan_id is None:
            self.current_plan_id = snapshot.id
        return snapshot.id
    
    def set_current_plan(self, plan_id: UUID) -> bool:
        """Set the current active plan."""
        for snapshot in self.snapshots:
            if snapshot.id == plan_id:
                self.current_plan_id = plan_id
                return True
        return False
    
    def get_current_plan(self) -> Optional[PlanSnapshot]:
        """Get the current active plan."""
        if self.current_plan_id is None:
            return None
            
        for snapshot in self.snapshots:
            if snapshot.id == self.current_plan_id:
                return snapshot
                
        return None
    
    def get_latest_plan(self) -> Optional[PlanSnapshot]:
        """Get the most recently added plan snapshot."""
        if not self.snapshots:
            return None
        
        # Return the most recently created snapshot
        return sorted(self.snapshots, key=lambda x: x.timestamp, reverse=True)[0]
    
    def get_plan_evolution(self) -> List[Tuple[datetime, float, float]]:
        """Get the evolution of plan confidence and risk over time."""
        return [
            (s.timestamp, s.confidence_score, sum(s.risk_assessment.values()) / len(s.risk_assessment) 
             if s.risk_assessment else 0.0)
            for s in sorted(self.snapshots, key=lambda x: x.timestamp)
        ]

#######################

#src\agendev\models\task_models.py#
#######################

# task_models.py
"""Pydantic models for tasks, epics, and dependencies."""

from __future__ import annotations
from typing import List, Dict, Optional, Union, Literal
from datetime import datetime
from uuid import UUID, uuid4
from enum import Enum
from pydantic import BaseModel, Field, field_validator, model_validator

class TaskStatus(str, Enum):
    """Status states for tasks and epics."""
    PLANNED = "planned"
    IN_PROGRESS = "in_progress"
    BLOCKED = "blocked"
    COMPLETED = "completed"
    FAILED = "failed"

class TaskPriority(str, Enum):
    """Priority levels for tasks and epics."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class TaskRisk(str, Enum):
    """Risk levels for tasks and epics."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class TaskType(str, Enum):
    """Types of tasks to support different parameter strategies."""
    IMPLEMENTATION = "implementation"
    REFACTOR = "refactor"
    BUGFIX = "bugfix"
    TEST = "test"
    DOCUMENTATION = "documentation"
    PLANNING = "planning"

class Dependency(BaseModel):
    """Represents a dependency between tasks."""
    source_id: UUID
    target_id: UUID
    dependency_type: Literal["blocks", "influences", "implements"] = "blocks"
    strength: float = Field(1.0, ge=0.0, le=1.0, description="How strong the dependency is (0.0-1.0)")
    
    @field_validator("source_id", "target_id")
    @classmethod
    def validate_not_same(cls, v, info):
        """Ensure source and target are not the same."""
        if info.data.get("source_id") == info.data.get("target_id") == v:
            raise ValueError("Source and target tasks cannot be the same")
        return v

class BaseTask(BaseModel):
    """Base model with common fields for tasks and epics."""
    id: UUID = Field(default_factory=uuid4)
    title: str
    description: str
    status: TaskStatus = TaskStatus.PLANNED
    priority: TaskPriority = TaskPriority.MEDIUM
    risk: TaskRisk = TaskRisk.MEDIUM
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)
    
    # Metadata for tracking
    completion_probability: float = Field(0.5, ge=0.0, le=1.0)
    estimated_complexity: float = Field(1.0, ge=0.1)
    estimated_duration_hours: float = Field(1.0, ge=0.0)
    actual_duration_hours: Optional[float] = None
    
    def mark_updated(self) -> None:
        """Update the updated_at timestamp."""
        self.updated_at = datetime.now()

class Task(BaseTask):
    """Represents a specific unit of work in the system."""
    epic_id: Optional[UUID] = None
    task_type: TaskType = TaskType.IMPLEMENTATION
    parent_task_id: Optional[UUID] = None
    subtasks: List[UUID] = Field(default_factory=list)
    dependencies: List[UUID] = Field(default_factory=list, description="Tasks that this task depends on")
    dependents: List[UUID] = Field(default_factory=list, description="Tasks that depend on this task")
    artifact_paths: List[str] = Field(default_factory=list, description="Paths to artifacts produced by this task")
    
    # LLM parameters to use for this task
    temperature: float = Field(0.7, ge=0.0, le=1.0)
    max_tokens: int = Field(2000, ge=0)
    
    # Context and progress tracking
    context_ids: List[str] = Field(default_factory=list, description="IDs of context elements relevant to this task")
    completion_percentage: float = Field(0.0, ge=0.0, le=100.0)
    
    @model_validator(mode='after')
    def validate_task_structure(self) -> 'Task':
        """Validate the task structure relationships."""
        if self.parent_task_id is not None and self.epic_id is not None:
            raise ValueError("A task cannot have both a parent task and an epic")
        return self

class Epic(BaseTask):
    """Represents a high-level goal containing multiple tasks."""
    tasks: List[UUID] = Field(default_factory=list)
    parent_epic_id: Optional[UUID] = None
    sub_epics: List[UUID] = Field(default_factory=list)
    
    # Milestone tracking
    target_completion_date: Optional[datetime] = None
    milestone_percentage: float = Field(0.0, ge=0.0, le=100.0)
    
    def calculate_progress(self, task_map: Dict[UUID, Task]) -> float:
        """Calculate the overall progress of this epic."""
        if not self.tasks:
            return 0.0
            
        total_weight = sum(task_map[task_id].estimated_complexity for task_id in self.tasks 
                           if task_id in task_map)
        
        if total_weight == 0:
            return 0.0
            
        weighted_completion = sum(
            task_map[task_id].completion_percentage * task_map[task_id].estimated_complexity
            for task_id in self.tasks
            if task_id in task_map
        )
        
        return weighted_completion / total_weight if total_weight > 0 else 0.0

class TaskGraph(BaseModel):
    """Represents the complete graph of tasks, epics, and their dependencies."""
    epics: Dict[UUID, Epic] = Field(default_factory=dict)
    tasks: Dict[UUID, Task] = Field(default_factory=dict)
    dependencies: List[Dependency] = Field(default_factory=list)
    
    def add_task(self, task: Task) -> UUID:
        """Add a task to the graph and return its ID."""
        self.tasks[task.id] = task
        if task.epic_id and task.epic_id in self.epics:
            epic = self.epics[task.epic_id]
            if task.id not in epic.tasks:
                epic.tasks.append(task.id)
                epic.mark_updated()
        return task.id
    
    def add_epic(self, epic: Epic) -> UUID:
        """Add an epic to the graph and return its ID."""
        self.epics[epic.id] = epic
        if epic.parent_epic_id and epic.parent_epic_id in self.epics:
            parent_epic = self.epics[epic.parent_epic_id]
            if epic.id not in parent_epic.sub_epics:
                parent_epic.sub_epics.append(epic.id)
                parent_epic.mark_updated()
        return epic.id
    
    def add_dependency(self, dependency: Dependency) -> None:
        """Add a dependency between tasks."""
        self.dependencies.append(dependency)
        
        # Update the task dependency references
        if dependency.source_id in self.tasks and dependency.target_id in self.tasks:
            source_task = self.tasks[dependency.source_id]
            target_task = self.tasks[dependency.target_id]
            
            if dependency.target_id not in source_task.dependencies:
                source_task.dependencies.append(dependency.target_id)
                source_task.mark_updated()
                
            if dependency.source_id not in target_task.dependents:
                target_task.dependents.append(dependency.source_id)
                target_task.mark_updated()
    
    def get_critical_path(self) -> List[UUID]:
        """Identify the critical path through the task graph."""
        # Simple implementation that finds the longest path in terms of estimated duration
        # A full implementation would use A* pathfinding
        
        # Start with tasks with no dependencies
        start_tasks = [task_id for task_id, task in self.tasks.items() if not task.dependencies]
        if not start_tasks:
            return []
            
        # Find paths from each starting task
        paths = []
        for start_task_id in start_tasks:
            paths.extend(self._find_paths_from(start_task_id))
            
        if not paths:
            return []
            
        # Return the path with the longest total duration
        return max(paths, key=lambda path: sum(
            self.tasks[task_id].estimated_duration_hours for task_id in path
        ))
    
    def _find_paths_from(self, task_id: UUID, current_path: List[UUID] = None) -> List[List[UUID]]:
        """Find all paths from the given task."""
        if current_path is None:
            current_path = []
            
        # Avoid cycles
        if task_id in current_path:
            return [current_path]
            
        current_path = current_path + [task_id]
        
        if task_id not in self.tasks:
            return [current_path]
            
        task = self.tasks[task_id]
        if not task.dependents:
            return [current_path]
            
        paths = []
        for dependent_id in task.dependents:
            new_paths = self._find_paths_from(dependent_id, current_path)
            paths.extend(new_paths)
            
        return paths if paths else [current_path]
    
    def update_task_statuses(self) -> None:
        """Update the status of tasks based on their dependencies."""
        for task_id, task in self.tasks.items():
            # Check if task is blocked
            if task.status in [TaskStatus.PLANNED, TaskStatus.BLOCKED]:
                is_blocked = False
                for dep_id in task.dependencies:
                    if dep_id in self.tasks and self.tasks[dep_id].status != TaskStatus.COMPLETED:
                        is_blocked = True
                        break
                
                task.status = TaskStatus.BLOCKED if is_blocked else TaskStatus.PLANNED
                task.mark_updated()
    
    def calculate_epic_progress(self) -> None:
        """Update progress for all epics based on task completion."""
        for epic_id, epic in self.epics.items():
            epic.milestone_percentage = epic.calculate_progress(self.tasks)
            epic.mark_updated()

#######################

#src\agendev\models\__init__.py#
#######################

# Data Models 
"""Pydantic models for AgenDev data structures.""" 


#######################

#src\agendev\utils\fs_utils.py#
#######################

# fs_utils.py
"""File system utilities for AgenDev."""

import os
import json
import shutil
import tempfile
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
from datetime import datetime
import pickle
import hashlib

# Base workspace structure definition
WORKSPACE_STRUCTURE = {
    "src": "Generated source code",
    "artifacts": {
        "snapshots": "Code snapshots (local version control)",
        "audio": "Voice feedback and summaries",
        "models": "Context and memory models"
    },
    "planning": {
        "search_trees": "MCTS simulation data",
        "pathfinding": "A* planning results",
        "simulations": "Outcome probabilities"
    },
    "progress": {
        "history": "Historical snapshots"
    },
    "quality": {
        "tests": "Generated tests",
        "reviews": "Implementation reviews",
        "benchmarks": "Performance benchmarks"
    }
}

# Default files to create in the workspace
DEFAULT_FILES = {
    "planning/epics.json": {"epics": []},
    "planning/tasks.json": {"tasks": []},
    "planning/roadmap.md": "# AgenDev Implementation Roadmap\nThis document outlines the implementation timeline for the AgenDev project.\n",
    "progress/current.md": "# Current Project State\nThis document tracks the current state of the AgenDev project.\n",
    "progress/metrics.json": {"metrics": []}
}

def get_workspace_root() -> Path:
    """Get the root path of the workspace."""
    # Try to find the workspace directory by looking for markers
    try:
        # Start with current working directory
        current_dir = Path.cwd()
        current_dir = Path(os.path.normpath(str(current_dir)))
        
        # Look for workspace directory up to 3 levels up
        for _ in range(4):
            # Check if workspace directory exists at this level
            workspace_path = current_dir / "workspace"
            if workspace_path.exists() and workspace_path.is_dir():
                return workspace_path
                
            # Check if we're already in a workspace directory
            if current_dir.name == "workspace":
                return current_dir
                
            # Check if we're at root directory (can't go up further)
            if current_dir.parent == current_dir:
                break
                
            # Go up one level
            current_dir = current_dir.parent
        
        # If not found, check execution directory
        app_dir = Path(os.path.abspath(os.path.dirname(__file__))).parent.parent.parent.parent
        workspace_path = app_dir / "workspace"
        if workspace_path.exists() and workspace_path.is_dir():
            return workspace_path
        
        # If still not found, create in current working directory
        workspace_path = Path.cwd() / "workspace"
        os.makedirs(workspace_path, exist_ok=True)
        print(f"Created workspace directory at: {workspace_path}")
        return workspace_path
        
    except Exception as e:
        print(f"Error locating workspace root: {e}")
        # Fallback to current directory + workspace
        workspace_path = Path.cwd() / "workspace"
        os.makedirs(workspace_path, exist_ok=True)
        return workspace_path

def ensure_workspace_structure() -> Path:
    """Create the workspace directory structure if it doesn't exist."""
    workspace_root = get_workspace_root()
    
    def create_nested_directories(parent_path: Path, structure: Union[Dict, str]) -> None:
        """Recursively create nested directories from the structure definition."""
        if isinstance(structure, dict):
            for dirname, substructure in structure.items():
                dir_path = parent_path / dirname
                os.makedirs(dir_path, exist_ok=True)
                
                # Create README.md with description
                if isinstance(substructure, str):
                    with open(dir_path / "README.md", "w") as f:
                        f.write(f"# {substructure}\n")
                
                # Process subdirectories
                if isinstance(substructure, dict):
                    create_nested_directories(dir_path, substructure)
        
    # Create the directory structure
    create_nested_directories(workspace_root, WORKSPACE_STRUCTURE)
    
    # Create default files
    for file_path, content in DEFAULT_FILES.items():
        full_path = workspace_root / file_path
        os.makedirs(full_path.parent, exist_ok=True)
        
        if isinstance(content, dict):
            save_json(content, full_path)
        else:
            with open(full_path, "w") as f:
                f.write(content)
    
    return workspace_root

def resolve_path(path: Union[str, Path], create_parents: bool = False) -> Path:
    """
    Resolve a path relative to the workspace root.
    
    Args:
        path: Path to resolve (absolute or relative to workspace)
        create_parents: Whether to create parent directories
        
    Returns:
        Absolute Path object
    """
    try:
        # Convert to Path if it's a string and normalize for Windows
        path_obj = Path(os.path.normpath(str(path))) if isinstance(path, str) else Path(os.path.normpath(str(path)))
        
        # If path is already absolute, use it directly
        if path_obj.is_absolute():
            if create_parents and path_obj.parent:
                os.makedirs(path_obj.parent, exist_ok=True)
            return path_obj
        
        # Otherwise, join with workspace root
        workspace_root = get_workspace_root()
        full_path = workspace_root / path_obj
        
        # Ensure the path is normalized (especially important for Windows)
        full_path = Path(os.path.normpath(str(full_path)))
        
        if create_parents and full_path.parent:
            try:
                os.makedirs(full_path.parent, exist_ok=True)
                print(f"Created directories for: {full_path.parent}")
            except Exception as e:
                print(f"Error creating directories for {full_path.parent}: {e}")
                
        return full_path
        
    except Exception as e:
        print(f"Error resolving path {path}: {e}")
        # Fall back to a safe option
        workspace_root = get_workspace_root()
        if isinstance(path, (str, Path)):
            filename = os.path.basename(str(path))
            safe_path = workspace_root / filename
            if create_parents:
                os.makedirs(safe_path.parent, exist_ok=True)
            return safe_path
        else:
            return workspace_root

def load_json(path: Union[str, Path]) -> Dict:
    """
    Load a JSON file.
    
    Args:
        path: Path to the JSON file, absolute or relative to workspace
        
    Returns:
        Parsed JSON data
    """
    full_path = resolve_path(path)
    
    if not full_path.exists():
        return {}
    
    try:
        with open(full_path, 'r') as f:
            return json.load(f)
    except json.JSONDecodeError:
        # If file exists but is not valid JSON, return empty dict
        return {}

def save_json(data: Dict, path: Union[str, Path], pretty: bool = True) -> None:
    """
    Save data to a JSON file.
    
    Args:
        data: Data to save
        path: Path to save to, absolute or relative to workspace
        pretty: Whether to format the JSON for readability
    """
    full_path = resolve_path(path, create_parents=True)
    
    indent = 2 if pretty else None
    with open(full_path, 'w') as f:
        json.dump(data, f, indent=indent, default=str)

def safe_save_json(data: Dict, path: Union[str, Path], pretty: bool = True) -> bool:
    """
    Safely save data to a JSON file using a temporary file to prevent corruption.
    
    Args:
        data: Data to save
        path: Path to save to, absolute or relative to workspace
        pretty: Whether to format the JSON for readability
        
    Returns:
        True if save was successful, False otherwise
    """
    full_path = resolve_path(path, create_parents=True)
    
    # Create temporary file in the same directory
    temp_path = None
    try:
        # Create a temporary file in the same directory
        dir_path = full_path.parent
        os.makedirs(dir_path, exist_ok=True)
        
        # Use NamedTemporaryFile which properly handles cleanup
        with tempfile.NamedTemporaryFile(dir=dir_path, suffix=".json.tmp", delete=False, mode='w') as temp_file:
            temp_path = temp_file.name
            # Write data directly to the file object
            json.dump(data, temp_file, indent=2 if pretty else None, default=str)
            # Ensure file is flushed to disk
            temp_file.flush()
            os.fsync(temp_file.fileno())
        
        # Replace the original file with the temporary file (after temp_file is closed)
        shutil.move(temp_path, full_path)
        return True
        
    except Exception as e:
        print(f"Error saving JSON to {full_path}: {e}")
        # Attempt to clean up temporary file if it exists
        if temp_path and os.path.exists(temp_path):
            try:
                os.remove(temp_path)
            except:
                pass
        return False

def append_to_file(content: str, path: Union[str, Path], ensure_newline: bool = True) -> None:
    """
    Append content to a file.
    
    Args:
        content: Content to append
        path: Path to the file, absolute or relative to workspace
        ensure_newline: Whether to ensure the file ends with a newline
    """
    full_path = resolve_path(path, create_parents=True)
    
    # Ensure content ends with newline if requested
    if ensure_newline and not content.endswith('\n'):
        content += '\n'
    
    # Create the file if it doesn't exist
    if not full_path.exists():
        with open(full_path, 'w') as f:
            f.write(content)
        return
    
    # Check if the file ends with a newline
    needs_newline = False
    if ensure_newline:
        try:
            with open(full_path, 'r') as f:
                f.seek(max(0, os.path.getsize(full_path) - 1))
                last_char = f.read(1)
                needs_newline = last_char != '\n'
        except:
            # If an error occurs, assume no newline
            needs_newline = True
    
    # Append to the file
    with open(full_path, 'a') as f:
        if needs_newline:
            f.write('\n')
        f.write(content)

def save_pickle(data: Any, path: Union[str, Path]) -> None:
    """
    Save data to a pickle file.
    
    Args:
        data: Data to save
        path: Path to save to, absolute or relative to workspace
    """
    full_path = resolve_path(path, create_parents=True)
    
    with open(full_path, 'wb') as f:
        pickle.dump(data, f)

def load_pickle(path: Union[str, Path], default=None) -> Any:
    """
    Load data from a pickle file.
    
    Args:
        path: Path to the pickle file, absolute or relative to workspace
        default: Value to return if file doesn't exist
        
    Returns:
        Unpickled data or default value
    """
    full_path = resolve_path(path)
    
    if not full_path.exists():
        return default
    
    with open(full_path, 'rb') as f:
        return pickle.load(f)

def content_hash(content: str) -> str:
    """
    Generate a hash of the content.
    
    Args:
        content: Content to hash
        
    Returns:
        SHA256 hash of the content
    """
    return hashlib.sha256(content.encode('utf-8')).hexdigest()

def save_snapshot(content: str, file_path: Union[str, Path], 
                 metadata: Optional[Dict] = None) -> Tuple[str, Path]:
    """
    Save a snapshot of content with metadata.
    
    Args:
        content: Content to save
        file_path: Original file path, used to determine snapshot location
        metadata: Additional metadata to store with the snapshot
        
    Returns:
        Tuple of (content_hash, snapshot_path)
    """
    # Convert the file path to a relative path if it's not already
    if isinstance(file_path, str):
        file_path = Path(file_path)
    
    # Normalize path for Windows
    file_path = Path(os.path.normpath(str(file_path)))
    
    if file_path.is_absolute():
        try:
            workspace_root = get_workspace_root()
            # Handle Windows paths
            workspace_root_str = str(workspace_root)
            file_path_str = str(file_path)
            
            # Ensure we handle Windows paths correctly
            if file_path_str.startswith(workspace_root_str):
                rel_path = Path(file_path_str[len(workspace_root_str):].lstrip('/\\'))
            else:
                # If the file is outside the workspace, use the filename only
                rel_path = Path(file_path.name)
        except ValueError as e:
            # If the file is outside the workspace, use the filename only
            print(f"Path resolution error: {e}")
            rel_path = Path(file_path.name)
    else:
        rel_path = file_path
    
    # Generate hash for the content
    hash_value = content_hash(content)
    
    # Prepare metadata
    if metadata is None:
        metadata = {}
    
    snapshot_metadata = {
        "hash": hash_value,
        "timestamp": datetime.now().isoformat(),
        "original_path": str(rel_path),
        **metadata
    }
    
    # Create parent directories safely by splitting into parts
    # First create the main snapshot directory
    snapshot_base = resolve_path("artifacts/snapshots", create_parents=True)
    
    # Then create the subdirectories based on rel_path.parent
    rel_parent = rel_path.parent
    if rel_parent and str(rel_parent) != '.':
        # Create each part of the directory path separately to avoid issues
        snapshot_dir = snapshot_base / rel_parent
        os.makedirs(snapshot_dir, exist_ok=True)
    else:
        snapshot_dir = snapshot_base
    
    # Generate unique filenames for content and metadata
    snapshot_stem = f"{rel_path.stem}_{hash_value[:8]}"
    snapshot_content_path = snapshot_dir / f"{snapshot_stem}{rel_path.suffix}"
    snapshot_metadata_path = snapshot_dir / f"{snapshot_stem}.meta.json"
    
    # Save content and metadata with improved error handling
    try:
        with open(snapshot_content_path, 'w') as f:
            f.write(content)
        
        save_json(snapshot_metadata, snapshot_metadata_path)
        print(f"Successfully saved snapshot to {snapshot_content_path}")
        return hash_value, snapshot_content_path
    except Exception as e:
        print(f"Error saving snapshot: {e}")
        # Return the hash and an empty path as fallback
        return hash_value, Path("")

def list_snapshots(file_path: Union[str, Path]) -> List[Dict]:
    """
    List all snapshots for a file.
    
    Args:
        file_path: Original file path
        
    Returns:
        List of snapshot metadata dictionaries
    """
    # Convert the file path to a relative path if it's not already
    if isinstance(file_path, str):
        file_path = Path(file_path)
    
    if file_path.is_absolute():
        try:
            workspace_root = get_workspace_root()
            rel_path = file_path.relative_to(workspace_root)
        except ValueError:
            # If the file is outside the workspace, use the filename only
            rel_path = Path(file_path.name)
    else:
        rel_path = file_path
    
    # Find all metadata files for this path
    snapshot_dir = resolve_path(f"artifacts/snapshots/{rel_path.parent}")
    if not snapshot_dir.exists():
        return []
    
    snapshots = []
    for meta_file in snapshot_dir.glob(f"{rel_path.stem}_*.meta.json"):
        try:
            metadata = load_json(meta_file)
            if metadata.get("original_path") == str(rel_path):
                snapshots.append(metadata)
        except:
            continue
    
    # Sort by timestamp
    return sorted(snapshots, key=lambda x: x.get("timestamp", ""), reverse=True)

def get_latest_snapshot(file_path: Union[str, Path]) -> Optional[Tuple[str, Dict]]:
    """
    Get the latest snapshot for a file.
    
    Args:
        file_path: Original file path
        
    Returns:
        Tuple of (content, metadata) or None if no snapshots exist
    """
    snapshots = list_snapshots(file_path)
    
    if not snapshots:
        return None
    
    latest = snapshots[0]
    hash_value = latest.get("hash", "")
    
    if not hash_value:
        return None
    
    # Convert the file path to a relative path
    if isinstance(file_path, str):
        file_path = Path(file_path)
    
    if file_path.is_absolute():
        try:
            workspace_root = get_workspace_root()
            rel_path = file_path.relative_to(workspace_root)
        except ValueError:
            rel_path = Path(file_path.name)
    else:
        rel_path = file_path
    
    # Construct the content path
    snapshot_dir = resolve_path(f"artifacts/snapshots/{rel_path.parent}")
    snapshot_content_path = snapshot_dir / f"{rel_path.stem}_{hash_value[:8]}{rel_path.suffix}"
    
    if not snapshot_content_path.exists():
        return None
    
    with open(snapshot_content_path, 'r') as f:
        content = f.read()
    
    return content, latest

#######################

#src\agendev\utils\visualization.py#
#######################

# Visualization Utilities 
"""Visualization utilities for planning algorithms.""" 


#######################

#src\agendev\utils\__init__.py#
#######################

# Utility Functions 
"""Utility functions for the AgenDev system.""" 


#######################

#src\agendev.egg-info\dependency_links.txt#
#######################




#######################

#src\agendev.egg-info\entry_points.txt#
#######################

[console_scripts]
agendev = agendev.app:main


#######################

#src\agendev.egg-info\requires.txt#
#######################

fastapi>=0.95.0
uvicorn>=0.21.0
httpx>=0.24.0
pydantic>=2.0.0
python-dotenv>=1.0.0
websockets>=11.0.3
requests>=2.28.0
numpy>=1.24.0
soundfile>=0.12.1
sounddevice>=0.4.6


#######################

#src\agendev.egg-info\SOURCES.txt#
#######################

README.md
setup.py
src/agendev/__init__.py
src/agendev/agenflow_manager.py
src/agendev/app.py
src/agendev/context_management.py
src/agendev/core.py
src/agendev/llm_integration.py
src/agendev/llm_module.py
src/agendev/parameter_controller.py
src/agendev/probability_modeling.py
src/agendev/search_algorithms.py
src/agendev/snapshot_engine.py
src/agendev/test_generation.py
src/agendev/tts_module.py
src/agendev/tts_notification.py
src/agendev.egg-info/PKG-INFO
src/agendev.egg-info/SOURCES.txt
src/agendev.egg-info/dependency_links.txt
src/agendev.egg-info/entry_points.txt
src/agendev.egg-info/requires.txt
src/agendev.egg-info/top_level.txt
src/agendev/agents/__init__.py
src/agendev/agents/agent_base.py
src/agendev/agents/code_agent.py
src/agendev/agents/deployment_agent.py
src/agendev/agents/executor_agent.py
src/agendev/agents/integration_agent.py
src/agendev/agents/knowledge_agent.py
src/agendev/agents/planner_agent.py
src/agendev/agents/web_automation_agent.py
src/agendev/models/__init__.py
src/agendev/models/planning_models.py
src/agendev/models/task_models.py
src/agendev/utils/__init__.py
src/agendev/utils/config.py
src/agendev/utils/fs_utils.py
src/agendev/utils/llm.py
src/agendev/utils/visualization.py

#######################

#src\agendev.egg-info\top_level.txt#
#######################

agendev


#######################

